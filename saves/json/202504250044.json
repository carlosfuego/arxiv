[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v1",
                "updated": "2025-04-23T11:18:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV3Sb5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV3Sb5"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, Fc, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of Tc = 78 K. Notably, the pump-induced band shifts at\nFc were comparable to those caused by thermal effects at Tc. These findings\nsuggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges\nabove 150 K, with out-of-plane electronic correlations leading to the $2\\times2\n\\times 2$ CDW near Tc, offering key insights into the interplay between the\nelectronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, Fc, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of Tc = 78 K. Notably, the pump-induced band shifts at\nFc were comparable to those caused by thermal effects at Tc. These findings\nsuggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges\nabove 150 K, with out-of-plane electronic correlations leading to the $2\\times2\n\\times 2$ CDW near Tc, offering key insights into the interplay between the\nelectronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Ho√üfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max L√ºbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v2",
                "updated": "2025-04-17T21:19:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    21,
                    19,
                    19,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camg√∂z"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Sch√∂ne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Sch√∂ne"
                },
                "author": "Robert Sch√∂ne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr√© No√´l"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr√© No√´l"
                },
                "author": "Pierre-Andr√© No√´l",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.16932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16932v1",
                "updated": "2025-04-23T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    59,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    59,
                    56,
                    2,
                    113,
                    0
                ],
                "title": "Dispu$œÑ$able: the high cost of a low optical depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dispu$œÑ$able: the high cost of a low optical depth"
                },
                "summary": "Recent Baryonic Acoustic Oscillation (BAO) measurements from the Dark Energy\nSpectroscopic Instrument (DESI) are mildly discrepant ($2.2\\sigma$) with the\nCosmic Microwave Background (CMB) when interpreted within $\\Lambda$CDM. When\nanalyzing these data with extended cosmologies this inconsistency manifests as\na $\\simeq3\\sigma$ preference for sub-minimal neutrino mass or evolving dark\nenergy. It is known that the preference for sub-minimal neutrino mass from the\nsuppression of structure growth could be alleviated by increasing the optical\ndepth to reionization $\\tau$. We show that, because the CMB-inferred $\\tau$ is\nnegatively correlated with the matter fraction, a larger optical depth resolves\na similar preference from geometric constraints. Optical depths large enough to\nresolve the neutrino mass tension ($\\tau\\sim0.09)$ also reduce the preference\nfor evolving dark energy from $\\simeq3\\sigma$ to $\\simeq1.5\\sigma$. Conversely,\nwithin $\\Lambda$CDM the combination of DESI BAO, high-$\\ell$ CMB and CMB\nlensing yields $\\tau = 0.090 \\pm 0.012$. The required increase in $\\tau$ is in\n$\\simeq3-5\\sigma$ tension with Planck low-$\\ell$ polarization data when taken\nat face value. While there is no evidence for systematics in the large-scale\nPlanck data, $\\tau$ remains the least well-constrained $\\Lambda$CDM parameter\nand is far from its cosmic variance limit. The importance of $\\tau$ for several\ncosmological measurements strengthens the case for future large-scale CMB\nexperiments as well as direct probes of the epoch of reionization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Baryonic Acoustic Oscillation (BAO) measurements from the Dark Energy\nSpectroscopic Instrument (DESI) are mildly discrepant ($2.2\\sigma$) with the\nCosmic Microwave Background (CMB) when interpreted within $\\Lambda$CDM. When\nanalyzing these data with extended cosmologies this inconsistency manifests as\na $\\simeq3\\sigma$ preference for sub-minimal neutrino mass or evolving dark\nenergy. It is known that the preference for sub-minimal neutrino mass from the\nsuppression of structure growth could be alleviated by increasing the optical\ndepth to reionization $\\tau$. We show that, because the CMB-inferred $\\tau$ is\nnegatively correlated with the matter fraction, a larger optical depth resolves\na similar preference from geometric constraints. Optical depths large enough to\nresolve the neutrino mass tension ($\\tau\\sim0.09)$ also reduce the preference\nfor evolving dark energy from $\\simeq3\\sigma$ to $\\simeq1.5\\sigma$. Conversely,\nwithin $\\Lambda$CDM the combination of DESI BAO, high-$\\ell$ CMB and CMB\nlensing yields $\\tau = 0.090 \\pm 0.012$. The required increase in $\\tau$ is in\n$\\simeq3-5\\sigma$ tension with Planck low-$\\ell$ polarization data when taken\nat face value. While there is no evidence for systematics in the large-scale\nPlanck data, $\\tau$ remains the least well-constrained $\\Lambda$CDM parameter\nand is far from its cosmic variance limit. The importance of $\\tau$ for several\ncosmological measurements strengthens the case for future large-scale CMB\nexperiments as well as direct probes of the epoch of reionization."
                },
                "authors": [
                    {
                        "name": "Noah Sailer"
                    },
                    {
                        "name": "Gerrit S. Farren"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Martin White"
                    }
                ],
                "author_detail": {
                    "name": "Martin White"
                },
                "author": "Martin White",
                "arxiv_comment": "4 pages, 4 figures, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16921v1",
                "updated": "2025-04-23T17:48:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    48,
                    25,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:48:25Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    48,
                    25,
                    2,
                    113,
                    0
                ],
                "title": "IberBench: LLM Evaluation on Iberian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IberBench: LLM Evaluation on Iberian Languages"
                },
                "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard."
                },
                "authors": [
                    {
                        "name": "Jos√© √Ångel Gonz√°lez"
                    },
                    {
                        "name": "Ian Borrego Obrador"
                    },
                    {
                        "name": "√Ålvaro Romo Herrero"
                    },
                    {
                        "name": "Areg Mikael Sarvazyan"
                    },
                    {
                        "name": "Mara Chinea-R√≠os"
                    },
                    {
                        "name": "Angelo Basile"
                    },
                    {
                        "name": "Marc Franco-Salvador"
                    }
                ],
                "author_detail": {
                    "name": "Marc Franco-Salvador"
                },
                "author": "Marc Franco-Salvador",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16419v3",
                "updated": "2025-04-23T17:46:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    46,
                    54,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-20T17:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."
                },
                "authors": [
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Jiamu Zhang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Andrew Wen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15842v3",
                "updated": "2025-04-23T17:46:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    46,
                    8,
                    2,
                    113,
                    0
                ],
                "published": "2024-07-22T17:58:05Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    17,
                    58,
                    5,
                    0,
                    204,
                    0
                ],
                "title": "DiffArtist: Towards Structure and Appearance Controllable Image\n  Stylization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffArtist: Towards Structure and Appearance Controllable Image\n  Stylization"
                },
                "summary": "Artistic style includes both structural and appearance elements. Existing\nneural stylization techniques primarily focus on transferring appearance\nfeatures such as color and texture, often neglecting the equally crucial aspect\nof structural stylization. In this paper, we present a comprehensive study on\nthe simultaneous stylization of structure and appearance of 2D images.\nSpecifically, we introduce DiffArtist, which, to the best of our knowledge, is\nthe first stylization method to allow for dual controllability over structure\nand appearance. Our key insight is to represent structure and appearance as\nseparate diffusion processes to achieve complete disentanglement without\nrequiring any training, thereby endowing users with unprecedented\ncontrollability for both components. The evaluation of stylization of both\nappearance and structure, however, remains challenging as it necessitates\nsemantic understanding. To this end, we further propose a Multimodal LLM-based\nstyle evaluator, which better aligns with human preferences than metrics\nlacking semantic understanding. With this powerful evaluator, we conduct\nextensive analysis, demonstrating that DiffArtist achieves superior style\nfidelity, editability, and structure-appearance disentanglement. These merits\nmake DiffArtist a highly versatile solution for creative applications. Project\nhomepage: https://github.com/songrise/Artist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artistic style includes both structural and appearance elements. Existing\nneural stylization techniques primarily focus on transferring appearance\nfeatures such as color and texture, often neglecting the equally crucial aspect\nof structural stylization. In this paper, we present a comprehensive study on\nthe simultaneous stylization of structure and appearance of 2D images.\nSpecifically, we introduce DiffArtist, which, to the best of our knowledge, is\nthe first stylization method to allow for dual controllability over structure\nand appearance. Our key insight is to represent structure and appearance as\nseparate diffusion processes to achieve complete disentanglement without\nrequiring any training, thereby endowing users with unprecedented\ncontrollability for both components. The evaluation of stylization of both\nappearance and structure, however, remains challenging as it necessitates\nsemantic understanding. To this end, we further propose a Multimodal LLM-based\nstyle evaluator, which better aligns with human preferences than metrics\nlacking semantic understanding. With this powerful evaluator, we conduct\nextensive analysis, demonstrating that DiffArtist achieves superior style\nfidelity, editability, and structure-appearance disentanglement. These merits\nmake DiffArtist a highly versatile solution for creative applications. Project\nhomepage: https://github.com/songrise/Artist."
                },
                "authors": [
                    {
                        "name": "Ruixiang Jiang"
                    },
                    {
                        "name": "Changwen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changwen Chen"
                },
                "author": "Changwen Chen",
                "arxiv_comment": "Homepage: https://DiffusionArtist.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16918v1",
                "updated": "2025-04-23T17:45:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    45,
                    5,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:45:05Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    45,
                    5,
                    2,
                    113,
                    0
                ],
                "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents"
                },
                "summary": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results."
                },
                "authors": [
                    {
                        "name": "Raghav Thind"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Haizhao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haizhao Yang"
                },
                "author": "Haizhao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16913v1",
                "updated": "2025-04-23T17:39:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    39,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:39:49Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    39,
                    49,
                    2,
                    113,
                    0
                ],
                "title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM\n  Behind AI-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM\n  Behind AI-Generated Text"
                },
                "summary": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability."
                },
                "authors": [
                    {
                        "name": "Shifali Agrahari"
                    },
                    {
                        "name": "Sanasam Ranbir Singh"
                    }
                ],
                "author_detail": {
                    "name": "Sanasam Ranbir Singh"
                },
                "author": "Sanasam Ranbir Singh",
                "arxiv_comment": "De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate\n  Speech Detection, co-located with AAAI 2025. Pennsylvania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21615v2",
                "updated": "2025-04-23T17:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    39,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-27T15:36:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "A Measure Based Generalizable Approach to Understandability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Measure Based Generalizable Approach to Understandability"
                },
                "summary": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future."
                },
                "authors": [
                    {
                        "name": "Vikas Kushwaha"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    },
                    {
                        "name": "Subhajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhajit Roy"
                },
                "author": "Subhajit Roy",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16884v1",
                "updated": "2025-04-23T17:00:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    45,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:00:45Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    45,
                    2,
                    113,
                    0
                ],
                "title": "Do Large Language Models know who did what to whom?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models know who did what to whom?"
                },
                "summary": "Large Language Models (LLMs) are commonly criticized for not understanding\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\nare distinct from language processing. Here, we instead study a kind of\nunderstanding tightly linked to language: inferring who did what to whom\n(thematic roles) in a sentence. Does the central training objective of\nLLMs-word prediction-result in sentence representations that capture thematic\nroles? In two experiments, we characterized sentence representations in four\nLLMs. In contrast to human similarity judgments, in LLMs the overall\nrepresentational similarity of sentence pairs reflected syntactic similarity\nbut not whether their agent and patient assignments were identical vs.\nreversed. Furthermore, we found little evidence that thematic role information\nwas available in any subset of hidden units. However, some attention heads\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\nextract thematic roles but, relative to humans, this information influences\ntheir representations more weakly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly criticized for not understanding\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\nare distinct from language processing. Here, we instead study a kind of\nunderstanding tightly linked to language: inferring who did what to whom\n(thematic roles) in a sentence. Does the central training objective of\nLLMs-word prediction-result in sentence representations that capture thematic\nroles? In two experiments, we characterized sentence representations in four\nLLMs. In contrast to human similarity judgments, in LLMs the overall\nrepresentational similarity of sentence pairs reflected syntactic similarity\nbut not whether their agent and patient assignments were identical vs.\nreversed. Furthermore, we found little evidence that thematic role information\nwas available in any subset of hidden units. However, some attention heads\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\nextract thematic roles but, relative to humans, this information influences\ntheir representations more weakly."
                },
                "authors": [
                    {
                        "name": "Joseph M. Denning"
                    },
                    {
                        "name": "Xiaohan"
                    },
                    {
                        "name": "Guo"
                    },
                    {
                        "name": "Bryor Snefjella"
                    },
                    {
                        "name": "Idan A. Blank"
                    }
                ],
                "author_detail": {
                    "name": "Idan A. Blank"
                },
                "arxiv_affiliation": "Hannah",
                "author": "Idan A. Blank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16883v1",
                "updated": "2025-04-23T17:00:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    25,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:00:25Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    25,
                    2,
                    113,
                    0
                ],
                "title": "Enhancing Critical Thinking with AI: A Tailored Warning System for RAG\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Critical Thinking with AI: A Tailored Warning System for RAG\n  Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems offer a powerful approach to\nenhancing large language model (LLM) outputs by incorporating fact-checked,\ncontextually relevant information. However, fairness and reliability concerns\npersist, as hallucinations can emerge at both the retrieval and generation\nstages, affecting users' reasoning and decision-making. Our research explores\nhow tailored warning messages -- whose content depends on the specific context\nof hallucination -- shape user reasoning and actions in an educational quiz\nsetting. Preliminary findings suggest that while warnings improve accuracy and\nawareness of high-level hallucinations, they may also introduce cognitive\nfriction, leading to confusion and diminished trust in the system. By examining\nthese interactions, this work contributes to the broader goal of AI-augmented\nreasoning: developing systems that actively support human reflection, critical\nthinking, and informed decision-making rather than passive information\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems offer a powerful approach to\nenhancing large language model (LLM) outputs by incorporating fact-checked,\ncontextually relevant information. However, fairness and reliability concerns\npersist, as hallucinations can emerge at both the retrieval and generation\nstages, affecting users' reasoning and decision-making. Our research explores\nhow tailored warning messages -- whose content depends on the specific context\nof hallucination -- shape user reasoning and actions in an educational quiz\nsetting. Preliminary findings suggest that while warnings improve accuracy and\nawareness of high-level hallucinations, they may also introduce cognitive\nfriction, leading to confusion and diminished trust in the system. By examining\nthese interactions, this work contributes to the broader goal of AI-augmented\nreasoning: developing systems that actively support human reflection, critical\nthinking, and informed decision-making rather than passive information\nconsumption."
                },
                "authors": [
                    {
                        "name": "Xuyang Zhu"
                    },
                    {
                        "name": "Sejoon Chang"
                    },
                    {
                        "name": "Andrew Kuik"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Kuik"
                },
                "author": "Andrew Kuik",
                "arxiv_comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction\n  for Augmented Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16877v1",
                "updated": "2025-04-23T16:54:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    54,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:54:16Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    54,
                    16,
                    2,
                    113,
                    0
                ],
                "title": "Context-Enhanced Vulnerability Detection Based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Enhanced Vulnerability Detection Based on Large Language Model"
                },
                "summary": "Vulnerability detection is a critical aspect of software security. Accurate\ndetection is essential to prevent potential security breaches and protect\nsoftware systems from malicious attacks. Recently, vulnerability detection\nmethods leveraging deep learning and large language models (LLMs) have garnered\nincreasing attention. However, existing approaches often focus on analyzing\nindividual files or functions, which limits their ability to gather sufficient\ncontextual information. Analyzing entire repositories to gather context\nintroduces significant noise and computational overhead. To address these\nchallenges, we propose a context-enhanced vulnerability detection approach that\ncombines program analysis with LLMs. Specifically, we use program analysis to\nextract contextual information at various levels of abstraction, thereby\nfiltering out irrelevant noise. The abstracted context along with source code\nare provided to LLM for vulnerability detection. We investigate how different\nlevels of contextual granularity improve LLM-based vulnerability detection\nperformance. Our goal is to strike a balance between providing sufficient\ndetail to accurately capture vulnerabilities and minimizing unnecessary\ncomplexity that could hinder model performance. Based on an extensive study\nusing GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key\nfindings includes: (1) incorporating abstracted context significantly enhances\nvulnerability detection effectiveness; (2) different models benefit from\ndistinct levels of abstraction depending on their code understanding\ncapabilities; and (3) capturing program behavior through program analysis for\ngeneral LLM-based code analysis tasks can be a direction that requires further\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability detection is a critical aspect of software security. Accurate\ndetection is essential to prevent potential security breaches and protect\nsoftware systems from malicious attacks. Recently, vulnerability detection\nmethods leveraging deep learning and large language models (LLMs) have garnered\nincreasing attention. However, existing approaches often focus on analyzing\nindividual files or functions, which limits their ability to gather sufficient\ncontextual information. Analyzing entire repositories to gather context\nintroduces significant noise and computational overhead. To address these\nchallenges, we propose a context-enhanced vulnerability detection approach that\ncombines program analysis with LLMs. Specifically, we use program analysis to\nextract contextual information at various levels of abstraction, thereby\nfiltering out irrelevant noise. The abstracted context along with source code\nare provided to LLM for vulnerability detection. We investigate how different\nlevels of contextual granularity improve LLM-based vulnerability detection\nperformance. Our goal is to strike a balance between providing sufficient\ndetail to accurately capture vulnerabilities and minimizing unnecessary\ncomplexity that could hinder model performance. Based on an extensive study\nusing GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key\nfindings includes: (1) incorporating abstracted context significantly enhances\nvulnerability detection effectiveness; (2) different models benefit from\ndistinct levels of abstraction depending on their code understanding\ncapabilities; and (3) capturing program behavior through program analysis for\ngeneral LLM-based code analysis tasks can be a direction that requires further\nattention."
                },
                "authors": [
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Bowen Xu"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Hailong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Sun"
                },
                "author": "Hailong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14985v2",
                "updated": "2025-04-23T16:52:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    52,
                    54,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T09:26:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    26,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "aiXamine: Simplified LLM Safety and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXamine: Simplified LLM Safety and Security"
                },
                "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
                },
                "authors": [
                    {
                        "name": "Fatih Deniz"
                    },
                    {
                        "name": "Dorde Popovic"
                    },
                    {
                        "name": "Yazan Boshmaf"
                    },
                    {
                        "name": "Euisuh Jeong"
                    },
                    {
                        "name": "Minhaj Ahmad"
                    },
                    {
                        "name": "Sanjay Chawla"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04105v3",
                "updated": "2025-04-23T16:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    48,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2024-03-06T23:17:16Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    23,
                    17,
                    16,
                    2,
                    66,
                    0
                ],
                "title": "Natural Language Processing in the Patent Domain: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing in the Patent Domain: A Survey"
                },
                "summary": "Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_doi": "10.1007/s10462-025-11168-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10462-025-11168-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.04105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Artificial Intelligence Review",
                "arxiv_journal_ref": "Artif Intell Rev 58, 214 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16871v1",
                "updated": "2025-04-23T16:46:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    46,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:46:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    46,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge"
                },
                "summary": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks"
                },
                "authors": [
                    {
                        "name": "Mirian Hipolito Garcia"
                    },
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Anastasios Kyrillidis"
                    },
                    {
                        "name": "Robert Sim"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16868v1",
                "updated": "2025-04-23T16:44:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    44,
                    24,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    44,
                    24,
                    2,
                    113,
                    0
                ],
                "title": "Hint towards inconsistency between BAO and Supernovae Dataset: The\n  Evidence of Redshift Evolving Dark Energy from DESI DR2 is Absent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hint towards inconsistency between BAO and Supernovae Dataset: The\n  Evidence of Redshift Evolving Dark Energy from DESI DR2 is Absent"
                },
                "summary": "The combination of independent cosmological datasets is a route towards\nprecision and accurate inference of the cosmological parameters if these\nobservations are not contaminated by systematic effects. However, the presence\nof unknown systematics present in differrent datasets can lead to a biased\ninference of the cosmological parameters. In this work, we test the consistency\nof the two independent tracers of the low-redshift cosmic expansion, namely the\nsupernovae dataset from Pantheon$+$ and the BAO dataset from DESI DR2 using the\ndistance duality relation which is a cornerstone relation in cosmology under\nthe framework of General Relativity. We find that these datasets violate the\ndistance duality relation and show a signature of redshift evolution, hinting\ntoward unaccounted physical effects or observational artifacts. Coincidentally\nthis effect mimics a redshift evolving dark energy scenario when supernovae\ndataset and DESI datasets are combined without accounting for this\ninconsistency. Accounting for this effect in the likelihood refutes the\nprevious claim of evidence of non-cosmological constant as dark energy model\nfrom DESI DR2, and shows a result consistent with cosmological constant with\n$w_0= -0.92\\pm 0.08$ and $w_a= -0.49^{+0.33}_{-0.36}$. This indicates that the\ncurrent conclusion from DESI DR2 in combination with Pantheon$+$ is likely due\nto the combination of two inconsistent datasets resulting in precise but\ninaccurate inference of cosmological parameters. In the future, tests of this\nkind for the consistency between different cosmological datasets will be\nessential for robust inference of cosmological parameters and for deciphering\nunaccounted physical effects or observational artifacts from supernovae and BAO\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of independent cosmological datasets is a route towards\nprecision and accurate inference of the cosmological parameters if these\nobservations are not contaminated by systematic effects. However, the presence\nof unknown systematics present in differrent datasets can lead to a biased\ninference of the cosmological parameters. In this work, we test the consistency\nof the two independent tracers of the low-redshift cosmic expansion, namely the\nsupernovae dataset from Pantheon$+$ and the BAO dataset from DESI DR2 using the\ndistance duality relation which is a cornerstone relation in cosmology under\nthe framework of General Relativity. We find that these datasets violate the\ndistance duality relation and show a signature of redshift evolution, hinting\ntoward unaccounted physical effects or observational artifacts. Coincidentally\nthis effect mimics a redshift evolving dark energy scenario when supernovae\ndataset and DESI datasets are combined without accounting for this\ninconsistency. Accounting for this effect in the likelihood refutes the\nprevious claim of evidence of non-cosmological constant as dark energy model\nfrom DESI DR2, and shows a result consistent with cosmological constant with\n$w_0= -0.92\\pm 0.08$ and $w_a= -0.49^{+0.33}_{-0.36}$. This indicates that the\ncurrent conclusion from DESI DR2 in combination with Pantheon$+$ is likely due\nto the combination of two inconsistent datasets resulting in precise but\ninaccurate inference of cosmological parameters. In the future, tests of this\nkind for the consistency between different cosmological datasets will be\nessential for robust inference of cosmological parameters and for deciphering\nunaccounted physical effects or observational artifacts from supernovae and BAO\ndatasets."
                },
                "authors": [
                    {
                        "name": "Samsuzzaman Afroz"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Suvodip Mukherjee"
                },
                "author": "Suvodip Mukherjee",
                "arxiv_comment": "19 Pages, 7 figures, To be submitted to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14128v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14128v4",
                "updated": "2025-04-24T02:50:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    50,
                    28,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-19T01:02:42Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    1,
                    2,
                    42,
                    5,
                    109,
                    0
                ],
                "title": "TALES: Text Adventure Learning Environment Suite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALES: Text Adventure Learning Environment Suite"
                },
                "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite."
                },
                "authors": [
                    {
                        "name": "Christopher Zhang Cui"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "name": "Marc-Alexandre C√¥t√©"
                    }
                ],
                "author_detail": {
                    "name": "Marc-Alexandre C√¥t√©"
                },
                "author": "Marc-Alexandre C√¥t√©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14128v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14128v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15166v2",
                "updated": "2025-04-23T16:31:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    31,
                    5,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T15:16:30Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    16,
                    30,
                    0,
                    111,
                    0
                ],
                "title": "Simulating biochemical reactions: The Linear Noise Approximation can\n  capture non-linear dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating biochemical reactions: The Linear Noise Approximation can\n  capture non-linear dynamics"
                },
                "summary": "There is a plethora of highly stochastic non-linear dynamical systems in\nfields such as molecular biology, chemistry, epidemiology, and ecology. Yet,\nnone of the currently available stochastic models are both accurate and\ncomputationally efficient for long-term predictions of large systems. The\nLinear Noise Approximation (LNA) model for biochemical reaction networks is\nanalytically tractable, which makes it computationally efficient for\nsimulation, analysis, and inference. However, it is only accurate for linear\nsystems and short-time transitions. Other methods can achieve greater accuracy\nacross a wider range of systems, including non-linear ones, but lack analytical\ntractability. This paper seeks to challenge the prevailing view by\ndemonstrating that the Linear Noise Approximation can indeed capture non-linear\ndynamics after certain modifications. We introduce a new framework that\nutilises centre manifold theory allowing us to identify simple interventions to\nthe LNA that do not significantly compromise its computational efficiency. We\ndevelop specific algorithms for systems that exhibit oscillations or\nbi-stability and demonstrate their accuracy and computational efficiency across\nmultiple examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a plethora of highly stochastic non-linear dynamical systems in\nfields such as molecular biology, chemistry, epidemiology, and ecology. Yet,\nnone of the currently available stochastic models are both accurate and\ncomputationally efficient for long-term predictions of large systems. The\nLinear Noise Approximation (LNA) model for biochemical reaction networks is\nanalytically tractable, which makes it computationally efficient for\nsimulation, analysis, and inference. However, it is only accurate for linear\nsystems and short-time transitions. Other methods can achieve greater accuracy\nacross a wider range of systems, including non-linear ones, but lack analytical\ntractability. This paper seeks to challenge the prevailing view by\ndemonstrating that the Linear Noise Approximation can indeed capture non-linear\ndynamics after certain modifications. We introduce a new framework that\nutilises centre manifold theory allowing us to identify simple interventions to\nthe LNA that do not significantly compromise its computational efficiency. We\ndevelop specific algorithms for systems that exhibit oscillations or\nbi-stability and demonstrate their accuracy and computational efficiency across\nmultiple examples."
                },
                "authors": [
                    {
                        "name": "Frederick Truman-Williams"
                    },
                    {
                        "name": "Giorgos Minas"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Minas"
                },
                "author": "Giorgos Minas",
                "arxiv_comment": "44 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C40, 82C31 (Primary) 60J20, 92C42 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16858v1",
                "updated": "2025-04-23T16:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    27,
                    15,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:27:15Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    27,
                    15,
                    2,
                    113,
                    0
                ],
                "title": "Planning with Diffusion Models for Target-Oriented Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with Diffusion Models for Target-Oriented Dialogue Systems"
                },
                "summary": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM\nera, where strategic dialogue planning is crucial for directing conversations\ntoward specific targets. However, existing dialogue planning methods generate\ndialogue plans in a step-by-step sequential manner, and may suffer from\ncompounding errors and myopic actions. To address these limitations, we\nintroduce a novel dialogue planning framework, DiffTOD, which leverages\ndiffusion models to enable non-sequential dialogue planning. DiffTOD formulates\ndialogue planning as a trajectory generation problem with conditional guidance,\nand leverages a diffusion language model to estimate the likelihood of the\ndialogue trajectory. To optimize the dialogue action strategies, DiffTOD\nintroduces three tailored guidance mechanisms for different target types,\noffering flexible guidance towards diverse TOD targets at test time. Extensive\nexperiments across three diverse TOD settings show that DiffTOD can effectively\nperform non-myopic lookahead exploration and optimize action strategies over a\nlong horizon through non-sequential dialogue planning, and demonstrates strong\nflexibility across complex and diverse dialogue scenarios. Our code and data\nare accessible through https://anonymous.4open.science/r/DiffTOD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM\nera, where strategic dialogue planning is crucial for directing conversations\ntoward specific targets. However, existing dialogue planning methods generate\ndialogue plans in a step-by-step sequential manner, and may suffer from\ncompounding errors and myopic actions. To address these limitations, we\nintroduce a novel dialogue planning framework, DiffTOD, which leverages\ndiffusion models to enable non-sequential dialogue planning. DiffTOD formulates\ndialogue planning as a trajectory generation problem with conditional guidance,\nand leverages a diffusion language model to estimate the likelihood of the\ndialogue trajectory. To optimize the dialogue action strategies, DiffTOD\nintroduces three tailored guidance mechanisms for different target types,\noffering flexible guidance towards diverse TOD targets at test time. Extensive\nexperiments across three diverse TOD settings show that DiffTOD can effectively\nperform non-myopic lookahead exploration and optimize action strategies over a\nlong horizon through non-sequential dialogue planning, and demonstrates strong\nflexibility across complex and diverse dialogue scenarios. Our code and data\nare accessible through https://anonymous.4open.science/r/DiffTOD."
                },
                "authors": [
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Bo Peng"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16856v1",
                "updated": "2025-04-23T16:23:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    17,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:23:17Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    17,
                    2,
                    113,
                    0
                ],
                "title": "Emo Pillars: Knowledge Distillation to Support Fine-Grained\n  Context-Aware and Context-Less Emotion Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emo Pillars: Knowledge Distillation to Support Fine-Grained\n  Context-Aware and Context-Less Emotion Classification"
                },
                "summary": "Most datasets for sentiment analysis lack context in which an opinion was\nexpressed, often crucial for emotion understanding, and are mainly limited by a\nfew emotion categories. Foundation large language models (LLMs) like GPT-4\nsuffer from over-predicting emotions and are too resource-intensive. We design\nan LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,\nfor the generation of training examples for more accessible, lightweight\nBERT-type encoder models. We focus on enlarging the semantic diversity of\nexamples and propose grounding the generation into a corpus of narratives to\nproduce non-repetitive story-character-centered utterances with unique contexts\nover 28 emotion classes. By running 700K inferences in 450 GPU hours, we\ncontribute with the dataset of 100K contextual and also 300K context-less\nexamples to cover both scenarios. We use it for fine-tuning pre-trained\nencoders, which results in several Emo Pillars models. We show that Emo Pillars\nmodels are highly adaptive to new domains when tuned to specific tasks such as\nGoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on\nthe first three. We also validate our dataset, conducting statistical analysis\nand human evaluation, and confirm the success of our measures in utterance\ndiversification (although less for the neutral class) and context\npersonalization, while pointing out the need for improved handling of\nout-of-taxonomy labels within the pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datasets for sentiment analysis lack context in which an opinion was\nexpressed, often crucial for emotion understanding, and are mainly limited by a\nfew emotion categories. Foundation large language models (LLMs) like GPT-4\nsuffer from over-predicting emotions and are too resource-intensive. We design\nan LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,\nfor the generation of training examples for more accessible, lightweight\nBERT-type encoder models. We focus on enlarging the semantic diversity of\nexamples and propose grounding the generation into a corpus of narratives to\nproduce non-repetitive story-character-centered utterances with unique contexts\nover 28 emotion classes. By running 700K inferences in 450 GPU hours, we\ncontribute with the dataset of 100K contextual and also 300K context-less\nexamples to cover both scenarios. We use it for fine-tuning pre-trained\nencoders, which results in several Emo Pillars models. We show that Emo Pillars\nmodels are highly adaptive to new domains when tuned to specific tasks such as\nGoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on\nthe first three. We also validate our dataset, conducting statistical analysis\nand human evaluation, and confirm the success of our measures in utterance\ndiversification (although less for the neutral class) and context\npersonalization, while pointing out the need for improved handling of\nout-of-taxonomy labels within the pipeline."
                },
                "authors": [
                    {
                        "name": "Alexander Shvets"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Shvets"
                },
                "author": "Alexander Shvets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16855v1",
                "updated": "2025-04-23T16:23:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    15,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:23:15Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    15,
                    2,
                    113,
                    0
                ],
                "title": "Monte Carlo Planning with Large Language Model for Text-Based Game\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Planning with Large Language Model for Text-Based Game\n  Agents"
                },
                "summary": "Text-based games provide valuable environments for language-based autonomous\nagents. However, planning-then-learning paradigms, such as those combining\nMonte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably\ntime-consuming due to extensive iterations. Additionally, these algorithms\nperform uncertainty-driven exploration but lack language understanding and\nreasoning abilities. In this paper, we introduce the Monte Carlo planning with\nDynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages\nthe language understanding and reasoning capabilities of Large Language Models\n(LLMs) alongside the exploratory advantages of tree search algorithms.\nSpecifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,\nenabling them to learn from past experiences and dynamically adjust action\nevaluations during planning. We conduct experiments on a series of text-based\ngames from the Jericho benchmark. Our results demonstrate that the MC-DML\nalgorithm significantly enhances performance across various games at the\ninitial planning phase, outperforming strong contemporary methods that require\nmultiple iterations. This demonstrates the effectiveness of our algorithm,\npaving the way for more efficient language-grounded planning in complex\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-based games provide valuable environments for language-based autonomous\nagents. However, planning-then-learning paradigms, such as those combining\nMonte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably\ntime-consuming due to extensive iterations. Additionally, these algorithms\nperform uncertainty-driven exploration but lack language understanding and\nreasoning abilities. In this paper, we introduce the Monte Carlo planning with\nDynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages\nthe language understanding and reasoning capabilities of Large Language Models\n(LLMs) alongside the exploratory advantages of tree search algorithms.\nSpecifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,\nenabling them to learn from past experiences and dynamically adjust action\nevaluations during planning. We conduct experiments on a series of text-based\ngames from the Jericho benchmark. Our results demonstrate that the MC-DML\nalgorithm significantly enhances performance across various games at the\ninitial planning phase, outperforming strong contemporary methods that require\nmultiple iterations. This demonstrates the effectiveness of our algorithm,\npaving the way for more efficient language-grounded planning in complex\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zijing Shi"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08813v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08813v4",
                "updated": "2025-04-23T16:14:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    14,
                    18,
                    2,
                    113,
                    0
                ],
                "published": "2023-07-17T20:01:11Z",
                "published_parsed": [
                    2023,
                    7,
                    17,
                    20,
                    1,
                    11,
                    0,
                    198,
                    0
                ],
                "title": "Comparative Performance Evaluation of Large Language Models for\n  Extracting Molecular Interactions and Pathway Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Performance Evaluation of Large Language Models for\n  Extracting Molecular Interactions and Pathway Knowledge"
                },
                "summary": "Background: Identification of the interactions and regulatory relations\nbetween biomolecules play pivotal roles in understanding complex biological\nsystems and the mechanisms underlying diverse biological functions. However,\nthe collection of such molecular interactions has heavily relied on expert\ncuration in the past, making it labor-intensive and time-consuming. To mitigate\nthese challenges, we propose leveraging the capabilities of large language\nmodels (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results: In this study, we investigate the efficacy of various LLMs in\naddressing biological tasks, such as the recognition of protein interactions,\nidentification of genes linked to pathways affected by low-dose radiation, and\nthe delineation of gene regulatory relationships. Overall, the larger models\nexhibited superior performance, indicating their potential for specific tasks\nthat involve the extraction of complex interactions among genes and proteins.\nAlthough these models possessed detailed information for distinct gene and\nprotein groups, they faced challenges in identifying groups with diverse\nfunctions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions: By conducting a comprehensive assessment of the state-of-the-art\nmodels using well-established molecular interaction and pathway databases, our\nstudy reveals that LLMs can identify genes/proteins associated with pathways of\ninterest and predict their interactions to a certain extent. Furthermore, these\nmodels can provide important insights, marking a noteworthy stride toward\nadvancing our understanding of biological systems through AI-assisted knowledge\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Identification of the interactions and regulatory relations\nbetween biomolecules play pivotal roles in understanding complex biological\nsystems and the mechanisms underlying diverse biological functions. However,\nthe collection of such molecular interactions has heavily relied on expert\ncuration in the past, making it labor-intensive and time-consuming. To mitigate\nthese challenges, we propose leveraging the capabilities of large language\nmodels (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results: In this study, we investigate the efficacy of various LLMs in\naddressing biological tasks, such as the recognition of protein interactions,\nidentification of genes linked to pathways affected by low-dose radiation, and\nthe delineation of gene regulatory relationships. Overall, the larger models\nexhibited superior performance, indicating their potential for specific tasks\nthat involve the extraction of complex interactions among genes and proteins.\nAlthough these models possessed detailed information for distinct gene and\nprotein groups, they faced challenges in identifying groups with diverse\nfunctions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions: By conducting a comprehensive assessment of the state-of-the-art\nmodels using well-established molecular interaction and pathway databases, our\nstudy reveals that LLMs can identify genes/proteins associated with pathways of\ninterest and predict their interactions to a certain extent. Furthermore, these\nmodels can provide important insights, marking a noteworthy stride toward\nadvancing our understanding of biological systems through AI-assisted knowledge\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Gilchan Park"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "name": "Xihaier Luo"
                    },
                    {
                        "name": "Vanessa L√≥pez-Marrero"
                    },
                    {
                        "name": "Shinjae Yoo"
                    },
                    {
                        "name": "Shantenu Jha"
                    }
                ],
                "author_detail": {
                    "name": "Shantenu Jha"
                },
                "author": "Shantenu Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08813v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08813v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16046v2",
                "updated": "2025-04-23T16:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    13,
                    8,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T17:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    16,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Mitigation of Worst-Case LLM Copyright Infringement"
                },
                "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Jiacan Yu"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16845v1",
                "updated": "2025-04-23T16:09:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    9,
                    33,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:09:33Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    9,
                    33,
                    2,
                    113,
                    0
                ],
                "title": "An accreting dwarf star orbiting the S-type giant star pi1 Gru",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An accreting dwarf star orbiting the S-type giant star pi1 Gru"
                },
                "summary": "Aims. We aim to characterize the properties of the inner companion of the\nS-type AGB star pi1 Gru, and to identify plausible future evolution scenarios\nfor this triple system. Methods. We observed pi1 Gru with ALMA and VLT/SPHERE.\nIn addition, we collected archival photometry data and used the Hipparcos-Gaia\nproper motion anomaly. We derive the best orbital parameters from Bayesian\ninference. Results. The inner companion, pi1 Gru C was located at 37.4 +/- 2.0\nmas from the primary in June-July 2019 (projected separation of 6.05 +/- 0.55\nau at 161.7 +/- 11.7 pc). The best orbital solution gives a companion mass of\n0.86 (+0.22/-0.20) Msun (using the derived mass of the primary), and a\nsemi-major axis of 7.05 (+0.54/-0.57) au. This leads to an orbital period of\n11.0 (+1.7/-1.5) yr. The best solution is an elliptical orbit with eccentricity\ne = 0.35 (+0.18/-0.17), but a circular orbit cannot be totally excluded. The\nclose companion can either be a K1V (F9.5V/K7V) star or a white dwarf. The\nultraviolet and millimeter continuum photometry are consistent with the\npresence of an accretion disk around the close companion. The ultraviolet\nemission could then either originate in hot spots in an overall cooler disk, or\nalso from a hot disk in case the companion is a white dwarf. Conclusions.\nThough the close companion and the AGB star are interacting, and an accretion\ndisk is observed around the companion, the mass-accretion rate is too low to\ncause a Ia supernova but could produce novae every ~900 yr. Short wavelength\nspatially resolved observations are needed to further constrain the nature of\nthe C companion. Searches for close-in companions similar to this system will\nhelp to better understand the physics of mass- and angular-momentum transfer,\nand orbital evolution in the late evolutionary stages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims. We aim to characterize the properties of the inner companion of the\nS-type AGB star pi1 Gru, and to identify plausible future evolution scenarios\nfor this triple system. Methods. We observed pi1 Gru with ALMA and VLT/SPHERE.\nIn addition, we collected archival photometry data and used the Hipparcos-Gaia\nproper motion anomaly. We derive the best orbital parameters from Bayesian\ninference. Results. The inner companion, pi1 Gru C was located at 37.4 +/- 2.0\nmas from the primary in June-July 2019 (projected separation of 6.05 +/- 0.55\nau at 161.7 +/- 11.7 pc). The best orbital solution gives a companion mass of\n0.86 (+0.22/-0.20) Msun (using the derived mass of the primary), and a\nsemi-major axis of 7.05 (+0.54/-0.57) au. This leads to an orbital period of\n11.0 (+1.7/-1.5) yr. The best solution is an elliptical orbit with eccentricity\ne = 0.35 (+0.18/-0.17), but a circular orbit cannot be totally excluded. The\nclose companion can either be a K1V (F9.5V/K7V) star or a white dwarf. The\nultraviolet and millimeter continuum photometry are consistent with the\npresence of an accretion disk around the close companion. The ultraviolet\nemission could then either originate in hot spots in an overall cooler disk, or\nalso from a hot disk in case the companion is a white dwarf. Conclusions.\nThough the close companion and the AGB star are interacting, and an accretion\ndisk is observed around the companion, the mass-accretion rate is too low to\ncause a Ia supernova but could produce novae every ~900 yr. Short wavelength\nspatially resolved observations are needed to further constrain the nature of\nthe C companion. Searches for close-in companions similar to this system will\nhelp to better understand the physics of mass- and angular-momentum transfer,\nand orbital evolution in the late evolutionary stages."
                },
                "authors": [
                    {
                        "name": "M. Montarg√®s"
                    },
                    {
                        "name": "J. Malfait"
                    },
                    {
                        "name": "M. Esseldeurs"
                    },
                    {
                        "name": "A. de Koter"
                    },
                    {
                        "name": "F. Baron"
                    },
                    {
                        "name": "P. Kervella"
                    },
                    {
                        "name": "T. Danilovich"
                    },
                    {
                        "name": "A. M. S. Richards"
                    },
                    {
                        "name": "R. Sahai"
                    },
                    {
                        "name": "I. McDonald"
                    },
                    {
                        "name": "T. Khouri"
                    },
                    {
                        "name": "S. Shetye"
                    },
                    {
                        "name": "A. Zijlstra"
                    },
                    {
                        "name": "M. Van de Sande"
                    },
                    {
                        "name": "I. El Mellah"
                    },
                    {
                        "name": "F. Herpin"
                    },
                    {
                        "name": "L. Siess"
                    },
                    {
                        "name": "S. Etoka"
                    },
                    {
                        "name": "D. Gobrecht"
                    },
                    {
                        "name": "L. Marinho"
                    },
                    {
                        "name": "S. H. J. Wallstr√∂m"
                    },
                    {
                        "name": "K. T. Wong"
                    },
                    {
                        "name": "aJ. Yates"
                    }
                ],
                "author_detail": {
                    "name": "aJ. Yates"
                },
                "author": "aJ. Yates",
                "arxiv_comment": "Accepted for publications in Astronomy & Astrophysics. 21 pages, 10+2\n  figures, 3+4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17277v2",
                "updated": "2025-04-23T16:07:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    7,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2024-09-25T18:43:58Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    43,
                    58,
                    2,
                    269,
                    0
                ],
                "title": "Building Real-time Awareness of Out-of-distribution in Trajectory\n  Prediction for Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Real-time Awareness of Out-of-distribution in Trajectory\n  Prediction for Autonomous Vehicles"
                },
                "summary": "Accurate trajectory prediction is essential for the safe operation of\nautonomous vehicles in real-world environments. Even well-trained machine\nlearning models may produce unreliable predictions due to discrepancies between\ntraining data and real-world conditions encountered during inference. In\nparticular, the training dataset tends to overrepresent common scenes (e.g.,\nstraight lanes) while underrepresenting less frequent ones (e.g., traffic\ncircles). In addition, it often overlooks unpredictable real-world events such\nas sudden braking or falling objects. To ensure safety, it is critical to\ndetect in real-time when a model's predictions become unreliable. Leveraging\nthe intuition that in-distribution (ID) scenes exhibit error patterns similar\nto training data, while out-of-distribution (OOD) scenes do not, we introduce a\nprincipled, real-time approach for OOD detection by framing it as a\nchange-point detection problem. We address the challenging settings where the\nOOD scenes are deceptive, meaning that they are not easily detectable by human\nintuitions. Our lightweight solutions can handle the occurrence of OOD at any\ntime during trajectory prediction inference. Experimental results on multiple\nreal-world datasets using a benchmark trajectory prediction model demonstrate\nthe effectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate trajectory prediction is essential for the safe operation of\nautonomous vehicles in real-world environments. Even well-trained machine\nlearning models may produce unreliable predictions due to discrepancies between\ntraining data and real-world conditions encountered during inference. In\nparticular, the training dataset tends to overrepresent common scenes (e.g.,\nstraight lanes) while underrepresenting less frequent ones (e.g., traffic\ncircles). In addition, it often overlooks unpredictable real-world events such\nas sudden braking or falling objects. To ensure safety, it is critical to\ndetect in real-time when a model's predictions become unreliable. Leveraging\nthe intuition that in-distribution (ID) scenes exhibit error patterns similar\nto training data, while out-of-distribution (OOD) scenes do not, we introduce a\nprincipled, real-time approach for OOD detection by framing it as a\nchange-point detection problem. We address the challenging settings where the\nOOD scenes are deceptive, meaning that they are not easily detectable by human\nintuitions. Our lightweight solutions can handle the occurrence of OOD at any\ntime during trajectory prediction inference. Experimental results on multiple\nreal-world datasets using a benchmark trajectory prediction model demonstrate\nthe effectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Tongfe Guo"
                    },
                    {
                        "name": "Taposh Banerjee"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Lili Su"
                    }
                ],
                "author_detail": {
                    "name": "Lili Su"
                },
                "author": "Lili Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00694v2",
                "updated": "2025-04-23T16:07:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    7,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-01T12:05:49Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    5,
                    49,
                    1,
                    91,
                    0
                ],
                "title": "On Benchmarking Code LLMs for Android Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Benchmarking Code LLMs for Android Malware Analysis"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\ncode intelligence tasks. However, their effectiveness for Android malware\nanalysis remains underexplored. Decompiled Android malware code presents unique\nchallenges for analysis, due to the malicious logic being buried within a large\nnumber of functions and the frequent lack of meaningful function names. This\npaper presents CAMA, a benchmarking framework designed to systematically\nevaluate the effectiveness of Code LLMs in Android malware analysis. CAMA\nspecifies structured model outputs to support key malware analysis tasks,\nincluding malicious function identification and malware purpose summarization.\nBuilt on these, it integrates three domain-specific evaluation metrics\n(consistency, fidelity, and semantic relevance), enabling rigorous stability\nand effectiveness assessment and cross-model comparison. We construct a\nbenchmark dataset of 118 Android malware samples from 13 families collected in\nrecent years, encompassing over 7.5 million distinct functions, and use CAMA to\nevaluate four popular open-source Code LLMs. Our experiments provide insights\ninto how Code LLMs interpret decompiled code and quantify the sensitivity to\nfunction renaming, highlighting both their potential and current limitations in\nmalware analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in various\ncode intelligence tasks. However, their effectiveness for Android malware\nanalysis remains underexplored. Decompiled Android malware code presents unique\nchallenges for analysis, due to the malicious logic being buried within a large\nnumber of functions and the frequent lack of meaningful function names. This\npaper presents CAMA, a benchmarking framework designed to systematically\nevaluate the effectiveness of Code LLMs in Android malware analysis. CAMA\nspecifies structured model outputs to support key malware analysis tasks,\nincluding malicious function identification and malware purpose summarization.\nBuilt on these, it integrates three domain-specific evaluation metrics\n(consistency, fidelity, and semantic relevance), enabling rigorous stability\nand effectiveness assessment and cross-model comparison. We construct a\nbenchmark dataset of 118 Android malware samples from 13 families collected in\nrecent years, encompassing over 7.5 million distinct functions, and use CAMA to\nevaluate four popular open-source Code LLMs. Our experiments provide insights\ninto how Code LLMs interpret decompiled code and quantify the sensitivity to\nfunction renaming, highlighting both their potential and current limitations in\nmalware analysis."
                },
                "authors": [
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Hongyu She"
                    },
                    {
                        "name": "Xingzhi Qian"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "arxiv_doi": "10.1145/3713081.3731745",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713081.3731745",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.00694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted to the 34th ACM SIGSOFT ISSTA Companion\n  (LLMSC Workshop 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16834v1",
                "updated": "2025-04-23T15:56:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    56,
                    28,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:56:28Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    56,
                    28,
                    2,
                    113,
                    0
                ],
                "title": "Improving Significant Wave Height Prediction Using Chronos Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Significant Wave Height Prediction Using Chronos Models"
                },
                "summary": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling."
                },
                "authors": [
                    {
                        "name": "Yilin Zhai"
                    },
                    {
                        "name": "Hongyuan Shi"
                    },
                    {
                        "name": "Chao Zhan"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Zaijin You"
                    },
                    {
                        "name": "Nan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wang"
                },
                "author": "Nan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08924v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08924v4",
                "updated": "2025-04-23T15:53:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    53,
                    19,
                    2,
                    113,
                    0
                ],
                "published": "2024-09-13T15:39:29Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    39,
                    29,
                    4,
                    257,
                    0
                ],
                "title": "Regression-based proximal causal inference for right-censored\n  time-to-event data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression-based proximal causal inference for right-censored\n  time-to-event data"
                },
                "summary": "Unmeasured confounding is one of the major concerns in causal inference from\nobservational data. Proximal causal inference (PCI) is an emerging\nmethodological framework to detect and potentially account for confounding bias\nby carefully leveraging a pair of negative control exposure (NCE) and outcome\n(NCO) variables, also known as treatment and outcome confounding proxies.\nAlthough regression-based PCI is well developed for binary and continuous\noutcomes, analogous PCI regression methods for right-censored time-to-event\noutcomes are currently lacking. In this paper, we propose a novel two-stage\nregression PCI approach for right-censored survival data under an additive\nhazard structural model. We provide theoretical justification for the proposed\napproach tailored to different types of NCOs, including continuous, count, and\nright-censored time-to-event variables. We illustrate the approach with an\nevaluation of the effectiveness of right heart catheterization among critically\nill patients using data from the SUPPORT study. Our method is implemented in\nthe open-access R package 'pci2s'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmeasured confounding is one of the major concerns in causal inference from\nobservational data. Proximal causal inference (PCI) is an emerging\nmethodological framework to detect and potentially account for confounding bias\nby carefully leveraging a pair of negative control exposure (NCE) and outcome\n(NCO) variables, also known as treatment and outcome confounding proxies.\nAlthough regression-based PCI is well developed for binary and continuous\noutcomes, analogous PCI regression methods for right-censored time-to-event\noutcomes are currently lacking. In this paper, we propose a novel two-stage\nregression PCI approach for right-censored survival data under an additive\nhazard structural model. We provide theoretical justification for the proposed\napproach tailored to different types of NCOs, including continuous, count, and\nright-censored time-to-event variables. We illustrate the approach with an\nevaluation of the effectiveness of right heart catheterization among critically\nill patients using data from the SUPPORT study. Our method is implemented in\nthe open-access R package 'pci2s'."
                },
                "authors": [
                    {
                        "name": "Kendrick Li"
                    },
                    {
                        "name": "George C. Linderman"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J. Tchetgen Tchetgen"
                },
                "author": "Eric J. Tchetgen Tchetgen",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08924v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08924v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16833v1",
                "updated": "2025-04-23T15:52:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    52,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:52:50Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    52,
                    50,
                    2,
                    113,
                    0
                ],
                "title": "LRASGen: LLM-based RESTful API Specification Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRASGen: LLM-based RESTful API Specification Generation"
                },
                "summary": "REpresentation State Transfer (REST) is an architectural style for designing\nweb applications that enable scalable, stateless communication between clients\nand servers via common HTTP techniques. Web APIs that employ the REST style are\nknown as RESTful (or REST) APIs. When using or testing a RESTful API,\ndevelopers may need to employ its specification, which is often defined by\nopen-source standards such as the OpenAPI Specification (OAS). However, it can\nbe very time-consuming and error-prone to write and update these\nspecifications, which may negatively impact the use of RESTful APIs, especially\nwhen the software requirements change. Many tools and methods have been\nproposed to solve this problem, such as Respector and Swagger Core. OAS\ngeneration can be regarded as a common text-generation task that creates a\nformal description of API endpoints derived from the source code. A potential\nsolution for this may involve using Large Language Models (LLMs), which have\nstrong capabilities in both code understanding and text generation. Motivated\nby this, we propose a novel approach for generating the OASs of RESTful APIs\nusing LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the\nbest of our knowledge, this is the first use of LLMs and API source code to\ngenerate OASs for RESTful APIs. Compared with existing tools and methods,\nLRASGen can generate the OASs, even when the implementation is incomplete (with\npartial code, and/or missing annotations/comments, etc.). To evaluate the\nLRASGen performance, we conducted a series of empirical studies on 20\nreal-world RESTful APIs. The results show that two LLMs (GPT-4o mini and\nDeepSeek V3) can both support LARSGen to generate accurate specifications, and\nLRASGen-generated specifications cover an average of 48.85% more missed\nentities than the developer-provided specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REpresentation State Transfer (REST) is an architectural style for designing\nweb applications that enable scalable, stateless communication between clients\nand servers via common HTTP techniques. Web APIs that employ the REST style are\nknown as RESTful (or REST) APIs. When using or testing a RESTful API,\ndevelopers may need to employ its specification, which is often defined by\nopen-source standards such as the OpenAPI Specification (OAS). However, it can\nbe very time-consuming and error-prone to write and update these\nspecifications, which may negatively impact the use of RESTful APIs, especially\nwhen the software requirements change. Many tools and methods have been\nproposed to solve this problem, such as Respector and Swagger Core. OAS\ngeneration can be regarded as a common text-generation task that creates a\nformal description of API endpoints derived from the source code. A potential\nsolution for this may involve using Large Language Models (LLMs), which have\nstrong capabilities in both code understanding and text generation. Motivated\nby this, we propose a novel approach for generating the OASs of RESTful APIs\nusing LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the\nbest of our knowledge, this is the first use of LLMs and API source code to\ngenerate OASs for RESTful APIs. Compared with existing tools and methods,\nLRASGen can generate the OASs, even when the implementation is incomplete (with\npartial code, and/or missing annotations/comments, etc.). To evaluate the\nLRASGen performance, we conducted a series of empirical studies on 20\nreal-world RESTful APIs. The results show that two LLMs (GPT-4o mini and\nDeepSeek V3) can both support LARSGen to generate accurate specifications, and\nLRASGen-generated specifications cover an average of 48.85% more missed\nentities than the developer-provided specifications."
                },
                "authors": [
                    {
                        "name": "Sida Deng"
                    },
                    {
                        "name": "Rubing Huang"
                    },
                    {
                        "name": "Man Zhang"
                    },
                    {
                        "name": "Chenhui Cui"
                    },
                    {
                        "name": "Dave Towey"
                    },
                    {
                        "name": "Rongcun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rongcun Wang"
                },
                "author": "Rongcun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16832v1",
                "updated": "2025-04-23T15:48:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    48,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:48:55Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    48,
                    55,
                    2,
                    113,
                    0
                ],
                "title": "GreenMind: A Next-Generation Vietnamese Large Language Model for\n  Structured and Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenMind: A Next-Generation Vietnamese Large Language Model for\n  Structured and Logical Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that\nrequire intermediate reasoning steps prior to generating a final answer. In\nthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model\ninspired by the finetuning strategy based on Group Relative Policy\nOptimization. We also leverage a high-quality Vietnamese synthesized reasoning\ndataset and design two reward functions to tackle the main limitations of this\ntechnique: (i) language mixing, where we explicitly detect the presence of\nbiased language characters during the process of sampling tokens, and (ii) we\nleverage Sentence Transformer-based models to ensure that the generated\nreasoning content maintains factual correctness and does not distort the final\noutput. Experimental results on the Vietnamese dataset from the VLSP 2023\nChallenge demonstrate that our model outperforms prior works and enhances\nlinguistic consistency in its responses. Furthermore, we extend our evaluation\nto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of\nour reasoning method compared to few-shot prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that\nrequire intermediate reasoning steps prior to generating a final answer. In\nthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model\ninspired by the finetuning strategy based on Group Relative Policy\nOptimization. We also leverage a high-quality Vietnamese synthesized reasoning\ndataset and design two reward functions to tackle the main limitations of this\ntechnique: (i) language mixing, where we explicitly detect the presence of\nbiased language characters during the process of sampling tokens, and (ii) we\nleverage Sentence Transformer-based models to ensure that the generated\nreasoning content maintains factual correctness and does not distort the final\noutput. Experimental results on the Vietnamese dataset from the VLSP 2023\nChallenge demonstrate that our model outperforms prior works and enhances\nlinguistic consistency in its responses. Furthermore, we extend our evaluation\nto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of\nour reasoning method compared to few-shot prompting techniques."
                },
                "authors": [
                    {
                        "name": "Luu Quy Tung"
                    },
                    {
                        "name": "Hoang Quoc Viet"
                    },
                    {
                        "name": "Vo Trong Thu"
                    }
                ],
                "author_detail": {
                    "name": "Vo Trong Thu"
                },
                "author": "Vo Trong Thu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16354v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16354v4",
                "updated": "2025-04-23T15:48:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    48,
                    42,
                    2,
                    113,
                    0
                ],
                "published": "2024-03-25T01:12:57Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    1,
                    12,
                    57,
                    0,
                    85,
                    0
                ],
                "title": "ChatDBG: Augmenting Debugging with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatDBG: Augmenting Debugging with Large Language Models"
                },
                "summary": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like \"why is x null?\". To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 75,000 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like \"why is x null?\". To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 75,000 times."
                },
                "authors": [
                    {
                        "name": "Kyla H. Levin"
                    },
                    {
                        "name": "Nicolas van Kempen"
                    },
                    {
                        "name": "Emery D. Berger"
                    },
                    {
                        "name": "Stephen N. Freund"
                    }
                ],
                "author_detail": {
                    "name": "Stephen N. Freund"
                },
                "author": "Stephen N. Freund",
                "arxiv_doi": "10.1145/3729355",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729355",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16354v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16354v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, to appear at FSE 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16828v1",
                "updated": "2025-04-23T15:44:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    44,
                    54,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:44:54Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    44,
                    54,
                    2,
                    113,
                    0
                ],
                "title": "Process Reward Models That Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models That Think"
                },
                "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm."
                },
                "authors": [
                    {
                        "name": "Muhammad Khalifa"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16813v1",
                "updated": "2025-04-23T15:31:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    31,
                    11,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:31:11Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    31,
                    11,
                    2,
                    113,
                    0
                ],
                "title": "LLM-assisted Graph-RAG Information Extraction from IFC Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Graph-RAG Information Extraction from IFC Data"
                },
                "summary": "IFC data has become the general building information standard for\ncollaborative work in the construction industry. However, IFC data can be very\ncomplicated because it allows for multiple ways to represent the same product\ninformation. In this research, we utilise the capabilities of LLMs to parse the\nIFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to\nretrieve building object properties and their relations. We will show that,\ndespite limitations due to the complex hierarchy of the IFC data, the Graph-RAG\nparsing enhances generative LLMs like GPT-4o with graph-based knowledge,\nenabling natural language query-response retrieval without the need for a\ncomplex pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFC data has become the general building information standard for\ncollaborative work in the construction industry. However, IFC data can be very\ncomplicated because it allows for multiple ways to represent the same product\ninformation. In this research, we utilise the capabilities of LLMs to parse the\nIFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to\nretrieve building object properties and their relations. We will show that,\ndespite limitations due to the complex hierarchy of the IFC data, the Graph-RAG\nparsing enhances generative LLMs like GPT-4o with graph-based knowledge,\nenabling natural language query-response retrieval without the need for a\ncomplex pipeline."
                },
                "authors": [
                    {
                        "name": "Sima Iranmanesh"
                    },
                    {
                        "name": "Hadeel Saadany"
                    },
                    {
                        "name": "Edlira Vakaj"
                    }
                ],
                "author_detail": {
                    "name": "Edlira Vakaj"
                },
                "author": "Edlira Vakaj",
                "arxiv_comment": "2025 European Conference on Computing in Construction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16801v1",
                "updated": "2025-04-23T15:20:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    53,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:20:53Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    53,
                    2,
                    113,
                    0
                ],
                "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA"
                },
                "authors": [
                    {
                        "name": "Xiaoxing Hu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Yupei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yupei Wang"
                },
                "author": "Yupei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16800v1",
                "updated": "2025-04-23T15:20:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    39,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:20:39Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    39,
                    2,
                    113,
                    0
                ],
                "title": "Array Partitioning Based Near-Field Attitude and Location Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Array Partitioning Based Near-Field Attitude and Location Estimation"
                },
                "summary": "This paper studies a passive source localization system, where a single base\nstation (BS) is employed to estimate the positions and attitudes of multiple\nmobile stations (MSs). The BS and the MSs are equipped with uniform rectangular\narrays, and the MSs are located in the near-field region of the BS array. To\navoid the difficulty of tackling the problem directly based on the near-field\nsignal model, we establish a subarray-wise far-field received signal model. In\nthis model, the entire BS array is divided into multiple subarrays to ensure\nthat each MS is in the far-field region of each BS subarray. By exploiting the\nangles of arrival (AoAs) of an MS antenna at different BS subarrays, we\nformulate the attitude and location estimation problem under the Bayesian\ninference framework. Based on the factor graph representation of the\nprobabilistic problem model, a message passing algorithm named array\npartitioning based pose and location estimation (APPLE) is developed to solve\nthis problem. An estimation-error lower bound is obtained as a performance\nbenchmark of the proposed algorithm. Numerical results demonstrate that the\nproposed APPLE algorithm outperforms other baseline methods in the accuracy of\nposition and attitude estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a passive source localization system, where a single base\nstation (BS) is employed to estimate the positions and attitudes of multiple\nmobile stations (MSs). The BS and the MSs are equipped with uniform rectangular\narrays, and the MSs are located in the near-field region of the BS array. To\navoid the difficulty of tackling the problem directly based on the near-field\nsignal model, we establish a subarray-wise far-field received signal model. In\nthis model, the entire BS array is divided into multiple subarrays to ensure\nthat each MS is in the far-field region of each BS subarray. By exploiting the\nangles of arrival (AoAs) of an MS antenna at different BS subarrays, we\nformulate the attitude and location estimation problem under the Bayesian\ninference framework. Based on the factor graph representation of the\nprobabilistic problem model, a message passing algorithm named array\npartitioning based pose and location estimation (APPLE) is developed to solve\nthis problem. An estimation-error lower bound is obtained as a performance\nbenchmark of the proposed algorithm. Numerical results demonstrate that the\nproposed APPLE algorithm outperforms other baseline methods in the accuracy of\nposition and attitude estimation."
                },
                "authors": [
                    {
                        "name": "Mingchen Zhang"
                    },
                    {
                        "name": "Xiaojun Yuan"
                    },
                    {
                        "name": "Boyu Teng"
                    },
                    {
                        "name": "Li Wang"
                    }
                ],
                "author_detail": {
                    "name": "Li Wang"
                },
                "author": "Li Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16797v1",
                "updated": "2025-04-23T15:18:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "The extended adjoint state and nonlinearity in correlation-based passive\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extended adjoint state and nonlinearity in correlation-based passive\n  imaging"
                },
                "summary": "This articles investigates physics-based passive imaging problem, wherein one\ninfers an unknown medium using ambient noise and correlation of the noise\nsignal. We develop a general backpropagation framework via the so-called\nextended adjoint state, suitable for any linear PDE; crucially, this approach\nreduces by half the number of required PDE solves. Applications to several\ndifferent PDE models demonstrate the universality of our method. In addition,\nwe analyze the nonlinearity of the correlated model, revealing a surprising\ntangential cone condition-like structure, thereby advancing the state of the\nart towards a convergence guarantee for regularized reconstruction in passive\nimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This articles investigates physics-based passive imaging problem, wherein one\ninfers an unknown medium using ambient noise and correlation of the noise\nsignal. We develop a general backpropagation framework via the so-called\nextended adjoint state, suitable for any linear PDE; crucially, this approach\nreduces by half the number of required PDE solves. Applications to several\ndifferent PDE models demonstrate the universality of our method. In addition,\nwe analyze the nonlinearity of the correlated model, revealing a surprising\ntangential cone condition-like structure, thereby advancing the state of the\nart towards a convergence guarantee for regularized reconstruction in passive\nimaging."
                },
                "authors": [
                    {
                        "name": "Tram Thi Ngoc Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Tram Thi Ngoc Nguyen"
                },
                "author": "Tram Thi Ngoc Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M32, 65J22, 35R30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16795v1",
                "updated": "2025-04-23T15:15:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    15,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:15:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    15,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Random Long-Context Access for Mamba via Hardware-aligned Hierarchical\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Long-Context Access for Mamba via Hardware-aligned Hierarchical\n  Sparse Attention"
                },
                "summary": "A key advantage of Recurrent Neural Networks (RNNs) over Transformers is\ntheir linear computational and space complexity enables faster training and\ninference for long sequences. However, RNNs are fundamentally unable to\nrandomly access historical context, and simply integrating attention mechanisms\nmay undermine their efficiency advantages. To overcome this limitation, we\npropose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel\nattention mechanism that enhances RNNs with long-range random access\nflexibility while preserving their merits in efficiency and length\ngeneralization. HSA divides inputs into chunks, selecting the top-$k$ chunks\nand hierarchically aggregates information. The core innovation lies in learning\ntoken-to-chunk relevance based on fine-grained token-level information inside\neach chunk. This approach enhances the precision of chunk selection across both\nin-domain and out-of-domain context lengths. To make HSA efficient, we further\nintroduce a hardware-aligned kernel design. By combining HSA with Mamba, we\nintroduce RAMba, which achieves perfect accuracy in passkey retrieval across 64\nmillion contexts despite pre-training on only 4K-length contexts, and\nsignificant improvements on various downstream tasks, with nearly constant\nmemory footprint. These results show RAMba's huge potential in long-context\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key advantage of Recurrent Neural Networks (RNNs) over Transformers is\ntheir linear computational and space complexity enables faster training and\ninference for long sequences. However, RNNs are fundamentally unable to\nrandomly access historical context, and simply integrating attention mechanisms\nmay undermine their efficiency advantages. To overcome this limitation, we\npropose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel\nattention mechanism that enhances RNNs with long-range random access\nflexibility while preserving their merits in efficiency and length\ngeneralization. HSA divides inputs into chunks, selecting the top-$k$ chunks\nand hierarchically aggregates information. The core innovation lies in learning\ntoken-to-chunk relevance based on fine-grained token-level information inside\neach chunk. This approach enhances the precision of chunk selection across both\nin-domain and out-of-domain context lengths. To make HSA efficient, we further\nintroduce a hardware-aligned kernel design. By combining HSA with Mamba, we\nintroduce RAMba, which achieves perfect accuracy in passkey retrieval across 64\nmillion contexts despite pre-training on only 4K-length contexts, and\nsignificant improvements on various downstream tasks, with nearly constant\nmemory footprint. These results show RAMba's huge potential in long-context\nmodeling."
                },
                "authors": [
                    {
                        "name": "Xiang Hu"
                    },
                    {
                        "name": "Jiaqi Leng"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kewei Tu"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16792v1",
                "updated": "2025-04-23T15:10:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    10,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:10:55Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    10,
                    55,
                    2,
                    113,
                    0
                ],
                "title": "Preemption Aware Task Scheduling for Priority and Deadline Constrained\n  DNN Inference Task Offloading in Homogeneous Mobile-Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preemption Aware Task Scheduling for Priority and Deadline Constrained\n  DNN Inference Task Offloading in Homogeneous Mobile-Edge Networks"
                },
                "summary": "This paper addresses the computational offloading of Deep Neural Networks\n(DNNs) to nearby devices with similar processing capabilities, to avoid the\nlarger communication delays incurred for cloud offloading. We present a\npreemption aware scheduling approach for priority and deadline constrained task\noffloading in homogeneous edge networks. Our scheduling approach consists of\ntwo distinct scheduling algorithms, designed to accommodate the differing\nrequirements of high and low priority tasks. To satisfy a task's deadline, our\nscheduling approach considers the availability of both communication and\ncomputational resources in the network when making placements in both the\ncurrent time-slot and future time-slots. The scheduler implements a\ndeadline-aware preemption mechanism to guarantee resource access to high\npriority tasks. When low-priority tasks are selected for preemption, the\nscheduler will attempt to reallocate them if possible before their deadline. We\nimplement this scheduling approach into a task offloading system which we\nevaluate empirically in the real-world on a network of edge devices composed of\nfour Raspberry Pi 2 Model B's. We evaluate this system under against a version\nwithout a task preemption mechanism as well as workstealing approaches to\ncompare the impact on high priority task completion and the ability to complete\noverall frames. These solutions are evaluated under a workload of 1296 frames.\nOur findings show that our scheduling approach allows for 99\\% of high-priority\ntasks to complete while also providing a 3 - 8\\% increase in the number of\nframes fully classified end-to-end over both workstealing approaches and\nsystems without a preemption mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the computational offloading of Deep Neural Networks\n(DNNs) to nearby devices with similar processing capabilities, to avoid the\nlarger communication delays incurred for cloud offloading. We present a\npreemption aware scheduling approach for priority and deadline constrained task\noffloading in homogeneous edge networks. Our scheduling approach consists of\ntwo distinct scheduling algorithms, designed to accommodate the differing\nrequirements of high and low priority tasks. To satisfy a task's deadline, our\nscheduling approach considers the availability of both communication and\ncomputational resources in the network when making placements in both the\ncurrent time-slot and future time-slots. The scheduler implements a\ndeadline-aware preemption mechanism to guarantee resource access to high\npriority tasks. When low-priority tasks are selected for preemption, the\nscheduler will attempt to reallocate them if possible before their deadline. We\nimplement this scheduling approach into a task offloading system which we\nevaluate empirically in the real-world on a network of edge devices composed of\nfour Raspberry Pi 2 Model B's. We evaluate this system under against a version\nwithout a task preemption mechanism as well as workstealing approaches to\ncompare the impact on high priority task completion and the ability to complete\noverall frames. These solutions are evaluated under a workload of 1296 frames.\nOur findings show that our scheduling approach allows for 99\\% of high-priority\ntasks to complete while also providing a 3 - 8\\% increase in the number of\nframes fully classified end-to-end over both workstealing approaches and\nsystems without a preemption mechanism."
                },
                "authors": [
                    {
                        "name": "Jamie Cotter"
                    },
                    {
                        "name": "Ignacio Castineiras"
                    },
                    {
                        "name": "Donna O'Shea"
                    },
                    {
                        "name": "Victor Cionca"
                    }
                ],
                "author_detail": {
                    "name": "Victor Cionca"
                },
                "author": "Victor Cionca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11193v3",
                "updated": "2025-04-23T15:10:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    10,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2024-08-20T20:59:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    20,
                    59,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "Inference with Many Weak Instruments and Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Many Weak Instruments and Heterogeneity"
                },
                "summary": "This paper considers inference in a linear instrumental variable regression\nmodel with many potentially weak instruments, in the presence of heterogeneous\ntreatment effects. I first show that existing test procedures, including those\nthat are robust to either weak instruments or heterogeneous treatment effects,\ncan be arbitrarily oversized. I propose a novel and valid test based on a score\nstatistic and a ``leave-three-out\" variance estimator. In the presence of\nheterogeneity and within the class of tests that are functions of the\nleave-one-out analog of a maximal invariant, this test is asymptotically the\nuniformly most powerful unbiased test. In two applications to judge and\nquarter-of-birth instruments, the proposed inference procedure also yields a\nbounded confidence set while some existing methods yield unbounded or empty\nconfidence sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers inference in a linear instrumental variable regression\nmodel with many potentially weak instruments, in the presence of heterogeneous\ntreatment effects. I first show that existing test procedures, including those\nthat are robust to either weak instruments or heterogeneous treatment effects,\ncan be arbitrarily oversized. I propose a novel and valid test based on a score\nstatistic and a ``leave-three-out\" variance estimator. In the presence of\nheterogeneity and within the class of tests that are functions of the\nleave-one-out analog of a maximal invariant, this test is asymptotically the\nuniformly most powerful unbiased test. In two applications to judge and\nquarter-of-birth instruments, the proposed inference procedure also yields a\nbounded confidence set while some existing methods yield unbounded or empty\nconfidence sets."
                },
                "authors": [
                    {
                        "name": "Luther Yap"
                    }
                ],
                "author_detail": {
                    "name": "Luther Yap"
                },
                "author": "Luther Yap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16786v1",
                "updated": "2025-04-23T15:02:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    53,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:02:53Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    53,
                    2,
                    113,
                    0
                ],
                "title": "MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating\n  Over-Smoothing and Incorporating Outlier Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating\n  Over-Smoothing and Incorporating Outlier Scores"
                },
                "summary": "Recent advances in large language models have significantly improved their\nability to process long-context input, but practical applications are\nchallenged by increased inference time and resource consumption, particularly\nin resource-constrained environments. To address these challenges, we propose\nMOOSComp, a token-classification-based long-context compression method that\nenhances the performance of a BERT-based compressor by mitigating the\nover-smoothing problem and incorporating outlier scores. In the training phase,\nwe add an inter-class cosine similarity loss term to penalize excessively\nsimilar token representations, thereby improving the token classification\naccuracy. During the compression phase, we introduce outlier scores to preserve\nrare but critical tokens that are prone to be discarded in task-agnostic\ncompression. These scores are integrated with the classifier's output, making\nthe compressor more generalizable to various tasks. Superior performance is\nachieved at various compression ratios on long-context understanding and\nreasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x\ncompression ratio on a resource-constrained mobile device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have significantly improved their\nability to process long-context input, but practical applications are\nchallenged by increased inference time and resource consumption, particularly\nin resource-constrained environments. To address these challenges, we propose\nMOOSComp, a token-classification-based long-context compression method that\nenhances the performance of a BERT-based compressor by mitigating the\nover-smoothing problem and incorporating outlier scores. In the training phase,\nwe add an inter-class cosine similarity loss term to penalize excessively\nsimilar token representations, thereby improving the token classification\naccuracy. During the compression phase, we introduce outlier scores to preserve\nrare but critical tokens that are prone to be discarded in task-agnostic\ncompression. These scores are integrated with the classifier's output, making\nthe compressor more generalizable to various tasks. Superior performance is\nachieved at various compression ratios on long-context understanding and\nreasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x\ncompression ratio on a resource-constrained mobile device."
                },
                "authors": [
                    {
                        "name": "Fengwei Zhou"
                    },
                    {
                        "name": "Jiafei Song"
                    },
                    {
                        "name": "Wenjin Jason Li"
                    },
                    {
                        "name": "Gengjian Xue"
                    },
                    {
                        "name": "Zhikang Zhao"
                    },
                    {
                        "name": "Yichao Lu"
                    },
                    {
                        "name": "Bailin Na"
                    }
                ],
                "author_detail": {
                    "name": "Bailin Na"
                },
                "author": "Bailin Na",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17163v2",
                "updated": "2025-04-23T14:42:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    42,
                    19,
                    2,
                    113,
                    0
                ],
                "published": "2024-11-26T07:07:48Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    7,
                    7,
                    48,
                    1,
                    331,
                    0
                ],
                "title": "OSDFace: One-Step Diffusion Model for Face Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSDFace: One-Step Diffusion Model for Face Restoration"
                },
                "summary": "Diffusion models have demonstrated impressive performance in face\nrestoration. Yet, their multi-step inference process remains computationally\nintensive, limiting their applicability in real-world scenarios. Moreover,\nexisting methods often struggle to generate face images that are harmonious,\nrealistic, and consistent with the subject's identity. In this work, we propose\nOSDFace, a novel one-step diffusion model for face restoration. Specifically,\nwe propose a visual representation embedder (VRE) to better capture prior\ninformation and understand the input face. In VRE, low-quality faces are\nprocessed by a visual tokenizer and subsequently embedded with a\nvector-quantized dictionary to generate visual prompts. Additionally, we\nincorporate a facial identity loss derived from face recognition to further\nensure identity consistency. We further employ a generative adversarial network\n(GAN) as a guidance model to encourage distribution alignment between the\nrestored face and the ground truth. Experimental results demonstrate that\nOSDFace surpasses current state-of-the-art (SOTA) methods in both visual\nquality and quantitative metrics, generating high-fidelity, natural face images\nwith high identity consistency. The code and model will be released at\nhttps://github.com/jkwang28/OSDFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive performance in face\nrestoration. Yet, their multi-step inference process remains computationally\nintensive, limiting their applicability in real-world scenarios. Moreover,\nexisting methods often struggle to generate face images that are harmonious,\nrealistic, and consistent with the subject's identity. In this work, we propose\nOSDFace, a novel one-step diffusion model for face restoration. Specifically,\nwe propose a visual representation embedder (VRE) to better capture prior\ninformation and understand the input face. In VRE, low-quality faces are\nprocessed by a visual tokenizer and subsequently embedded with a\nvector-quantized dictionary to generate visual prompts. Additionally, we\nincorporate a facial identity loss derived from face recognition to further\nensure identity consistency. We further employ a generative adversarial network\n(GAN) as a guidance model to encourage distribution alignment between the\nrestored face and the ground truth. Experimental results demonstrate that\nOSDFace surpasses current state-of-the-art (SOTA) methods in both visual\nquality and quantitative metrics, generating high-fidelity, natural face images\nwith high identity consistency. The code and model will be released at\nhttps://github.com/jkwang28/OSDFace."
                },
                "authors": [
                    {
                        "name": "Jingkai Wang"
                    },
                    {
                        "name": "Jue Gong"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "Accepted to CVPR 2025. The code and model will be available at\n  https://github.com/jkwang28/OSDFace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16768v1",
                "updated": "2025-04-23T14:41:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    41,
                    11,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:41:11Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    41,
                    11,
                    2,
                    113,
                    0
                ],
                "title": "How Effective are Generative Large Language Models in Performing\n  Requirements Classification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Effective are Generative Large Language Models in Performing\n  Requirements Classification?"
                },
                "summary": "In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance."
                },
                "authors": [
                    {
                        "name": "Waad Alhoshan"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Liping Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhao"
                },
                "author": "Liping Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16760v1",
                "updated": "2025-04-23T14:33:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    33,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:33:20Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    33,
                    20,
                    2,
                    113,
                    0
                ],
                "title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies"
                },
                "summary": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications."
                },
                "authors": [
                    {
                        "name": "Bartosz Piotrowski"
                    },
                    {
                        "name": "Witold Drzewakowski"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Mi≈Ço≈õ"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Mi≈Ço≈õ"
                },
                "author": "Piotr Mi≈Ço≈õ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16754v1",
                "updated": "2025-04-23T14:27:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    27,
                    12,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:27:12Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    27,
                    12,
                    2,
                    113,
                    0
                ],
                "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for\n  Long-Context AI Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for\n  Long-Context AI Conversations"
                },
                "summary": "Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining."
                },
                "authors": [
                    {
                        "name": "Kwangseob Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Kwangseob Ahn"
                },
                "author": "Kwangseob Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05812v2",
                "updated": "2025-04-23T14:25:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    25,
                    51,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-08T08:48:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    48,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization"
                },
                "summary": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervision--such as human-labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form natural reasoning tasks. Specifically,\nwithout any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy\nof Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervision--such as human-labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form natural reasoning tasks. Specifically,\nwithout any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy\nof Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro."
                },
                "authors": [
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Haitao Wu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "arxiv_comment": "Ongoing work. First released on April 8, 2025. Updated the natural\n  reasoning results on April 23, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03004v2",
                "updated": "2025-04-23T14:25:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    25,
                    46,
                    2,
                    113,
                    0
                ],
                "published": "2024-07-03T11:02:12Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    11,
                    2,
                    12,
                    2,
                    185,
                    0
                ],
                "title": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from\n  Unstructured Clinical Narratives in Epilepsy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from\n  Unstructured Clinical Narratives in Epilepsy"
                },
                "summary": "Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare."
                },
                "authors": [
                    {
                        "name": "Meghal Dani"
                    },
                    {
                        "name": "Muthu Jeyanthi Prakash"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Stefanie Liebe"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Liebe"
                },
                "author": "Stefanie Liebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16736v1",
                "updated": "2025-04-23T14:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    7,
                    26,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    7,
                    26,
                    2,
                    113,
                    0
                ],
                "title": "A Survey of AI Agent Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of AI Agent Protocols"
                },
                "summary": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents."
                },
                "authors": [
                    {
                        "name": "Yingxuan Yang"
                    },
                    {
                        "name": "Huacan Chai"
                    },
                    {
                        "name": "Yuanyi Song"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Haoyi Hu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Gaowei Chang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16728v1",
                "updated": "2025-04-23T14:01:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    1,
                    36,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:01:36Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    1,
                    36,
                    2,
                    113,
                    0
                ],
                "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRIS: Interactive Research Ideation System for Accelerating Scientific\n  Discovery"
                },
                "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System"
                },
                "authors": [
                    {
                        "name": "Aniketh Garikaparthi"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "6 pages main-text, 2 pages appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16729v1",
                "updated": "2025-04-23T14:01:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    1,
                    36,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:01:36Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    1,
                    36,
                    2,
                    113,
                    0
                ],
                "title": "MEC Task Offloading in AIoT: A User-Centric DRL Model Splitting\n  Inference Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEC Task Offloading in AIoT: A User-Centric DRL Model Splitting\n  Inference Scheme"
                },
                "summary": "With the rapid development of the Artificial Intelligence of Things (AIoT),\nmobile edge computing (MEC) becomes an essential technology underpinning AIoT\napplications. However, multi-angle resource constraints, multi-user task\ncompetition, and the complexity of task offloading decisions in dynamic MEC\nenvironments present new technical challenges. Therefore, a user-centric deep\nreinforcement learning (DRL) model splitting inference scheme is proposed to\naddress the problem. This scheme combines model splitting inference technology\nand designs a UCMS_MADDPG-based offloading algorithm to realize efficient model\nsplitting inference responses in the dynamic MEC environment with multi-angle\nresource constraints. Specifically, we formulate a joint optimization problem\nthat integrates resource allocation, server selection, and task offloading,\naiming to minimize the weighted sum of task execution delay and energy\nconsumption. We also introduce a user-server co-selection algorithm to address\nthe selection issue between users and servers. Furthermore, we design an\nalgorithm centered on user pre-decision to coordinate the outputs of continuous\nand discrete hybrid decisions, and introduce a priority sampling mechanism\nbased on reward-error trade-off to optimize the experience replay mechanism of\nthe network. Simulation results show that the proposed UCMS_MADDPG-based\noffloading algorithm demonstrates superior overall performance compared with\nother benchmark algorithms in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of the Artificial Intelligence of Things (AIoT),\nmobile edge computing (MEC) becomes an essential technology underpinning AIoT\napplications. However, multi-angle resource constraints, multi-user task\ncompetition, and the complexity of task offloading decisions in dynamic MEC\nenvironments present new technical challenges. Therefore, a user-centric deep\nreinforcement learning (DRL) model splitting inference scheme is proposed to\naddress the problem. This scheme combines model splitting inference technology\nand designs a UCMS_MADDPG-based offloading algorithm to realize efficient model\nsplitting inference responses in the dynamic MEC environment with multi-angle\nresource constraints. Specifically, we formulate a joint optimization problem\nthat integrates resource allocation, server selection, and task offloading,\naiming to minimize the weighted sum of task execution delay and energy\nconsumption. We also introduce a user-server co-selection algorithm to address\nthe selection issue between users and servers. Furthermore, we design an\nalgorithm centered on user pre-decision to coordinate the outputs of continuous\nand discrete hybrid decisions, and introduce a priority sampling mechanism\nbased on reward-error trade-off to optimize the experience replay mechanism of\nthe network. Simulation results show that the proposed UCMS_MADDPG-based\noffloading algorithm demonstrates superior overall performance compared with\nother benchmark algorithms in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Weixi Li"
                    },
                    {
                        "name": "Rongzuo Guo"
                    },
                    {
                        "name": "Yuning Wang"
                    },
                    {
                        "name": "Fangying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fangying Chen"
                },
                "author": "Fangying Chen",
                "arxiv_comment": "39 pages,11 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10982v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10982v4",
                "updated": "2025-04-23T13:54:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    54,
                    1,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-15T08:46:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    46,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs"
                },
                "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."
                },
                "authors": [
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Xingyu Song"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Zixin Xu"
                    },
                    {
                        "name": "Xiujie Chen"
                    },
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10982v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10982v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00060v2",
                "updated": "2025-04-23T13:49:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    49,
                    48,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-31T12:20:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    20,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "CF-CAM: Cluster Filter Class Activation Mapping for Reliable\n  Gradient-Based Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CF-CAM: Cluster Filter Class Activation Mapping for Reliable\n  Gradient-Based Interpretability"
                },
                "summary": "As deep learning continues to advance, the transparency of neural network\ndecision-making remains a critical challenge, limiting trust and applicability\nin high-stakes domains. Class Activation Mapping (CAM) techniques have emerged\nas a key approach toward visualizing model decisions, yet existing methods face\ninherent trade-offs. Gradient-based CAM variants suffer from sensitivity to\ngradient perturbations due to gradient noise, leading to unstable and\nunreliable explanations. Conversely, gradient-free approaches mitigate gradient\ninstability but incur significant computational overhead and inference latency.\nTo address these limitations, we propose a Cluster Filter Class Activation Map\n(CF-CAM) technique, a novel framework that reintroduces gradient-based\nweighting while enhancing robustness against gradient noise. CF-CAM utilizes\nhierarchical importance weighting strategy to balance discriminative feature\npreservation and noise elimination. A density-aware channel clustering method\nvia Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups\nsemantically relevant feature channels and discard noise-prone activations.\nAdditionally, cluster-conditioned gradient filtering leverages Gaussian filters\nto refine gradient signals, preserving edge-aware localization while\nsuppressing noise impact. Experiment results demonstrate that CF-CAM achieves\nsuperior interpretability performance while enhancing computational efficiency,\noutperforming state-of-the-art CAM methods in faithfulness and robustness. By\neffectively mitigating gradient instability without excessive computational\ncost, CF-CAM provides a competitive solution for enhancing the interpretability\nof deep neural networks in critical applications such as autonomous driving and\nmedical diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning continues to advance, the transparency of neural network\ndecision-making remains a critical challenge, limiting trust and applicability\nin high-stakes domains. Class Activation Mapping (CAM) techniques have emerged\nas a key approach toward visualizing model decisions, yet existing methods face\ninherent trade-offs. Gradient-based CAM variants suffer from sensitivity to\ngradient perturbations due to gradient noise, leading to unstable and\nunreliable explanations. Conversely, gradient-free approaches mitigate gradient\ninstability but incur significant computational overhead and inference latency.\nTo address these limitations, we propose a Cluster Filter Class Activation Map\n(CF-CAM) technique, a novel framework that reintroduces gradient-based\nweighting while enhancing robustness against gradient noise. CF-CAM utilizes\nhierarchical importance weighting strategy to balance discriminative feature\npreservation and noise elimination. A density-aware channel clustering method\nvia Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups\nsemantically relevant feature channels and discard noise-prone activations.\nAdditionally, cluster-conditioned gradient filtering leverages Gaussian filters\nto refine gradient signals, preserving edge-aware localization while\nsuppressing noise impact. Experiment results demonstrate that CF-CAM achieves\nsuperior interpretability performance while enhancing computational efficiency,\noutperforming state-of-the-art CAM methods in faithfulness and robustness. By\neffectively mitigating gradient instability without excessive computational\ncost, CF-CAM provides a competitive solution for enhancing the interpretability\nof deep neural networks in critical applications such as autonomous driving and\nmedical diagnosis."
                },
                "authors": [
                    {
                        "name": "Hongjie He"
                    },
                    {
                        "name": "Xu Pan"
                    },
                    {
                        "name": "Yudong Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yudong Yao"
                },
                "author": "Yudong Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15965v2",
                "updated": "2025-04-23T13:47:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    47,
                    27,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T15:05:04Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    5,
                    4,
                    1,
                    112,
                    0
                ],
                "title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs"
                },
                "summary": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "26 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16717v1",
                "updated": "2025-04-23T13:45:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    45,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T13:45:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    45,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Random walks on random networks of cliques: Inferring the network\n  structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random walks on random networks of cliques: Inferring the network\n  structure"
                },
                "summary": "We study the properties of discrete-time random walks on networks formed by\nrandomly interconnected cliques, namely, random networks of cliques. Our\npurpose is to derive the parameters that define the network structure --\nspecifically, the distribution of clique size and the abundance of inter-clique\nlinks -- from the observation of selected statistical features along the random\nwalk. To this end, we apply a Bayesian approach based on recording the times\nspent by the walker inside successively visited cliques. The procedure is\nillustrated with some numerical examples of diverse complexity, where the\nrelevant structural parameters are successfully recovered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the properties of discrete-time random walks on networks formed by\nrandomly interconnected cliques, namely, random networks of cliques. Our\npurpose is to derive the parameters that define the network structure --\nspecifically, the distribution of clique size and the abundance of inter-clique\nlinks -- from the observation of selected statistical features along the random\nwalk. To this end, we apply a Bayesian approach based on recording the times\nspent by the walker inside successively visited cliques. The procedure is\nillustrated with some numerical examples of diverse complexity, where the\nrelevant structural parameters are successfully recovered."
                },
                "authors": [
                    {
                        "name": "Albano Nannini"
                    },
                    {
                        "name": "Dami√°n Zanette"
                    }
                ],
                "author_detail": {
                    "name": "Dami√°n Zanette"
                },
                "author": "Dami√°n Zanette",
                "arxiv_comment": "9 pages 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15308v2",
                "updated": "2025-04-23T13:36:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    36,
                    33,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T21:39:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    21,
                    39,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "Surface to Seafloor: A Generative AI Framework for Decoding the Ocean\n  Interior State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface to Seafloor: A Generative AI Framework for Decoding the Ocean\n  Interior State"
                },
                "summary": "Understanding subsurface ocean dynamics is essential for quantifying oceanic\nheat and mass transport, but direct observations at depth remain sparse due to\nlogistical and technological constraints. In contrast, satellite missions\nprovide rich surface datasets-such as sea surface height, temperature, and\nsalinity-that offer indirect but potentially powerful constraints on the ocean\ninterior. Here, we present a probabilistic framework based on score-based\ndiffusion models to reconstruct three-dimensional subsurface velocity and\nbuoyancy fields, including the energetic ocean eddy field, from surface\nobservations. Using a 15-level primitive equation simulation of an idealized\ndouble-gyre system, we evaluate the skill of the model in inferring the mean\ncirculation and the mesoscale variability at depth under varying levels of\nsurface information. We find that the generative model successfully recovers\nkey dynamical structures and provides physically meaningful uncertainty\nestimates, with predictive skill diminishing systematically as the surface\nresolution decreases or the inference depth increases. These results\ndemonstrate the potential of generative approaches for ocean state estimation\nand uncertainty quantification, particularly in regimes where traditional\ndeterministic methods are underconstrained or ill-posed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding subsurface ocean dynamics is essential for quantifying oceanic\nheat and mass transport, but direct observations at depth remain sparse due to\nlogistical and technological constraints. In contrast, satellite missions\nprovide rich surface datasets-such as sea surface height, temperature, and\nsalinity-that offer indirect but potentially powerful constraints on the ocean\ninterior. Here, we present a probabilistic framework based on score-based\ndiffusion models to reconstruct three-dimensional subsurface velocity and\nbuoyancy fields, including the energetic ocean eddy field, from surface\nobservations. Using a 15-level primitive equation simulation of an idealized\ndouble-gyre system, we evaluate the skill of the model in inferring the mean\ncirculation and the mesoscale variability at depth under varying levels of\nsurface information. We find that the generative model successfully recovers\nkey dynamical structures and provides physically meaningful uncertainty\nestimates, with predictive skill diminishing systematically as the surface\nresolution decreases or the inference depth increases. These results\ndemonstrate the potential of generative approaches for ocean state estimation\nand uncertainty quantification, particularly in regimes where traditional\ndeterministic methods are underconstrained or ill-posed."
                },
                "authors": [
                    {
                        "name": "Andre N. Souza"
                    },
                    {
                        "name": "Simone Silvestri"
                    },
                    {
                        "name": "Katherine Deck"
                    },
                    {
                        "name": "Tobias Bischoff"
                    },
                    {
                        "name": "Raffaele Ferrari"
                    },
                    {
                        "name": "Glenn R. Flierl"
                    }
                ],
                "author_detail": {
                    "name": "Glenn R. Flierl"
                },
                "author": "Glenn R. Flierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00476v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00476v4",
                "updated": "2025-04-23T13:25:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    25,
                    10,
                    2,
                    113,
                    0
                ],
                "published": "2024-10-01T08:06:45Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    8,
                    6,
                    45,
                    1,
                    275,
                    0
                ],
                "title": "Importance sampling-based gradient method for dimension reduction in\n  Poisson log-normal model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance sampling-based gradient method for dimension reduction in\n  Poisson log-normal model"
                },
                "summary": "High-dimensional count data poses significant challenges for statistical\nanalysis, necessitating effective methods that also preserve explainability. We\nfocus on a low rank constrained variant of the Poisson log-normal model, which\nrelates the observed data to a latent low-dimensional multivariate Gaussian\nvariable via a Poisson distribution. Variational inference methods have become\na golden standard solution to infer such a model. While computationally\nefficient, they usually lack theoretical statistical properties with respect to\nthe model. To address this issue we propose a projected stochastic gradient\nscheme that directly maximizes the log-likelihood. We prove the convergence of\nthe proposed method when using importance sampling for estimating the gradient.\nSpecifically, we obtain a rate of convergence of $O(T^{-1/2} + N^{-1})$ with\n$T$ the number of iterations and $N$ the number of Monte Carlo draws. The\nlatter follows from a novel descent lemma for non convex $L$-smooth objective\nfunctions, and random biased gradient estimate. We also demonstrate numerically\nthe efficiency of our solution compared to its variational competitor. Our\nmethod not only scales with respect to the number of observed samples but also\nprovides access to the desirable properties of the maximum likelihood\nestimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional count data poses significant challenges for statistical\nanalysis, necessitating effective methods that also preserve explainability. We\nfocus on a low rank constrained variant of the Poisson log-normal model, which\nrelates the observed data to a latent low-dimensional multivariate Gaussian\nvariable via a Poisson distribution. Variational inference methods have become\na golden standard solution to infer such a model. While computationally\nefficient, they usually lack theoretical statistical properties with respect to\nthe model. To address this issue we propose a projected stochastic gradient\nscheme that directly maximizes the log-likelihood. We prove the convergence of\nthe proposed method when using importance sampling for estimating the gradient.\nSpecifically, we obtain a rate of convergence of $O(T^{-1/2} + N^{-1})$ with\n$T$ the number of iterations and $N$ the number of Monte Carlo draws. The\nlatter follows from a novel descent lemma for non convex $L$-smooth objective\nfunctions, and random biased gradient estimate. We also demonstrate numerically\nthe efficiency of our solution compared to its variational competitor. Our\nmethod not only scales with respect to the number of observed samples but also\nprovides access to the desirable properties of the maximum likelihood\nestimator."
                },
                "authors": [
                    {
                        "name": "Bastien Batardi√®re"
                    },
                    {
                        "name": "Julien Chiquet"
                    },
                    {
                        "name": "Joon Kwon"
                    },
                    {
                        "name": "Julien Stoehr"
                    }
                ],
                "author_detail": {
                    "name": "Julien Stoehr"
                },
                "arxiv_affiliation": "CEREMADE",
                "author": "Julien Stoehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00476v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00476v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16691v1",
                "updated": "2025-04-23T13:23:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    23,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T13:23:56Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    23,
                    56,
                    2,
                    113,
                    0
                ],
                "title": "Rethinking Vision Transformer for Large-Scale Fine-Grained Image\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Vision Transformer for Large-Scale Fine-Grained Image\n  Retrieval"
                },
                "summary": "Large-scale fine-grained image retrieval (FGIR) aims to retrieve images\nbelonging to the same subcategory as a given query by capturing subtle\ndifferences in a large-scale setting. Recently, Vision Transformers (ViT) have\nbeen employed in FGIR due to their powerful self-attention mechanism for\nmodeling long-range dependencies. However, most Transformer-based methods focus\nprimarily on leveraging self-attention to distinguish fine-grained details,\nwhile overlooking the high computational complexity and redundant dependencies\ninherent to these models, limiting their scalability and effectiveness in\nlarge-scale FGIR. In this paper, we propose an Efficient and Effective\nViT-based framework, termed \\textbf{EET}, which integrates token pruning module\nwith a discriminative transfer strategy to address these limitations.\nSpecifically, we introduce a content-based token pruning scheme to enhance the\nefficiency of the vanilla ViT, progressively removing background or\nlow-discriminative tokens at different stages by exploiting feature responses\nand self-attention mechanism. To ensure the resulting efficient ViT retains\nstrong discriminative power, we further present a discriminative transfer\nstrategy comprising both \\textit{discriminative knowledge transfer} and\n\\textit{discriminative region guidance}. Using a distillation paradigm, these\ncomponents transfer knowledge from a larger ``teacher'' ViT to a more efficient\n``student'' model, guiding the latter to focus on subtle yet crucial regions in\na cost-free manner. Extensive experiments on two widely-used fine-grained\ndatasets and four large-scale fine-grained datasets demonstrate the\neffectiveness of our method. Specifically, EET reduces the inference latency of\nViT-Small by 42.7\\% and boosts the retrieval performance of 16-bit hash codes\nby 5.15\\% on the challenging NABirds dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale fine-grained image retrieval (FGIR) aims to retrieve images\nbelonging to the same subcategory as a given query by capturing subtle\ndifferences in a large-scale setting. Recently, Vision Transformers (ViT) have\nbeen employed in FGIR due to their powerful self-attention mechanism for\nmodeling long-range dependencies. However, most Transformer-based methods focus\nprimarily on leveraging self-attention to distinguish fine-grained details,\nwhile overlooking the high computational complexity and redundant dependencies\ninherent to these models, limiting their scalability and effectiveness in\nlarge-scale FGIR. In this paper, we propose an Efficient and Effective\nViT-based framework, termed \\textbf{EET}, which integrates token pruning module\nwith a discriminative transfer strategy to address these limitations.\nSpecifically, we introduce a content-based token pruning scheme to enhance the\nefficiency of the vanilla ViT, progressively removing background or\nlow-discriminative tokens at different stages by exploiting feature responses\nand self-attention mechanism. To ensure the resulting efficient ViT retains\nstrong discriminative power, we further present a discriminative transfer\nstrategy comprising both \\textit{discriminative knowledge transfer} and\n\\textit{discriminative region guidance}. Using a distillation paradigm, these\ncomponents transfer knowledge from a larger ``teacher'' ViT to a more efficient\n``student'' model, guiding the latter to focus on subtle yet crucial regions in\na cost-free manner. Extensive experiments on two widely-used fine-grained\ndatasets and four large-scale fine-grained datasets demonstrate the\neffectiveness of our method. Specifically, EET reduces the inference latency of\nViT-Small by 42.7\\% and boosts the retrieval performance of 16-bit hash codes\nby 5.15\\% on the challenging NABirds dataset."
                },
                "authors": [
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Yonghua Pan"
                    },
                    {
                        "name": "Zechao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zechao Li"
                },
                "author": "Zechao Li",
                "arxiv_comment": "Accepted by IEEE TMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15231v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15231v4",
                "updated": "2025-04-24T07:21:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    21,
                    44,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-21T15:19:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    15,
                    19,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "Synthetic Lyrics Detection Across Languages and Genres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Lyrics Detection Across Languages and Genres"
                },
                "summary": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users."
                },
                "authors": [
                    {
                        "name": "Yanis Labrak"
                    },
                    {
                        "name": "Markus Frohmann"
                    },
                    {
                        "name": "Gabriel Meseguer-Brocal"
                    },
                    {
                        "name": "Elena V. Epure"
                    }
                ],
                "author_detail": {
                    "name": "Elena V. Epure"
                },
                "author": "Elena V. Epure",
                "arxiv_comment": "Published in the TrustNLP Workshop at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15231v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15231v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16683v1",
                "updated": "2025-04-23T13:10:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    10,
                    37,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T13:10:37Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    10,
                    37,
                    2,
                    113,
                    0
                ],
                "title": "MCMC for Bayesian estimation of Differential Privacy from Membership\n  Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCMC for Bayesian estimation of Differential Privacy from Membership\n  Inference Attacks"
                },
                "summary": "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data."
                },
                "authors": [
                    {
                        "name": "Ceren Yildirim"
                    },
                    {
                        "name": "Kamer Kaya"
                    },
                    {
                        "name": "Sinan Yildirim"
                    },
                    {
                        "name": "Erkay Savas"
                    }
                ],
                "author_detail": {
                    "name": "Erkay Savas"
                },
                "author": "Erkay Savas",
                "arxiv_comment": "Code available:\n  https://github.com/cerenyildirim/MCMC_for_Bayesian_estimation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12766v3",
                "updated": "2025-04-23T12:52:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    52,
                    18,
                    2,
                    113,
                    0
                ],
                "published": "2024-03-18T17:32:32Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    17,
                    32,
                    32,
                    0,
                    78,
                    0
                ],
                "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K\n  Tokens"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have pushed the\nboundaries of natural language processing, especially in long-context\nunderstanding. However, the evaluation of these models' long-context abilities\nremains a challenge due to the limitations of current benchmarks. To address\nthis gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with\ncomplex, extended narratives. Constructed from English novels, NovelQA offers a\nunique blend of complexity, length, and narrative coherence, making it an ideal\ntool for assessing deep textual understanding in LLMs. This paper details the\ndesign and construction of NovelQA, focusing on its comprehensive manual\nannotation process and the variety of question types aimed at evaluating\nnuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals\nsignificant insights into their strengths and weaknesses. Notably, the models\nstruggle with multi-hop reasoning, detail-oriented questions, and handling\nextremely long inputs, with average lengths exceeding 200,000 tokens. Results\nhighlight the need for substantial advancements in LLMs to enhance their\nlong-context comprehension and contribute effectively to computational literary\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have pushed the\nboundaries of natural language processing, especially in long-context\nunderstanding. However, the evaluation of these models' long-context abilities\nremains a challenge due to the limitations of current benchmarks. To address\nthis gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with\ncomplex, extended narratives. Constructed from English novels, NovelQA offers a\nunique blend of complexity, length, and narrative coherence, making it an ideal\ntool for assessing deep textual understanding in LLMs. This paper details the\ndesign and construction of NovelQA, focusing on its comprehensive manual\nannotation process and the variety of question types aimed at evaluating\nnuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals\nsignificant insights into their strengths and weaknesses. Notably, the models\nstruggle with multi-hop reasoning, detail-oriented questions, and handling\nextremely long inputs, with average lengths exceeding 200,000 tokens. Results\nhighlight the need for substantial advancements in LLMs to enhance their\nlong-context comprehension and contribute effectively to computational literary\nanalysis."
                },
                "authors": [
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Ruoxi Ning"
                    },
                    {
                        "name": "Boqi Pan"
                    },
                    {
                        "name": "Tonghui Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "Accepted by ICLR-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16671v1",
                "updated": "2025-04-23T12:39:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    39,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T12:39:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    39,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative\n  Analysis"
                },
                "summary": "The use of large language models (LLMs) in qualitative analysis offers\nenhanced efficiency but raises questions about their alignment with the\ncontextual nature of research for design (RfD). This research examines the\ntrustworthiness of LLM-driven design insights, using qualitative coding as a\ncase study to explore the interpretive processes central to RfD. We introduce\nLLMCode, an open-source tool integrating two metrics, namely Intersection over\nUnion (IoU) and Modified Hausdorff Distance, to assess the alignment between\nhuman and LLM-generated insights. Across two studies involving 26 designers, we\nfind that while the model performs well with deductive coding, its ability to\nemulate a designer's deeper interpretive lens over the data is limited,\nemphasising the importance of human-AI collaboration. Our results highlight a\nreciprocal dynamic where users refine LLM outputs and adapt their own\nperspectives based on the model's suggestions. These findings underscore the\nimportance of fostering appropriate reliance on LLMs by designing tools that\npreserve interpretive depth while facilitating intuitive collaboration between\ndesigners and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) in qualitative analysis offers\nenhanced efficiency but raises questions about their alignment with the\ncontextual nature of research for design (RfD). This research examines the\ntrustworthiness of LLM-driven design insights, using qualitative coding as a\ncase study to explore the interpretive processes central to RfD. We introduce\nLLMCode, an open-source tool integrating two metrics, namely Intersection over\nUnion (IoU) and Modified Hausdorff Distance, to assess the alignment between\nhuman and LLM-generated insights. Across two studies involving 26 designers, we\nfind that while the model performs well with deductive coding, its ability to\nemulate a designer's deeper interpretive lens over the data is limited,\nemphasising the importance of human-AI collaboration. Our results highlight a\nreciprocal dynamic where users refine LLM outputs and adapt their own\nperspectives based on the model's suggestions. These findings underscore the\nimportance of fostering appropriate reliance on LLMs by designing tools that\npreserve interpretive depth while facilitating intuitive collaboration between\ndesigners and AI."
                },
                "authors": [
                    {
                        "name": "Joel Oksanen"
                    },
                    {
                        "name": "Andr√©s Lucero"
                    },
                    {
                        "name": "Perttu H√§m√§l√§inen"
                    }
                ],
                "author_detail": {
                    "name": "Perttu H√§m√§l√§inen"
                },
                "author": "Perttu H√§m√§l√§inen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16669v1",
                "updated": "2025-04-23T12:36:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    36,
                    32,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T12:36:32Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    36,
                    32,
                    2,
                    113,
                    0
                ],
                "title": "Binarity at LOw Metallicity (BLOeM): Bayesian inference of natal kicks\n  from inert black hole binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarity at LOw Metallicity (BLOeM): Bayesian inference of natal kicks\n  from inert black hole binaries"
                },
                "summary": "Context. The emerging population of inert black hole binaries (BHBs) provides\na unique opportunity to constrain black hole (BH) formation physics. These\nsystems are composed of a stellar-mass BH in a wide orbit around a\nnon-degenerate star with no observed Xray emission. Inert BHBs allow for narrow\nconstraints to be inferred on the natal kick and mass loss during BH-forming\ncore-collapse events. Aims. In anticipation of the upcoming BLOeM survey, we\naim to provide tight constraints on BH natal kicks by exploiting the full\nparameter space obtained from combined spectroscopic and astrometric data to\ncharacterize the orbits of inert BHBs. Multi-epoch spectroscopy from the BLOeM\nproject will provide measurements of periods, eccentricities, and radial\nvelocities for inert BHBs in the SMC, which complements Gaia astrometric\nobservations of proper motions. Methods. We present a Bayesian parameter\nestimation framework to infer natal kicks and mass loss during core-collapse\nfrom inert BHBs, accounting for all available observables, including the\nsystemic velocity and its orientation relative to the orbital plane. The\nframework further allows for circumstances when some of the observables are\nunavailable, such as for the distant BLOeM sources which preclude resolved\norbits. Results. With our new framework, we are able to distinguish between BH\nformation channels, even in the absence of a resolved orbit. In cases when the\npre-explosion orbit can be assumed to be circular, we precisely recover the\nparameters of the core-collapse, highlighting the importance of understanding\nthe eccentricity landscape of pre-explosion binaries, both theoretically and\nobservationally. Treating the near-circular, inert BHB, VFTS 243, as a\nrepresentative of the anticipated BLOeM systems, we constrain the natal kick to\nless than 27 km/s and the mass loss to less than 2.9 Msun within a 90% credible\ninterval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The emerging population of inert black hole binaries (BHBs) provides\na unique opportunity to constrain black hole (BH) formation physics. These\nsystems are composed of a stellar-mass BH in a wide orbit around a\nnon-degenerate star with no observed Xray emission. Inert BHBs allow for narrow\nconstraints to be inferred on the natal kick and mass loss during BH-forming\ncore-collapse events. Aims. In anticipation of the upcoming BLOeM survey, we\naim to provide tight constraints on BH natal kicks by exploiting the full\nparameter space obtained from combined spectroscopic and astrometric data to\ncharacterize the orbits of inert BHBs. Multi-epoch spectroscopy from the BLOeM\nproject will provide measurements of periods, eccentricities, and radial\nvelocities for inert BHBs in the SMC, which complements Gaia astrometric\nobservations of proper motions. Methods. We present a Bayesian parameter\nestimation framework to infer natal kicks and mass loss during core-collapse\nfrom inert BHBs, accounting for all available observables, including the\nsystemic velocity and its orientation relative to the orbital plane. The\nframework further allows for circumstances when some of the observables are\nunavailable, such as for the distant BLOeM sources which preclude resolved\norbits. Results. With our new framework, we are able to distinguish between BH\nformation channels, even in the absence of a resolved orbit. In cases when the\npre-explosion orbit can be assumed to be circular, we precisely recover the\nparameters of the core-collapse, highlighting the importance of understanding\nthe eccentricity landscape of pre-explosion binaries, both theoretically and\nobservationally. Treating the near-circular, inert BHB, VFTS 243, as a\nrepresentative of the anticipated BLOeM systems, we constrain the natal kick to\nless than 27 km/s and the mass loss to less than 2.9 Msun within a 90% credible\ninterval."
                },
                "authors": [
                    {
                        "name": "R. Willcox"
                    },
                    {
                        "name": "P. Marchant"
                    },
                    {
                        "name": "A. Vigna-G√≥mez"
                    },
                    {
                        "name": "H. Sana"
                    },
                    {
                        "name": "J. Bodensteiner"
                    },
                    {
                        "name": "K. Deshmukh"
                    },
                    {
                        "name": "M. Esseldeurs"
                    },
                    {
                        "name": "M. Fabry"
                    },
                    {
                        "name": "V. H√©nault-Brunet"
                    },
                    {
                        "name": "S. Janssens"
                    },
                    {
                        "name": "L. Mahy"
                    },
                    {
                        "name": "L. Patrick"
                    },
                    {
                        "name": "D. Pauli"
                    },
                    {
                        "name": "M. Renzo"
                    },
                    {
                        "name": "A. A. C. Sander"
                    },
                    {
                        "name": "T. Shenar"
                    },
                    {
                        "name": "L. A. C. van Son"
                    },
                    {
                        "name": "M. Stoop"
                    }
                ],
                "author_detail": {
                    "name": "M. Stoop"
                },
                "arxiv_affiliation": "the BLOeM Collaboration",
                "author": "M. Stoop",
                "arxiv_comment": "Article is 15 pages + 8 of appendix, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14373v2",
                "updated": "2025-04-23T12:27:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    27,
                    3,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-19T18:23:31Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    23,
                    31,
                    5,
                    109,
                    0
                ],
                "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image"
                },
                "summary": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications."
                },
                "authors": [
                    {
                        "name": "Chen Guo"
                    },
                    {
                        "name": "Zhuo Su"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Shuang Li"
                    },
                    {
                        "name": "Xu Chang"
                    },
                    {
                        "name": "Zhaohu Li"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Guidong Wang"
                    },
                    {
                        "name": "Ruqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Huang"
                },
                "author": "Ruqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10199v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10199v6",
                "updated": "2025-04-23T12:15:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    15,
                    48,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-13T09:34:33Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    34,
                    33,
                    3,
                    72,
                    0
                ],
                "title": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation and Uncertainty Quantification for Stochastic Inverse\n  Problems via Variational Bayesian Methods"
                },
                "summary": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples."
                },
                "authors": [
                    {
                        "name": "Ruibiao Song"
                    },
                    {
                        "name": "Liying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liying Zhang"
                },
                "author": "Liying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10199v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10199v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14898v2",
                "updated": "2025-04-23T12:05:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    5,
                    19,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T07:09:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    9,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "Expected Free Energy-based Planning as Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Free Energy-based Planning as Variational Inference"
                },
                "summary": "We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, provides such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives, such as ambiguity resolution and novelty seeking.\nHowever, the computational burden of EFE minimization had remained a\nsignificant obstacle to its scalability. In this paper, we show that EFE-based\nplanning arises naturally from minimizing a variational free energy functional\non a generative model augmented with preference and epistemic priors. This\nresult reinforces theoretical consistency with the Free Energy Principle by\ncasting planning under uncertainty itself as a form of variational inference.\nOur formulation yields policies that jointly support goal achievement and\ninformation gain, while incorporating a complexity term that accounts for\nbounded computational resources. This unifying framework connects and extends\nexisting methods, enabling scalable, resource-aware implementations of active\ninference agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, provides such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives, such as ambiguity resolution and novelty seeking.\nHowever, the computational burden of EFE minimization had remained a\nsignificant obstacle to its scalability. In this paper, we show that EFE-based\nplanning arises naturally from minimizing a variational free energy functional\non a generative model augmented with preference and epistemic priors. This\nresult reinforces theoretical consistency with the Free Energy Principle by\ncasting planning under uncertainty itself as a form of variational inference.\nOur formulation yields policies that jointly support goal achievement and\ninformation gain, while incorporating a complexity term that accounts for\nbounded computational resources. This unifying framework connects and extends\nexisting methods, enabling scalable, resource-aware implementations of active\ninference agents."
                },
                "authors": [
                    {
                        "name": "Bert de Vries"
                    },
                    {
                        "name": "Wouter Nuijten"
                    },
                    {
                        "name": "Thijs van de Laar"
                    },
                    {
                        "name": "Wouter Kouw"
                    },
                    {
                        "name": "Sepideh Adamiat"
                    },
                    {
                        "name": "Tim Nisslbeck"
                    },
                    {
                        "name": "Mykola Lukashchuk"
                    },
                    {
                        "name": "Hoang Minh Huu Nguyen"
                    },
                    {
                        "name": "Marco Hidalgo Araya"
                    },
                    {
                        "name": "Raphael Tresor"
                    },
                    {
                        "name": "Thijs Jenneskens"
                    },
                    {
                        "name": "Ivana Nikoloska"
                    },
                    {
                        "name": "Raaja Ganapathy Subramanian"
                    },
                    {
                        "name": "Bart van Erp"
                    },
                    {
                        "name": "Dmitry Bagaev"
                    },
                    {
                        "name": "Albert Podusenko"
                    }
                ],
                "author_detail": {
                    "name": "Albert Podusenko"
                },
                "author": "Albert Podusenko",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16628v1",
                "updated": "2025-04-23T11:35:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    35,
                    57,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:35:57Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    35,
                    57,
                    2,
                    113,
                    0
                ],
                "title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\n  Models using Pareto High-quality Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\n  Models using Pareto High-quality Data"
                },
                "summary": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks."
                },
                "authors": [
                    {
                        "name": "Haoran Gu"
                    },
                    {
                        "name": "Handing Wang"
                    },
                    {
                        "name": "Yi Mei"
                    },
                    {
                        "name": "Mengjie Zhang"
                    },
                    {
                        "name": "Yaochu Jin"
                    }
                ],
                "author_detail": {
                    "name": "Yaochu Jin"
                },
                "author": "Yaochu Jin",
                "arxiv_comment": "19 pages, 6 figure, Multiobjective Alignment of LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16627v1",
                "updated": "2025-04-23T11:34:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    34,
                    35,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:34:35Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    34,
                    35,
                    2,
                    113,
                    0
                ],
                "title": "TIFIN India at SemEval-2025: Harnessing Translation to Overcome\n  Multilingual IR Challenges in Fact-Checked Claim Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIFIN India at SemEval-2025: Harnessing Translation to Overcome\n  Multilingual IR Challenges in Fact-Checked Claim Retrieval"
                },
                "summary": "We address the challenge of retrieving previously fact-checked claims in\nmonolingual and crosslingual settings - a critical task given the global\nprevalence of disinformation. Our approach follows a two-stage strategy: a\nreliable baseline retrieval system using a fine-tuned embedding model and an\nLLM-based reranker. Our key contribution is demonstrating how LLM-based\ntranslation can overcome the hurdles of multilingual information retrieval.\nAdditionally, we focus on ensuring that the bulk of the pipeline can be\nreplicated on a consumer GPU. Our final integrated system achieved a success@10\nscore of 0.938 and 0.81025 on the monolingual and crosslingual test sets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of retrieving previously fact-checked claims in\nmonolingual and crosslingual settings - a critical task given the global\nprevalence of disinformation. Our approach follows a two-stage strategy: a\nreliable baseline retrieval system using a fine-tuned embedding model and an\nLLM-based reranker. Our key contribution is demonstrating how LLM-based\ntranslation can overcome the hurdles of multilingual information retrieval.\nAdditionally, we focus on ensuring that the bulk of the pipeline can be\nreplicated on a consumer GPU. Our final integrated system achieved a success@10\nscore of 0.938 and 0.81025 on the monolingual and crosslingual test sets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Prasanna Devadiga"
                    },
                    {
                        "name": "Arya Suneesh"
                    },
                    {
                        "name": "Pawan Kumar Rajpoot"
                    },
                    {
                        "name": "Bharatdeep Hazarika"
                    },
                    {
                        "name": "Aditya U Baliga"
                    }
                ],
                "author_detail": {
                    "name": "Aditya U Baliga"
                },
                "author": "Aditya U Baliga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16624v1",
                "updated": "2025-04-23T11:30:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    30,
                    1,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:30:01Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    30,
                    1,
                    2,
                    113,
                    0
                ],
                "title": "Compositional Active Learning of Synchronous Systems through Automated\n  Alphabet Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Active Learning of Synchronous Systems through Automated\n  Alphabet Refinement"
                },
                "summary": "Active automata learning infers automaton models of systems from behavioral\nobservations, a technique successfully applied to a wide range of domains.\nCompositional approaches for concurrent systems have recently emerged. We take\na significant step beyond available results, including those by the authors,\nand develop a general technique for compositional learning of a synchronizing\nparallel system with an unknown decomposition. Our approach automatically\nrefines the global alphabet into component alphabets while learning the\ncomponent models. We develop a theoretical treatment of distributions of\nalphabets, i.e., sets of possibly overlapping component alphabets. We\ncharacterize counter-examples that reveal inconsistencies with global\nobservations, and show how to systematically update the distribution to restore\nconsistency. We present a compositional learning algorithm implementing these\nideas, where learning counterexamples precisely correspond to distribution\ncounterexamples under well-defined conditions. We provide an implementation,\ncalled CoalA, using the state-of-the-art active learning library LearnLib. Our\nexperiments show that in more than 630 subject systems, CoalA delivers orders\nof magnitude improvements (up to five orders) in membership queries and in\nsystems with significant concurrency, it also achieves better scalability in\nthe number of equivalence queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active automata learning infers automaton models of systems from behavioral\nobservations, a technique successfully applied to a wide range of domains.\nCompositional approaches for concurrent systems have recently emerged. We take\na significant step beyond available results, including those by the authors,\nand develop a general technique for compositional learning of a synchronizing\nparallel system with an unknown decomposition. Our approach automatically\nrefines the global alphabet into component alphabets while learning the\ncomponent models. We develop a theoretical treatment of distributions of\nalphabets, i.e., sets of possibly overlapping component alphabets. We\ncharacterize counter-examples that reveal inconsistencies with global\nobservations, and show how to systematically update the distribution to restore\nconsistency. We present a compositional learning algorithm implementing these\nideas, where learning counterexamples precisely correspond to distribution\ncounterexamples under well-defined conditions. We provide an implementation,\ncalled CoalA, using the state-of-the-art active learning library LearnLib. Our\nexperiments show that in more than 630 subject systems, CoalA delivers orders\nof magnitude improvements (up to five orders) in membership queries and in\nsystems with significant concurrency, it also achieves better scalability in\nthe number of equivalence queries."
                },
                "authors": [
                    {
                        "name": "Leo Henry"
                    },
                    {
                        "name": "Thomas Neele"
                    },
                    {
                        "name": "Mohammad Mousavi"
                    },
                    {
                        "name": "Matteo Sammartino"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Sammartino"
                },
                "author": "Matteo Sammartino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16623v1",
                "updated": "2025-04-23T11:29:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    29,
                    31,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:29:31Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    29,
                    31,
                    2,
                    113,
                    0
                ],
                "title": "Censored lifespans in a double-truncated sample: Maximum likelihood\n  inference for the exponential distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Censored lifespans in a double-truncated sample: Maximum likelihood\n  inference for the exponential distribution"
                },
                "summary": "The analysis of a truncated sample can be hindered by censoring. Survival\ninformation may be lost to follow-up or the birthdate may be missing. The data\ncan still be modeled as a truncated point process and it is close to a Poisson\nprocess, in the Hellinger distance, as long as the sample is small relative to\nthe population. We assume an exponential distribution for the lifespan, derive\nthe likelihood and profile out the unobservable sample size. Identification of\nthe exponential parameter is shown, together with consistency and asymptotic\nnormality of its M-estimator. Even though the estimator sequence is indexed in\nthe sample size, both the point estimator and the standard error are\nobservable. Enterprise lifespans in Germany constitute our example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of a truncated sample can be hindered by censoring. Survival\ninformation may be lost to follow-up or the birthdate may be missing. The data\ncan still be modeled as a truncated point process and it is close to a Poisson\nprocess, in the Hellinger distance, as long as the sample is small relative to\nthe population. We assume an exponential distribution for the lifespan, derive\nthe likelihood and profile out the unobservable sample size. Identification of\nthe exponential parameter is shown, together with consistency and asymptotic\nnormality of its M-estimator. Even though the estimator sequence is indexed in\nthe sample size, both the point estimator and the standard error are\nobservable. Enterprise lifespans in Germany constitute our example."
                },
                "authors": [
                    {
                        "name": "Fiete Sieg"
                    },
                    {
                        "name": "Anne-Marie Toparkus"
                    },
                    {
                        "name": "Rafael Weissbach"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Weissbach"
                },
                "author": "Rafael Weissbach",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62N01, 62D10, 60G55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00513v2",
                "updated": "2025-04-23T11:26:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    26,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-01T08:03:40Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    3,
                    40,
                    1,
                    91,
                    0
                ],
                "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset"
                },
                "summary": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "arxiv_doi": "10.1145/3727582.3728689",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3727582.3728689",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.00513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16622v1",
                "updated": "2025-04-23T11:24:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    24,
                    30,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:24:30Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    24,
                    30,
                    2,
                    113,
                    0
                ],
                "title": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial\n  Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial\n  Computing Systems"
                },
                "summary": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion."
                },
                "authors": [
                    {
                        "name": "Christoforus Yoga Haryanto"
                    },
                    {
                        "name": "Emily Lomempow"
                    }
                ],
                "author_detail": {
                    "name": "Emily Lomempow"
                },
                "author": "Emily Lomempow",
                "arxiv_comment": "Working Paper, 37 pages, 1 figure, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16615v1",
                "updated": "2025-04-23T11:00:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    0,
                    17,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:00:17Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    0,
                    17,
                    2,
                    113,
                    0
                ],
                "title": "Algorithmic Mirror: Designing an Interactive Tool to Promote\n  Self-Reflection for YouTube Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Mirror: Designing an Interactive Tool to Promote\n  Self-Reflection for YouTube Recommendations"
                },
                "summary": "Big Data analytics and Artificial Intelligence systems derive non-intuitive\nand often unverifiable inferences about individuals' behaviors, preferences,\nand private lives. Drawing on diverse, feature-rich datasets of unpredictable\nvalue, these systems erode the intuitive connection between our actions and how\nwe are perceived, diminishing control over our digital identities. While\nExplainable Artificial Intelligence scholars have attempted to explain the\ninner workings of algorithms, their visualizations frequently overwhelm\nend-users with complexity. This research introduces 'hypothetical inference', a\nnovel approach that uses language models to simulate how algorithms might\ninterpret users' digital footprints and infer personal characteristics without\nrequiring access to proprietary platform algorithms. Through empirical studies\nwith fourteen adult participants, we identified three key design opportunities\nto foster critical algorithmic literacy: (1) reassembling scattered digital\nfootprints into a unified map, (2) simulating algorithmic inference through\nLLM-generated interpretations, and (3) incorporating temporal dimensions to\nvisualize evolving patterns. This research lays the groundwork for tools that\ncan help users recognize the influence of data on platforms and develop greater\nautonomy in increasingly algorithm-mediated digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Big Data analytics and Artificial Intelligence systems derive non-intuitive\nand often unverifiable inferences about individuals' behaviors, preferences,\nand private lives. Drawing on diverse, feature-rich datasets of unpredictable\nvalue, these systems erode the intuitive connection between our actions and how\nwe are perceived, diminishing control over our digital identities. While\nExplainable Artificial Intelligence scholars have attempted to explain the\ninner workings of algorithms, their visualizations frequently overwhelm\nend-users with complexity. This research introduces 'hypothetical inference', a\nnovel approach that uses language models to simulate how algorithms might\ninterpret users' digital footprints and infer personal characteristics without\nrequiring access to proprietary platform algorithms. Through empirical studies\nwith fourteen adult participants, we identified three key design opportunities\nto foster critical algorithmic literacy: (1) reassembling scattered digital\nfootprints into a unified map, (2) simulating algorithmic inference through\nLLM-generated interpretations, and (3) incorporating temporal dimensions to\nvisualize evolving patterns. This research lays the groundwork for tools that\ncan help users recognize the influence of data on platforms and develop greater\nautonomy in increasingly algorithm-mediated digital environments."
                },
                "authors": [
                    {
                        "name": "Yui Kondo"
                    },
                    {
                        "name": "Kevin Dunnell"
                    },
                    {
                        "name": "Qing Xiao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Luc Rocher"
                    }
                ],
                "author_detail": {
                    "name": "Luc Rocher"
                },
                "author": "Luc Rocher",
                "arxiv_comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction\n  for Augmented Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11094v2",
                "updated": "2025-04-23T11:00:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    0,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2023-11-18T14:49:04Z",
                "published_parsed": [
                    2023,
                    11,
                    18,
                    14,
                    49,
                    4,
                    5,
                    322,
                    0
                ],
                "title": "Reinforcement Learning With LLMs Interaction For Distributed Diffusion\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning With LLMs Interaction For Distributed Diffusion\n  Model Services"
                },
                "summary": "Distributed Artificial Intelligence-Generated Content (AIGC) has attracted\nsignificant attention, but two key challenges remain: maximizing subjective\nQuality of Experience (QoE) and improving energy efficiency, which are\nparticularly pronounced in widely adopted Generative Diffusion Model\n(GDM)-based image generation services. In this paper, we propose a novel\nuser-centric Interactive AI (IAI) approach for service management, with a\ndistributed GDM-based AIGC framework that emphasizes efficient and cooperative\ndeployment. The proposed method restructures the GDM inference process by\nallowing users with semantically similar prompts to share parts of the\ndenoising chain. Furthermore, to maximize the users' subjective QoE, we propose\nan IAI approach, i.e., Reinforcement Learning With Large Language Models\nInteraction (RLLI), which utilizes Large Language Model (LLM)-empowered\ngenerative agents to replicate user interaction, providing real-time and\nsubjective QoE feedback aligned with diverse user personalities. Lastly, we\npresent the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm,\nadapted to the proposed RLLI framework, to allocate communication and computing\nresources effectively while accounting for subjective user traits and dynamic\nwireless conditions. Simulation results demonstrate that G-DDPG improves total\nQoE by 15% compared with the standard DDPG algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Artificial Intelligence-Generated Content (AIGC) has attracted\nsignificant attention, but two key challenges remain: maximizing subjective\nQuality of Experience (QoE) and improving energy efficiency, which are\nparticularly pronounced in widely adopted Generative Diffusion Model\n(GDM)-based image generation services. In this paper, we propose a novel\nuser-centric Interactive AI (IAI) approach for service management, with a\ndistributed GDM-based AIGC framework that emphasizes efficient and cooperative\ndeployment. The proposed method restructures the GDM inference process by\nallowing users with semantically similar prompts to share parts of the\ndenoising chain. Furthermore, to maximize the users' subjective QoE, we propose\nan IAI approach, i.e., Reinforcement Learning With Large Language Models\nInteraction (RLLI), which utilizes Large Language Model (LLM)-empowered\ngenerative agents to replicate user interaction, providing real-time and\nsubjective QoE feedback aligned with diverse user personalities. Lastly, we\npresent the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm,\nadapted to the proposed RLLI framework, to allocate communication and computing\nresources effectively while accounting for subjective user traits and dynamic\nwireless conditions. Simulation results demonstrate that G-DDPG improves total\nQoE by 15% compared with the standard DDPG algorithm."
                },
                "authors": [
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Xuemin Shen"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08637v3",
                "updated": "2025-04-23T10:44:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    44,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2023-12-11T23:30:01Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    30,
                    1,
                    0,
                    345,
                    0
                ],
                "title": "Synergy: Towards On-Body AI via Tiny AI Accelerator Collaboration on\n  Wearables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergy: Towards On-Body AI via Tiny AI Accelerator Collaboration on\n  Wearables"
                },
                "summary": "The advent of tiny artificial intelligence (AI) accelerators enables AI to\nrun at the extreme edge, offering reduced latency, lower power cost, and\nimproved privacy. When integrated into wearable devices, these accelerators\nopen exciting opportunities, allowing various AI apps to run directly on the\nbody. We present Synergy that provides AI apps with best-effort performance via\nsystem-driven holistic collaboration over AI accelerator-equipped wearables. To\nachieve this, Synergy provides device-agnostic programming interfaces to AI\napps, giving the system visibility and controllability over the app's resource\nuse. Then, Synergy maximizes the inference throughput of concurrent AI models\nby creating various execution plans for each app considering AI accelerator\navailability and intelligently selecting the best set of execution plans.\nSynergy further improves throughput by leveraging parallelization opportunities\nover multiple computation units. Our evaluations with 7 baselines and 8 models\ndemonstrate that, on average, Synergy achieves a 23.0 times improvement in\nthroughput, while reducing latency by 73.9% and power consumption by 15.8%,\ncompared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of tiny artificial intelligence (AI) accelerators enables AI to\nrun at the extreme edge, offering reduced latency, lower power cost, and\nimproved privacy. When integrated into wearable devices, these accelerators\nopen exciting opportunities, allowing various AI apps to run directly on the\nbody. We present Synergy that provides AI apps with best-effort performance via\nsystem-driven holistic collaboration over AI accelerator-equipped wearables. To\nachieve this, Synergy provides device-agnostic programming interfaces to AI\napps, giving the system visibility and controllability over the app's resource\nuse. Then, Synergy maximizes the inference throughput of concurrent AI models\nby creating various execution plans for each app considering AI accelerator\navailability and intelligently selecting the best set of execution plans.\nSynergy further improves throughput by leveraging parallelization opportunities\nover multiple computation units. Our evaluations with 7 baselines and 8 models\ndemonstrate that, on average, Synergy achieves a 23.0 times improvement in\nthroughput, while reducing latency by 73.9% and power consumption by 15.8%,\ncompared to the baselines."
                },
                "authors": [
                    {
                        "name": "Taesik Gong"
                    },
                    {
                        "name": "Si Young Jang"
                    },
                    {
                        "name": "Utku G√ºnay Acer"
                    },
                    {
                        "name": "Fahim Kawsar"
                    },
                    {
                        "name": "Chulhong Min"
                    }
                ],
                "author_detail": {
                    "name": "Chulhong Min"
                },
                "author": "Chulhong Min",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Mobile Computing\n  (TMC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18322v2",
                "updated": "2025-04-23T10:33:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    33,
                    46,
                    2,
                    113,
                    0
                ],
                "published": "2023-11-30T07:47:38Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    7,
                    47,
                    38,
                    3,
                    334,
                    0
                ],
                "title": "Bayesian Nonparametric Inference in Elliptic PDEs: Convergence Rates and\n  Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonparametric Inference in Elliptic PDEs: Convergence Rates and\n  Implementation"
                },
                "summary": "Parameter identification problems in partial differential equations (PDEs)\nconsist in determining one or more functional coefficient in a PDE. In this\narticle, the Bayesian nonparametric approach to such problems is considered.\nFocusing on the representative example of inferring the diffusivity function in\nan elliptic PDE from noisy observations of the PDE solution, the performance of\nBayesian procedures based on Gaussian process priors is investigated. Building\non recent developments in the literature, we derive novel asymptotic\ntheoretical guarantees that establish posterior consistency and convergence\nrates for methodologically attractive Gaussian series priors based on the\nDirichlet-Laplacian eigenbasis. An implementation of the associated\nposterior-based inference is provided and illustrated via a numerical\nsimulation study, where excellent agreement with the theory is obtained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter identification problems in partial differential equations (PDEs)\nconsist in determining one or more functional coefficient in a PDE. In this\narticle, the Bayesian nonparametric approach to such problems is considered.\nFocusing on the representative example of inferring the diffusivity function in\nan elliptic PDE from noisy observations of the PDE solution, the performance of\nBayesian procedures based on Gaussian process priors is investigated. Building\non recent developments in the literature, we derive novel asymptotic\ntheoretical guarantees that establish posterior consistency and convergence\nrates for methodologically attractive Gaussian series priors based on the\nDirichlet-Laplacian eigenbasis. An implementation of the associated\nposterior-based inference is provided and illustrated via a numerical\nsimulation study, where excellent agreement with the theory is obtained."
                },
                "authors": [
                    {
                        "name": "Matteo Giordano"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Giordano"
                },
                "author": "Matteo Giordano",
                "arxiv_comment": "20 pages, 6 figures, 2 tables. To appear in Foundations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16604v1",
                "updated": "2025-04-23T10:32:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    32,
                    45,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:32:45Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    32,
                    45,
                    2,
                    113,
                    0
                ],
                "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to\n  Challenge Conspiracy Theories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to\n  Challenge Conspiracy Theories"
                },
                "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic."
                },
                "authors": [
                    {
                        "name": "Mareike Lisker"
                    },
                    {
                        "name": "Christina Gottschalk"
                    },
                    {
                        "name": "Helena Mihaljeviƒá"
                    }
                ],
                "author_detail": {
                    "name": "Helena Mihaljeviƒá"
                },
                "author": "Helena Mihaljeviƒá",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16601v1",
                "updated": "2025-04-23T10:31:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    31,
                    33,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:31:33Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    31,
                    33,
                    2,
                    113,
                    0
                ],
                "title": "Comparing Large Language Models and Traditional Machine Translation\n  Tools for Translating Medical Consultation Summaries: A Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Large Language Models and Traditional Machine Translation\n  Tools for Translating Medical Consultation Summaries: A Pilot Study"
                },
                "summary": "This study evaluates how well large language models (LLMs) and traditional\nmachine translation (MT) tools translate medical consultation summaries from\nEnglish into Arabic, Chinese, and Vietnamese. It assesses both patient,\nfriendly and clinician, focused texts using standard automated metrics. Results\nshowed that traditional MT tools generally performed better, especially for\ncomplex texts, while LLMs showed promise, particularly in Vietnamese and\nChinese, when translating simpler summaries. Arabic translations improved with\ncomplexity due to the language's morphology. Overall, while LLMs offer\ncontextual flexibility, they remain inconsistent, and current evaluation\nmetrics fail to capture clinical relevance. The study highlights the need for\ndomain-specific training, improved evaluation methods, and human oversight in\nmedical translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates how well large language models (LLMs) and traditional\nmachine translation (MT) tools translate medical consultation summaries from\nEnglish into Arabic, Chinese, and Vietnamese. It assesses both patient,\nfriendly and clinician, focused texts using standard automated metrics. Results\nshowed that traditional MT tools generally performed better, especially for\ncomplex texts, while LLMs showed promise, particularly in Vietnamese and\nChinese, when translating simpler summaries. Arabic translations improved with\ncomplexity due to the language's morphology. Overall, while LLMs offer\ncontextual flexibility, they remain inconsistent, and current evaluation\nmetrics fail to capture clinical relevance. The study highlights the need for\ndomain-specific training, improved evaluation methods, and human oversight in\nmedical translation."
                },
                "authors": [
                    {
                        "name": "Andy Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Rashina Hoda"
                    },
                    {
                        "name": "Chris Bain"
                    },
                    {
                        "name": "Peter Poon"
                    }
                ],
                "author_detail": {
                    "name": "Peter Poon"
                },
                "author": "Peter Poon",
                "arxiv_comment": "8 pages, 2 tables and 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00530v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00530v5",
                "updated": "2025-04-23T10:26:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    26,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2023-11-01T14:08:56Z",
                "published_parsed": [
                    2023,
                    11,
                    1,
                    14,
                    8,
                    56,
                    2,
                    305,
                    0
                ],
                "title": "Advances in Embodied Navigation Using Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Embodied Navigation Using Large Language Models: A Survey"
                },
                "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN."
                },
                "authors": [
                    {
                        "name": "Jinzhou Lin"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Xuxiang Feng"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Man Zhang"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "arxiv_comment": "Submited to IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS:\n  SYSTEMS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.00530v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00530v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13460v2",
                "updated": "2025-04-23T10:12:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    12,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T04:35:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization"
                },
                "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark."
                },
                "authors": [
                    {
                        "name": "Hongwei Ji"
                    },
                    {
                        "name": "Wulian Yun"
                    },
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04067v2",
                "updated": "2025-04-23T10:10:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    10,
                    57,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-06T03:52:46Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    3,
                    52,
                    46,
                    3,
                    65,
                    0
                ],
                "title": "FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven\n  Talking Portrait Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven\n  Talking Portrait Synthesis"
                },
                "summary": "Achieving high-fidelity lip-speech synchronization in audio-driven talking\nportrait synthesis remains challenging. While multi-stage pipelines or\ndiffusion models yield high-quality results, they suffer from high\ncomputational costs. Some approaches perform well on specific individuals with\nlow resources, yet still exhibit mismatched lip movements. The aforementioned\nmethods are modeled in the pixel domain. We observed that there are noticeable\ndiscrepancies in the frequency domain between the synthesized talking videos\nand natural videos. Currently, no research on talking portrait synthesis has\nconsidered this aspect. To address this, we propose a FREquency-modulated,\nhigh-fidelity, and real-time Audio-driven talKing portrait synthesis framework,\nnamed FREAK, which models talking portraits from the frequency domain\nperspective, enhancing the fidelity and naturalness of the synthesized\nportraits. FREAK introduces two novel frequency-based modules: 1) the Visual\nEncoding Frequency Modulator (VEFM) to couple multi-scale visual features in\nthe frequency domain, better preserving visual frequency information and\nreducing the gap in the frequency spectrum between synthesized and natural\nframes. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model\nlearn the talking pattern in the frequency domain and improve audio-visual\nsynchronization. Additionally, we optimize the model in both pixel domain and\nfrequency domain jointly. Furthermore, FREAK supports seamless switching\nbetween one-shot and video dubbing settings, offering enhanced flexibility. Due\nto its superior performance, it can simultaneously support high-resolution\nvideo results and real-time inference. Extensive experiments demonstrate that\nour method synthesizes high-fidelity talking portraits with detailed facial\ntextures and precise lip synchronization in real-time, outperforming\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving high-fidelity lip-speech synchronization in audio-driven talking\nportrait synthesis remains challenging. While multi-stage pipelines or\ndiffusion models yield high-quality results, they suffer from high\ncomputational costs. Some approaches perform well on specific individuals with\nlow resources, yet still exhibit mismatched lip movements. The aforementioned\nmethods are modeled in the pixel domain. We observed that there are noticeable\ndiscrepancies in the frequency domain between the synthesized talking videos\nand natural videos. Currently, no research on talking portrait synthesis has\nconsidered this aspect. To address this, we propose a FREquency-modulated,\nhigh-fidelity, and real-time Audio-driven talKing portrait synthesis framework,\nnamed FREAK, which models talking portraits from the frequency domain\nperspective, enhancing the fidelity and naturalness of the synthesized\nportraits. FREAK introduces two novel frequency-based modules: 1) the Visual\nEncoding Frequency Modulator (VEFM) to couple multi-scale visual features in\nthe frequency domain, better preserving visual frequency information and\nreducing the gap in the frequency spectrum between synthesized and natural\nframes. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model\nlearn the talking pattern in the frequency domain and improve audio-visual\nsynchronization. Additionally, we optimize the model in both pixel domain and\nfrequency domain jointly. Furthermore, FREAK supports seamless switching\nbetween one-shot and video dubbing settings, offering enhanced flexibility. Due\nto its superior performance, it can simultaneously support high-resolution\nvideo results and real-time inference. Extensive experiments demonstrate that\nour method synthesizes high-fidelity talking portraits with detailed facial\ntextures and precise lip synchronization in real-time, outperforming\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Ziqi Ni"
                    },
                    {
                        "name": "Ao Fu"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "arxiv_comment": "Accepted by ICMR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06601v3",
                "updated": "2025-04-24T03:29:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    29,
                    59,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-10T15:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "title": "LaMsS: When Large Language Models Meet Self-Skepticism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMsS: When Large Language Models Meet Self-Skepticism"
                },
                "summary": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76."
                },
                "authors": [
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 6 figures, Published at ICLR 2025 Workshop on Scaling\n  Self-Improving Foundation Models,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16584v1",
                "updated": "2025-04-23T10:05:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    5,
                    27,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:05:27Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    5,
                    27,
                    2,
                    113,
                    0
                ],
                "title": "Case Study: Fine-tuning Small Language Models for Accurate and Private\n  CWE Detection in Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Study: Fine-tuning Small Language Models for Accurate and Private\n  CWE Detection in Python Code"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and analyzing code for security vulnerabilities, such as Common\nWeakness Enumerations (CWEs). However, their reliance on cloud infrastructure\nand substantial computational requirements pose challenges for analyzing\nsensitive or proprietary codebases due to privacy concerns and inference costs.\nThis work explores the potential of Small Language Models (SLMs) as a viable\nalternative for accurate, on-premise vulnerability detection. We investigated\nwhether a 350-million parameter pre-trained code model (codegen-mono) could be\neffectively fine-tuned to detect the MITRE Top 25 CWEs specifically within\nPython code. To facilitate this, we developed a targeted dataset of 500\nexamples using a semi-supervised approach involving LLM-driven synthetic data\ngeneration coupled with meticulous human review. Initial tests confirmed that\nthe base codegen-mono model completely failed to identify CWEs in our samples.\nHowever, after applying instruction-following fine-tuning, the specialized SLM\nachieved remarkable performance on our test set, yielding approximately 99%\naccuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results\nstrongly suggest that fine-tuned SLMs can serve as highly accurate and\nefficient tools for CWE detection, offering a practical and privacy-preserving\nsolution for integrating advanced security analysis directly into development\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and analyzing code for security vulnerabilities, such as Common\nWeakness Enumerations (CWEs). However, their reliance on cloud infrastructure\nand substantial computational requirements pose challenges for analyzing\nsensitive or proprietary codebases due to privacy concerns and inference costs.\nThis work explores the potential of Small Language Models (SLMs) as a viable\nalternative for accurate, on-premise vulnerability detection. We investigated\nwhether a 350-million parameter pre-trained code model (codegen-mono) could be\neffectively fine-tuned to detect the MITRE Top 25 CWEs specifically within\nPython code. To facilitate this, we developed a targeted dataset of 500\nexamples using a semi-supervised approach involving LLM-driven synthetic data\ngeneration coupled with meticulous human review. Initial tests confirmed that\nthe base codegen-mono model completely failed to identify CWEs in our samples.\nHowever, after applying instruction-following fine-tuning, the specialized SLM\nachieved remarkable performance on our test set, yielding approximately 99%\naccuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results\nstrongly suggest that fine-tuned SLMs can serve as highly accurate and\nefficient tools for CWE detection, offering a practical and privacy-preserving\nsolution for integrating advanced security analysis directly into development\nworkflows."
                },
                "authors": [
                    {
                        "name": "Md. Azizul Hakim Bappy"
                    },
                    {
                        "name": "Hossen A Mustafa"
                    },
                    {
                        "name": "Prottoy Saha"
                    },
                    {
                        "name": "Rajinus Salehat"
                    }
                ],
                "author_detail": {
                    "name": "Rajinus Salehat"
                },
                "arxiv_affiliation": "Hajee Mohammad Danesh Science and Technology University, Dinajpur, Bangladesh",
                "author": "Rajinus Salehat",
                "arxiv_comment": "11 pages, 2 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/floxihunter/synthetic_python_cwe. Model\n  available at https://huggingface.co/floxihunter/codegen-mono-CWEdetect.\n  Keywords: Small Language Models (SLMs), Vulnerability Detection, CWE,\n  Fine-tuning, Python Security, Privacy-Preserving Code Analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16005v2",
                "updated": "2025-04-23T09:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    59,
                    31,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T16:14:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    14,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "CAPO: Cost-Aware Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Cost-Aware Prompt Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Timo Hei√ü"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer",
                "arxiv_comment": "Submitted to AutoML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16574v1",
                "updated": "2025-04-23T09:53:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    53,
                    1,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:53:01Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    53,
                    1,
                    2,
                    113,
                    0
                ],
                "title": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient\n  Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient\n  Prompt Compression"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress, demonstrating\nunprecedented capabilities across various natural language processing tasks.\nHowever, the high costs associated with such exceptional performance limit the\nwidespread adoption of LLMs, highlighting the need for prompt compression.\nExisting prompt compression methods primarily rely on heuristic truncation or\nabstractive summarization techniques, which fundamentally overlook the\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\nimportance for generation. In this work, we introduce Prompt Importance\nSampling (PIS), a novel compression framework that dynamically compresses\nprompts by sampling important tokens based on the analysis of attention scores\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\ntoken level, we quantify saliency using LLM-native attention scores and\nimplement adaptive compression through a lightweight 9-layer reinforcement\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\nsampling strategy for sentence-level importance sampling. Comprehensive\nevaluations across multiple domain benchmarks demonstrate that our method\nachieves state-of-the-art compression performance. Notably, our framework\nserendipitously enhances reasoning efficiency through optimized context\nstructuring. This work advances prompt engineering by offering both theoretical\ngrounding and practical efficiency in context management for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress, demonstrating\nunprecedented capabilities across various natural language processing tasks.\nHowever, the high costs associated with such exceptional performance limit the\nwidespread adoption of LLMs, highlighting the need for prompt compression.\nExisting prompt compression methods primarily rely on heuristic truncation or\nabstractive summarization techniques, which fundamentally overlook the\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\nimportance for generation. In this work, we introduce Prompt Importance\nSampling (PIS), a novel compression framework that dynamically compresses\nprompts by sampling important tokens based on the analysis of attention scores\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\ntoken level, we quantify saliency using LLM-native attention scores and\nimplement adaptive compression through a lightweight 9-layer reinforcement\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\nsampling strategy for sentence-level importance sampling. Comprehensive\nevaluations across multiple domain benchmarks demonstrate that our method\nachieves state-of-the-art compression performance. Notably, our framework\nserendipitously enhances reasoning efficiency through optimized context\nstructuring. This work advances prompt engineering by offering both theoretical\ngrounding and practical efficiency in context management for LLMs."
                },
                "authors": [
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Binjia Zhou"
                    },
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Shiguang NI"
                    }
                ],
                "author_detail": {
                    "name": "Shiguang NI"
                },
                "author": "Shiguang NI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16573v1",
                "updated": "2025-04-23T09:49:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    49,
                    5,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:49:05Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    49,
                    5,
                    2,
                    113,
                    0
                ],
                "title": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling\n  Assistant System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling\n  Assistant System"
                },
                "summary": "Psychological counseling is a highly personalized and dynamic process that\nrequires therapists to continuously monitor emotional changes, document session\ninsights, and maintain therapeutic continuity. In this paper, we introduce\nPsyCounAssist, a comprehensive AI-powered counseling assistant system\nspecifically designed to augment psychological counseling practices.\nPsyCounAssist integrates multimodal emotion recognition combining speech and\nphotoplethysmography (PPG) signals for accurate real-time affective analysis,\nautomated structured session reporting using large language models (LLMs), and\npersonalized AI-generated follow-up support. Deployed on Android-based tablet\ndevices, the system demonstrates practical applicability and flexibility in\nreal-world counseling scenarios. Experimental evaluation confirms the\nreliability of PPG-based emotional classification and highlights the system's\npotential for non-intrusive, privacy-aware emotional support. PsyCounAssist\nrepresents a novel approach to ethically and effectively integrating AI into\npsychological counseling workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological counseling is a highly personalized and dynamic process that\nrequires therapists to continuously monitor emotional changes, document session\ninsights, and maintain therapeutic continuity. In this paper, we introduce\nPsyCounAssist, a comprehensive AI-powered counseling assistant system\nspecifically designed to augment psychological counseling practices.\nPsyCounAssist integrates multimodal emotion recognition combining speech and\nphotoplethysmography (PPG) signals for accurate real-time affective analysis,\nautomated structured session reporting using large language models (LLMs), and\npersonalized AI-generated follow-up support. Deployed on Android-based tablet\ndevices, the system demonstrates practical applicability and flexibility in\nreal-world counseling scenarios. Experimental evaluation confirms the\nreliability of PPG-based emotional classification and highlights the system's\npotential for non-intrusive, privacy-aware emotional support. PsyCounAssist\nrepresents a novel approach to ethically and effectively integrating AI into\npsychological counseling workflows."
                },
                "authors": [
                    {
                        "name": "Xianghe Liu"
                    },
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Tao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Tao Sun"
                },
                "author": "Tao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16570v1",
                "updated": "2025-04-23T09:48:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    48,
                    8,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:48:08Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    48,
                    8,
                    2,
                    113,
                    0
                ],
                "title": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using\n  Unsupervised Backbones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using\n  Unsupervised Backbones"
                },
                "summary": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we outperform a baseline under the same label-free setting.\nOur method also achieves competitive -- and in some cases superior -- results\ncompared to training-free approaches relying on supervised backbones, as well\nas several fully supervised state-of-the-art methods. This demonstrates that\ntraining-free CAC can be both scalable and competitive. Website:\nhttps://lorebianchi98.github.io/CountingDINO/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we outperform a baseline under the same label-free setting.\nOur method also achieves competitive -- and in some cases superior -- results\ncompared to training-free approaches relying on supervised backbones, as well\nas several fully supervised state-of-the-art methods. This demonstrates that\ntraining-free CAC can be both scalable and competitive. Website:\nhttps://lorebianchi98.github.io/CountingDINO/"
                },
                "authors": [
                    {
                        "name": "Giacomo Pacini"
                    },
                    {
                        "name": "Lorenzo Bianchi"
                    },
                    {
                        "name": "Luca Ciampi"
                    },
                    {
                        "name": "Nicola Messina"
                    },
                    {
                        "name": "Giuseppe Amato"
                    },
                    {
                        "name": "Fabrizio Falchi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Falchi"
                },
                "author": "Fabrizio Falchi",
                "arxiv_comment": "13 pages, 2 figures, 2 tables. Project website:\n  https://lorebianchi98.github.io/CountingDINO/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16563v1",
                "updated": "2025-04-23T09:43:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    43,
                    40,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:43:40Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    43,
                    40,
                    2,
                    113,
                    0
                ],
                "title": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution"
                },
                "summary": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Jingli Yang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16552v1",
                "updated": "2025-04-23T09:28:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    28,
                    9,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:28:09Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    28,
                    9,
                    2,
                    113,
                    0
                ],
                "title": "DTVM: Revolutionizing Smart Contract Execution with Determinism and\n  Compatibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTVM: Revolutionizing Smart Contract Execution with Determinism and\n  Compatibility"
                },
                "summary": "We introduce the DeTerministic Virtual Machine (DTVM) Stack, a\nnext-generation smart contract execution framework designed to address critical\nperformance, determinism, and ecosystem compatibility challenges in blockchain\nnetworks. Building upon WebAssembly (Wasm) while maintaining full Ethereum\nVirtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle\nIntermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to\nbalance compilation speed and execution efficiency. DTVM further accommodates\ndiverse instruction set architectures (e.g., EVM, RISC-V) through modular\nadaptation layers. This enables seamless integration with DTVM's hybrid\nlazy-JIT compilation engine, which dynamically optimizes performance while\npreserving deterministic execution guarantees across heterogeneous\nenvironments. The key contributions including: 1). The framework achieves up to\n2$\\times$ acceleration over evmone in dominant Ethereum contract (e.g.\nERC20/721/1155) execution and reduces fibonacci computation latency by\n11.8$\\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch\nmechanism enables sub-millisecond (0.95ms) post-deployment invocation times,\noutperforming up to about 23$\\times$ in compilation and invocation efficiency.\n3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and\nAssemblyScript) through unified bytecode conversion while maintaining EVM ABI\ncompatibility for seamless invocation. It reduces machine code object sizes by\n30.0$\\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers\nSmartCogent, an AI-driven full-stack development experience, leveraging\nfine-tuned LLMs and retrieval-augmented generation to automate tasks across the\nsmart contract lifecycle: development, debugging, security auditing, and\ndeployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the DeTerministic Virtual Machine (DTVM) Stack, a\nnext-generation smart contract execution framework designed to address critical\nperformance, determinism, and ecosystem compatibility challenges in blockchain\nnetworks. Building upon WebAssembly (Wasm) while maintaining full Ethereum\nVirtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle\nIntermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to\nbalance compilation speed and execution efficiency. DTVM further accommodates\ndiverse instruction set architectures (e.g., EVM, RISC-V) through modular\nadaptation layers. This enables seamless integration with DTVM's hybrid\nlazy-JIT compilation engine, which dynamically optimizes performance while\npreserving deterministic execution guarantees across heterogeneous\nenvironments. The key contributions including: 1). The framework achieves up to\n2$\\times$ acceleration over evmone in dominant Ethereum contract (e.g.\nERC20/721/1155) execution and reduces fibonacci computation latency by\n11.8$\\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch\nmechanism enables sub-millisecond (0.95ms) post-deployment invocation times,\noutperforming up to about 23$\\times$ in compilation and invocation efficiency.\n3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and\nAssemblyScript) through unified bytecode conversion while maintaining EVM ABI\ncompatibility for seamless invocation. It reduces machine code object sizes by\n30.0$\\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers\nSmartCogent, an AI-driven full-stack development experience, leveraging\nfine-tuned LLMs and retrieval-augmented generation to automate tasks across the\nsmart contract lifecycle: development, debugging, security auditing, and\ndeployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack)."
                },
                "authors": [
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Changzheng Wei"
                    },
                    {
                        "name": "Ying Yan"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Zhihao Chen"
                    },
                    {
                        "name": "Xiong Xu"
                    },
                    {
                        "name": "Xuebing Huang"
                    },
                    {
                        "name": "Wengang Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Xiaofu Zheng"
                    },
                    {
                        "name": "Hanghang Wu"
                    },
                    {
                        "name": "Shenglong Chen"
                    },
                    {
                        "name": "Ermei Wang"
                    },
                    {
                        "name": "Xiangfei Chen"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Meng Wu"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Liwei Yuan"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Alex Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ji Luo"
                    },
                    {
                        "name": "Zhengyu He"
                    },
                    {
                        "name": "Wenbiao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wenbiao Zhao"
                },
                "author": "Wenbiao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16548v1",
                "updated": "2025-04-23T09:25:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    22,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:25:22Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    22,
                    2,
                    113,
                    0
                ],
                "title": "Exploring human-SAV interaction using large language models: The impact\n  of psychological ownership and anthropomorphism on user experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring human-SAV interaction using large language models: The impact\n  of psychological ownership and anthropomorphism on user experience"
                },
                "summary": "There has been extensive prior work exploring how psychological factors such\nas anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).\nHowever, limited research has been conducted on how prompt strategies in large\nlanguage model (LLM)-powered SAV User Interfaces (UIs) affect users'\nperceptions, experiences, and intentions to adopt such technology. In this\nwork, we investigate how conversational UIs powered by LLMs drive these\npsychological factors and psychological ownership, the sense of possession a\nuser may come to feel towards an entity or object they may not legally own. We\ndesigned four SAV UIs with varying levels of anthropomorphic characteristics\nand psychological ownership triggers. Quantitative measures of psychological\nownership, anthropomorphism, quality of service, disclosure tendency, sentiment\nof SAV responses, and overall acceptance were collected after participants\ninteracted with each SAV. Qualitative feedback was also gathered regarding the\nexperience of psychological ownership during the interactions. The results\nindicate that an SAV conversational UI designed to be more anthropomorphic and\nto induce psychological ownership improved users' perceptions of the SAV's\nhuman-like qualities and improved the sentiment of responses compared to a\ncontrol condition. These findings provide practical guidance for designing\nLLM-based conversational UIs that enhance user experience and adoption of SAVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been extensive prior work exploring how psychological factors such\nas anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).\nHowever, limited research has been conducted on how prompt strategies in large\nlanguage model (LLM)-powered SAV User Interfaces (UIs) affect users'\nperceptions, experiences, and intentions to adopt such technology. In this\nwork, we investigate how conversational UIs powered by LLMs drive these\npsychological factors and psychological ownership, the sense of possession a\nuser may come to feel towards an entity or object they may not legally own. We\ndesigned four SAV UIs with varying levels of anthropomorphic characteristics\nand psychological ownership triggers. Quantitative measures of psychological\nownership, anthropomorphism, quality of service, disclosure tendency, sentiment\nof SAV responses, and overall acceptance were collected after participants\ninteracted with each SAV. Qualitative feedback was also gathered regarding the\nexperience of psychological ownership during the interactions. The results\nindicate that an SAV conversational UI designed to be more anthropomorphic and\nto induce psychological ownership improved users' perceptions of the SAV's\nhuman-like qualities and improved the sentiment of responses compared to a\ncontrol condition. These findings provide practical guidance for designing\nLLM-based conversational UIs that enhance user experience and adoption of SAVs."
                },
                "authors": [
                    {
                        "name": "Lirui Guo"
                    },
                    {
                        "name": "Michael G. Burke"
                    },
                    {
                        "name": "Wynita M. Griggs"
                    }
                ],
                "author_detail": {
                    "name": "Wynita M. Griggs"
                },
                "author": "Wynita M. Griggs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16538v1",
                "updated": "2025-04-23T09:08:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    8,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:08:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    8,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language\n  Assessment and Mapping of Urban Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language\n  Assessment and Mapping of Urban Scenes"
                },
                "summary": "Streetscapes are an essential component of urban space. Their assessment is\npresently either limited to morphometric properties of their mass skeleton or\nrequires labor-intensive qualitative evaluations of visually perceived\nqualities. This paper introduces SAGAI: Streetscape Analysis with Generative\nArtificial Intelligence, a modular workflow for scoring street-level urban\nscenes using open-access data and vision-language models. SAGAI integrates\nOpenStreetMap geometries, Google Street View imagery, and a lightweight version\nof the LLaVA model to generate structured spatial indicators from images via\ncustomizable natural language prompts. The pipeline includes an automated\nmapping module that aggregates visual scores at both the point and street\nlevels, enabling direct cartographic interpretation. It operates without\ntask-specific training or proprietary software dependencies, supporting\nscalable and interpretable analysis of urban environments. Two exploratory case\nstudies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial\noutputs from vision-language inference. The initial results show strong\nperformance for binary urban-rural scene classification, moderate precision in\ncommercial feature detection, and lower estimates, but still informative, of\nsidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a\nwide range of urban research themes, such as walkability, safety, or urban\ndesign, through prompt modification alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streetscapes are an essential component of urban space. Their assessment is\npresently either limited to morphometric properties of their mass skeleton or\nrequires labor-intensive qualitative evaluations of visually perceived\nqualities. This paper introduces SAGAI: Streetscape Analysis with Generative\nArtificial Intelligence, a modular workflow for scoring street-level urban\nscenes using open-access data and vision-language models. SAGAI integrates\nOpenStreetMap geometries, Google Street View imagery, and a lightweight version\nof the LLaVA model to generate structured spatial indicators from images via\ncustomizable natural language prompts. The pipeline includes an automated\nmapping module that aggregates visual scores at both the point and street\nlevels, enabling direct cartographic interpretation. It operates without\ntask-specific training or proprietary software dependencies, supporting\nscalable and interpretable analysis of urban environments. Two exploratory case\nstudies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial\noutputs from vision-language inference. The initial results show strong\nperformance for binary urban-rural scene classification, moderate precision in\ncommercial feature detection, and lower estimates, but still informative, of\nsidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a\nwide range of urban research themes, such as walkability, safety, or urban\ndesign, through prompt modification alone."
                },
                "authors": [
                    {
                        "name": "Joan Perez"
                    },
                    {
                        "name": "Giovanni Fusco"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Fusco"
                },
                "arxiv_affiliation": "Universite Cote-Azur-CNRS-AMU-Avignon Universite, ESPACE, France",
                "author": "Giovanni Fusco",
                "arxiv_comment": "25 pages, 6 figures in main paper, 6 figures in appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16529v1",
                "updated": "2025-04-23T08:54:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    54,
                    10,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:54:10Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    54,
                    10,
                    2,
                    113,
                    0
                ],
                "title": "6G EdgeAI: Performance Evaluation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G EdgeAI: Performance Evaluation and Analysis"
                },
                "summary": "Generative AI (GenAI) services powered by large language models (LLMs)\nincreasingly deliver real-time interactions, yet existing 5G multi-access edge\ncomputing (MEC) architectures often treat communication and computing as\nseparate domains, limiting their ability to meet stringent latency\nrequirements. To address this challenge, we introduce an Integrated\nCommunication and Computing (ICC) framework where computing capabilities are\nenabled to reside directly in radio access network (RAN) nodes and jointly\nmanage bandwidth and computing resources. Our queueing-theoretic analysis shows\nthat ICC outperforms 5G MEC, achieving higher service capacity (defined as the\nmaximum arrival rate that maintains a specified fraction of jobs completed\nwithin a given delay budget) by 98%. We corroborate these gains through\nsystem-level simulations that account for transformer-based LLM workloads,\nrealistic GPU specifications, and a priority-based scheduling scheme. The\nsimulations show that ICC improves service capacity by 60%, demonstrating its\npotential to enable efficient, cost-effective real-time GenAI services in 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) services powered by large language models (LLMs)\nincreasingly deliver real-time interactions, yet existing 5G multi-access edge\ncomputing (MEC) architectures often treat communication and computing as\nseparate domains, limiting their ability to meet stringent latency\nrequirements. To address this challenge, we introduce an Integrated\nCommunication and Computing (ICC) framework where computing capabilities are\nenabled to reside directly in radio access network (RAN) nodes and jointly\nmanage bandwidth and computing resources. Our queueing-theoretic analysis shows\nthat ICC outperforms 5G MEC, achieving higher service capacity (defined as the\nmaximum arrival rate that maintains a specified fraction of jobs completed\nwithin a given delay budget) by 98%. We corroborate these gains through\nsystem-level simulations that account for transformer-based LLM workloads,\nrealistic GPU specifications, and a priority-based scheduling scheme. The\nsimulations show that ICC improves service capacity by 60%, demonstrating its\npotential to enable efficient, cost-effective real-time GenAI services in 6G."
                },
                "authors": [
                    {
                        "name": "Chien-Sheng Yang"
                    },
                    {
                        "name": "Yu-Jen Ku"
                    },
                    {
                        "name": "Yuan-Yao Lou"
                    },
                    {
                        "name": "Nathan Tenny"
                    },
                    {
                        "name": "Alex C. -C. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Alex C. -C. Hsu"
                },
                "author": "Alex C. -C. Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16526v1",
                "updated": "2025-04-23T08:50:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    50,
                    24,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:50:24Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    50,
                    24,
                    2,
                    113,
                    0
                ],
                "title": "Using Causal Inference to Test Systems with Hidden and Interacting\n  Variables: An Evaluative Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Causal Inference to Test Systems with Hidden and Interacting\n  Variables: An Evaluative Case Study"
                },
                "summary": "Software systems with large parameter spaces, nondeterminism and high\ncomputational cost are challenging to test. Recently, software testing\ntechniques based on causal inference have been successfully applied to systems\nthat exhibit such characteristics, including scientific models and autonomous\ndriving systems. One significant limitation is that these are restricted to\ntest properties where all of the variables involved can be observed and where\nthere are no interactions between variables. In practice, this is rarely\nguaranteed; the logging infrastructure may not be available to record all of\nthe necessary runtime variable values, and it can often be the case that an\noutput of the system can be affected by complex interactions between variables.\nTo address this, we leverage two additional concepts from causal inference,\nnamely effect modification and instrumental variable methods. We build these\nconcepts into an existing causal testing tool and conduct an evaluative case\nstudy which uses the concepts to test three system-level requirements of CARLA,\na high-fidelity driving simulator widely used in autonomous vehicle development\nand testing. The results show that we can obtain reliable test outcomes without\nrequiring large amounts of highly controlled test data or instrumentation of\nthe code, even when variables interact with each other and are not recorded in\nthe test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems with large parameter spaces, nondeterminism and high\ncomputational cost are challenging to test. Recently, software testing\ntechniques based on causal inference have been successfully applied to systems\nthat exhibit such characteristics, including scientific models and autonomous\ndriving systems. One significant limitation is that these are restricted to\ntest properties where all of the variables involved can be observed and where\nthere are no interactions between variables. In practice, this is rarely\nguaranteed; the logging infrastructure may not be available to record all of\nthe necessary runtime variable values, and it can often be the case that an\noutput of the system can be affected by complex interactions between variables.\nTo address this, we leverage two additional concepts from causal inference,\nnamely effect modification and instrumental variable methods. We build these\nconcepts into an existing causal testing tool and conduct an evaluative case\nstudy which uses the concepts to test three system-level requirements of CARLA,\na high-fidelity driving simulator widely used in autonomous vehicle development\nand testing. The results show that we can obtain reliable test outcomes without\nrequiring large amounts of highly controlled test data or instrumentation of\nthe code, even when variables interact with each other and are not recorded in\nthe test data."
                },
                "authors": [
                    {
                        "name": "Michael Foster"
                    },
                    {
                        "name": "Robert M. Hierons"
                    },
                    {
                        "name": "Donghwan Shin"
                    },
                    {
                        "name": "Neil Walkinshaw"
                    },
                    {
                        "name": "Christopher Wild"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Wild"
                },
                "author": "Christopher Wild",
                "arxiv_comment": "10 pages (plus two containing only references), 3 tables, 2 figures,\n  EASE 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16516v1",
                "updated": "2025-04-23T08:41:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    41,
                    27,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:41:27Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    41,
                    27,
                    2,
                    113,
                    0
                ],
                "title": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion\n  and Reasoning for Vision-and-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion\n  and Reasoning for Vision-and-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow\nnatural language instructions and reach target locations in real-world\nenvironments. While prior methods often rely on either global scene\nrepresentations or object-level features, these approaches are insufficient for\ncapturing the complex interactions across modalities required for accurate\nnavigation. In this paper, we propose a Multi-level Fusion and Reasoning\nArchitecture (MFRA) to enhance the agent's ability to reason over visual\nobservations, language instructions and navigation history. Specifically, MFRA\nintroduces a hierarchical fusion mechanism that aggregates multi-level\nfeatures-ranging from low-level visual cues to high-level semantic\nconcepts-across multiple modalities. We further design a reasoning module that\nleverages fused representations to infer navigation actions through\ninstruction-guided attention and dynamic context integration. By selectively\ncapturing and combining relevant visual, linguistic, and temporal signals, MFRA\nimproves decision-making accuracy in complex navigation scenarios. Extensive\nexperiments on benchmark VLN datasets including REVERIE, R2R, and SOON\ndemonstrate that MFRA achieves superior performance compared to\nstate-of-the-art methods, validating the effectiveness of multi-level modal\nfusion for embodied navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow\nnatural language instructions and reach target locations in real-world\nenvironments. While prior methods often rely on either global scene\nrepresentations or object-level features, these approaches are insufficient for\ncapturing the complex interactions across modalities required for accurate\nnavigation. In this paper, we propose a Multi-level Fusion and Reasoning\nArchitecture (MFRA) to enhance the agent's ability to reason over visual\nobservations, language instructions and navigation history. Specifically, MFRA\nintroduces a hierarchical fusion mechanism that aggregates multi-level\nfeatures-ranging from low-level visual cues to high-level semantic\nconcepts-across multiple modalities. We further design a reasoning module that\nleverages fused representations to infer navigation actions through\ninstruction-guided attention and dynamic context integration. By selectively\ncapturing and combining relevant visual, linguistic, and temporal signals, MFRA\nimproves decision-making accuracy in complex navigation scenarios. Extensive\nexperiments on benchmark VLN datasets including REVERIE, R2R, and SOON\ndemonstrate that MFRA achieves superior performance compared to\nstate-of-the-art methods, validating the effectiveness of multi-level modal\nfusion for embodied navigation."
                },
                "authors": [
                    {
                        "name": "Junrong Yue"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Chuan Qin"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaomin Lie"
                    },
                    {
                        "name": "Xinlei Yu"
                    },
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Zhendong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Zhao"
                },
                "author": "Zhendong Zhao",
                "arxiv_comment": "11 pages, 4 figures, Submitted to ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16511v1",
                "updated": "2025-04-23T08:36:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    36,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:36:50Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    36,
                    50,
                    2,
                    113,
                    0
                ],
                "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining"
                },
                "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity."
                },
                "authors": [
                    {
                        "name": "Fengze Liu"
                    },
                    {
                        "name": "Weidong Zhou"
                    },
                    {
                        "name": "Binbin Liu"
                    },
                    {
                        "name": "Zhimiao Yu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Haobin Lin"
                    },
                    {
                        "name": "Yifeng Yu"
                    },
                    {
                        "name": "Xiaohuan Zhou"
                    },
                    {
                        "name": "Taifeng Wang"
                    },
                    {
                        "name": "Yong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cao"
                },
                "author": "Yong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16506v1",
                "updated": "2025-04-23T08:33:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    33,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:33:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    33,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "A Comprehensive Survey of Synthetic Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Synthetic Tabular Data Generation"
                },
                "summary": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area."
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Yili Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16504v1",
                "updated": "2025-04-23T08:31:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    31,
                    51,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:31:51Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    31,
                    51,
                    2,
                    113,
                    0
                ],
                "title": "Intelligent Depression Prevention via LLM-Based Dialogue Analysis:\n  Overcoming the Limitations of Scale-Dependent Diagnosis through Precise\n  Emotional Pattern Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Depression Prevention via LLM-Based Dialogue Analysis:\n  Overcoming the Limitations of Scale-Dependent Diagnosis through Precise\n  Emotional Pattern Recognition"
                },
                "summary": "Existing depression screening predominantly relies on standardized\nquestionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates\n(18-34% in clinical studies) due to their static, symptom-counting nature and\nsusceptibility to patient recall bias. This paper presents an AI-powered\ndepression prevention system that leverages large language models (LLMs) to\nanalyze real-time conversational cues--including subtle emotional expressions\n(e.g., micro-sentiment shifts, self-referential language patterns)--for more\naccurate and dynamic mental state assessment. Our system achieves three key\ninnovations: (1) Continuous monitoring through natural dialogue, detecting\ndepression-indicative linguistic features (anhedonia markers, hopelessness\nsemantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk\nstratification that updates severity levels based on conversational context,\nreducing false positives by 41% compared to scale-based thresholds; and (3)\nPersonalized intervention strategies tailored to users' emotional granularity,\ndemonstrating 2.3x higher adherence rates than generic advice. Clinical\nvalidation with 450 participants shows the system identifies 92% of at-risk\ncases missed by traditional scales, while its explainable AI interface bridges\nthe gap between automated analysis and clinician judgment. This work\nestablishes conversational AI as a paradigm shift from episodic scale-dependent\ndiagnosis to continuous, emotionally intelligent mental health monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing depression screening predominantly relies on standardized\nquestionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates\n(18-34% in clinical studies) due to their static, symptom-counting nature and\nsusceptibility to patient recall bias. This paper presents an AI-powered\ndepression prevention system that leverages large language models (LLMs) to\nanalyze real-time conversational cues--including subtle emotional expressions\n(e.g., micro-sentiment shifts, self-referential language patterns)--for more\naccurate and dynamic mental state assessment. Our system achieves three key\ninnovations: (1) Continuous monitoring through natural dialogue, detecting\ndepression-indicative linguistic features (anhedonia markers, hopelessness\nsemantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk\nstratification that updates severity levels based on conversational context,\nreducing false positives by 41% compared to scale-based thresholds; and (3)\nPersonalized intervention strategies tailored to users' emotional granularity,\ndemonstrating 2.3x higher adherence rates than generic advice. Clinical\nvalidation with 450 participants shows the system identifies 92% of at-risk\ncases missed by traditional scales, while its explainable AI interface bridges\nthe gap between automated analysis and clinician judgment. This work\nestablishes conversational AI as a paradigm shift from episodic scale-dependent\ndiagnosis to continuous, emotionally intelligent mental health monitoring."
                },
                "authors": [
                    {
                        "name": "Zhenguang Zhong"
                    },
                    {
                        "name": "Zhixuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Wang"
                },
                "author": "Zhixuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16489v1",
                "updated": "2025-04-23T08:01:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    1,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:01:50Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    1,
                    50,
                    2,
                    113,
                    0
                ],
                "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based\n  Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based\n  Multi-Agent Debate"
                },
                "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment."
                },
                "authors": [
                    {
                        "name": "Senmao Qi"
                    },
                    {
                        "name": "Yifei Zou"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ziyi Lin"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "arxiv_comment": "33 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05000v2",
                "updated": "2025-04-23T07:50:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    50,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?"
                },
                "summary": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Samuel Albanie"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Albanie"
                },
                "author": "Samuel Albanie",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16472v1",
                "updated": "2025-04-23T07:32:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    32,
                    43,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T07:32:43Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    32,
                    43,
                    2,
                    113,
                    0
                ],
                "title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges"
                },
                "summary": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\n`just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper\\footnote{Author order is alphabetical. The\ncorresponding author is Mark Harman.} was written to accompany the keynote by\nthe authors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\n`just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper\\footnote{Author order is alphabetical. The\ncorresponding author is Mark Harman.} was written to accompany the keynote by\nthe authors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025."
                },
                "authors": [
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Peter O'Hearn"
                    },
                    {
                        "name": "Shubho Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Shubho Sengupta"
                },
                "author": "Shubho Sengupta",
                "arxiv_comment": "To Appear as keynote paper at FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v4",
                "updated": "2025-04-23T07:27:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    27,
                    35,
                    2,
                    113,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15922v2",
                "updated": "2025-04-23T07:24:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    24,
                    40,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T14:06:02Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    2,
                    1,
                    112,
                    0
                ],
                "title": "Language Models to Support Multi-Label Classification of Industrial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models to Support Multi-Label Classification of Industrial Data"
                },
                "summary": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem"
                },
                "authors": [
                    {
                        "name": "Waleed Abdeen"
                    },
                    {
                        "name": "Michael Unterkalmsteiner"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Panagiota Chatzipetrou"
                    }
                ],
                "author_detail": {
                    "name": "Panagiota Chatzipetrou"
                },
                "author": "Panagiota Chatzipetrou",
                "arxiv_comment": "Accepted at SANER Conference 2025. Awaiting publication by IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13612v3",
                "updated": "2025-04-23T07:09:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    9,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2024-12-18T08:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    42,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Large Language Models for Automated Literature Review: An Evaluation of\n  Reference Generation, Abstract Writing, and Review Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Automated Literature Review: An Evaluation of\n  Reference Generation, Abstract Writing, and Review Composition"
                },
                "summary": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "12 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01840v3",
                "updated": "2025-04-23T07:08:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    8,
                    17,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-03T18:59:04Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    59,
                    4,
                    0,
                    62,
                    0
                ],
                "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test"
                },
                "summary": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves\na 1.38x throughput improvement at a batch size of 64. The code is available at\nhttps://github.com/SafeAILab/EAGLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves\na 1.38x throughput improvement at a batch size of 64. The code is available at\nhttps://github.com/SafeAILab/EAGLE."
                },
                "authors": [
                    {
                        "name": "Yuhui Li"
                    },
                    {
                        "name": "Fangyun Wei"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Hongyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Zhang"
                },
                "author": "Hongyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.16921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16921v1",
                "updated": "2025-04-23T17:48:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    48,
                    25,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:48:25Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    48,
                    25,
                    2,
                    113,
                    0
                ],
                "title": "IberBench: LLM Evaluation on Iberian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IberBench: LLM Evaluation on Iberian Languages"
                },
                "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard."
                },
                "authors": [
                    {
                        "name": "Jos√© √Ångel Gonz√°lez"
                    },
                    {
                        "name": "Ian Borrego Obrador"
                    },
                    {
                        "name": "√Ålvaro Romo Herrero"
                    },
                    {
                        "name": "Areg Mikael Sarvazyan"
                    },
                    {
                        "name": "Mara Chinea-R√≠os"
                    },
                    {
                        "name": "Angelo Basile"
                    },
                    {
                        "name": "Marc Franco-Salvador"
                    }
                ],
                "author_detail": {
                    "name": "Marc Franco-Salvador"
                },
                "author": "Marc Franco-Salvador",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16419v3",
                "updated": "2025-04-23T17:46:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    46,
                    54,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-20T17:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    59,
                    38,
                    3,
                    79,
                    0
                ],
                "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."
                },
                "authors": [
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Jiamu Zhang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Andrew Wen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15842v3",
                "updated": "2025-04-23T17:46:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    46,
                    8,
                    2,
                    113,
                    0
                ],
                "published": "2024-07-22T17:58:05Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    17,
                    58,
                    5,
                    0,
                    204,
                    0
                ],
                "title": "DiffArtist: Towards Structure and Appearance Controllable Image\n  Stylization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffArtist: Towards Structure and Appearance Controllable Image\n  Stylization"
                },
                "summary": "Artistic style includes both structural and appearance elements. Existing\nneural stylization techniques primarily focus on transferring appearance\nfeatures such as color and texture, often neglecting the equally crucial aspect\nof structural stylization. In this paper, we present a comprehensive study on\nthe simultaneous stylization of structure and appearance of 2D images.\nSpecifically, we introduce DiffArtist, which, to the best of our knowledge, is\nthe first stylization method to allow for dual controllability over structure\nand appearance. Our key insight is to represent structure and appearance as\nseparate diffusion processes to achieve complete disentanglement without\nrequiring any training, thereby endowing users with unprecedented\ncontrollability for both components. The evaluation of stylization of both\nappearance and structure, however, remains challenging as it necessitates\nsemantic understanding. To this end, we further propose a Multimodal LLM-based\nstyle evaluator, which better aligns with human preferences than metrics\nlacking semantic understanding. With this powerful evaluator, we conduct\nextensive analysis, demonstrating that DiffArtist achieves superior style\nfidelity, editability, and structure-appearance disentanglement. These merits\nmake DiffArtist a highly versatile solution for creative applications. Project\nhomepage: https://github.com/songrise/Artist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artistic style includes both structural and appearance elements. Existing\nneural stylization techniques primarily focus on transferring appearance\nfeatures such as color and texture, often neglecting the equally crucial aspect\nof structural stylization. In this paper, we present a comprehensive study on\nthe simultaneous stylization of structure and appearance of 2D images.\nSpecifically, we introduce DiffArtist, which, to the best of our knowledge, is\nthe first stylization method to allow for dual controllability over structure\nand appearance. Our key insight is to represent structure and appearance as\nseparate diffusion processes to achieve complete disentanglement without\nrequiring any training, thereby endowing users with unprecedented\ncontrollability for both components. The evaluation of stylization of both\nappearance and structure, however, remains challenging as it necessitates\nsemantic understanding. To this end, we further propose a Multimodal LLM-based\nstyle evaluator, which better aligns with human preferences than metrics\nlacking semantic understanding. With this powerful evaluator, we conduct\nextensive analysis, demonstrating that DiffArtist achieves superior style\nfidelity, editability, and structure-appearance disentanglement. These merits\nmake DiffArtist a highly versatile solution for creative applications. Project\nhomepage: https://github.com/songrise/Artist."
                },
                "authors": [
                    {
                        "name": "Ruixiang Jiang"
                    },
                    {
                        "name": "Changwen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changwen Chen"
                },
                "author": "Changwen Chen",
                "arxiv_comment": "Homepage: https://DiffusionArtist.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16918v1",
                "updated": "2025-04-23T17:45:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    45,
                    5,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:45:05Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    45,
                    5,
                    2,
                    113,
                    0
                ],
                "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents"
                },
                "summary": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results."
                },
                "authors": [
                    {
                        "name": "Raghav Thind"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Haizhao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Haizhao Yang"
                },
                "author": "Haizhao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16913v1",
                "updated": "2025-04-23T17:39:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    39,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:39:49Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    39,
                    49,
                    2,
                    113,
                    0
                ],
                "title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM\n  Behind AI-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM\n  Behind AI-Generated Text"
                },
                "summary": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability."
                },
                "authors": [
                    {
                        "name": "Shifali Agrahari"
                    },
                    {
                        "name": "Sanasam Ranbir Singh"
                    }
                ],
                "author_detail": {
                    "name": "Sanasam Ranbir Singh"
                },
                "author": "Sanasam Ranbir Singh",
                "arxiv_comment": "De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate\n  Speech Detection, co-located with AAAI 2025. Pennsylvania",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21615v2",
                "updated": "2025-04-23T17:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    39,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-27T15:36:49Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    36,
                    49,
                    3,
                    86,
                    0
                ],
                "title": "A Measure Based Generalizable Approach to Understandability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Measure Based Generalizable Approach to Understandability"
                },
                "summary": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future."
                },
                "authors": [
                    {
                        "name": "Vikas Kushwaha"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    },
                    {
                        "name": "Subhajit Roy"
                    }
                ],
                "author_detail": {
                    "name": "Subhajit Roy"
                },
                "author": "Subhajit Roy",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16902v1",
                "updated": "2025-04-23T17:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    27,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:27:49Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    27,
                    49,
                    2,
                    113,
                    0
                ],
                "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building A Secure Agentic AI Application Leveraging A2A Protocol"
                },
                "summary": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications."
                },
                "authors": [
                    {
                        "name": "Idan Habler"
                    },
                    {
                        "name": "Ken Huang"
                    },
                    {
                        "name": "Vineeth Sai Narajala"
                    },
                    {
                        "name": "Prashant Kulkarni"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Kulkarni"
                },
                "author": "Prashant Kulkarni",
                "arxiv_comment": "13 pages, 4 figures, 1 table, Authors contributed equally to this\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16884v1",
                "updated": "2025-04-23T17:00:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    45,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:00:45Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    45,
                    2,
                    113,
                    0
                ],
                "title": "Do Large Language Models know who did what to whom?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models know who did what to whom?"
                },
                "summary": "Large Language Models (LLMs) are commonly criticized for not understanding\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\nare distinct from language processing. Here, we instead study a kind of\nunderstanding tightly linked to language: inferring who did what to whom\n(thematic roles) in a sentence. Does the central training objective of\nLLMs-word prediction-result in sentence representations that capture thematic\nroles? In two experiments, we characterized sentence representations in four\nLLMs. In contrast to human similarity judgments, in LLMs the overall\nrepresentational similarity of sentence pairs reflected syntactic similarity\nbut not whether their agent and patient assignments were identical vs.\nreversed. Furthermore, we found little evidence that thematic role information\nwas available in any subset of hidden units. However, some attention heads\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\nextract thematic roles but, relative to humans, this information influences\ntheir representations more weakly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly criticized for not understanding\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\nare distinct from language processing. Here, we instead study a kind of\nunderstanding tightly linked to language: inferring who did what to whom\n(thematic roles) in a sentence. Does the central training objective of\nLLMs-word prediction-result in sentence representations that capture thematic\nroles? In two experiments, we characterized sentence representations in four\nLLMs. In contrast to human similarity judgments, in LLMs the overall\nrepresentational similarity of sentence pairs reflected syntactic similarity\nbut not whether their agent and patient assignments were identical vs.\nreversed. Furthermore, we found little evidence that thematic role information\nwas available in any subset of hidden units. However, some attention heads\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\nextract thematic roles but, relative to humans, this information influences\ntheir representations more weakly."
                },
                "authors": [
                    {
                        "name": "Joseph M. Denning"
                    },
                    {
                        "name": "Xiaohan"
                    },
                    {
                        "name": "Guo"
                    },
                    {
                        "name": "Bryor Snefjella"
                    },
                    {
                        "name": "Idan A. Blank"
                    }
                ],
                "author_detail": {
                    "name": "Idan A. Blank"
                },
                "arxiv_affiliation": "Hannah",
                "author": "Idan A. Blank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16883v1",
                "updated": "2025-04-23T17:00:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    25,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T17:00:25Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    17,
                    0,
                    25,
                    2,
                    113,
                    0
                ],
                "title": "Enhancing Critical Thinking with AI: A Tailored Warning System for RAG\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Critical Thinking with AI: A Tailored Warning System for RAG\n  Models"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems offer a powerful approach to\nenhancing large language model (LLM) outputs by incorporating fact-checked,\ncontextually relevant information. However, fairness and reliability concerns\npersist, as hallucinations can emerge at both the retrieval and generation\nstages, affecting users' reasoning and decision-making. Our research explores\nhow tailored warning messages -- whose content depends on the specific context\nof hallucination -- shape user reasoning and actions in an educational quiz\nsetting. Preliminary findings suggest that while warnings improve accuracy and\nawareness of high-level hallucinations, they may also introduce cognitive\nfriction, leading to confusion and diminished trust in the system. By examining\nthese interactions, this work contributes to the broader goal of AI-augmented\nreasoning: developing systems that actively support human reflection, critical\nthinking, and informed decision-making rather than passive information\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems offer a powerful approach to\nenhancing large language model (LLM) outputs by incorporating fact-checked,\ncontextually relevant information. However, fairness and reliability concerns\npersist, as hallucinations can emerge at both the retrieval and generation\nstages, affecting users' reasoning and decision-making. Our research explores\nhow tailored warning messages -- whose content depends on the specific context\nof hallucination -- shape user reasoning and actions in an educational quiz\nsetting. Preliminary findings suggest that while warnings improve accuracy and\nawareness of high-level hallucinations, they may also introduce cognitive\nfriction, leading to confusion and diminished trust in the system. By examining\nthese interactions, this work contributes to the broader goal of AI-augmented\nreasoning: developing systems that actively support human reflection, critical\nthinking, and informed decision-making rather than passive information\nconsumption."
                },
                "authors": [
                    {
                        "name": "Xuyang Zhu"
                    },
                    {
                        "name": "Sejoon Chang"
                    },
                    {
                        "name": "Andrew Kuik"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Kuik"
                },
                "author": "Andrew Kuik",
                "arxiv_comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction\n  for Augmented Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16877v1",
                "updated": "2025-04-23T16:54:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    54,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:54:16Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    54,
                    16,
                    2,
                    113,
                    0
                ],
                "title": "Context-Enhanced Vulnerability Detection Based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Enhanced Vulnerability Detection Based on Large Language Model"
                },
                "summary": "Vulnerability detection is a critical aspect of software security. Accurate\ndetection is essential to prevent potential security breaches and protect\nsoftware systems from malicious attacks. Recently, vulnerability detection\nmethods leveraging deep learning and large language models (LLMs) have garnered\nincreasing attention. However, existing approaches often focus on analyzing\nindividual files or functions, which limits their ability to gather sufficient\ncontextual information. Analyzing entire repositories to gather context\nintroduces significant noise and computational overhead. To address these\nchallenges, we propose a context-enhanced vulnerability detection approach that\ncombines program analysis with LLMs. Specifically, we use program analysis to\nextract contextual information at various levels of abstraction, thereby\nfiltering out irrelevant noise. The abstracted context along with source code\nare provided to LLM for vulnerability detection. We investigate how different\nlevels of contextual granularity improve LLM-based vulnerability detection\nperformance. Our goal is to strike a balance between providing sufficient\ndetail to accurately capture vulnerabilities and minimizing unnecessary\ncomplexity that could hinder model performance. Based on an extensive study\nusing GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key\nfindings includes: (1) incorporating abstracted context significantly enhances\nvulnerability detection effectiveness; (2) different models benefit from\ndistinct levels of abstraction depending on their code understanding\ncapabilities; and (3) capturing program behavior through program analysis for\ngeneral LLM-based code analysis tasks can be a direction that requires further\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability detection is a critical aspect of software security. Accurate\ndetection is essential to prevent potential security breaches and protect\nsoftware systems from malicious attacks. Recently, vulnerability detection\nmethods leveraging deep learning and large language models (LLMs) have garnered\nincreasing attention. However, existing approaches often focus on analyzing\nindividual files or functions, which limits their ability to gather sufficient\ncontextual information. Analyzing entire repositories to gather context\nintroduces significant noise and computational overhead. To address these\nchallenges, we propose a context-enhanced vulnerability detection approach that\ncombines program analysis with LLMs. Specifically, we use program analysis to\nextract contextual information at various levels of abstraction, thereby\nfiltering out irrelevant noise. The abstracted context along with source code\nare provided to LLM for vulnerability detection. We investigate how different\nlevels of contextual granularity improve LLM-based vulnerability detection\nperformance. Our goal is to strike a balance between providing sufficient\ndetail to accurately capture vulnerabilities and minimizing unnecessary\ncomplexity that could hinder model performance. Based on an extensive study\nusing GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key\nfindings includes: (1) incorporating abstracted context significantly enhances\nvulnerability detection effectiveness; (2) different models benefit from\ndistinct levels of abstraction depending on their code understanding\ncapabilities; and (3) capturing program behavior through program analysis for\ngeneral LLM-based code analysis tasks can be a direction that requires further\nattention."
                },
                "authors": [
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Bowen Xu"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Hailong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Sun"
                },
                "author": "Hailong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14985v2",
                "updated": "2025-04-23T16:52:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    52,
                    54,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T09:26:05Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    26,
                    5,
                    0,
                    111,
                    0
                ],
                "title": "aiXamine: Simplified LLM Safety and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aiXamine: Simplified LLM Safety and Security"
                },
                "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."
                },
                "authors": [
                    {
                        "name": "Fatih Deniz"
                    },
                    {
                        "name": "Dorde Popovic"
                    },
                    {
                        "name": "Yazan Boshmaf"
                    },
                    {
                        "name": "Euisuh Jeong"
                    },
                    {
                        "name": "Minhaj Ahmad"
                    },
                    {
                        "name": "Sanjay Chawla"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04105v3",
                "updated": "2025-04-23T16:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    48,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2024-03-06T23:17:16Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    23,
                    17,
                    16,
                    2,
                    66,
                    0
                ],
                "title": "Natural Language Processing in the Patent Domain: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing in the Patent Domain: A Survey"
                },
                "summary": "Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_doi": "10.1007/s10462-025-11168-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10462-025-11168-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.04105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Artificial Intelligence Review",
                "arxiv_journal_ref": "Artif Intell Rev 58, 214 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16871v1",
                "updated": "2025-04-23T16:46:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    46,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:46:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    46,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge"
                },
                "summary": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks"
                },
                "authors": [
                    {
                        "name": "Mirian Hipolito Garcia"
                    },
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Anastasios Kyrillidis"
                    },
                    {
                        "name": "Robert Sim"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14128v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14128v4",
                "updated": "2025-04-24T02:50:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    50,
                    28,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-19T01:02:42Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    1,
                    2,
                    42,
                    5,
                    109,
                    0
                ],
                "title": "TALES: Text Adventure Learning Environment Suite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALES: Text Adventure Learning Environment Suite"
                },
                "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite."
                },
                "authors": [
                    {
                        "name": "Christopher Zhang Cui"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "name": "Marc-Alexandre C√¥t√©"
                    }
                ],
                "author_detail": {
                    "name": "Marc-Alexandre C√¥t√©"
                },
                "author": "Marc-Alexandre C√¥t√©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14128v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14128v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16858v1",
                "updated": "2025-04-23T16:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    27,
                    15,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:27:15Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    27,
                    15,
                    2,
                    113,
                    0
                ],
                "title": "Planning with Diffusion Models for Target-Oriented Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with Diffusion Models for Target-Oriented Dialogue Systems"
                },
                "summary": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM\nera, where strategic dialogue planning is crucial for directing conversations\ntoward specific targets. However, existing dialogue planning methods generate\ndialogue plans in a step-by-step sequential manner, and may suffer from\ncompounding errors and myopic actions. To address these limitations, we\nintroduce a novel dialogue planning framework, DiffTOD, which leverages\ndiffusion models to enable non-sequential dialogue planning. DiffTOD formulates\ndialogue planning as a trajectory generation problem with conditional guidance,\nand leverages a diffusion language model to estimate the likelihood of the\ndialogue trajectory. To optimize the dialogue action strategies, DiffTOD\nintroduces three tailored guidance mechanisms for different target types,\noffering flexible guidance towards diverse TOD targets at test time. Extensive\nexperiments across three diverse TOD settings show that DiffTOD can effectively\nperform non-myopic lookahead exploration and optimize action strategies over a\nlong horizon through non-sequential dialogue planning, and demonstrates strong\nflexibility across complex and diverse dialogue scenarios. Our code and data\nare accessible through https://anonymous.4open.science/r/DiffTOD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM\nera, where strategic dialogue planning is crucial for directing conversations\ntoward specific targets. However, existing dialogue planning methods generate\ndialogue plans in a step-by-step sequential manner, and may suffer from\ncompounding errors and myopic actions. To address these limitations, we\nintroduce a novel dialogue planning framework, DiffTOD, which leverages\ndiffusion models to enable non-sequential dialogue planning. DiffTOD formulates\ndialogue planning as a trajectory generation problem with conditional guidance,\nand leverages a diffusion language model to estimate the likelihood of the\ndialogue trajectory. To optimize the dialogue action strategies, DiffTOD\nintroduces three tailored guidance mechanisms for different target types,\noffering flexible guidance towards diverse TOD targets at test time. Extensive\nexperiments across three diverse TOD settings show that DiffTOD can effectively\nperform non-myopic lookahead exploration and optimize action strategies over a\nlong horizon through non-sequential dialogue planning, and demonstrates strong\nflexibility across complex and diverse dialogue scenarios. Our code and data\nare accessible through https://anonymous.4open.science/r/DiffTOD."
                },
                "authors": [
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Bo Peng"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16856v1",
                "updated": "2025-04-23T16:23:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    17,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:23:17Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    17,
                    2,
                    113,
                    0
                ],
                "title": "Emo Pillars: Knowledge Distillation to Support Fine-Grained\n  Context-Aware and Context-Less Emotion Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emo Pillars: Knowledge Distillation to Support Fine-Grained\n  Context-Aware and Context-Less Emotion Classification"
                },
                "summary": "Most datasets for sentiment analysis lack context in which an opinion was\nexpressed, often crucial for emotion understanding, and are mainly limited by a\nfew emotion categories. Foundation large language models (LLMs) like GPT-4\nsuffer from over-predicting emotions and are too resource-intensive. We design\nan LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,\nfor the generation of training examples for more accessible, lightweight\nBERT-type encoder models. We focus on enlarging the semantic diversity of\nexamples and propose grounding the generation into a corpus of narratives to\nproduce non-repetitive story-character-centered utterances with unique contexts\nover 28 emotion classes. By running 700K inferences in 450 GPU hours, we\ncontribute with the dataset of 100K contextual and also 300K context-less\nexamples to cover both scenarios. We use it for fine-tuning pre-trained\nencoders, which results in several Emo Pillars models. We show that Emo Pillars\nmodels are highly adaptive to new domains when tuned to specific tasks such as\nGoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on\nthe first three. We also validate our dataset, conducting statistical analysis\nand human evaluation, and confirm the success of our measures in utterance\ndiversification (although less for the neutral class) and context\npersonalization, while pointing out the need for improved handling of\nout-of-taxonomy labels within the pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datasets for sentiment analysis lack context in which an opinion was\nexpressed, often crucial for emotion understanding, and are mainly limited by a\nfew emotion categories. Foundation large language models (LLMs) like GPT-4\nsuffer from over-predicting emotions and are too resource-intensive. We design\nan LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,\nfor the generation of training examples for more accessible, lightweight\nBERT-type encoder models. We focus on enlarging the semantic diversity of\nexamples and propose grounding the generation into a corpus of narratives to\nproduce non-repetitive story-character-centered utterances with unique contexts\nover 28 emotion classes. By running 700K inferences in 450 GPU hours, we\ncontribute with the dataset of 100K contextual and also 300K context-less\nexamples to cover both scenarios. We use it for fine-tuning pre-trained\nencoders, which results in several Emo Pillars models. We show that Emo Pillars\nmodels are highly adaptive to new domains when tuned to specific tasks such as\nGoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on\nthe first three. We also validate our dataset, conducting statistical analysis\nand human evaluation, and confirm the success of our measures in utterance\ndiversification (although less for the neutral class) and context\npersonalization, while pointing out the need for improved handling of\nout-of-taxonomy labels within the pipeline."
                },
                "authors": [
                    {
                        "name": "Alexander Shvets"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Shvets"
                },
                "author": "Alexander Shvets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16855v1",
                "updated": "2025-04-23T16:23:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    15,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:23:15Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    23,
                    15,
                    2,
                    113,
                    0
                ],
                "title": "Monte Carlo Planning with Large Language Model for Text-Based Game\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Planning with Large Language Model for Text-Based Game\n  Agents"
                },
                "summary": "Text-based games provide valuable environments for language-based autonomous\nagents. However, planning-then-learning paradigms, such as those combining\nMonte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably\ntime-consuming due to extensive iterations. Additionally, these algorithms\nperform uncertainty-driven exploration but lack language understanding and\nreasoning abilities. In this paper, we introduce the Monte Carlo planning with\nDynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages\nthe language understanding and reasoning capabilities of Large Language Models\n(LLMs) alongside the exploratory advantages of tree search algorithms.\nSpecifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,\nenabling them to learn from past experiences and dynamically adjust action\nevaluations during planning. We conduct experiments on a series of text-based\ngames from the Jericho benchmark. Our results demonstrate that the MC-DML\nalgorithm significantly enhances performance across various games at the\ninitial planning phase, outperforming strong contemporary methods that require\nmultiple iterations. This demonstrates the effectiveness of our algorithm,\npaving the way for more efficient language-grounded planning in complex\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-based games provide valuable environments for language-based autonomous\nagents. However, planning-then-learning paradigms, such as those combining\nMonte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably\ntime-consuming due to extensive iterations. Additionally, these algorithms\nperform uncertainty-driven exploration but lack language understanding and\nreasoning abilities. In this paper, we introduce the Monte Carlo planning with\nDynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages\nthe language understanding and reasoning capabilities of Large Language Models\n(LLMs) alongside the exploratory advantages of tree search algorithms.\nSpecifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,\nenabling them to learn from past experiences and dynamically adjust action\nevaluations during planning. We conduct experiments on a series of text-based\ngames from the Jericho benchmark. Our results demonstrate that the MC-DML\nalgorithm significantly enhances performance across various games at the\ninitial planning phase, outperforming strong contemporary methods that require\nmultiple iterations. This demonstrates the effectiveness of our algorithm,\npaving the way for more efficient language-grounded planning in complex\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zijing Shi"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08813v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08813v4",
                "updated": "2025-04-23T16:14:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    14,
                    18,
                    2,
                    113,
                    0
                ],
                "published": "2023-07-17T20:01:11Z",
                "published_parsed": [
                    2023,
                    7,
                    17,
                    20,
                    1,
                    11,
                    0,
                    198,
                    0
                ],
                "title": "Comparative Performance Evaluation of Large Language Models for\n  Extracting Molecular Interactions and Pathway Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Performance Evaluation of Large Language Models for\n  Extracting Molecular Interactions and Pathway Knowledge"
                },
                "summary": "Background: Identification of the interactions and regulatory relations\nbetween biomolecules play pivotal roles in understanding complex biological\nsystems and the mechanisms underlying diverse biological functions. However,\nthe collection of such molecular interactions has heavily relied on expert\ncuration in the past, making it labor-intensive and time-consuming. To mitigate\nthese challenges, we propose leveraging the capabilities of large language\nmodels (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results: In this study, we investigate the efficacy of various LLMs in\naddressing biological tasks, such as the recognition of protein interactions,\nidentification of genes linked to pathways affected by low-dose radiation, and\nthe delineation of gene regulatory relationships. Overall, the larger models\nexhibited superior performance, indicating their potential for specific tasks\nthat involve the extraction of complex interactions among genes and proteins.\nAlthough these models possessed detailed information for distinct gene and\nprotein groups, they faced challenges in identifying groups with diverse\nfunctions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions: By conducting a comprehensive assessment of the state-of-the-art\nmodels using well-established molecular interaction and pathway databases, our\nstudy reveals that LLMs can identify genes/proteins associated with pathways of\ninterest and predict their interactions to a certain extent. Furthermore, these\nmodels can provide important insights, marking a noteworthy stride toward\nadvancing our understanding of biological systems through AI-assisted knowledge\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Identification of the interactions and regulatory relations\nbetween biomolecules play pivotal roles in understanding complex biological\nsystems and the mechanisms underlying diverse biological functions. However,\nthe collection of such molecular interactions has heavily relied on expert\ncuration in the past, making it labor-intensive and time-consuming. To mitigate\nthese challenges, we propose leveraging the capabilities of large language\nmodels (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results: In this study, we investigate the efficacy of various LLMs in\naddressing biological tasks, such as the recognition of protein interactions,\nidentification of genes linked to pathways affected by low-dose radiation, and\nthe delineation of gene regulatory relationships. Overall, the larger models\nexhibited superior performance, indicating their potential for specific tasks\nthat involve the extraction of complex interactions among genes and proteins.\nAlthough these models possessed detailed information for distinct gene and\nprotein groups, they faced challenges in identifying groups with diverse\nfunctions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions: By conducting a comprehensive assessment of the state-of-the-art\nmodels using well-established molecular interaction and pathway databases, our\nstudy reveals that LLMs can identify genes/proteins associated with pathways of\ninterest and predict their interactions to a certain extent. Furthermore, these\nmodels can provide important insights, marking a noteworthy stride toward\nadvancing our understanding of biological systems through AI-assisted knowledge\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Gilchan Park"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "name": "Xihaier Luo"
                    },
                    {
                        "name": "Vanessa L√≥pez-Marrero"
                    },
                    {
                        "name": "Shinjae Yoo"
                    },
                    {
                        "name": "Shantenu Jha"
                    }
                ],
                "author_detail": {
                    "name": "Shantenu Jha"
                },
                "author": "Shantenu Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08813v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08813v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16046v2",
                "updated": "2025-04-23T16:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    13,
                    8,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T17:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    16,
                    53,
                    1,
                    112,
                    0
                ],
                "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Mitigation of Worst-Case LLM Copyright Infringement"
                },
                "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Jiacan Yu"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16848v1",
                "updated": "2025-04-23T16:10:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    10,
                    31,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T16:10:31Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    10,
                    31,
                    2,
                    113,
                    0
                ],
                "title": "Improving QoS Prediction in Urban V2X Networks by Leveraging Data from\n  Leading Vehicles and Historical Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving QoS Prediction in Urban V2X Networks by Leveraging Data from\n  Leading Vehicles and Historical Trends"
                },
                "summary": "With the evolution of Vehicle-to-Everything (V2X) technology and increased\ndeployment of 5G networks and edge computing, Predictive Quality of Service\n(PQoS) is seen as an enabler for resilient and adaptive V2X communication\nsystems. PQoS incorporates data-driven techniques, such as Machine Learning\n(ML), to forecast/predict Key Performing Indicators (KPIs) such as throughput,\nlatency, etc. In this paper, we aim to predict downlink throughput in an urban\nenvironment using the Berlin V2X cellular dataset. We select features from the\nego and lead vehicles to train different ML models to help improve the\npredicted throughput for the ego vehicle. We identify these features based on\nan in-depth exploratory data analysis. Results show an improvement in model\nperformance when adding features from the lead vehicle. Moreover, we show that\nthe improvement in model performance is model-agnostic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the evolution of Vehicle-to-Everything (V2X) technology and increased\ndeployment of 5G networks and edge computing, Predictive Quality of Service\n(PQoS) is seen as an enabler for resilient and adaptive V2X communication\nsystems. PQoS incorporates data-driven techniques, such as Machine Learning\n(ML), to forecast/predict Key Performing Indicators (KPIs) such as throughput,\nlatency, etc. In this paper, we aim to predict downlink throughput in an urban\nenvironment using the Berlin V2X cellular dataset. We select features from the\nego and lead vehicles to train different ML models to help improve the\npredicted throughput for the ego vehicle. We identify these features based on\nan in-depth exploratory data analysis. Results show an improvement in model\nperformance when adding features from the lead vehicle. Moreover, we show that\nthe improvement in model performance is model-agnostic."
                },
                "authors": [
                    {
                        "name": "Sanket Partani"
                    },
                    {
                        "name": "Michael Zentarra"
                    },
                    {
                        "name": "Anthony Kiggundu"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00694v2",
                "updated": "2025-04-23T16:07:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    7,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-01T12:05:49Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    5,
                    49,
                    1,
                    91,
                    0
                ],
                "title": "On Benchmarking Code LLMs for Android Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Benchmarking Code LLMs for Android Malware Analysis"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\ncode intelligence tasks. However, their effectiveness for Android malware\nanalysis remains underexplored. Decompiled Android malware code presents unique\nchallenges for analysis, due to the malicious logic being buried within a large\nnumber of functions and the frequent lack of meaningful function names. This\npaper presents CAMA, a benchmarking framework designed to systematically\nevaluate the effectiveness of Code LLMs in Android malware analysis. CAMA\nspecifies structured model outputs to support key malware analysis tasks,\nincluding malicious function identification and malware purpose summarization.\nBuilt on these, it integrates three domain-specific evaluation metrics\n(consistency, fidelity, and semantic relevance), enabling rigorous stability\nand effectiveness assessment and cross-model comparison. We construct a\nbenchmark dataset of 118 Android malware samples from 13 families collected in\nrecent years, encompassing over 7.5 million distinct functions, and use CAMA to\nevaluate four popular open-source Code LLMs. Our experiments provide insights\ninto how Code LLMs interpret decompiled code and quantify the sensitivity to\nfunction renaming, highlighting both their potential and current limitations in\nmalware analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in various\ncode intelligence tasks. However, their effectiveness for Android malware\nanalysis remains underexplored. Decompiled Android malware code presents unique\nchallenges for analysis, due to the malicious logic being buried within a large\nnumber of functions and the frequent lack of meaningful function names. This\npaper presents CAMA, a benchmarking framework designed to systematically\nevaluate the effectiveness of Code LLMs in Android malware analysis. CAMA\nspecifies structured model outputs to support key malware analysis tasks,\nincluding malicious function identification and malware purpose summarization.\nBuilt on these, it integrates three domain-specific evaluation metrics\n(consistency, fidelity, and semantic relevance), enabling rigorous stability\nand effectiveness assessment and cross-model comparison. We construct a\nbenchmark dataset of 118 Android malware samples from 13 families collected in\nrecent years, encompassing over 7.5 million distinct functions, and use CAMA to\nevaluate four popular open-source Code LLMs. Our experiments provide insights\ninto how Code LLMs interpret decompiled code and quantify the sensitivity to\nfunction renaming, highlighting both their potential and current limitations in\nmalware analysis."
                },
                "authors": [
                    {
                        "name": "Yiling He"
                    },
                    {
                        "name": "Hongyu She"
                    },
                    {
                        "name": "Xingzhi Qian"
                    },
                    {
                        "name": "Xinran Zheng"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "arxiv_doi": "10.1145/3713081.3731745",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713081.3731745",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.00694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted to the 34th ACM SIGSOFT ISSTA Companion\n  (LLMSC Workshop 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16834v1",
                "updated": "2025-04-23T15:56:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    56,
                    28,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:56:28Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    56,
                    28,
                    2,
                    113,
                    0
                ],
                "title": "Improving Significant Wave Height Prediction Using Chronos Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Significant Wave Height Prediction Using Chronos Models"
                },
                "summary": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling."
                },
                "authors": [
                    {
                        "name": "Yilin Zhai"
                    },
                    {
                        "name": "Hongyuan Shi"
                    },
                    {
                        "name": "Chao Zhan"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Zaijin You"
                    },
                    {
                        "name": "Nan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wang"
                },
                "author": "Nan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16833v1",
                "updated": "2025-04-23T15:52:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    52,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:52:50Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    52,
                    50,
                    2,
                    113,
                    0
                ],
                "title": "LRASGen: LLM-based RESTful API Specification Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRASGen: LLM-based RESTful API Specification Generation"
                },
                "summary": "REpresentation State Transfer (REST) is an architectural style for designing\nweb applications that enable scalable, stateless communication between clients\nand servers via common HTTP techniques. Web APIs that employ the REST style are\nknown as RESTful (or REST) APIs. When using or testing a RESTful API,\ndevelopers may need to employ its specification, which is often defined by\nopen-source standards such as the OpenAPI Specification (OAS). However, it can\nbe very time-consuming and error-prone to write and update these\nspecifications, which may negatively impact the use of RESTful APIs, especially\nwhen the software requirements change. Many tools and methods have been\nproposed to solve this problem, such as Respector and Swagger Core. OAS\ngeneration can be regarded as a common text-generation task that creates a\nformal description of API endpoints derived from the source code. A potential\nsolution for this may involve using Large Language Models (LLMs), which have\nstrong capabilities in both code understanding and text generation. Motivated\nby this, we propose a novel approach for generating the OASs of RESTful APIs\nusing LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the\nbest of our knowledge, this is the first use of LLMs and API source code to\ngenerate OASs for RESTful APIs. Compared with existing tools and methods,\nLRASGen can generate the OASs, even when the implementation is incomplete (with\npartial code, and/or missing annotations/comments, etc.). To evaluate the\nLRASGen performance, we conducted a series of empirical studies on 20\nreal-world RESTful APIs. The results show that two LLMs (GPT-4o mini and\nDeepSeek V3) can both support LARSGen to generate accurate specifications, and\nLRASGen-generated specifications cover an average of 48.85% more missed\nentities than the developer-provided specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REpresentation State Transfer (REST) is an architectural style for designing\nweb applications that enable scalable, stateless communication between clients\nand servers via common HTTP techniques. Web APIs that employ the REST style are\nknown as RESTful (or REST) APIs. When using or testing a RESTful API,\ndevelopers may need to employ its specification, which is often defined by\nopen-source standards such as the OpenAPI Specification (OAS). However, it can\nbe very time-consuming and error-prone to write and update these\nspecifications, which may negatively impact the use of RESTful APIs, especially\nwhen the software requirements change. Many tools and methods have been\nproposed to solve this problem, such as Respector and Swagger Core. OAS\ngeneration can be regarded as a common text-generation task that creates a\nformal description of API endpoints derived from the source code. A potential\nsolution for this may involve using Large Language Models (LLMs), which have\nstrong capabilities in both code understanding and text generation. Motivated\nby this, we propose a novel approach for generating the OASs of RESTful APIs\nusing LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the\nbest of our knowledge, this is the first use of LLMs and API source code to\ngenerate OASs for RESTful APIs. Compared with existing tools and methods,\nLRASGen can generate the OASs, even when the implementation is incomplete (with\npartial code, and/or missing annotations/comments, etc.). To evaluate the\nLRASGen performance, we conducted a series of empirical studies on 20\nreal-world RESTful APIs. The results show that two LLMs (GPT-4o mini and\nDeepSeek V3) can both support LARSGen to generate accurate specifications, and\nLRASGen-generated specifications cover an average of 48.85% more missed\nentities than the developer-provided specifications."
                },
                "authors": [
                    {
                        "name": "Sida Deng"
                    },
                    {
                        "name": "Rubing Huang"
                    },
                    {
                        "name": "Man Zhang"
                    },
                    {
                        "name": "Chenhui Cui"
                    },
                    {
                        "name": "Dave Towey"
                    },
                    {
                        "name": "Rongcun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rongcun Wang"
                },
                "author": "Rongcun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16832v1",
                "updated": "2025-04-23T15:48:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    48,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:48:55Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    48,
                    55,
                    2,
                    113,
                    0
                ],
                "title": "GreenMind: A Next-Generation Vietnamese Large Language Model for\n  Structured and Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenMind: A Next-Generation Vietnamese Large Language Model for\n  Structured and Logical Reasoning"
                },
                "summary": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that\nrequire intermediate reasoning steps prior to generating a final answer. In\nthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model\ninspired by the finetuning strategy based on Group Relative Policy\nOptimization. We also leverage a high-quality Vietnamese synthesized reasoning\ndataset and design two reward functions to tackle the main limitations of this\ntechnique: (i) language mixing, where we explicitly detect the presence of\nbiased language characters during the process of sampling tokens, and (ii) we\nleverage Sentence Transformer-based models to ensure that the generated\nreasoning content maintains factual correctness and does not distort the final\noutput. Experimental results on the Vietnamese dataset from the VLSP 2023\nChallenge demonstrate that our model outperforms prior works and enhances\nlinguistic consistency in its responses. Furthermore, we extend our evaluation\nto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of\nour reasoning method compared to few-shot prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that\nrequire intermediate reasoning steps prior to generating a final answer. In\nthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model\ninspired by the finetuning strategy based on Group Relative Policy\nOptimization. We also leverage a high-quality Vietnamese synthesized reasoning\ndataset and design two reward functions to tackle the main limitations of this\ntechnique: (i) language mixing, where we explicitly detect the presence of\nbiased language characters during the process of sampling tokens, and (ii) we\nleverage Sentence Transformer-based models to ensure that the generated\nreasoning content maintains factual correctness and does not distort the final\noutput. Experimental results on the Vietnamese dataset from the VLSP 2023\nChallenge demonstrate that our model outperforms prior works and enhances\nlinguistic consistency in its responses. Furthermore, we extend our evaluation\nto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of\nour reasoning method compared to few-shot prompting techniques."
                },
                "authors": [
                    {
                        "name": "Luu Quy Tung"
                    },
                    {
                        "name": "Hoang Quoc Viet"
                    },
                    {
                        "name": "Vo Trong Thu"
                    }
                ],
                "author_detail": {
                    "name": "Vo Trong Thu"
                },
                "author": "Vo Trong Thu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16354v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16354v4",
                "updated": "2025-04-23T15:48:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    48,
                    42,
                    2,
                    113,
                    0
                ],
                "published": "2024-03-25T01:12:57Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    1,
                    12,
                    57,
                    0,
                    85,
                    0
                ],
                "title": "ChatDBG: Augmenting Debugging with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatDBG: Augmenting Debugging with Large Language Models"
                },
                "summary": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like \"why is x null?\". To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 75,000 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like \"why is x null?\". To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 75,000 times."
                },
                "authors": [
                    {
                        "name": "Kyla H. Levin"
                    },
                    {
                        "name": "Nicolas van Kempen"
                    },
                    {
                        "name": "Emery D. Berger"
                    },
                    {
                        "name": "Stephen N. Freund"
                    }
                ],
                "author_detail": {
                    "name": "Stephen N. Freund"
                },
                "author": "Stephen N. Freund",
                "arxiv_doi": "10.1145/3729355",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729355",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16354v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16354v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, to appear at FSE 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16828v1",
                "updated": "2025-04-23T15:44:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    44,
                    54,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:44:54Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    44,
                    54,
                    2,
                    113,
                    0
                ],
                "title": "Process Reward Models That Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models That Think"
                },
                "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm."
                },
                "authors": [
                    {
                        "name": "Muhammad Khalifa"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Jaekyeom Kim"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Honglak Lee"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16813v1",
                "updated": "2025-04-23T15:31:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    31,
                    11,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:31:11Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    31,
                    11,
                    2,
                    113,
                    0
                ],
                "title": "LLM-assisted Graph-RAG Information Extraction from IFC Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Graph-RAG Information Extraction from IFC Data"
                },
                "summary": "IFC data has become the general building information standard for\ncollaborative work in the construction industry. However, IFC data can be very\ncomplicated because it allows for multiple ways to represent the same product\ninformation. In this research, we utilise the capabilities of LLMs to parse the\nIFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to\nretrieve building object properties and their relations. We will show that,\ndespite limitations due to the complex hierarchy of the IFC data, the Graph-RAG\nparsing enhances generative LLMs like GPT-4o with graph-based knowledge,\nenabling natural language query-response retrieval without the need for a\ncomplex pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFC data has become the general building information standard for\ncollaborative work in the construction industry. However, IFC data can be very\ncomplicated because it allows for multiple ways to represent the same product\ninformation. In this research, we utilise the capabilities of LLMs to parse the\nIFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to\nretrieve building object properties and their relations. We will show that,\ndespite limitations due to the complex hierarchy of the IFC data, the Graph-RAG\nparsing enhances generative LLMs like GPT-4o with graph-based knowledge,\nenabling natural language query-response retrieval without the need for a\ncomplex pipeline."
                },
                "authors": [
                    {
                        "name": "Sima Iranmanesh"
                    },
                    {
                        "name": "Hadeel Saadany"
                    },
                    {
                        "name": "Edlira Vakaj"
                    }
                ],
                "author_detail": {
                    "name": "Edlira Vakaj"
                },
                "author": "Edlira Vakaj",
                "arxiv_comment": "2025 European Conference on Computing in Construction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16801v1",
                "updated": "2025-04-23T15:20:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    53,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T15:20:53Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    53,
                    2,
                    113,
                    0
                ],
                "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA"
                },
                "authors": [
                    {
                        "name": "Xiaoxing Hu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Yupei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yupei Wang"
                },
                "author": "Yupei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16768v1",
                "updated": "2025-04-23T14:41:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    41,
                    11,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:41:11Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    41,
                    11,
                    2,
                    113,
                    0
                ],
                "title": "How Effective are Generative Large Language Models in Performing\n  Requirements Classification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Effective are Generative Large Language Models in Performing\n  Requirements Classification?"
                },
                "summary": "In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance."
                },
                "authors": [
                    {
                        "name": "Waad Alhoshan"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Liping Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhao"
                },
                "author": "Liping Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16763v1",
                "updated": "2025-04-23T14:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    34,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    34,
                    20,
                    2,
                    113,
                    0
                ],
                "title": "Noise-Tolerant Coreset-Based Class Incremental Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Coreset-Based Class Incremental Continual Learning"
                },
                "summary": "Many applications of computer vision require the ability to adapt to novel\ndata distributions after deployment. Adaptation requires algorithms capable of\ncontinual learning (CL). Continual learners must be plastic to adapt to novel\ntasks while minimizing forgetting of previous tasks.However, CL opens up\navenues for noise to enter the training pipeline and disrupt the CL. This work\nfocuses on label noise and instance noise in the context of class-incremental\nlearning (CIL), where new classes are added to a classifier over time, and\nthere is no access to external data from past classes. We aim to understand the\nsensitivity of CL methods that work by replaying items from a memory\nconstructed using the idea of Coresets. We derive a new bound for the\nrobustness of such a method to uncorrelated instance noise under a general\nadditive noise threat model, revealing several insights. Putting the theory\ninto practice, we create two continual learning algorithms to construct\nnoise-tolerant replay buffers. We empirically compare the effectiveness of\nprior memory-based continual learners and the proposed algorithms under label\nand uncorrelated instance noise on five diverse datasets. We show that existing\nmemory-based CL are not robust whereas the proposed methods exhibit significant\nimprovements in maximizing classification accuracy and minimizing forgetting in\nthe noisy CIL setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications of computer vision require the ability to adapt to novel\ndata distributions after deployment. Adaptation requires algorithms capable of\ncontinual learning (CL). Continual learners must be plastic to adapt to novel\ntasks while minimizing forgetting of previous tasks.However, CL opens up\navenues for noise to enter the training pipeline and disrupt the CL. This work\nfocuses on label noise and instance noise in the context of class-incremental\nlearning (CIL), where new classes are added to a classifier over time, and\nthere is no access to external data from past classes. We aim to understand the\nsensitivity of CL methods that work by replaying items from a memory\nconstructed using the idea of Coresets. We derive a new bound for the\nrobustness of such a method to uncorrelated instance noise under a general\nadditive noise threat model, revealing several insights. Putting the theory\ninto practice, we create two continual learning algorithms to construct\nnoise-tolerant replay buffers. We empirically compare the effectiveness of\nprior memory-based continual learners and the proposed algorithms under label\nand uncorrelated instance noise on five diverse datasets. We show that existing\nmemory-based CL are not robust whereas the proposed methods exhibit significant\nimprovements in maximizing classification accuracy and minimizing forgetting in\nthe noisy CIL setting."
                },
                "authors": [
                    {
                        "name": "Edison Mucllari"
                    },
                    {
                        "name": "Aswin Raghavan"
                    },
                    {
                        "name": "Zachary Alan Daniels"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Alan Daniels"
                },
                "author": "Zachary Alan Daniels",
                "arxiv_comment": "Work-in-Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16760v1",
                "updated": "2025-04-23T14:33:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    33,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:33:20Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    33,
                    20,
                    2,
                    113,
                    0
                ],
                "title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies"
                },
                "summary": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications."
                },
                "authors": [
                    {
                        "name": "Bartosz Piotrowski"
                    },
                    {
                        "name": "Witold Drzewakowski"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Mi≈Ço≈õ"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Mi≈Ço≈õ"
                },
                "author": "Piotr Mi≈Ço≈õ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16754v1",
                "updated": "2025-04-23T14:27:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    27,
                    12,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:27:12Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    27,
                    12,
                    2,
                    113,
                    0
                ],
                "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for\n  Long-Context AI Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for\n  Long-Context AI Conversations"
                },
                "summary": "Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining."
                },
                "authors": [
                    {
                        "name": "Kwangseob Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Kwangseob Ahn"
                },
                "author": "Kwangseob Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05812v2",
                "updated": "2025-04-23T14:25:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    25,
                    51,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-08T08:48:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    48,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization"
                },
                "summary": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervision--such as human-labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form natural reasoning tasks. Specifically,\nwithout any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy\nof Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervision--such as human-labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form natural reasoning tasks. Specifically,\nwithout any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy\nof Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro."
                },
                "authors": [
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Haitao Wu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "arxiv_comment": "Ongoing work. First released on April 8, 2025. Updated the natural\n  reasoning results on April 23, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03004v2",
                "updated": "2025-04-23T14:25:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    25,
                    46,
                    2,
                    113,
                    0
                ],
                "published": "2024-07-03T11:02:12Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    11,
                    2,
                    12,
                    2,
                    185,
                    0
                ],
                "title": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from\n  Unstructured Clinical Narratives in Epilepsy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from\n  Unstructured Clinical Narratives in Epilepsy"
                },
                "summary": "Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare."
                },
                "authors": [
                    {
                        "name": "Meghal Dani"
                    },
                    {
                        "name": "Muthu Jeyanthi Prakash"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Stefanie Liebe"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Liebe"
                },
                "author": "Stefanie Liebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16736v1",
                "updated": "2025-04-23T14:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    7,
                    26,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    7,
                    26,
                    2,
                    113,
                    0
                ],
                "title": "A Survey of AI Agent Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of AI Agent Protocols"
                },
                "summary": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents."
                },
                "authors": [
                    {
                        "name": "Yingxuan Yang"
                    },
                    {
                        "name": "Huacan Chai"
                    },
                    {
                        "name": "Yuanyi Song"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Haoyi Hu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Gaowei Chang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10100v3",
                "updated": "2025-04-24T05:33:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    5,
                    33,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-01-17T10:39:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    39,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics"
                },
                "summary": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chenhao Li"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16728v1",
                "updated": "2025-04-23T14:01:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    1,
                    36,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T14:01:36Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    14,
                    1,
                    36,
                    2,
                    113,
                    0
                ],
                "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRIS: Interactive Research Ideation System for Accelerating Scientific\n  Discovery"
                },
                "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System"
                },
                "authors": [
                    {
                        "name": "Aniketh Garikaparthi"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "6 pages main-text, 2 pages appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10982v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10982v4",
                "updated": "2025-04-23T13:54:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    54,
                    1,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-15T08:46:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    46,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical\n  Question Answering with Small-Scale LLMs"
                },
                "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."
                },
                "authors": [
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Xingyu Song"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Zixin Xu"
                    },
                    {
                        "name": "Xiujie Chen"
                    },
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10982v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10982v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15965v2",
                "updated": "2025-04-23T13:47:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    47,
                    27,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T15:05:04Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    5,
                    4,
                    1,
                    112,
                    0
                ],
                "title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era\n  of LLMs"
                },
                "summary": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the process of encoding, storing, and retrieving information,\nallowing humans to retain experiences, knowledge, skills, and facts over time,\nand serving as the foundation for growth and effective interaction with the\nworld. It plays a crucial role in shaping our identity, making decisions,\nlearning from past experiences, building relationships, and adapting to\nchanges. In the era of large language models (LLMs), memory refers to the\nability of an AI system to retain, recall, and use information from past\ninteractions to improve future responses and interactions. Although previous\nresearch and reviews have provided detailed descriptions of memory mechanisms,\nthere is still a lack of a systematic review that summarizes and analyzes the\nrelationship between the memory of LLM-driven AI systems and human memory, as\nwell as how we can be inspired by human memory to construct more powerful\nmemory systems. To achieve this, in this paper, we propose a comprehensive\nsurvey on the memory of LLM-driven AI systems. In particular, we first conduct\na detailed analysis of the categories of human memory and relate them to the\nmemory of AI systems. Second, we systematically organize existing\nmemory-related work and propose a categorization method based on three\ndimensions (object, form, and time) and eight quadrants. Finally, we illustrate\nsome open problems regarding the memory of current AI systems and outline\npossible future directions for memory in the era of large language models."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "26 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16709v1",
                "updated": "2025-04-23T13:40:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    40,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T13:40:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    40,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Resource Reduction in Multiparty Quantum Secret Sharing of both\n  Classical and Quantum Information under Noisy Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Reduction in Multiparty Quantum Secret Sharing of both\n  Classical and Quantum Information under Noisy Scenario"
                },
                "summary": "Quantum secret sharing (QSS) enables secure distribution of information among\nmultiple parties but remains vulnerable to noise. We analyze the effects of\nbit-flip, phase-flip, and amplitude damping noise on the multiparty QSS for\nclassical message (QSSCM) and secret sharing of quantum information (SSQI)\nprotocols proposed by Zhang et al. (Phys. Rev. A, 71:044301, 2005). To scale\ndown these effects, we introduce an efficient quantum error correction (QEC)\nscheme based on a simplified version of Shor's code. Leveraging the specific\nstructure of the QSS protocols, we reduce the qubit overhead from the standard\n9 of Shor's code to as few as 3 while still achieving lower average error rates\nthan existing QEC methods. Thus, our approach can also be adopted for other\nsingle-qubit-based quantum protocols. Simulations demonstrate that our approach\nsignificantly enhances the protocols' resilience, improving their practicality\nfor real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum secret sharing (QSS) enables secure distribution of information among\nmultiple parties but remains vulnerable to noise. We analyze the effects of\nbit-flip, phase-flip, and amplitude damping noise on the multiparty QSS for\nclassical message (QSSCM) and secret sharing of quantum information (SSQI)\nprotocols proposed by Zhang et al. (Phys. Rev. A, 71:044301, 2005). To scale\ndown these effects, we introduce an efficient quantum error correction (QEC)\nscheme based on a simplified version of Shor's code. Leveraging the specific\nstructure of the QSS protocols, we reduce the qubit overhead from the standard\n9 of Shor's code to as few as 3 while still achieving lower average error rates\nthan existing QEC methods. Thus, our approach can also be adopted for other\nsingle-qubit-based quantum protocols. Simulations demonstrate that our approach\nsignificantly enhances the protocols' resilience, improving their practicality\nfor real-world deployment."
                },
                "authors": [
                    {
                        "name": "Nirupam Basak"
                    },
                    {
                        "name": "Goutam Paul"
                    }
                ],
                "author_detail": {
                    "name": "Goutam Paul"
                },
                "author": "Goutam Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15231v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15231v4",
                "updated": "2025-04-24T07:21:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    21,
                    44,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-21T15:19:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    15,
                    19,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "Synthetic Lyrics Detection Across Languages and Genres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Lyrics Detection Across Languages and Genres"
                },
                "summary": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users."
                },
                "authors": [
                    {
                        "name": "Yanis Labrak"
                    },
                    {
                        "name": "Markus Frohmann"
                    },
                    {
                        "name": "Gabriel Meseguer-Brocal"
                    },
                    {
                        "name": "Elena V. Epure"
                    }
                ],
                "author_detail": {
                    "name": "Elena V. Epure"
                },
                "author": "Elena V. Epure",
                "arxiv_comment": "Published in the TrustNLP Workshop at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15231v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15231v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16688v2",
                "updated": "2025-04-24T08:37:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    37,
                    38,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T13:19:35Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    19,
                    35,
                    2,
                    113,
                    0
                ],
                "title": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation\n  for 6G: MLR, ANOVA, and Residual Distribution Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation\n  for 6G: MLR, ANOVA, and Residual Distribution Analysis"
                },
                "summary": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models with one to five components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models with one to five components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments."
                },
                "authors": [
                    {
                        "name": "Nahshon Mokua Obiri"
                    },
                    {
                        "name": "Kristof Van Laerhoven"
                    }
                ],
                "author_detail": {
                    "name": "Kristof Van Laerhoven"
                },
                "author": "Kristof Van Laerhoven",
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media. This is the accepted version of the article: To appear in the\n  2025 Joint European Conference on Networks and Communications & 6G Summit\n  (EuCNC/6G Summit)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2008.08050v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2008.08050v6",
                "updated": "2025-04-23T13:12:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    12,
                    39,
                    2,
                    113,
                    0
                ],
                "published": "2020-08-18T17:28:55Z",
                "published_parsed": [
                    2020,
                    8,
                    18,
                    17,
                    28,
                    55,
                    1,
                    231,
                    0
                ],
                "title": "The MRS UAV System: Pushing the Frontiers of Reproducible Research,\n  Real-world Deployment, and Education with Autonomous Unmanned Aerial Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MRS UAV System: Pushing the Frontiers of Reproducible Research,\n  Real-world Deployment, and Education with Autonomous Unmanned Aerial Vehicles"
                },
                "summary": "We present a multirotor Unmanned Aerial Vehicle control (UAV) and estimation\nsystem for supporting replicable research through realistic simulations and\nreal-world experiments. We propose a unique multi-frame localization paradigm\nfor estimating the states of a UAV in various frames of reference using\nmultiple sensors simultaneously. The system enables complex missions in GNSS\nand GNSS-denied environments, including outdoor-indoor transitions and the\nexecution of redundant estimators for backing up unreliable localization\nsources. Two feedback control designs are presented: one for precise and\naggressive maneuvers, and the other for stable and smooth flight with a noisy\nstate estimate. The proposed control and estimation pipeline are constructed\nwithout using the Euler/Tait-Bryan angle representation of orientation in 3D.\nInstead, we rely on rotation matrices and a novel heading-based convention to\nrepresent the one free rotational degree-of-freedom in 3D of a standard\nmultirotor helicopter. We provide an actively maintained and well-documented\nopen-source implementation, including realistic simulation of UAV, sensors, and\nlocalization systems. The proposed system is the product of years of applied\nresearch on multi-robot systems, aerial swarms, aerial manipulation, motion\nplanning, and remote sensing. All our results have been supported by real-world\nsystem deployment that shaped the system into the form presented here. In\naddition, the system was utilized during the participation of our team from the\nCTU in Prague in the prestigious MBZIRC 2017 and 2020 robotics competitions,\nand also in the DARPA SubT challenge. Each time, our team was able to secure\ntop places among the best competitors from all over the world. On each\noccasion, the challenges has motivated the team to improve the system and to\ngain a great amount of high-quality experience within tight deadlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multirotor Unmanned Aerial Vehicle control (UAV) and estimation\nsystem for supporting replicable research through realistic simulations and\nreal-world experiments. We propose a unique multi-frame localization paradigm\nfor estimating the states of a UAV in various frames of reference using\nmultiple sensors simultaneously. The system enables complex missions in GNSS\nand GNSS-denied environments, including outdoor-indoor transitions and the\nexecution of redundant estimators for backing up unreliable localization\nsources. Two feedback control designs are presented: one for precise and\naggressive maneuvers, and the other for stable and smooth flight with a noisy\nstate estimate. The proposed control and estimation pipeline are constructed\nwithout using the Euler/Tait-Bryan angle representation of orientation in 3D.\nInstead, we rely on rotation matrices and a novel heading-based convention to\nrepresent the one free rotational degree-of-freedom in 3D of a standard\nmultirotor helicopter. We provide an actively maintained and well-documented\nopen-source implementation, including realistic simulation of UAV, sensors, and\nlocalization systems. The proposed system is the product of years of applied\nresearch on multi-robot systems, aerial swarms, aerial manipulation, motion\nplanning, and remote sensing. All our results have been supported by real-world\nsystem deployment that shaped the system into the form presented here. In\naddition, the system was utilized during the participation of our team from the\nCTU in Prague in the prestigious MBZIRC 2017 and 2020 robotics competitions,\nand also in the DARPA SubT challenge. Each time, our team was able to secure\ntop places among the best competitors from all over the world. On each\noccasion, the challenges has motivated the team to improve the system and to\ngain a great amount of high-quality experience within tight deadlines."
                },
                "authors": [
                    {
                        "name": "Tomas Baca"
                    },
                    {
                        "name": "Matej Petrlik"
                    },
                    {
                        "name": "Matous Vrba"
                    },
                    {
                        "name": "Vojtech Spurny"
                    },
                    {
                        "name": "Robert Penicka"
                    },
                    {
                        "name": "Daniel Hert"
                    },
                    {
                        "name": "Martin Saska"
                    }
                ],
                "author_detail": {
                    "name": "Martin Saska"
                },
                "author": "Martin Saska",
                "arxiv_doi": "10.1007/s10846-021-01383-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10846-021-01383-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2008.08050v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2008.08050v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, 20 figures, accepted to Journal of Intelligent & Robotic\n  Systems (JINT), for the provided open-source software see\n  http://github.com/ctu-mrs, erratum for eq. 3, 15, 19, 24",
                "arxiv_journal_ref": "Journal of Intelligent & Robotic Systems, 102, Article number: 26\n  (2021)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12766v3",
                "updated": "2025-04-23T12:52:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    52,
                    18,
                    2,
                    113,
                    0
                ],
                "published": "2024-03-18T17:32:32Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    17,
                    32,
                    32,
                    0,
                    78,
                    0
                ],
                "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K\n  Tokens"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have pushed the\nboundaries of natural language processing, especially in long-context\nunderstanding. However, the evaluation of these models' long-context abilities\nremains a challenge due to the limitations of current benchmarks. To address\nthis gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with\ncomplex, extended narratives. Constructed from English novels, NovelQA offers a\nunique blend of complexity, length, and narrative coherence, making it an ideal\ntool for assessing deep textual understanding in LLMs. This paper details the\ndesign and construction of NovelQA, focusing on its comprehensive manual\nannotation process and the variety of question types aimed at evaluating\nnuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals\nsignificant insights into their strengths and weaknesses. Notably, the models\nstruggle with multi-hop reasoning, detail-oriented questions, and handling\nextremely long inputs, with average lengths exceeding 200,000 tokens. Results\nhighlight the need for substantial advancements in LLMs to enhance their\nlong-context comprehension and contribute effectively to computational literary\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have pushed the\nboundaries of natural language processing, especially in long-context\nunderstanding. However, the evaluation of these models' long-context abilities\nremains a challenge due to the limitations of current benchmarks. To address\nthis gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with\ncomplex, extended narratives. Constructed from English novels, NovelQA offers a\nunique blend of complexity, length, and narrative coherence, making it an ideal\ntool for assessing deep textual understanding in LLMs. This paper details the\ndesign and construction of NovelQA, focusing on its comprehensive manual\nannotation process and the variety of question types aimed at evaluating\nnuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals\nsignificant insights into their strengths and weaknesses. Notably, the models\nstruggle with multi-hop reasoning, detail-oriented questions, and handling\nextremely long inputs, with average lengths exceeding 200,000 tokens. Results\nhighlight the need for substantial advancements in LLMs to enhance their\nlong-context comprehension and contribute effectively to computational literary\nanalysis."
                },
                "authors": [
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Ruoxi Ning"
                    },
                    {
                        "name": "Boqi Pan"
                    },
                    {
                        "name": "Tonghui Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "Accepted by ICLR-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16671v1",
                "updated": "2025-04-23T12:39:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    39,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T12:39:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    39,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative\n  Analysis"
                },
                "summary": "The use of large language models (LLMs) in qualitative analysis offers\nenhanced efficiency but raises questions about their alignment with the\ncontextual nature of research for design (RfD). This research examines the\ntrustworthiness of LLM-driven design insights, using qualitative coding as a\ncase study to explore the interpretive processes central to RfD. We introduce\nLLMCode, an open-source tool integrating two metrics, namely Intersection over\nUnion (IoU) and Modified Hausdorff Distance, to assess the alignment between\nhuman and LLM-generated insights. Across two studies involving 26 designers, we\nfind that while the model performs well with deductive coding, its ability to\nemulate a designer's deeper interpretive lens over the data is limited,\nemphasising the importance of human-AI collaboration. Our results highlight a\nreciprocal dynamic where users refine LLM outputs and adapt their own\nperspectives based on the model's suggestions. These findings underscore the\nimportance of fostering appropriate reliance on LLMs by designing tools that\npreserve interpretive depth while facilitating intuitive collaboration between\ndesigners and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) in qualitative analysis offers\nenhanced efficiency but raises questions about their alignment with the\ncontextual nature of research for design (RfD). This research examines the\ntrustworthiness of LLM-driven design insights, using qualitative coding as a\ncase study to explore the interpretive processes central to RfD. We introduce\nLLMCode, an open-source tool integrating two metrics, namely Intersection over\nUnion (IoU) and Modified Hausdorff Distance, to assess the alignment between\nhuman and LLM-generated insights. Across two studies involving 26 designers, we\nfind that while the model performs well with deductive coding, its ability to\nemulate a designer's deeper interpretive lens over the data is limited,\nemphasising the importance of human-AI collaboration. Our results highlight a\nreciprocal dynamic where users refine LLM outputs and adapt their own\nperspectives based on the model's suggestions. These findings underscore the\nimportance of fostering appropriate reliance on LLMs by designing tools that\npreserve interpretive depth while facilitating intuitive collaboration between\ndesigners and AI."
                },
                "authors": [
                    {
                        "name": "Joel Oksanen"
                    },
                    {
                        "name": "Andr√©s Lucero"
                    },
                    {
                        "name": "Perttu H√§m√§l√§inen"
                    }
                ],
                "author_detail": {
                    "name": "Perttu H√§m√§l√§inen"
                },
                "author": "Perttu H√§m√§l√§inen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10049v2",
                "updated": "2025-04-23T12:12:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    12,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2024-09-16T07:21:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    21,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation"
                },
                "summary": "This paper proposes a lightweight systematic solution for multi-robot\ncoordinated navigation with decentralized cooperative perception. An\ninformation flow is first created to facilitate real-time observation sharing\nover unreliable ad-hoc networks. Then, the environmental uncertainties of each\nrobot are reduced by interaction fields that deliver complementary information.\nFinally, path optimization is achieved, enabling self-organized coordination\nwith effective convergence, divergence, and collision avoidance. Our method is\nfully interpretable and ready for deployment without gaps. Comprehensive\nsimulations and real-world experiments demonstrate reduced path redundancy,\nrobust performance across various tasks, and minimal demands on computation and\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a lightweight systematic solution for multi-robot\ncoordinated navigation with decentralized cooperative perception. An\ninformation flow is first created to facilitate real-time observation sharing\nover unreliable ad-hoc networks. Then, the environmental uncertainties of each\nrobot are reduced by interaction fields that deliver complementary information.\nFinally, path optimization is achieved, enabling self-organized coordination\nwith effective convergence, divergence, and collision avoidance. Our method is\nfully interpretable and ready for deployment without gaps. Comprehensive\nsimulations and real-world experiments demonstrate reduced path redundancy,\nrobust performance across various tasks, and minimal demands on computation and\ncommunication."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Weining Lu"
                    },
                    {
                        "name": "Qingquan Lin"
                    },
                    {
                        "name": "Litong Meng"
                    },
                    {
                        "name": "Haolu Li"
                    },
                    {
                        "name": "Bin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Liang"
                },
                "author": "Bin Liang",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16628v1",
                "updated": "2025-04-23T11:35:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    35,
                    57,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:35:57Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    35,
                    57,
                    2,
                    113,
                    0
                ],
                "title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\n  Models using Pareto High-quality Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\n  Models using Pareto High-quality Data"
                },
                "summary": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks."
                },
                "authors": [
                    {
                        "name": "Haoran Gu"
                    },
                    {
                        "name": "Handing Wang"
                    },
                    {
                        "name": "Yi Mei"
                    },
                    {
                        "name": "Mengjie Zhang"
                    },
                    {
                        "name": "Yaochu Jin"
                    }
                ],
                "author_detail": {
                    "name": "Yaochu Jin"
                },
                "author": "Yaochu Jin",
                "arxiv_comment": "19 pages, 6 figure, Multiobjective Alignment of LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16627v1",
                "updated": "2025-04-23T11:34:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    34,
                    35,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:34:35Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    34,
                    35,
                    2,
                    113,
                    0
                ],
                "title": "TIFIN India at SemEval-2025: Harnessing Translation to Overcome\n  Multilingual IR Challenges in Fact-Checked Claim Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIFIN India at SemEval-2025: Harnessing Translation to Overcome\n  Multilingual IR Challenges in Fact-Checked Claim Retrieval"
                },
                "summary": "We address the challenge of retrieving previously fact-checked claims in\nmonolingual and crosslingual settings - a critical task given the global\nprevalence of disinformation. Our approach follows a two-stage strategy: a\nreliable baseline retrieval system using a fine-tuned embedding model and an\nLLM-based reranker. Our key contribution is demonstrating how LLM-based\ntranslation can overcome the hurdles of multilingual information retrieval.\nAdditionally, we focus on ensuring that the bulk of the pipeline can be\nreplicated on a consumer GPU. Our final integrated system achieved a success@10\nscore of 0.938 and 0.81025 on the monolingual and crosslingual test sets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of retrieving previously fact-checked claims in\nmonolingual and crosslingual settings - a critical task given the global\nprevalence of disinformation. Our approach follows a two-stage strategy: a\nreliable baseline retrieval system using a fine-tuned embedding model and an\nLLM-based reranker. Our key contribution is demonstrating how LLM-based\ntranslation can overcome the hurdles of multilingual information retrieval.\nAdditionally, we focus on ensuring that the bulk of the pipeline can be\nreplicated on a consumer GPU. Our final integrated system achieved a success@10\nscore of 0.938 and 0.81025 on the monolingual and crosslingual test sets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Prasanna Devadiga"
                    },
                    {
                        "name": "Arya Suneesh"
                    },
                    {
                        "name": "Pawan Kumar Rajpoot"
                    },
                    {
                        "name": "Bharatdeep Hazarika"
                    },
                    {
                        "name": "Aditya U Baliga"
                    }
                ],
                "author_detail": {
                    "name": "Aditya U Baliga"
                },
                "author": "Aditya U Baliga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00513v2",
                "updated": "2025-04-23T11:26:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    26,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-01T08:03:40Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    3,
                    40,
                    1,
                    91,
                    0
                ],
                "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset"
                },
                "summary": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "arxiv_doi": "10.1145/3727582.3728689",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3727582.3728689",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.00513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16622v1",
                "updated": "2025-04-23T11:24:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    24,
                    30,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:24:30Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    24,
                    30,
                    2,
                    113,
                    0
                ],
                "title": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial\n  Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial\n  Computing Systems"
                },
                "summary": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion."
                },
                "authors": [
                    {
                        "name": "Christoforus Yoga Haryanto"
                    },
                    {
                        "name": "Emily Lomempow"
                    }
                ],
                "author_detail": {
                    "name": "Emily Lomempow"
                },
                "author": "Emily Lomempow",
                "arxiv_comment": "Working Paper, 37 pages, 1 figure, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16615v1",
                "updated": "2025-04-23T11:00:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    0,
                    17,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:00:17Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    0,
                    17,
                    2,
                    113,
                    0
                ],
                "title": "Algorithmic Mirror: Designing an Interactive Tool to Promote\n  Self-Reflection for YouTube Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Mirror: Designing an Interactive Tool to Promote\n  Self-Reflection for YouTube Recommendations"
                },
                "summary": "Big Data analytics and Artificial Intelligence systems derive non-intuitive\nand often unverifiable inferences about individuals' behaviors, preferences,\nand private lives. Drawing on diverse, feature-rich datasets of unpredictable\nvalue, these systems erode the intuitive connection between our actions and how\nwe are perceived, diminishing control over our digital identities. While\nExplainable Artificial Intelligence scholars have attempted to explain the\ninner workings of algorithms, their visualizations frequently overwhelm\nend-users with complexity. This research introduces 'hypothetical inference', a\nnovel approach that uses language models to simulate how algorithms might\ninterpret users' digital footprints and infer personal characteristics without\nrequiring access to proprietary platform algorithms. Through empirical studies\nwith fourteen adult participants, we identified three key design opportunities\nto foster critical algorithmic literacy: (1) reassembling scattered digital\nfootprints into a unified map, (2) simulating algorithmic inference through\nLLM-generated interpretations, and (3) incorporating temporal dimensions to\nvisualize evolving patterns. This research lays the groundwork for tools that\ncan help users recognize the influence of data on platforms and develop greater\nautonomy in increasingly algorithm-mediated digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Big Data analytics and Artificial Intelligence systems derive non-intuitive\nand often unverifiable inferences about individuals' behaviors, preferences,\nand private lives. Drawing on diverse, feature-rich datasets of unpredictable\nvalue, these systems erode the intuitive connection between our actions and how\nwe are perceived, diminishing control over our digital identities. While\nExplainable Artificial Intelligence scholars have attempted to explain the\ninner workings of algorithms, their visualizations frequently overwhelm\nend-users with complexity. This research introduces 'hypothetical inference', a\nnovel approach that uses language models to simulate how algorithms might\ninterpret users' digital footprints and infer personal characteristics without\nrequiring access to proprietary platform algorithms. Through empirical studies\nwith fourteen adult participants, we identified three key design opportunities\nto foster critical algorithmic literacy: (1) reassembling scattered digital\nfootprints into a unified map, (2) simulating algorithmic inference through\nLLM-generated interpretations, and (3) incorporating temporal dimensions to\nvisualize evolving patterns. This research lays the groundwork for tools that\ncan help users recognize the influence of data on platforms and develop greater\nautonomy in increasingly algorithm-mediated digital environments."
                },
                "authors": [
                    {
                        "name": "Yui Kondo"
                    },
                    {
                        "name": "Kevin Dunnell"
                    },
                    {
                        "name": "Qing Xiao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Luc Rocher"
                    }
                ],
                "author_detail": {
                    "name": "Luc Rocher"
                },
                "author": "Luc Rocher",
                "arxiv_comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM CHI Workshop on Human-AI Interaction\n  for Augmented Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.11094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.11094v2",
                "updated": "2025-04-23T11:00:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    0,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2023-11-18T14:49:04Z",
                "published_parsed": [
                    2023,
                    11,
                    18,
                    14,
                    49,
                    4,
                    5,
                    322,
                    0
                ],
                "title": "Reinforcement Learning With LLMs Interaction For Distributed Diffusion\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning With LLMs Interaction For Distributed Diffusion\n  Model Services"
                },
                "summary": "Distributed Artificial Intelligence-Generated Content (AIGC) has attracted\nsignificant attention, but two key challenges remain: maximizing subjective\nQuality of Experience (QoE) and improving energy efficiency, which are\nparticularly pronounced in widely adopted Generative Diffusion Model\n(GDM)-based image generation services. In this paper, we propose a novel\nuser-centric Interactive AI (IAI) approach for service management, with a\ndistributed GDM-based AIGC framework that emphasizes efficient and cooperative\ndeployment. The proposed method restructures the GDM inference process by\nallowing users with semantically similar prompts to share parts of the\ndenoising chain. Furthermore, to maximize the users' subjective QoE, we propose\nan IAI approach, i.e., Reinforcement Learning With Large Language Models\nInteraction (RLLI), which utilizes Large Language Model (LLM)-empowered\ngenerative agents to replicate user interaction, providing real-time and\nsubjective QoE feedback aligned with diverse user personalities. Lastly, we\npresent the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm,\nadapted to the proposed RLLI framework, to allocate communication and computing\nresources effectively while accounting for subjective user traits and dynamic\nwireless conditions. Simulation results demonstrate that G-DDPG improves total\nQoE by 15% compared with the standard DDPG algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Artificial Intelligence-Generated Content (AIGC) has attracted\nsignificant attention, but two key challenges remain: maximizing subjective\nQuality of Experience (QoE) and improving energy efficiency, which are\nparticularly pronounced in widely adopted Generative Diffusion Model\n(GDM)-based image generation services. In this paper, we propose a novel\nuser-centric Interactive AI (IAI) approach for service management, with a\ndistributed GDM-based AIGC framework that emphasizes efficient and cooperative\ndeployment. The proposed method restructures the GDM inference process by\nallowing users with semantically similar prompts to share parts of the\ndenoising chain. Furthermore, to maximize the users' subjective QoE, we propose\nan IAI approach, i.e., Reinforcement Learning With Large Language Models\nInteraction (RLLI), which utilizes Large Language Model (LLM)-empowered\ngenerative agents to replicate user interaction, providing real-time and\nsubjective QoE feedback aligned with diverse user personalities. Lastly, we\npresent the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm,\nadapted to the proposed RLLI framework, to allocate communication and computing\nresources effectively while accounting for subjective user traits and dynamic\nwireless conditions. Simulation results demonstrate that G-DDPG improves total\nQoE by 15% compared with the standard DDPG algorithm."
                },
                "authors": [
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Xuemin Shen"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.11094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.11094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16613v1",
                "updated": "2025-04-23T10:59:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    59,
                    3,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:59:03Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    59,
                    3,
                    2,
                    113,
                    0
                ],
                "title": "UAV-Mounted IRS (UMI) in the Presence of Hovering Fluctuations: 3D\n  Pattern Characterization and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-Mounted IRS (UMI) in the Presence of Hovering Fluctuations: 3D\n  Pattern Characterization and Performance Analysis"
                },
                "summary": "This paper investigates unmanned aerial vehicle (UAV)-mounted intelligent\nreflecting surfaces (IRS) to leverage the benefits of this technology for\nfuture communication networks, such as 6G. Key advantages include enhanced\nspectral and energy efficiency, expanded network coverage, and flexible\ndeployment. One of the main challenges in employing UAV-mounted IRS (UMI)\ntechnology is the random fluctuations of hovering UAVs. Focusing on this\nchallenge, this paper explores the capabilities of UMI with passive/active\nelements affected by UAV fluctuations in both horizontal and vertical angles,\nconsidering the three-dimensional (3D) radiation pattern of the IRS. The\nrelationship between UAV fluctuations and IRS pattern is investigated by taking\ninto account the random angular vibrations of UAVs. A tractable and closed-form\ndistribution function for the IRS pattern is derived, using linear\napproximation and by dividing it into several sectors. In addition, closed-form\nexpressions for outage probability (OP) are obtained using central limit\ntheorem (CLT) and Gamma approximation. The theoretical expressions are\nvalidated through Monte Carlo simulations. The findings indicate that the\nrandom fluctuations of hovering UAVs have a notable impact on the performance\nof UMI systems. To avoid link interruptions due to UAV instability, IRS should\nutilize fewer elements, even though this leads to a decrease in directivity. As\na result, unlike terrestrial IRS, incorporating more elements into aerial IRS\nsystems does not necessarily improve performance due to the fluctuations in\nUAV. Numerical results show that the OP can be minimized by selecting the\noptimal number of IRS elements and using active elements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates unmanned aerial vehicle (UAV)-mounted intelligent\nreflecting surfaces (IRS) to leverage the benefits of this technology for\nfuture communication networks, such as 6G. Key advantages include enhanced\nspectral and energy efficiency, expanded network coverage, and flexible\ndeployment. One of the main challenges in employing UAV-mounted IRS (UMI)\ntechnology is the random fluctuations of hovering UAVs. Focusing on this\nchallenge, this paper explores the capabilities of UMI with passive/active\nelements affected by UAV fluctuations in both horizontal and vertical angles,\nconsidering the three-dimensional (3D) radiation pattern of the IRS. The\nrelationship between UAV fluctuations and IRS pattern is investigated by taking\ninto account the random angular vibrations of UAVs. A tractable and closed-form\ndistribution function for the IRS pattern is derived, using linear\napproximation and by dividing it into several sectors. In addition, closed-form\nexpressions for outage probability (OP) are obtained using central limit\ntheorem (CLT) and Gamma approximation. The theoretical expressions are\nvalidated through Monte Carlo simulations. The findings indicate that the\nrandom fluctuations of hovering UAVs have a notable impact on the performance\nof UMI systems. To avoid link interruptions due to UAV instability, IRS should\nutilize fewer elements, even though this leads to a decrease in directivity. As\na result, unlike terrestrial IRS, incorporating more elements into aerial IRS\nsystems does not necessarily improve performance due to the fluctuations in\nUAV. Numerical results show that the OP can be minimized by selecting the\noptimal number of IRS elements and using active elements."
                },
                "authors": [
                    {
                        "name": "Mohammad Javad Zakavi"
                    },
                    {
                        "name": "Mahtab Mirmohseni"
                    },
                    {
                        "name": "Farid Ashtiani"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01444v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01444v8",
                "updated": "2025-04-23T10:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    58,
                    38,
                    2,
                    113,
                    0
                ],
                "published": "2023-03-02T18:03:03Z",
                "published_parsed": [
                    2023,
                    3,
                    2,
                    18,
                    3,
                    3,
                    3,
                    61,
                    0
                ],
                "title": "Routine haematological markers can predict and discriminate health\n  status and biological age even from noisy sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routine haematological markers can predict and discriminate health\n  status and biological age even from noisy sources"
                },
                "summary": "For more than two decades, advances in personalised medicine and precision\nhealthcare have largely been based on genomics and other omics data. These\nstrategies aim to tailor interventions to individual patient profiles,\npromising greater treatment efficacy and more efficient allocation of\nhealthcare resources. Here, we show that widely collected common hematologic\nmarkers can reliably predict and discriminate individual chronological age and\nhealth status from even noisy sources. Our analysis includes synthetic and real\nretrospective patient data, including medically relevant and extreme cases, and\ndraws on more than 100\\,000 complete blood count records over 13 years from the\nUnited States Centers for Disease Control and Prevention's National Health and\nNutrition Examination Survey (CDC NHANES). We combine fully explainable risk\nassessment scores with machine and deep learning techniques to focus on\nclinically significant characteristics without functioning purely as a\n``black-box model allowing interpretation and control. Unlike current\nbiological ageing indicators, this approach may offer rapid, and scalable\nimplementations of personalised, precision and predictive approaches to\nhealthcare and medicine without requiring specialised, uncommon or costly\ndeployment of tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For more than two decades, advances in personalised medicine and precision\nhealthcare have largely been based on genomics and other omics data. These\nstrategies aim to tailor interventions to individual patient profiles,\npromising greater treatment efficacy and more efficient allocation of\nhealthcare resources. Here, we show that widely collected common hematologic\nmarkers can reliably predict and discriminate individual chronological age and\nhealth status from even noisy sources. Our analysis includes synthetic and real\nretrospective patient data, including medically relevant and extreme cases, and\ndraws on more than 100\\,000 complete blood count records over 13 years from the\nUnited States Centers for Disease Control and Prevention's National Health and\nNutrition Examination Survey (CDC NHANES). We combine fully explainable risk\nassessment scores with machine and deep learning techniques to focus on\nclinically significant characteristics without functioning purely as a\n``black-box model allowing interpretation and control. Unlike current\nbiological ageing indicators, this approach may offer rapid, and scalable\nimplementations of personalised, precision and predictive approaches to\nhealthcare and medicine without requiring specialised, uncommon or costly\ndeployment of tests."
                },
                "authors": [
                    {
                        "name": "Santiago Hern√°ndez-Orozco"
                    },
                    {
                        "name": "Abicumaran Uthamacumaran"
                    },
                    {
                        "name": "Francisco Hern√°ndez-Quiroz"
                    },
                    {
                        "name": "Kourosh Saeb-Parsy"
                    },
                    {
                        "name": "Hector Zenil"
                    }
                ],
                "author_detail": {
                    "name": "Hector Zenil"
                },
                "author": "Hector Zenil",
                "arxiv_comment": "40 pages + appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.01444v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01444v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16604v1",
                "updated": "2025-04-23T10:32:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    32,
                    45,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:32:45Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    32,
                    45,
                    2,
                    113,
                    0
                ],
                "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to\n  Challenge Conspiracy Theories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to\n  Challenge Conspiracy Theories"
                },
                "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic."
                },
                "authors": [
                    {
                        "name": "Mareike Lisker"
                    },
                    {
                        "name": "Christina Gottschalk"
                    },
                    {
                        "name": "Helena Mihaljeviƒá"
                    }
                ],
                "author_detail": {
                    "name": "Helena Mihaljeviƒá"
                },
                "author": "Helena Mihaljeviƒá",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16601v1",
                "updated": "2025-04-23T10:31:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    31,
                    33,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:31:33Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    31,
                    33,
                    2,
                    113,
                    0
                ],
                "title": "Comparing Large Language Models and Traditional Machine Translation\n  Tools for Translating Medical Consultation Summaries: A Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Large Language Models and Traditional Machine Translation\n  Tools for Translating Medical Consultation Summaries: A Pilot Study"
                },
                "summary": "This study evaluates how well large language models (LLMs) and traditional\nmachine translation (MT) tools translate medical consultation summaries from\nEnglish into Arabic, Chinese, and Vietnamese. It assesses both patient,\nfriendly and clinician, focused texts using standard automated metrics. Results\nshowed that traditional MT tools generally performed better, especially for\ncomplex texts, while LLMs showed promise, particularly in Vietnamese and\nChinese, when translating simpler summaries. Arabic translations improved with\ncomplexity due to the language's morphology. Overall, while LLMs offer\ncontextual flexibility, they remain inconsistent, and current evaluation\nmetrics fail to capture clinical relevance. The study highlights the need for\ndomain-specific training, improved evaluation methods, and human oversight in\nmedical translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates how well large language models (LLMs) and traditional\nmachine translation (MT) tools translate medical consultation summaries from\nEnglish into Arabic, Chinese, and Vietnamese. It assesses both patient,\nfriendly and clinician, focused texts using standard automated metrics. Results\nshowed that traditional MT tools generally performed better, especially for\ncomplex texts, while LLMs showed promise, particularly in Vietnamese and\nChinese, when translating simpler summaries. Arabic translations improved with\ncomplexity due to the language's morphology. Overall, while LLMs offer\ncontextual flexibility, they remain inconsistent, and current evaluation\nmetrics fail to capture clinical relevance. The study highlights the need for\ndomain-specific training, improved evaluation methods, and human oversight in\nmedical translation."
                },
                "authors": [
                    {
                        "name": "Andy Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Rashina Hoda"
                    },
                    {
                        "name": "Chris Bain"
                    },
                    {
                        "name": "Peter Poon"
                    }
                ],
                "author_detail": {
                    "name": "Peter Poon"
                },
                "author": "Peter Poon",
                "arxiv_comment": "8 pages, 2 tables and 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00530v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00530v5",
                "updated": "2025-04-23T10:26:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    26,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2023-11-01T14:08:56Z",
                "published_parsed": [
                    2023,
                    11,
                    1,
                    14,
                    8,
                    56,
                    2,
                    305,
                    0
                ],
                "title": "Advances in Embodied Navigation Using Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Embodied Navigation Using Large Language Models: A Survey"
                },
                "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN."
                },
                "authors": [
                    {
                        "name": "Jinzhou Lin"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Xuxiang Feng"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Man Zhang"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "arxiv_comment": "Submited to IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS:\n  SYSTEMS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.00530v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00530v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13460v2",
                "updated": "2025-04-23T10:12:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    12,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T04:35:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization"
                },
                "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark."
                },
                "authors": [
                    {
                        "name": "Hongwei Ji"
                    },
                    {
                        "name": "Wulian Yun"
                    },
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06601v3",
                "updated": "2025-04-24T03:29:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    29,
                    59,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-10T15:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "title": "LaMsS: When Large Language Models Meet Self-Skepticism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMsS: When Large Language Models Meet Self-Skepticism"
                },
                "summary": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76."
                },
                "authors": [
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 6 figures, Published at ICLR 2025 Workshop on Scaling\n  Self-Improving Foundation Models,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16584v1",
                "updated": "2025-04-23T10:05:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    5,
                    27,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T10:05:27Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    5,
                    27,
                    2,
                    113,
                    0
                ],
                "title": "Case Study: Fine-tuning Small Language Models for Accurate and Private\n  CWE Detection in Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Study: Fine-tuning Small Language Models for Accurate and Private\n  CWE Detection in Python Code"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and analyzing code for security vulnerabilities, such as Common\nWeakness Enumerations (CWEs). However, their reliance on cloud infrastructure\nand substantial computational requirements pose challenges for analyzing\nsensitive or proprietary codebases due to privacy concerns and inference costs.\nThis work explores the potential of Small Language Models (SLMs) as a viable\nalternative for accurate, on-premise vulnerability detection. We investigated\nwhether a 350-million parameter pre-trained code model (codegen-mono) could be\neffectively fine-tuned to detect the MITRE Top 25 CWEs specifically within\nPython code. To facilitate this, we developed a targeted dataset of 500\nexamples using a semi-supervised approach involving LLM-driven synthetic data\ngeneration coupled with meticulous human review. Initial tests confirmed that\nthe base codegen-mono model completely failed to identify CWEs in our samples.\nHowever, after applying instruction-following fine-tuning, the specialized SLM\nachieved remarkable performance on our test set, yielding approximately 99%\naccuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results\nstrongly suggest that fine-tuned SLMs can serve as highly accurate and\nefficient tools for CWE detection, offering a practical and privacy-preserving\nsolution for integrating advanced security analysis directly into development\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and analyzing code for security vulnerabilities, such as Common\nWeakness Enumerations (CWEs). However, their reliance on cloud infrastructure\nand substantial computational requirements pose challenges for analyzing\nsensitive or proprietary codebases due to privacy concerns and inference costs.\nThis work explores the potential of Small Language Models (SLMs) as a viable\nalternative for accurate, on-premise vulnerability detection. We investigated\nwhether a 350-million parameter pre-trained code model (codegen-mono) could be\neffectively fine-tuned to detect the MITRE Top 25 CWEs specifically within\nPython code. To facilitate this, we developed a targeted dataset of 500\nexamples using a semi-supervised approach involving LLM-driven synthetic data\ngeneration coupled with meticulous human review. Initial tests confirmed that\nthe base codegen-mono model completely failed to identify CWEs in our samples.\nHowever, after applying instruction-following fine-tuning, the specialized SLM\nachieved remarkable performance on our test set, yielding approximately 99%\naccuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results\nstrongly suggest that fine-tuned SLMs can serve as highly accurate and\nefficient tools for CWE detection, offering a practical and privacy-preserving\nsolution for integrating advanced security analysis directly into development\nworkflows."
                },
                "authors": [
                    {
                        "name": "Md. Azizul Hakim Bappy"
                    },
                    {
                        "name": "Hossen A Mustafa"
                    },
                    {
                        "name": "Prottoy Saha"
                    },
                    {
                        "name": "Rajinus Salehat"
                    }
                ],
                "author_detail": {
                    "name": "Rajinus Salehat"
                },
                "arxiv_affiliation": "Hajee Mohammad Danesh Science and Technology University, Dinajpur, Bangladesh",
                "author": "Rajinus Salehat",
                "arxiv_comment": "11 pages, 2 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/floxihunter/synthetic_python_cwe. Model\n  available at https://huggingface.co/floxihunter/codegen-mono-CWEdetect.\n  Keywords: Small Language Models (SLMs), Vulnerability Detection, CWE,\n  Fine-tuning, Python Security, Privacy-Preserving Code Analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16005v2",
                "updated": "2025-04-23T09:59:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    59,
                    31,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T16:14:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    14,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "CAPO: Cost-Aware Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Cost-Aware Prompt Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Timo Hei√ü"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer",
                "arxiv_comment": "Submitted to AutoML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16574v1",
                "updated": "2025-04-23T09:53:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    53,
                    1,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:53:01Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    53,
                    1,
                    2,
                    113,
                    0
                ],
                "title": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient\n  Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient\n  Prompt Compression"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress, demonstrating\nunprecedented capabilities across various natural language processing tasks.\nHowever, the high costs associated with such exceptional performance limit the\nwidespread adoption of LLMs, highlighting the need for prompt compression.\nExisting prompt compression methods primarily rely on heuristic truncation or\nabstractive summarization techniques, which fundamentally overlook the\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\nimportance for generation. In this work, we introduce Prompt Importance\nSampling (PIS), a novel compression framework that dynamically compresses\nprompts by sampling important tokens based on the analysis of attention scores\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\ntoken level, we quantify saliency using LLM-native attention scores and\nimplement adaptive compression through a lightweight 9-layer reinforcement\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\nsampling strategy for sentence-level importance sampling. Comprehensive\nevaluations across multiple domain benchmarks demonstrate that our method\nachieves state-of-the-art compression performance. Notably, our framework\nserendipitously enhances reasoning efficiency through optimized context\nstructuring. This work advances prompt engineering by offering both theoretical\ngrounding and practical efficiency in context management for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress, demonstrating\nunprecedented capabilities across various natural language processing tasks.\nHowever, the high costs associated with such exceptional performance limit the\nwidespread adoption of LLMs, highlighting the need for prompt compression.\nExisting prompt compression methods primarily rely on heuristic truncation or\nabstractive summarization techniques, which fundamentally overlook the\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\nimportance for generation. In this work, we introduce Prompt Importance\nSampling (PIS), a novel compression framework that dynamically compresses\nprompts by sampling important tokens based on the analysis of attention scores\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\ntoken level, we quantify saliency using LLM-native attention scores and\nimplement adaptive compression through a lightweight 9-layer reinforcement\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\nsampling strategy for sentence-level importance sampling. Comprehensive\nevaluations across multiple domain benchmarks demonstrate that our method\nachieves state-of-the-art compression performance. Notably, our framework\nserendipitously enhances reasoning efficiency through optimized context\nstructuring. This work advances prompt engineering by offering both theoretical\ngrounding and practical efficiency in context management for LLMs."
                },
                "authors": [
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Binjia Zhou"
                    },
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Shiguang NI"
                    }
                ],
                "author_detail": {
                    "name": "Shiguang NI"
                },
                "author": "Shiguang NI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16573v1",
                "updated": "2025-04-23T09:49:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    49,
                    5,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:49:05Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    49,
                    5,
                    2,
                    113,
                    0
                ],
                "title": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling\n  Assistant System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling\n  Assistant System"
                },
                "summary": "Psychological counseling is a highly personalized and dynamic process that\nrequires therapists to continuously monitor emotional changes, document session\ninsights, and maintain therapeutic continuity. In this paper, we introduce\nPsyCounAssist, a comprehensive AI-powered counseling assistant system\nspecifically designed to augment psychological counseling practices.\nPsyCounAssist integrates multimodal emotion recognition combining speech and\nphotoplethysmography (PPG) signals for accurate real-time affective analysis,\nautomated structured session reporting using large language models (LLMs), and\npersonalized AI-generated follow-up support. Deployed on Android-based tablet\ndevices, the system demonstrates practical applicability and flexibility in\nreal-world counseling scenarios. Experimental evaluation confirms the\nreliability of PPG-based emotional classification and highlights the system's\npotential for non-intrusive, privacy-aware emotional support. PsyCounAssist\nrepresents a novel approach to ethically and effectively integrating AI into\npsychological counseling workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological counseling is a highly personalized and dynamic process that\nrequires therapists to continuously monitor emotional changes, document session\ninsights, and maintain therapeutic continuity. In this paper, we introduce\nPsyCounAssist, a comprehensive AI-powered counseling assistant system\nspecifically designed to augment psychological counseling practices.\nPsyCounAssist integrates multimodal emotion recognition combining speech and\nphotoplethysmography (PPG) signals for accurate real-time affective analysis,\nautomated structured session reporting using large language models (LLMs), and\npersonalized AI-generated follow-up support. Deployed on Android-based tablet\ndevices, the system demonstrates practical applicability and flexibility in\nreal-world counseling scenarios. Experimental evaluation confirms the\nreliability of PPG-based emotional classification and highlights the system's\npotential for non-intrusive, privacy-aware emotional support. PsyCounAssist\nrepresents a novel approach to ethically and effectively integrating AI into\npsychological counseling workflows."
                },
                "authors": [
                    {
                        "name": "Xianghe Liu"
                    },
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Tao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Tao Sun"
                },
                "author": "Tao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16563v1",
                "updated": "2025-04-23T09:43:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    43,
                    40,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:43:40Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    43,
                    40,
                    2,
                    113,
                    0
                ],
                "title": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution"
                },
                "summary": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Jingli Yang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16552v1",
                "updated": "2025-04-23T09:28:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    28,
                    9,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:28:09Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    28,
                    9,
                    2,
                    113,
                    0
                ],
                "title": "DTVM: Revolutionizing Smart Contract Execution with Determinism and\n  Compatibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTVM: Revolutionizing Smart Contract Execution with Determinism and\n  Compatibility"
                },
                "summary": "We introduce the DeTerministic Virtual Machine (DTVM) Stack, a\nnext-generation smart contract execution framework designed to address critical\nperformance, determinism, and ecosystem compatibility challenges in blockchain\nnetworks. Building upon WebAssembly (Wasm) while maintaining full Ethereum\nVirtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle\nIntermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to\nbalance compilation speed and execution efficiency. DTVM further accommodates\ndiverse instruction set architectures (e.g., EVM, RISC-V) through modular\nadaptation layers. This enables seamless integration with DTVM's hybrid\nlazy-JIT compilation engine, which dynamically optimizes performance while\npreserving deterministic execution guarantees across heterogeneous\nenvironments. The key contributions including: 1). The framework achieves up to\n2$\\times$ acceleration over evmone in dominant Ethereum contract (e.g.\nERC20/721/1155) execution and reduces fibonacci computation latency by\n11.8$\\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch\nmechanism enables sub-millisecond (0.95ms) post-deployment invocation times,\noutperforming up to about 23$\\times$ in compilation and invocation efficiency.\n3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and\nAssemblyScript) through unified bytecode conversion while maintaining EVM ABI\ncompatibility for seamless invocation. It reduces machine code object sizes by\n30.0$\\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers\nSmartCogent, an AI-driven full-stack development experience, leveraging\nfine-tuned LLMs and retrieval-augmented generation to automate tasks across the\nsmart contract lifecycle: development, debugging, security auditing, and\ndeployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the DeTerministic Virtual Machine (DTVM) Stack, a\nnext-generation smart contract execution framework designed to address critical\nperformance, determinism, and ecosystem compatibility challenges in blockchain\nnetworks. Building upon WebAssembly (Wasm) while maintaining full Ethereum\nVirtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle\nIntermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to\nbalance compilation speed and execution efficiency. DTVM further accommodates\ndiverse instruction set architectures (e.g., EVM, RISC-V) through modular\nadaptation layers. This enables seamless integration with DTVM's hybrid\nlazy-JIT compilation engine, which dynamically optimizes performance while\npreserving deterministic execution guarantees across heterogeneous\nenvironments. The key contributions including: 1). The framework achieves up to\n2$\\times$ acceleration over evmone in dominant Ethereum contract (e.g.\nERC20/721/1155) execution and reduces fibonacci computation latency by\n11.8$\\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch\nmechanism enables sub-millisecond (0.95ms) post-deployment invocation times,\noutperforming up to about 23$\\times$ in compilation and invocation efficiency.\n3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and\nAssemblyScript) through unified bytecode conversion while maintaining EVM ABI\ncompatibility for seamless invocation. It reduces machine code object sizes by\n30.0$\\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers\nSmartCogent, an AI-driven full-stack development experience, leveraging\nfine-tuned LLMs and retrieval-augmented generation to automate tasks across the\nsmart contract lifecycle: development, debugging, security auditing, and\ndeployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack)."
                },
                "authors": [
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Changzheng Wei"
                    },
                    {
                        "name": "Ying Yan"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Zhihao Chen"
                    },
                    {
                        "name": "Xiong Xu"
                    },
                    {
                        "name": "Xuebing Huang"
                    },
                    {
                        "name": "Wengang Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Xiaofu Zheng"
                    },
                    {
                        "name": "Hanghang Wu"
                    },
                    {
                        "name": "Shenglong Chen"
                    },
                    {
                        "name": "Ermei Wang"
                    },
                    {
                        "name": "Xiangfei Chen"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Meng Wu"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Liwei Yuan"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Alex Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ji Luo"
                    },
                    {
                        "name": "Zhengyu He"
                    },
                    {
                        "name": "Wenbiao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wenbiao Zhao"
                },
                "author": "Wenbiao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16550v1",
                "updated": "2025-04-23T09:25:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:25:52Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    52,
                    2,
                    113,
                    0
                ],
                "title": "A Collaborative Intrusion Detection System Using Snort IDS Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Collaborative Intrusion Detection System Using Snort IDS Nodes"
                },
                "summary": "Intrusion Detection Systems (IDSs) are integral to safeguarding networks by\ndetecting and responding to threats from malicious traffic or compromised\ndevices. However, standalone IDS deployments often fall short when addressing\nthe increasing complexity and scale of modern cyberattacks. This paper proposes\na Collaborative Intrusion Detection System (CIDS) that leverages Snort, an\nopen-source network intrusion detection system, to enhance detection accuracy\nand reduce false positives. The proposed architecture connects multiple Snort\nIDS nodes to a centralised node and integrates with a Security Information and\nEvent Management (SIEM) platform to facilitate real-time data sharing,\ncorrelation, and analysis. The CIDS design includes a scalable configuration of\nSnort sensors, a centralised database for log storage, and LogScale SIEM for\nadvanced analytics and visualisation. By aggregating and analysing intrusion\ndata from multiple nodes, the system enables improved detection of distributed\nand sophisticated attack patterns that standalone IDSs may miss. Performance\nevaluation against simulated attacks, including Nmap port scans and ICMP flood\nattacks, demonstrates our CIDS's ability to efficiently process large-scale\nnetwork traffic, detect threats with higher accuracy, and reduce alert fatigue.\nThis paper highlights the potential of CIDS in modern network environments and\nexplores future enhancements, such as integrating machine learning for advanced\nthreat detection and creating public datasets to support collaborative\nresearch. The proposed CIDS framework provides a promising foundation for\nbuilding more resilient and adaptive network security systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection Systems (IDSs) are integral to safeguarding networks by\ndetecting and responding to threats from malicious traffic or compromised\ndevices. However, standalone IDS deployments often fall short when addressing\nthe increasing complexity and scale of modern cyberattacks. This paper proposes\na Collaborative Intrusion Detection System (CIDS) that leverages Snort, an\nopen-source network intrusion detection system, to enhance detection accuracy\nand reduce false positives. The proposed architecture connects multiple Snort\nIDS nodes to a centralised node and integrates with a Security Information and\nEvent Management (SIEM) platform to facilitate real-time data sharing,\ncorrelation, and analysis. The CIDS design includes a scalable configuration of\nSnort sensors, a centralised database for log storage, and LogScale SIEM for\nadvanced analytics and visualisation. By aggregating and analysing intrusion\ndata from multiple nodes, the system enables improved detection of distributed\nand sophisticated attack patterns that standalone IDSs may miss. Performance\nevaluation against simulated attacks, including Nmap port scans and ICMP flood\nattacks, demonstrates our CIDS's ability to efficiently process large-scale\nnetwork traffic, detect threats with higher accuracy, and reduce alert fatigue.\nThis paper highlights the potential of CIDS in modern network environments and\nexplores future enhancements, such as integrating machine learning for advanced\nthreat detection and creating public datasets to support collaborative\nresearch. The proposed CIDS framework provides a promising foundation for\nbuilding more resilient and adaptive network security systems."
                },
                "authors": [
                    {
                        "name": "Tom Davies"
                    },
                    {
                        "name": "Max Hashem Eiza"
                    },
                    {
                        "name": "Nathan Shone"
                    },
                    {
                        "name": "Rob Lyon"
                    }
                ],
                "author_detail": {
                    "name": "Rob Lyon"
                },
                "author": "Rob Lyon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16548v1",
                "updated": "2025-04-23T09:25:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    22,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:25:22Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    22,
                    2,
                    113,
                    0
                ],
                "title": "Exploring human-SAV interaction using large language models: The impact\n  of psychological ownership and anthropomorphism on user experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring human-SAV interaction using large language models: The impact\n  of psychological ownership and anthropomorphism on user experience"
                },
                "summary": "There has been extensive prior work exploring how psychological factors such\nas anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).\nHowever, limited research has been conducted on how prompt strategies in large\nlanguage model (LLM)-powered SAV User Interfaces (UIs) affect users'\nperceptions, experiences, and intentions to adopt such technology. In this\nwork, we investigate how conversational UIs powered by LLMs drive these\npsychological factors and psychological ownership, the sense of possession a\nuser may come to feel towards an entity or object they may not legally own. We\ndesigned four SAV UIs with varying levels of anthropomorphic characteristics\nand psychological ownership triggers. Quantitative measures of psychological\nownership, anthropomorphism, quality of service, disclosure tendency, sentiment\nof SAV responses, and overall acceptance were collected after participants\ninteracted with each SAV. Qualitative feedback was also gathered regarding the\nexperience of psychological ownership during the interactions. The results\nindicate that an SAV conversational UI designed to be more anthropomorphic and\nto induce psychological ownership improved users' perceptions of the SAV's\nhuman-like qualities and improved the sentiment of responses compared to a\ncontrol condition. These findings provide practical guidance for designing\nLLM-based conversational UIs that enhance user experience and adoption of SAVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been extensive prior work exploring how psychological factors such\nas anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).\nHowever, limited research has been conducted on how prompt strategies in large\nlanguage model (LLM)-powered SAV User Interfaces (UIs) affect users'\nperceptions, experiences, and intentions to adopt such technology. In this\nwork, we investigate how conversational UIs powered by LLMs drive these\npsychological factors and psychological ownership, the sense of possession a\nuser may come to feel towards an entity or object they may not legally own. We\ndesigned four SAV UIs with varying levels of anthropomorphic characteristics\nand psychological ownership triggers. Quantitative measures of psychological\nownership, anthropomorphism, quality of service, disclosure tendency, sentiment\nof SAV responses, and overall acceptance were collected after participants\ninteracted with each SAV. Qualitative feedback was also gathered regarding the\nexperience of psychological ownership during the interactions. The results\nindicate that an SAV conversational UI designed to be more anthropomorphic and\nto induce psychological ownership improved users' perceptions of the SAV's\nhuman-like qualities and improved the sentiment of responses compared to a\ncontrol condition. These findings provide practical guidance for designing\nLLM-based conversational UIs that enhance user experience and adoption of SAVs."
                },
                "authors": [
                    {
                        "name": "Lirui Guo"
                    },
                    {
                        "name": "Michael G. Burke"
                    },
                    {
                        "name": "Wynita M. Griggs"
                    }
                ],
                "author_detail": {
                    "name": "Wynita M. Griggs"
                },
                "author": "Wynita M. Griggs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11449v2",
                "updated": "2025-04-23T09:25:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    25,
                    9,
                    2,
                    113,
                    0
                ],
                "published": "2024-11-18T10:31:24Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    31,
                    24,
                    0,
                    323,
                    0
                ],
                "title": "Better Together? The Role of Explanations in Supporting Novices in\n  Individual and Collective Deliberations about AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Together? The Role of Explanations in Supporting Novices in\n  Individual and Collective Deliberations about AI"
                },
                "summary": "Deploying AI systems in public institutions can have far-reaching\nconsequences for many people, making it a matter of public interest. Providing\nopportunities for stakeholders to come together, understand these systems, and\ndebate their merits and harms is thus essential. Explainable AI often focuses\non individuals, but deliberation benefits from group settings, which are\nunderexplored. To address this gap, we present findings from an interview study\nwith 8 focus groups and 12 individuals. Our findings provide insight into how\nexplanations support AI novices in deliberating alone and in groups.\nParticipants used modular explanations with four information categories to\nsolve tasks and decide about an AI system's deployment. We found that the\nexplanations supported groups in creating shared understanding and in finding\narguments for and against the system's deployment. In comparison, individual\nparticipants engaged with explanations in more depth and performed better in\nthe study tasks, but missed an exchange with others. Based on our findings, we\nprovide suggestions on how explanations should be designed to work in group\nsettings and describe their potential use in real-world contexts. With this,\nour contributions inform XAI research that aims to enable AI novices to\nunderstand and deliberate AI systems in the public sector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying AI systems in public institutions can have far-reaching\nconsequences for many people, making it a matter of public interest. Providing\nopportunities for stakeholders to come together, understand these systems, and\ndebate their merits and harms is thus essential. Explainable AI often focuses\non individuals, but deliberation benefits from group settings, which are\nunderexplored. To address this gap, we present findings from an interview study\nwith 8 focus groups and 12 individuals. Our findings provide insight into how\nexplanations support AI novices in deliberating alone and in groups.\nParticipants used modular explanations with four information categories to\nsolve tasks and decide about an AI system's deployment. We found that the\nexplanations supported groups in creating shared understanding and in finding\narguments for and against the system's deployment. In comparison, individual\nparticipants engaged with explanations in more depth and performed better in\nthe study tasks, but missed an exchange with others. Based on our findings, we\nprovide suggestions on how explanations should be designed to work in group\nsettings and describe their potential use in real-world contexts. With this,\nour contributions inform XAI research that aims to enable AI novices to\nunderstand and deliberate AI systems in the public sector."
                },
                "authors": [
                    {
                        "name": "Timoth√©e Schmude"
                    },
                    {
                        "name": "Laura Koesten"
                    },
                    {
                        "name": "Torsten M√∂ller"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Tschiatschek"
                },
                "author": "Sebastian Tschiatschek",
                "arxiv_comment": "30 pages main text, 8 figures, 4 tables. Supplementary material is\n  included in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16538v1",
                "updated": "2025-04-23T09:08:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    8,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T09:08:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    8,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language\n  Assessment and Mapping of Urban Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language\n  Assessment and Mapping of Urban Scenes"
                },
                "summary": "Streetscapes are an essential component of urban space. Their assessment is\npresently either limited to morphometric properties of their mass skeleton or\nrequires labor-intensive qualitative evaluations of visually perceived\nqualities. This paper introduces SAGAI: Streetscape Analysis with Generative\nArtificial Intelligence, a modular workflow for scoring street-level urban\nscenes using open-access data and vision-language models. SAGAI integrates\nOpenStreetMap geometries, Google Street View imagery, and a lightweight version\nof the LLaVA model to generate structured spatial indicators from images via\ncustomizable natural language prompts. The pipeline includes an automated\nmapping module that aggregates visual scores at both the point and street\nlevels, enabling direct cartographic interpretation. It operates without\ntask-specific training or proprietary software dependencies, supporting\nscalable and interpretable analysis of urban environments. Two exploratory case\nstudies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial\noutputs from vision-language inference. The initial results show strong\nperformance for binary urban-rural scene classification, moderate precision in\ncommercial feature detection, and lower estimates, but still informative, of\nsidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a\nwide range of urban research themes, such as walkability, safety, or urban\ndesign, through prompt modification alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streetscapes are an essential component of urban space. Their assessment is\npresently either limited to morphometric properties of their mass skeleton or\nrequires labor-intensive qualitative evaluations of visually perceived\nqualities. This paper introduces SAGAI: Streetscape Analysis with Generative\nArtificial Intelligence, a modular workflow for scoring street-level urban\nscenes using open-access data and vision-language models. SAGAI integrates\nOpenStreetMap geometries, Google Street View imagery, and a lightweight version\nof the LLaVA model to generate structured spatial indicators from images via\ncustomizable natural language prompts. The pipeline includes an automated\nmapping module that aggregates visual scores at both the point and street\nlevels, enabling direct cartographic interpretation. It operates without\ntask-specific training or proprietary software dependencies, supporting\nscalable and interpretable analysis of urban environments. Two exploratory case\nstudies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial\noutputs from vision-language inference. The initial results show strong\nperformance for binary urban-rural scene classification, moderate precision in\ncommercial feature detection, and lower estimates, but still informative, of\nsidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a\nwide range of urban research themes, such as walkability, safety, or urban\ndesign, through prompt modification alone."
                },
                "authors": [
                    {
                        "name": "Joan Perez"
                    },
                    {
                        "name": "Giovanni Fusco"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Fusco"
                },
                "arxiv_affiliation": "Universite Cote-Azur-CNRS-AMU-Avignon Universite, ESPACE, France",
                "author": "Giovanni Fusco",
                "arxiv_comment": "25 pages, 6 figures in main paper, 6 figures in appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15222v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15222v4",
                "updated": "2025-04-23T09:05:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    5,
                    22,
                    2,
                    113,
                    0
                ],
                "published": "2024-06-14T02:15:09Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    2,
                    15,
                    9,
                    4,
                    166,
                    0
                ],
                "title": "A Deep Learning System for Rapid and Accurate Warning of Acute Aortic\n  Syndrome on Non-contrast CT in China",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Learning System for Rapid and Accurate Warning of Acute Aortic\n  Syndrome on Non-contrast CT in China"
                },
                "summary": "The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients\npresenting with acute chest pain remains a clinical challenge. Aortic CT\nangiography (CTA) is the imaging protocol of choice in patients with suspected\nAAS. However, due to economic and workflow constraints in China, the majority\nof suspected patients initially undergo non-contrast CT as the initial imaging\ntesting, and CTA is reserved for those at higher risk. In this work, we present\nan artificial intelligence-based warning system, iAorta, using non-contrast CT\nfor AAS identification in China, which demonstrates remarkably high accuracy\nand provides clinicians with interpretable warnings. iAorta was evaluated\nthrough a comprehensive step-wise study. In the multi-center retrospective\nstudy (n = 20,750), iAorta achieved a mean area under the receiver operating\ncurve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study\n(n = 137,525), iAorta demonstrated consistently high performance across various\nnon-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a\nspecificity of 0.991-0.993. In the prospective comparative study (n = 13,846),\niAorta demonstrated the capability to significantly shorten the time to correct\ndiagnostic pathway. For the prospective pilot deployment that we conducted,\niAorta correctly identified 21 out of 22 patients with AAS among 15,584\nconsecutive patients presenting with acute chest pain and under non-contrast CT\nprotocol in the emergency department (ED) and enabled the average diagnostic\ntime of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the\niAorta can help avoid delayed or missed diagnosis of AAS in settings where\nnon-contrast CT remains the unavoidable the initial or only imaging test in\nresource-constrained regions and in patients who cannot or did not receive\nintravenous contrast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients\npresenting with acute chest pain remains a clinical challenge. Aortic CT\nangiography (CTA) is the imaging protocol of choice in patients with suspected\nAAS. However, due to economic and workflow constraints in China, the majority\nof suspected patients initially undergo non-contrast CT as the initial imaging\ntesting, and CTA is reserved for those at higher risk. In this work, we present\nan artificial intelligence-based warning system, iAorta, using non-contrast CT\nfor AAS identification in China, which demonstrates remarkably high accuracy\nand provides clinicians with interpretable warnings. iAorta was evaluated\nthrough a comprehensive step-wise study. In the multi-center retrospective\nstudy (n = 20,750), iAorta achieved a mean area under the receiver operating\ncurve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study\n(n = 137,525), iAorta demonstrated consistently high performance across various\nnon-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a\nspecificity of 0.991-0.993. In the prospective comparative study (n = 13,846),\niAorta demonstrated the capability to significantly shorten the time to correct\ndiagnostic pathway. For the prospective pilot deployment that we conducted,\niAorta correctly identified 21 out of 22 patients with AAS among 15,584\nconsecutive patients presenting with acute chest pain and under non-contrast CT\nprotocol in the emergency department (ED) and enabled the average diagnostic\ntime of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the\niAorta can help avoid delayed or missed diagnosis of AAS in settings where\nnon-contrast CT remains the unavoidable the initial or only imaging test in\nresource-constrained regions and in patients who cannot or did not receive\nintravenous contrast."
                },
                "authors": [
                    {
                        "name": "Yujian Hu"
                    },
                    {
                        "name": "Yilang Xiang"
                    },
                    {
                        "name": "Yan-Jie Zhou"
                    },
                    {
                        "name": "Yangyan He"
                    },
                    {
                        "name": "Dehai Lang"
                    },
                    {
                        "name": "Shifeng Yang"
                    },
                    {
                        "name": "Xiaolong Du"
                    },
                    {
                        "name": "Chunlan Den"
                    },
                    {
                        "name": "Youyao Xu"
                    },
                    {
                        "name": "Gaofeng Wang"
                    },
                    {
                        "name": "Zhengyao Ding"
                    },
                    {
                        "name": "Jingyong Huang"
                    },
                    {
                        "name": "Wenjun Zhao"
                    },
                    {
                        "name": "Xuejun Wu"
                    },
                    {
                        "name": "Donglin Li"
                    },
                    {
                        "name": "Qianqian Zhu"
                    },
                    {
                        "name": "Zhenjiang Li"
                    },
                    {
                        "name": "Chenyang Qiu"
                    },
                    {
                        "name": "Ziheng Wu"
                    },
                    {
                        "name": "Yunjun He"
                    },
                    {
                        "name": "Chen Tian"
                    },
                    {
                        "name": "Yihui Qiu"
                    },
                    {
                        "name": "Zuodong Lin"
                    },
                    {
                        "name": "Xiaolong Zhang"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Zhenpeng Yuan"
                    },
                    {
                        "name": "Xiaoxiang Zhou"
                    },
                    {
                        "name": "Rong Fan"
                    },
                    {
                        "name": "Ruihan Chen"
                    },
                    {
                        "name": "Wenchao Guo"
                    },
                    {
                        "name": "Jianpeng Zhang"
                    },
                    {
                        "name": "Tony C. W. Mok"
                    },
                    {
                        "name": "Zi Li"
                    },
                    {
                        "name": "Mannudeep K. Kalra"
                    },
                    {
                        "name": "Le Lu"
                    },
                    {
                        "name": "Wenbo Xiao"
                    },
                    {
                        "name": "Xiaoqiang Li"
                    },
                    {
                        "name": "Yun Bian"
                    },
                    {
                        "name": "Chengwei Shao"
                    },
                    {
                        "name": "Guofu Wang"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Zhengxing Huang"
                    },
                    {
                        "name": "Minfeng Xu"
                    },
                    {
                        "name": "Hongkun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongkun Zhang"
                },
                "author": "Hongkun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15222v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15222v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12589v2",
                "updated": "2025-04-23T08:57:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    57,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-22T02:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    29,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network"
                },
                "summary": "The deployment of LoRa networks necessitates joint performance optimization,\nincluding packet delivery rate, energy efficiency, and throughput.\nAdditionally, multiple LoRa parameters for packet transmission must be\ndynamically configured to tailor the performance metrics prioritization across\nvarying channel environments. Because of the coupling relationship between LoRa\nparameters and metrics, existing works have opted to focus on certain\nparameters or specific metrics to circumvent the intricate coupling\nrelationship, leading to limited adaptability. Therefore, we propose D-LoRa, a\ndistributed parameter adaptation scheme, based on reinforcement learning\ntowards network performance. We decompose the joint performance optimization\nproblem into multiple independent Multi-Armed Bandit (MAB) problems with\ndifferent reward functions. We have also built a comprehensive analytical model\nfor the LoRa network that considers path loss, quasi-orthogonality of spreading\nfactor, and packet collision. Experimental results show that our scheme can\nincrease packet delivery rate by up to 28.8% and demonstrates superior\nadaptability across different performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of LoRa networks necessitates joint performance optimization,\nincluding packet delivery rate, energy efficiency, and throughput.\nAdditionally, multiple LoRa parameters for packet transmission must be\ndynamically configured to tailor the performance metrics prioritization across\nvarying channel environments. Because of the coupling relationship between LoRa\nparameters and metrics, existing works have opted to focus on certain\nparameters or specific metrics to circumvent the intricate coupling\nrelationship, leading to limited adaptability. Therefore, we propose D-LoRa, a\ndistributed parameter adaptation scheme, based on reinforcement learning\ntowards network performance. We decompose the joint performance optimization\nproblem into multiple independent Multi-Armed Bandit (MAB) problems with\ndifferent reward functions. We have also built a comprehensive analytical model\nfor the LoRa network that considers path loss, quasi-orthogonality of spreading\nfactor, and packet collision. Experimental results show that our scheme can\nincrease packet delivery rate by up to 28.8% and demonstrates superior\nadaptability across different performance metrics."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Tongyu Song"
                    },
                    {
                        "name": "Jing Ren"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Shizhong Xu"
                    },
                    {
                        "name": "Sheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Wang"
                },
                "author": "Sheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16529v1",
                "updated": "2025-04-23T08:54:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    54,
                    10,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:54:10Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    54,
                    10,
                    2,
                    113,
                    0
                ],
                "title": "6G EdgeAI: Performance Evaluation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G EdgeAI: Performance Evaluation and Analysis"
                },
                "summary": "Generative AI (GenAI) services powered by large language models (LLMs)\nincreasingly deliver real-time interactions, yet existing 5G multi-access edge\ncomputing (MEC) architectures often treat communication and computing as\nseparate domains, limiting their ability to meet stringent latency\nrequirements. To address this challenge, we introduce an Integrated\nCommunication and Computing (ICC) framework where computing capabilities are\nenabled to reside directly in radio access network (RAN) nodes and jointly\nmanage bandwidth and computing resources. Our queueing-theoretic analysis shows\nthat ICC outperforms 5G MEC, achieving higher service capacity (defined as the\nmaximum arrival rate that maintains a specified fraction of jobs completed\nwithin a given delay budget) by 98%. We corroborate these gains through\nsystem-level simulations that account for transformer-based LLM workloads,\nrealistic GPU specifications, and a priority-based scheduling scheme. The\nsimulations show that ICC improves service capacity by 60%, demonstrating its\npotential to enable efficient, cost-effective real-time GenAI services in 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) services powered by large language models (LLMs)\nincreasingly deliver real-time interactions, yet existing 5G multi-access edge\ncomputing (MEC) architectures often treat communication and computing as\nseparate domains, limiting their ability to meet stringent latency\nrequirements. To address this challenge, we introduce an Integrated\nCommunication and Computing (ICC) framework where computing capabilities are\nenabled to reside directly in radio access network (RAN) nodes and jointly\nmanage bandwidth and computing resources. Our queueing-theoretic analysis shows\nthat ICC outperforms 5G MEC, achieving higher service capacity (defined as the\nmaximum arrival rate that maintains a specified fraction of jobs completed\nwithin a given delay budget) by 98%. We corroborate these gains through\nsystem-level simulations that account for transformer-based LLM workloads,\nrealistic GPU specifications, and a priority-based scheduling scheme. The\nsimulations show that ICC improves service capacity by 60%, demonstrating its\npotential to enable efficient, cost-effective real-time GenAI services in 6G."
                },
                "authors": [
                    {
                        "name": "Chien-Sheng Yang"
                    },
                    {
                        "name": "Yu-Jen Ku"
                    },
                    {
                        "name": "Yuan-Yao Lou"
                    },
                    {
                        "name": "Nathan Tenny"
                    },
                    {
                        "name": "Alex C. -C. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Alex C. -C. Hsu"
                },
                "author": "Alex C. -C. Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16515v1",
                "updated": "2025-04-23T08:40:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    40,
                    44,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:40:44Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    40,
                    44,
                    2,
                    113,
                    0
                ],
                "title": "Federated Learning of Low-Rank One-Shot Image Detection Models in Edge\n  Devices with Scalable Accuracy and Compute Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning of Low-Rank One-Shot Image Detection Models in Edge\n  Devices with Scalable Accuracy and Compute Complexity"
                },
                "summary": "This paper introduces a novel federated learning framework termed LoRa-FL\ndesigned for training low-rank one-shot image detection models deployed on edge\ndevices. By incorporating low-rank adaptation techniques into one-shot\ndetection architectures, our method significantly reduces both computational\nand communication overhead while maintaining scalable accuracy. The proposed\nframework leverages federated learning to collaboratively train lightweight\nimage recognition models, enabling rapid adaptation and efficient deployment\nacross heterogeneous, resource-constrained devices. Experimental evaluations on\nthe MNIST and CIFAR10 benchmark datasets, both in an\nindependent-and-identically-distributed (IID) and non-IID setting, demonstrate\nthat our approach achieves competitive detection performance while\nsignificantly reducing communication bandwidth and compute complexity. This\nmakes it a promising solution for adaptively reducing the communication and\ncompute power overheads, while not sacrificing model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel federated learning framework termed LoRa-FL\ndesigned for training low-rank one-shot image detection models deployed on edge\ndevices. By incorporating low-rank adaptation techniques into one-shot\ndetection architectures, our method significantly reduces both computational\nand communication overhead while maintaining scalable accuracy. The proposed\nframework leverages federated learning to collaboratively train lightweight\nimage recognition models, enabling rapid adaptation and efficient deployment\nacross heterogeneous, resource-constrained devices. Experimental evaluations on\nthe MNIST and CIFAR10 benchmark datasets, both in an\nindependent-and-identically-distributed (IID) and non-IID setting, demonstrate\nthat our approach achieves competitive detection performance while\nsignificantly reducing communication bandwidth and compute complexity. This\nmakes it a promising solution for adaptively reducing the communication and\ncompute power overheads, while not sacrificing model accuracy."
                },
                "authors": [
                    {
                        "name": "Abdul Hannaan"
                    },
                    {
                        "name": "Zubair Shah"
                    },
                    {
                        "name": "Aiman Erbad"
                    },
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Ali Safa"
                    }
                ],
                "author_detail": {
                    "name": "Ali Safa"
                },
                "author": "Ali Safa",
                "arxiv_comment": "accepted for publication at IEEE IWCMC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16511v1",
                "updated": "2025-04-23T08:36:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    36,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:36:50Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    36,
                    50,
                    2,
                    113,
                    0
                ],
                "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining"
                },
                "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity."
                },
                "authors": [
                    {
                        "name": "Fengze Liu"
                    },
                    {
                        "name": "Weidong Zhou"
                    },
                    {
                        "name": "Binbin Liu"
                    },
                    {
                        "name": "Zhimiao Yu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Haobin Lin"
                    },
                    {
                        "name": "Yifeng Yu"
                    },
                    {
                        "name": "Xiaohuan Zhou"
                    },
                    {
                        "name": "Taifeng Wang"
                    },
                    {
                        "name": "Yong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cao"
                },
                "author": "Yong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16506v1",
                "updated": "2025-04-23T08:33:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    33,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:33:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    33,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "A Comprehensive Survey of Synthetic Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Synthetic Tabular Data Generation"
                },
                "summary": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area."
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Yili Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16504v1",
                "updated": "2025-04-23T08:31:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    31,
                    51,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:31:51Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    31,
                    51,
                    2,
                    113,
                    0
                ],
                "title": "Intelligent Depression Prevention via LLM-Based Dialogue Analysis:\n  Overcoming the Limitations of Scale-Dependent Diagnosis through Precise\n  Emotional Pattern Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Depression Prevention via LLM-Based Dialogue Analysis:\n  Overcoming the Limitations of Scale-Dependent Diagnosis through Precise\n  Emotional Pattern Recognition"
                },
                "summary": "Existing depression screening predominantly relies on standardized\nquestionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates\n(18-34% in clinical studies) due to their static, symptom-counting nature and\nsusceptibility to patient recall bias. This paper presents an AI-powered\ndepression prevention system that leverages large language models (LLMs) to\nanalyze real-time conversational cues--including subtle emotional expressions\n(e.g., micro-sentiment shifts, self-referential language patterns)--for more\naccurate and dynamic mental state assessment. Our system achieves three key\ninnovations: (1) Continuous monitoring through natural dialogue, detecting\ndepression-indicative linguistic features (anhedonia markers, hopelessness\nsemantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk\nstratification that updates severity levels based on conversational context,\nreducing false positives by 41% compared to scale-based thresholds; and (3)\nPersonalized intervention strategies tailored to users' emotional granularity,\ndemonstrating 2.3x higher adherence rates than generic advice. Clinical\nvalidation with 450 participants shows the system identifies 92% of at-risk\ncases missed by traditional scales, while its explainable AI interface bridges\nthe gap between automated analysis and clinician judgment. This work\nestablishes conversational AI as a paradigm shift from episodic scale-dependent\ndiagnosis to continuous, emotionally intelligent mental health monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing depression screening predominantly relies on standardized\nquestionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates\n(18-34% in clinical studies) due to their static, symptom-counting nature and\nsusceptibility to patient recall bias. This paper presents an AI-powered\ndepression prevention system that leverages large language models (LLMs) to\nanalyze real-time conversational cues--including subtle emotional expressions\n(e.g., micro-sentiment shifts, self-referential language patterns)--for more\naccurate and dynamic mental state assessment. Our system achieves three key\ninnovations: (1) Continuous monitoring through natural dialogue, detecting\ndepression-indicative linguistic features (anhedonia markers, hopelessness\nsemantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk\nstratification that updates severity levels based on conversational context,\nreducing false positives by 41% compared to scale-based thresholds; and (3)\nPersonalized intervention strategies tailored to users' emotional granularity,\ndemonstrating 2.3x higher adherence rates than generic advice. Clinical\nvalidation with 450 participants shows the system identifies 92% of at-risk\ncases missed by traditional scales, while its explainable AI interface bridges\nthe gap between automated analysis and clinician judgment. This work\nestablishes conversational AI as a paradigm shift from episodic scale-dependent\ndiagnosis to continuous, emotionally intelligent mental health monitoring."
                },
                "authors": [
                    {
                        "name": "Zhenguang Zhong"
                    },
                    {
                        "name": "Zhixuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Wang"
                },
                "author": "Zhixuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16489v1",
                "updated": "2025-04-23T08:01:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    1,
                    50,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T08:01:50Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    1,
                    50,
                    2,
                    113,
                    0
                ],
                "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based\n  Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based\n  Multi-Agent Debate"
                },
                "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment."
                },
                "authors": [
                    {
                        "name": "Senmao Qi"
                    },
                    {
                        "name": "Yifei Zou"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Ziyi Lin"
                    },
                    {
                        "name": "Xiuzhen Cheng"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "arxiv_comment": "33 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05000v2",
                "updated": "2025-04-23T07:50:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    50,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2024-11-07T18:59:27Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    27,
                    3,
                    312,
                    0
                ],
                "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?"
                },
                "summary": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Samuel Albanie"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Albanie"
                },
                "author": "Samuel Albanie",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16472v1",
                "updated": "2025-04-23T07:32:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    32,
                    43,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T07:32:43Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    32,
                    43,
                    2,
                    113,
                    0
                ],
                "title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\n  Open Research Challenges"
                },
                "summary": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\n`just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper\\footnote{Author order is alphabetical. The\ncorresponding author is Mark Harman.} was written to accompany the keynote by\nthe authors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\n`just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper\\footnote{Author order is alphabetical. The\ncorresponding author is Mark Harman.} was written to accompany the keynote by\nthe authors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025."
                },
                "authors": [
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Peter O'Hearn"
                    },
                    {
                        "name": "Shubho Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Shubho Sengupta"
                },
                "author": "Shubho Sengupta",
                "arxiv_comment": "To Appear as keynote paper at FSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16469v1",
                "updated": "2025-04-23T07:29:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    29,
                    32,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T07:29:32Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    29,
                    32,
                    2,
                    113,
                    0
                ],
                "title": "Closed-form analysis of Multi-RIS Reflected Signals in RIS-Aided\n  Networks Using Stochastic Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closed-form analysis of Multi-RIS Reflected Signals in RIS-Aided\n  Networks Using Stochastic Geometry"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) enhance wireless communication by\ncreating engineered signal reflection paths in addition to direct links. This\nwork presents a stochastic geometry framework using point processes (PPs) to\nmodel multiple randomly deployed RISs conditioned on their associated base\nstation (BS) locations. By characterizing aggregated reflections from multiple\nRISs using the Laplace transform, we analytically assess the performance impact\nof RIS-reflected signals by integrating this characterization into\nwell-established stochastic geometry frameworks. Specifically, we derive\nclosed-form expressions for the Laplace transform of the reflected signal power\nin several deployment scenarios. These analytical results facilitate\nperformance evaluation of RIS-enabled enhancements. Numerical simulations\nvalidate that optimal RIS placement favors proximity to BSs or user equipment\n(UEs), and further quantify the impact of reflected interference, various\nfading assumptions, and diverse spatial deployment strategies. Importantly, our\nanalytical approach shows superior computational efficiency compared to Monte\nCarlo simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) enhance wireless communication by\ncreating engineered signal reflection paths in addition to direct links. This\nwork presents a stochastic geometry framework using point processes (PPs) to\nmodel multiple randomly deployed RISs conditioned on their associated base\nstation (BS) locations. By characterizing aggregated reflections from multiple\nRISs using the Laplace transform, we analytically assess the performance impact\nof RIS-reflected signals by integrating this characterization into\nwell-established stochastic geometry frameworks. Specifically, we derive\nclosed-form expressions for the Laplace transform of the reflected signal power\nin several deployment scenarios. These analytical results facilitate\nperformance evaluation of RIS-enabled enhancements. Numerical simulations\nvalidate that optimal RIS placement favors proximity to BSs or user equipment\n(UEs), and further quantify the impact of reflected interference, various\nfading assumptions, and diverse spatial deployment strategies. Importantly, our\nanalytical approach shows superior computational efficiency compared to Monte\nCarlo simulations."
                },
                "authors": [
                    {
                        "name": "Guodong Sun"
                    },
                    {
                        "name": "Francois Baccelli"
                    }
                ],
                "author_detail": {
                    "name": "Francois Baccelli"
                },
                "author": "Francois Baccelli",
                "arxiv_comment": "Accepted for the SpaSWiN 2025 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v4",
                "updated": "2025-04-23T07:27:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    27,
                    35,
                    2,
                    113,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\emph{LumosCore} to build high-bandwidth and large-scale data\ncenter networks for LLM jobs. By replacing the core-layer electrical packet\nswitches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$\nincrease in bandwidth or $8\\times$ increase in network size. We offer the\ndetailed design of \\emph{LumosCore} at both deployment stage and running stage.\nAt deployment stage, we propose Interleaved Wiring, which is compatible with\nall possible logical topologies. At running stage, we design polynomial-time\nalgorithms for GPU placement, logical topology generating and OCS\nreconfiguration to minimize network contention and reduce impact to scheduled\njobs. We evaluate \\emph{LumosCore} using both testbed experiments and\nlarge-scale simulation. Compared to traditional hybrid optical/electrical\narchitectures, \\emph{LumosCore} increases the end-to-end training throughput by\nup to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos\narchitectures, \\emph{LumosCore} reduces the average job completion time by up\nto 34.1\\% in a 16k simulation platform."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinbing Wang"
                },
                "author": "Xinbing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15922v2",
                "updated": "2025-04-23T07:24:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    24,
                    40,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T14:06:02Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    6,
                    2,
                    1,
                    112,
                    0
                ],
                "title": "Language Models to Support Multi-Label Classification of Industrial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models to Support Multi-Label Classification of Industrial Data"
                },
                "summary": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-label requirements classification is a challenging task, especially\nwhen dealing with numerous classes at varying levels of abstraction. The\ndifficulties increases when a limited number of requirements is available to\ntrain a supervised classifier. Zero-shot learning (ZSL) does not require\ntraining data and can potentially address this problem. This paper investigates\nthe performance of zero-shot classifiers (ZSCs) on a multi-label industrial\ndataset. We focuse on classifying requirements according to a taxonomy designed\nto support requirements tracing. We compare multiple variants of ZSCs using\ndifferent embeddings, including 9 language models (LMs) with a reduced number\nof parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large\nnumber of parameters (up to 70B), e.g., Llama. Our ground truth includes 377\nrequirements and 1968 labels from 6 output spaces. For the evaluation, we adopt\ntraditional metrics, i.e., precision, recall, F1, and $F_\\beta$, as well as a\nnovel label distance metric Dn. This aims to better capture the\nclassification's hierarchical nature and provides a more nuanced evaluation of\nhow far the results are from the ground truth. 1) The top-performing model on 5\nout of 6 output spaces is T5-xl, with maximum $F_\\beta$ = 0.78 and Dn = 0.04,\nwhile BERT base outperformed the other models in one case, with maximum\n$F_\\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the\nbest classification results compared to LLMs. Thus, addressing the problem in\npractice is feasible as limited computing power is needed. 3) The model\narchitecture (autoencoding, autoregression, and sentence-to-sentence)\nsignificantly affects the classifier's performance. We conclude that using ZSL\nfor multi-label requirements classification offers promising results. We also\npresent a novel metric that can be used to select the top-performing model for\nthis problem"
                },
                "authors": [
                    {
                        "name": "Waleed Abdeen"
                    },
                    {
                        "name": "Michael Unterkalmsteiner"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    },
                    {
                        "name": "Alessio Ferrari"
                    },
                    {
                        "name": "Panagiota Chatzipetrou"
                    }
                ],
                "author_detail": {
                    "name": "Panagiota Chatzipetrou"
                },
                "author": "Panagiota Chatzipetrou",
                "arxiv_comment": "Accepted at SANER Conference 2025. Awaiting publication by IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13612v3",
                "updated": "2025-04-23T07:09:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    9,
                    6,
                    2,
                    113,
                    0
                ],
                "published": "2024-12-18T08:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    8,
                    42,
                    25,
                    2,
                    353,
                    0
                ],
                "title": "Large Language Models for Automated Literature Review: An Evaluation of\n  Reference Generation, Abstract Writing, and Review Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Automated Literature Review: An Evaluation of\n  Reference Generation, Abstract Writing, and Review Composition"
                },
                "summary": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews."
                },
                "authors": [
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "12 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01840v3",
                "updated": "2025-04-23T07:08:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    7,
                    8,
                    17,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-03T18:59:04Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    59,
                    4,
                    0,
                    62,
                    0
                ],
                "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test"
                },
                "summary": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves\na 1.38x throughput improvement at a batch size of 64. The code is available at\nhttps://github.com/SafeAILab/EAGLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves\na 1.38x throughput improvement at a batch size of 64. The code is available at\nhttps://github.com/SafeAILab/EAGLE."
                },
                "authors": [
                    {
                        "name": "Yuhui Li"
                    },
                    {
                        "name": "Fangyun Wei"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Hongyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Zhang"
                },
                "author": "Hongyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12022v3",
                "updated": "2025-04-23T06:56:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    6,
                    56,
                    0,
                    2,
                    113,
                    0
                ],
                "published": "2024-06-28T01:44:57Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    1,
                    44,
                    57,
                    4,
                    180,
                    0
                ],
                "title": "ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code\n  Generation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated excellent\nperformance, inspiring researchers to explore their use in automating register\ntransfer level (RTL) code generation and improving hardware design efficiency.\nHowever, the existing approaches to fine-tune LLMs for RTL generation typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data, which are costly to acquire.\nTo mitigate these issues, we innovatively introduce an iterative training\nparadigm named ITERTL. During each iteration, samples are drawn from the model\ntrained in the previous cycle. Then these new samples are employed for training\nin current loop. Furthermore, we introduce a plug-and-play data filtering\nstrategy, thereby encouraging the model to generate high-quality,\nself-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA)\nopen-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human\nbenchmark. Under similar conditions of data quantity and quality, our approach\nsignificantly outperforms the baseline. Extensive experiments validate the\neffectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated excellent\nperformance, inspiring researchers to explore their use in automating register\ntransfer level (RTL) code generation and improving hardware design efficiency.\nHowever, the existing approaches to fine-tune LLMs for RTL generation typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data, which are costly to acquire.\nTo mitigate these issues, we innovatively introduce an iterative training\nparadigm named ITERTL. During each iteration, samples are drawn from the model\ntrained in the previous cycle. Then these new samples are employed for training\nin current loop. Furthermore, we introduce a plug-and-play data filtering\nstrategy, thereby encouraging the model to generate high-quality,\nself-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA)\nopen-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human\nbenchmark. Under similar conditions of data quantity and quality, our approach\nsignificantly outperforms the baseline. Extensive experiments validate the\neffectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Peiyang Wu"
                    },
                    {
                        "name": "Nan Guo"
                    },
                    {
                        "name": "Xiao Xiao"
                    },
                    {
                        "name": "Wenming Li"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Dongrui Fan"
                    }
                ],
                "author_detail": {
                    "name": "Dongrui Fan"
                },
                "author": "Dongrui Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v5",
                "updated": "2025-04-23T06:29:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    6,
                    29,
                    39,
                    2,
                    113,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ishneet Sukhvinder Singh"
                    },
                    {
                        "name": "Ritvik Aggarwal"
                    },
                    {
                        "name": "Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sean O'Brien"
                    }
                ],
                "author_detail": {
                    "name": "Sean O'Brien"
                },
                "author": "Sean O'Brien",
                "arxiv_comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16449v1",
                "updated": "2025-04-23T06:23:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    6,
                    23,
                    18,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T06:23:18Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    6,
                    23,
                    18,
                    2,
                    113,
                    0
                ],
                "title": "From Past to Present: A Survey of Malicious URL Detection Techniques,\n  Datasets and Code Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Past to Present: A Survey of Malicious URL Detection Techniques,\n  Datasets and Code Repositories"
                },
                "summary": "Malicious URLs persistently threaten the cybersecurity ecosystem, by either\ndeceiving users into divulging private data or distributing harmful payloads to\ninfiltrate host systems. Gaining timely insights into the current state of this\nongoing battle holds significant importance. However, existing reviews exhibit\n4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures\nunderstanding of how detection approaches exploit specific modal information\nchannels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;\n3) No open-source implementations are collected to facilitate benchmarking; 4)\nInsufficient dataset coverage.This paper presents a comprehensive review of\nmalicious URL detection technologies, systematically analyzing methods from\ntraditional blacklisting to advanced deep learning approaches (e.g.\nTransformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel\nmodality-based taxonomy that categorizes existing works according to their\nprimary data modalities (URL, HTML, Visual, etc.). This hierarchical\nclassification enables both rigorous technical analysis and clear understanding\nof multimodal information utilization. Furthermore, to establish a profile of\naccessible datasets and address the lack of standardized benchmarking (where\ncurrent studies often lack proper baseline comparisons), we curate and analyze:\n1) publicly available datasets (2016-2024), and 2) open-source implementations\nfrom published works(2013-2025). Then, we outline essential design principles\nand architectural frameworks for product-level implementations. The review\nconcludes by examining emerging challenges and proposing actionable directions\nfor future research. We maintain a GitHub repository for ongoing curating\ndatasets and open-source implementations:\nhttps://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious URLs persistently threaten the cybersecurity ecosystem, by either\ndeceiving users into divulging private data or distributing harmful payloads to\ninfiltrate host systems. Gaining timely insights into the current state of this\nongoing battle holds significant importance. However, existing reviews exhibit\n4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures\nunderstanding of how detection approaches exploit specific modal information\nchannels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;\n3) No open-source implementations are collected to facilitate benchmarking; 4)\nInsufficient dataset coverage.This paper presents a comprehensive review of\nmalicious URL detection technologies, systematically analyzing methods from\ntraditional blacklisting to advanced deep learning approaches (e.g.\nTransformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel\nmodality-based taxonomy that categorizes existing works according to their\nprimary data modalities (URL, HTML, Visual, etc.). This hierarchical\nclassification enables both rigorous technical analysis and clear understanding\nof multimodal information utilization. Furthermore, to establish a profile of\naccessible datasets and address the lack of standardized benchmarking (where\ncurrent studies often lack proper baseline comparisons), we curate and analyze:\n1) publicly available datasets (2016-2024), and 2) open-source implementations\nfrom published works(2013-2025). Then, we outline essential design principles\nand architectural frameworks for product-level implementations. The review\nconcludes by examining emerging challenges and proposing actionable directions\nfor future research. We maintain a GitHub repository for ongoing curating\ndatasets and open-source implementations:\nhttps://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Yanqiu Yu"
                    },
                    {
                        "name": "Jianguo Sun"
                    },
                    {
                        "name": "Yanbin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Wang"
                },
                "author": "Yanbin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10157v2",
                "updated": "2025-04-23T06:08:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    6,
                    8,
                    32,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-14T12:12:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    12,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users"
                },
                "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."
                },
                "authors": [
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Shiyue Yang"
                    },
                    {
                        "name": "Xiawei Liu"
                    },
                    {
                        "name": "Libo Sun"
                    },
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Yihang Yang"
                    },
                    {
                        "name": "Weihong Qi"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Guanying Li"
                    },
                    {
                        "name": "Ling Yan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Siming Chen"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Shiping Tang"
                    },
                    {
                        "name": "Libo Wu"
                    },
                    {
                        "name": "Baohua Zhou"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16438v1",
                "updated": "2025-04-23T05:57:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    57,
                    20,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T05:57:20Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    57,
                    20,
                    2,
                    113,
                    0
                ],
                "title": "Private Federated Learning using Preference-Optimized Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Federated Learning using Preference-Optimized Synthetic Data"
                },
                "summary": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as a preference ranking. Our algorithm, Preference Optimization for\nPrivate Client Data (POPri) harnesses client feedback using preference\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 68%, compared to 52% for prior synthetic data methods, and 10% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as a preference ranking. Our algorithm, Preference Optimization for\nPrivate Client Data (POPri) harnesses client feedback using preference\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 68%, compared to 52% for prior synthetic data methods, and 10% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri."
                },
                "authors": [
                    {
                        "name": "Charlie Hou"
                    },
                    {
                        "name": "Mei-Yu Wang"
                    },
                    {
                        "name": "Yige Zhu"
                    },
                    {
                        "name": "Daniel Lazar"
                    },
                    {
                        "name": "Giulia Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Fanti"
                },
                "author": "Giulia Fanti",
                "arxiv_comment": "Spotlight presentation at SynthData Workshop ICLR25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02263v3",
                "updated": "2025-04-23T05:47:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    47,
                    32,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-03T04:20:44Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    20,
                    44,
                    3,
                    93,
                    0
                ],
                "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\n  Expert Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\n  Expert Parallelism"
                },
                "summary": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Ruidong Zhu"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chao Jin"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Cesar A. Stuardo"
                    },
                    {
                        "name": "Dongyang Wang"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Huaping Zhou"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Jianzhe Xiao"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Lingjun Liu"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jianxi Ye"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16432v1",
                "updated": "2025-04-23T05:34:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    34,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T05:34:49Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    34,
                    49,
                    2,
                    113,
                    0
                ],
                "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold\n  Network"
                },
                "summary": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities."
                },
                "authors": [
                    {
                        "name": "Ziran Liang"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16429v1",
                "updated": "2025-04-23T05:27:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    27,
                    27,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T05:27:27Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    27,
                    27,
                    2,
                    113,
                    0
                ],
                "title": "Give LLMs a Security Course: Securing Retrieval-Augmented Code\n  Generation via Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Give LLMs a Security Course: Securing Retrieval-Augmented Code\n  Generation via Knowledge Injection"
                },
                "summary": "Retrieval-Augmented Code Generation (RACG) leverages external knowledge to\nenhance Large Language Models (LLMs) in code synthesis, improving the\nfunctional correctness of the generated code. However, existing RACG systems\nlargely overlook security, leading to substantial risks. Especially, the\npoisoning of malicious code into knowledge bases can mislead LLMs, resulting in\nthe generation of insecure outputs, which poses a critical threat in modern\nsoftware development. To address this, we propose a security-hardening\nframework for RACG systems, CodeGuarder, that shifts the paradigm from\nretrieving only functional code examples to incorporating both functional code\nand security knowledge. Our framework constructs a security knowledge base from\nreal-world vulnerability databases, including secure code samples and root\ncause annotations. For each code generation query, a retriever decomposes the\nquery into fine-grained sub-tasks and fetches relevant security knowledge. To\nprioritize critical security guidance, we introduce a re-ranking and filtering\nmechanism by leveraging the LLMs' susceptibility to different vulnerability\ntypes. This filtered security knowledge is seamlessly integrated into the\ngeneration prompt. Our evaluation shows CodeGuarder significantly improves code\nsecurity rates across various LLMs, achieving average improvements of 20.12\\%\nin standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning\nscenarios without compromising functional correctness. Furthermore, CodeGuarder\ndemonstrates strong generalization, enhancing security even when the targeted\nlanguage's security knowledge is lacking. This work presents CodeGuarder as a\npivotal advancement towards building secure and trustworthy RACG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Code Generation (RACG) leverages external knowledge to\nenhance Large Language Models (LLMs) in code synthesis, improving the\nfunctional correctness of the generated code. However, existing RACG systems\nlargely overlook security, leading to substantial risks. Especially, the\npoisoning of malicious code into knowledge bases can mislead LLMs, resulting in\nthe generation of insecure outputs, which poses a critical threat in modern\nsoftware development. To address this, we propose a security-hardening\nframework for RACG systems, CodeGuarder, that shifts the paradigm from\nretrieving only functional code examples to incorporating both functional code\nand security knowledge. Our framework constructs a security knowledge base from\nreal-world vulnerability databases, including secure code samples and root\ncause annotations. For each code generation query, a retriever decomposes the\nquery into fine-grained sub-tasks and fetches relevant security knowledge. To\nprioritize critical security guidance, we introduce a re-ranking and filtering\nmechanism by leveraging the LLMs' susceptibility to different vulnerability\ntypes. This filtered security knowledge is seamlessly integrated into the\ngeneration prompt. Our evaluation shows CodeGuarder significantly improves code\nsecurity rates across various LLMs, achieving average improvements of 20.12\\%\nin standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning\nscenarios without compromising functional correctness. Furthermore, CodeGuarder\ndemonstrates strong generalization, enhancing security even when the targeted\nlanguage's security knowledge is lacking. This work presents CodeGuarder as a\npivotal advancement towards building secure and trustworthy RACG systems."
                },
                "authors": [
                    {
                        "name": "Bo Lin"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Yihao Qin"
                    },
                    {
                        "name": "Liqian Chen"
                    },
                    {
                        "name": "Xiaoguang Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Mao"
                },
                "author": "Xiaoguang Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16427v2",
                "updated": "2025-04-24T07:35:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    35,
                    3,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T05:25:13Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    25,
                    13,
                    2,
                    113,
                    0
                ],
                "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark"
                },
                "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA."
                },
                "authors": [
                    {
                        "name": "Hanlei Zhang"
                    },
                    {
                        "name": "Zhuohang Li"
                    },
                    {
                        "name": "Yeshuang Zhu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Peiwu Wang"
                    },
                    {
                        "name": "Haige Zhu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jinchao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Zhang"
                },
                "author": "Jinchao Zhang",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20770v4",
                "updated": "2025-04-23T05:12:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    12,
                    45,
                    2,
                    113,
                    0
                ],
                "published": "2024-05-24T07:23:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    7,
                    23,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Sentinel: LLM Agent for Adversarial Purification"
                },
                "summary": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness."
                },
                "authors": [
                    {
                        "name": "Guang Lin"
                    },
                    {
                        "name": "Toshihisa Tanaka"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15610v2",
                "updated": "2025-04-23T04:59:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    59,
                    47,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-22T06:08:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    6,
                    8,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in\n  Resource-Constrained Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in\n  Resource-Constrained Settings"
                },
                "summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy."
                },
                "authors": [
                    {
                        "name": "Md Millat Hosen"
                    }
                ],
                "author_detail": {
                    "name": "Md Millat Hosen"
                },
                "author": "Md Millat Hosen",
                "arxiv_comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16414v1",
                "updated": "2025-04-23T04:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    36,
                    19,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T04:36:19Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    36,
                    19,
                    2,
                    113,
                    0
                ],
                "title": "Evaluating Multi-Hop Reasoning in Large Language Models: A\n  Chemistry-Centric Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multi-Hop Reasoning in Large Language Models: A\n  Chemistry-Centric Case Study"
                },
                "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics."
                },
                "authors": [
                    {
                        "name": "Mohammad Khodadad"
                    },
                    {
                        "name": "Ali Shiraee Kasmaee"
                    },
                    {
                        "name": "Mahdi Astaraki"
                    },
                    {
                        "name": "Nicholas Sherck"
                    },
                    {
                        "name": "Hamidreza Mahyar"
                    },
                    {
                        "name": "Soheila Samiee"
                    }
                ],
                "author_detail": {
                    "name": "Soheila Samiee"
                },
                "author": "Soheila Samiee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16408v1",
                "updated": "2025-04-23T04:19:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    19,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T04:19:52Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    19,
                    52,
                    2,
                    113,
                    0
                ],
                "title": "Less is More: Enhancing Structured Multi-Agent Reasoning via\n  Quality-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Enhancing Structured Multi-Agent Reasoning via\n  Quality-Guided Distillation"
                },
                "summary": "The XLLM@ACL2025 Shared Task-III formulates a low-resource structural\nreasoning task that challenges LLMs to generate interpretable, step-by-step\nrationales with minimal labeled data. We present Less is More, the third-place\nwinning approach in the XLLM@ACL2025 Shared Task-III, which focuses on\nstructured reasoning from only 24 labeled examples. Our approach leverages a\nmulti-agent framework with reverse-prompt induction, retrieval-augmented\nreasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to\ndistill high-quality supervision across three subtasks: question parsing, CoT\nparsing, and step-level verification. All modules are fine-tuned from\nMeta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure\nvalidation with reward filtering across few-shot and zero-shot prompts, our\npipeline consistently improves structure reasoning quality. These results\nunderscore the value of controllable data distillation in enhancing structured\ninference under low-resource constraints. Our code is available at\nhttps://github.com/Jiahao-Yuan/Less-is-More.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The XLLM@ACL2025 Shared Task-III formulates a low-resource structural\nreasoning task that challenges LLMs to generate interpretable, step-by-step\nrationales with minimal labeled data. We present Less is More, the third-place\nwinning approach in the XLLM@ACL2025 Shared Task-III, which focuses on\nstructured reasoning from only 24 labeled examples. Our approach leverages a\nmulti-agent framework with reverse-prompt induction, retrieval-augmented\nreasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to\ndistill high-quality supervision across three subtasks: question parsing, CoT\nparsing, and step-level verification. All modules are fine-tuned from\nMeta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure\nvalidation with reward filtering across few-shot and zero-shot prompts, our\npipeline consistently improves structure reasoning quality. These results\nunderscore the value of controllable data distillation in enhancing structured\ninference under low-resource constraints. Our code is available at\nhttps://github.com/Jiahao-Yuan/Less-is-More."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Xingzhe Sun"
                    },
                    {
                        "name": "Xing Yu"
                    },
                    {
                        "name": "Jingwen Wang"
                    },
                    {
                        "name": "Dehui Du"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Zixiang Di"
                    }
                ],
                "author_detail": {
                    "name": "Zixiang Di"
                },
                "author": "Zixiang Di",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04718v2",
                "updated": "2025-04-23T04:06:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    6,
                    56,
                    2,
                    113,
                    0
                ],
                "published": "2025-02-07T07:39:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    7,
                    39,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?"
                },
                "summary": "Text style transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, as is common in other natural language processing (NLP)\ntasks, however, automatic metrics for TST have not received as much attention\nas metrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks, sentiment transfer and\ndetoxification, in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nlarge language models (LLMs) as tools for TST evaluation. Our findings\nhighlight newly applied advanced NLP metrics and LLM-based evaluations provide\nbetter insights than existing TST metrics. Our oracle ensemble approaches show\neven more potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text style transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, as is common in other natural language processing (NLP)\ntasks, however, automatic metrics for TST have not received as much attention\nas metrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks, sentiment transfer and\ndetoxification, in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nlarge language models (LLMs) as tools for TST evaluation. Our findings\nhighlight newly applied advanced NLP metrics and LLM-based evaluations provide\nbetter insights than existing TST metrics. Our oracle ensemble approaches show\neven more potential."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atul Kr. Ojha"
                    },
                    {
                        "name": "John P. McCrae"
                    },
                    {
                        "name": "Ondrej Dusek"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Dusek"
                },
                "author": "Ondrej Dusek",
                "arxiv_comment": "Accepted at NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16397v1",
                "updated": "2025-04-23T03:57:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    3,
                    57,
                    24,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T03:57:24Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    3,
                    57,
                    24,
                    2,
                    113,
                    0
                ],
                "title": "Circinus: Efficient Query Planner for Compound ML Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circinus: Efficient Query Planner for Compound ML Serving"
                },
                "summary": "The rise of compound AI serving -- integrating multiple operators in a\npipeline that may span edge and cloud tiers -- enables end-user applications\nsuch as autonomous driving, generative AI-powered meeting companions, and\nimmersive gaming. Achieving high service goodput -- i.e., meeting service level\nobjectives (SLOs) for pipeline latency, accuracy, and costs -- requires\neffective planning of operator placement, configuration, and resource\nallocation across infrastructure tiers. However, the diverse SLO requirements,\nvarying edge capabilities, and high query volumes create an enormous planning\nsearch space, rendering current solutions fundamentally limited for real-time\nserving and cost-efficient deployments.\n  This paper presents Circinus, an SLO-aware query planner for large-scale\ncompound AI workloads. Circinus novelly decomposes multi-query planning and\nmulti-dimensional SLO objectives while preserving global decision quality. By\nexploiting plan similarities within and across queries, it significantly\nreduces search steps. It further improves per-step efficiency with a\nprecision-aware plan profiler that incrementally profiles and strategically\napplies early stopping based on imprecise estimates of plan performance. At\nscale, Circinus selects query-plan combinations to maximize global SLO goodput.\nEvaluations in real-world settings show that Circinus improves service goodput\nby 3.2-5.0$\\times$, accelerates query planning by 4.2-5.8$\\times$, achieving\nquery response in seconds, while reducing deployment costs by 3.2-4.0$\\times$\nover state of the arts even in their intended single-tier deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of compound AI serving -- integrating multiple operators in a\npipeline that may span edge and cloud tiers -- enables end-user applications\nsuch as autonomous driving, generative AI-powered meeting companions, and\nimmersive gaming. Achieving high service goodput -- i.e., meeting service level\nobjectives (SLOs) for pipeline latency, accuracy, and costs -- requires\neffective planning of operator placement, configuration, and resource\nallocation across infrastructure tiers. However, the diverse SLO requirements,\nvarying edge capabilities, and high query volumes create an enormous planning\nsearch space, rendering current solutions fundamentally limited for real-time\nserving and cost-efficient deployments.\n  This paper presents Circinus, an SLO-aware query planner for large-scale\ncompound AI workloads. Circinus novelly decomposes multi-query planning and\nmulti-dimensional SLO objectives while preserving global decision quality. By\nexploiting plan similarities within and across queries, it significantly\nreduces search steps. It further improves per-step efficiency with a\nprecision-aware plan profiler that incrementally profiles and strategically\napplies early stopping based on imprecise estimates of plan performance. At\nscale, Circinus selects query-plan combinations to maximize global SLO goodput.\nEvaluations in real-world settings show that Circinus improves service goodput\nby 3.2-5.0$\\times$, accelerates query planning by 4.2-5.8$\\times$, achieving\nquery response in seconds, while reducing deployment costs by 3.2-4.0$\\times$\nover state of the arts even in their intended single-tier deployments."
                },
                "authors": [
                    {
                        "name": "Banruo Liu"
                    },
                    {
                        "name": "Wei-Yu Lin"
                    },
                    {
                        "name": "Minghao Fang"
                    },
                    {
                        "name": "Yihan Jiang"
                    },
                    {
                        "name": "Fan Lai"
                    }
                ],
                "author_detail": {
                    "name": "Fan Lai"
                },
                "author": "Fan Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13217v2",
                "updated": "2025-04-23T03:43:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    3,
                    43,
                    30,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-17T04:00:40Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    4,
                    0,
                    40,
                    3,
                    107,
                    0
                ],
                "title": "Sustainability via LLM Right-sizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainability via LLM Right-sizing"
                },
                "summary": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice."
                },
                "authors": [
                    {
                        "name": "Jennifer Haase"
                    },
                    {
                        "name": "Finn Klessascheck"
                    },
                    {
                        "name": "Jan Mendling"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "arxiv_comment": "17 pages, 2 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]