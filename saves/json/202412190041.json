[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v3",
                "updated": "2024-12-17T03:56:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    3,
                    56,
                    26,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v1",
                "updated": "2024-12-17T02:43:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Information Seeking via Query-Guided Activation\n  Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Information Seeking via Query-Guided Activation\n  Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v1",
                "updated": "2024-12-16T18:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10382v1",
                "updated": "2024-11-28T10:40:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    40,
                    47,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T10:40:47Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    40,
                    47,
                    3,
                    333,
                    0
                ],
                "title": "Many Hands Make Light Work: Accelerating Edge Inference via Multi-Client\n  Collaborative Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many Hands Make Light Work: Accelerating Edge Inference via Multi-Client\n  Collaborative Caching"
                },
                "summary": "Edge inference is a technology that enables real-time data processing and\nanalysis on clients near the data source. To ensure compliance with the\nService-Level Objectives (SLOs), such as a 30% latency reduction target,\ncaching is usually adopted to reduce redundant computations in inference tasks\non stream data. Due to task and data correlations, sharing cache information\namong clients can improve the inference performance. However, the\nnon-independent and identically distributed (non-IID) nature of data across\ndifferent clients and the long-tail distributions, where some classes have\nsignificantly more samples than others, will reduce cache hit ratios and\nincrease latency. To address the aforementioned challenges, we propose an\nefficient inference framework, CoCa, which leverages a multi-client\ncollaborative caching mechanism to accelerate edge inference. On the client\nside, the model is pre-set with multiple cache layers to achieve a quick\ninference. During inference, the model performs sequential lookups at cache\nlayers activated by the edge server. On the server side, CoCa uses a\ntwo-dimensional global cache to periodically aggregate information from\nclients, mitigating the effects of non-IID data. For client cache allocation,\nCoCa first evaluates the importance of classes based on how frequently and\nrecently their samples have been accessed. CoCa then selects frequently\nrecurring classes to address long-tail distribution challenges. Finally, CoCa\ndynamically activates cache layers to balance lookup overhead and accuracy.\nExtensive experiments demonstrate that CoCa reduces inference latency by 23.0%\nto 45.2% on the VGG, ResNet and AST models with a slight loss of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge inference is a technology that enables real-time data processing and\nanalysis on clients near the data source. To ensure compliance with the\nService-Level Objectives (SLOs), such as a 30% latency reduction target,\ncaching is usually adopted to reduce redundant computations in inference tasks\non stream data. Due to task and data correlations, sharing cache information\namong clients can improve the inference performance. However, the\nnon-independent and identically distributed (non-IID) nature of data across\ndifferent clients and the long-tail distributions, where some classes have\nsignificantly more samples than others, will reduce cache hit ratios and\nincrease latency. To address the aforementioned challenges, we propose an\nefficient inference framework, CoCa, which leverages a multi-client\ncollaborative caching mechanism to accelerate edge inference. On the client\nside, the model is pre-set with multiple cache layers to achieve a quick\ninference. During inference, the model performs sequential lookups at cache\nlayers activated by the edge server. On the server side, CoCa uses a\ntwo-dimensional global cache to periodically aggregate information from\nclients, mitigating the effects of non-IID data. For client cache allocation,\nCoCa first evaluates the importance of classes based on how frequently and\nrecently their samples have been accessed. CoCa then selects frequently\nrecurring classes to address long-tail distribution challenges. Finally, CoCa\ndynamically activates cache layers to balance lookup overhead and accuracy.\nExtensive experiments demonstrate that CoCa reduces inference latency by 23.0%\nto 45.2% on the VGG, ResNet and AST models with a slight loss of accuracy."
                },
                "authors": [
                    {
                        "name": "Wenyi Liang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "arxiv_comment": "IEEE International Conference on Data Engineering (ICDE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.13178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v1",
                "updated": "2024-12-17T18:55:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "21 pages, 14 tables, 7 figures, submitted to ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06215v2",
                "updated": "2024-12-17T18:54:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    54,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-08T17:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    20,
                    37,
                    1,
                    282,
                    0
                ],
                "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback"
                },
                "summary": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project Page: https://DataEnvGym.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13175v1",
                "updated": "2024-12-17T18:54:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    54,
                    1,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:54:01Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    54,
                    1,
                    1,
                    352,
                    0
                ],
                "title": "DnDScore: Decontextualization and Decomposition for Factuality\n  Verification in Long-Form Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DnDScore: Decontextualization and Decomposition for Factuality\n  Verification in Long-Form Text Generation"
                },
                "summary": "The decompose-then-verify strategy for verification of Large Language Model\n(LLM) generations decomposes claims that are then independently verified.\nDecontextualization augments text (claims) to ensure it can be verified outside\nof the original context, enabling reliable verification. While decomposition\nand decontextualization have been explored independently, their interactions in\na complete system have not been investigated. Their conflicting purposes can\ncreate tensions: decomposition isolates atomic facts while decontextualization\ninserts relevant information. Furthermore, a decontextualized subclaim presents\na challenge to the verification step: what part of the augmented text should be\nverified as it now contains multiple atomic facts? We conduct an evaluation of\ndifferent decomposition, decontextualization, and verification strategies and\nfind that the choice of strategy matters in the resulting factuality scores.\nAdditionally, we introduce DnDScore, a decontextualization aware verification\nmethod which validates subclaims in the context of contextual information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The decompose-then-verify strategy for verification of Large Language Model\n(LLM) generations decomposes claims that are then independently verified.\nDecontextualization augments text (claims) to ensure it can be verified outside\nof the original context, enabling reliable verification. While decomposition\nand decontextualization have been explored independently, their interactions in\na complete system have not been investigated. Their conflicting purposes can\ncreate tensions: decomposition isolates atomic facts while decontextualization\ninserts relevant information. Furthermore, a decontextualized subclaim presents\na challenge to the verification step: what part of the augmented text should be\nverified as it now contains multiple atomic facts? We conduct an evaluation of\ndifferent decomposition, decontextualization, and verification strategies and\nfind that the choice of strategy matters in the resulting factuality scores.\nAdditionally, we introduce DnDScore, a decontextualization aware verification\nmethod which validates subclaims in the context of contextual information."
                },
                "authors": [
                    {
                        "name": "Miriam Wanner"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13171v1",
                "updated": "2024-12-17T18:50:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    50,
                    33,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:50:33Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    50,
                    33,
                    1,
                    352,
                    0
                ],
                "title": "Compressed Chain of Thought: Efficient Reasoning Through Dense\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Chain of Thought: Efficient Reasoning Through Dense\n  Representations"
                },
                "summary": "Chain-of-thought (CoT) decoding enables language models to improve reasoning\nperformance at the cost of high generation latency in decoding. Recent\nproposals have explored variants of contemplation tokens, a term we introduce\nthat refers to special tokens used during inference to allow for extra\ncomputation. Prior work has considered fixed-length sequences drawn from a\ndiscrete set of embeddings as contemplation tokens. Here we propose Compressed\nChain-of-Thought (CCoT), a framework to generate contentful and continuous\ncontemplation tokens of variable sequence length. The generated contemplation\ntokens are compressed representations of explicit reasoning chains, and our\nmethod can be applied to off-the-shelf decoder language models. Through\nexperiments, we illustrate how CCoT enables additional reasoning over dense\ncontentful representations to achieve corresponding improvements in accuracy.\nMoreover, the reasoning improvements can be adaptively modified on demand by\ncontrolling the number of contemplation tokens generated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) decoding enables language models to improve reasoning\nperformance at the cost of high generation latency in decoding. Recent\nproposals have explored variants of contemplation tokens, a term we introduce\nthat refers to special tokens used during inference to allow for extra\ncomputation. Prior work has considered fixed-length sequences drawn from a\ndiscrete set of embeddings as contemplation tokens. Here we propose Compressed\nChain-of-Thought (CCoT), a framework to generate contentful and continuous\ncontemplation tokens of variable sequence length. The generated contemplation\ntokens are compressed representations of explicit reasoning chains, and our\nmethod can be applied to off-the-shelf decoder language models. Through\nexperiments, we illustrate how CCoT enables additional reasoning over dense\ncontentful representations to achieve corresponding improvements in accuracy.\nMoreover, the reasoning improvements can be adaptively modified on demand by\ncontrolling the number of contemplation tokens generated."
                },
                "authors": [
                    {
                        "name": "Jeffrey Cheng"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13169v1",
                "updated": "2024-12-17T18:46:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    46,
                    32,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:46:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    46,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study"
                },
                "summary": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness."
                },
                "authors": [
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Berk Yoztyurk"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Frauke Kreuter"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Assenmacher"
                },
                "author": "Matthias Assenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13163v1",
                "updated": "2024-12-17T18:42:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    42,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:42:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    42,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System"
                },
                "summary": "Organizations seeking to utilize Large Language Models (LLMs) for knowledge\nquerying and analysis often encounter challenges in maintaining an LLM\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\nsolution for organizations looking to overcome the challenges of maintaining\nproprietary models and to help reduce LLM hallucinations in their query\nresponses. However, RAG comes with its own issues regarding scaling data\npipelines across tiered-access and disparate data sources. In many scenarios,\nit is necessary to query beyond a single data silo to provide richer and more\nrelevant context for an LLM. Analyzing data sources within and across\norganizational trust boundaries is often limited by complex data-sharing\npolicies that prohibit centralized data storage, therefore, inhibit the fast\nand effective setup and scaling of RAG solutions. In this paper, we introduce\nConfidential Computing (CC) techniques as a solution for secure Federated\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\nacross a decentralized network of data providers by ensuring context\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\nMIRAGE benchmarking dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations seeking to utilize Large Language Models (LLMs) for knowledge\nquerying and analysis often encounter challenges in maintaining an LLM\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\nsolution for organizations looking to overcome the challenges of maintaining\nproprietary models and to help reduce LLM hallucinations in their query\nresponses. However, RAG comes with its own issues regarding scaling data\npipelines across tiered-access and disparate data sources. In many scenarios,\nit is necessary to query beyond a single data silo to provide richer and more\nrelevant context for an LLM. Analyzing data sources within and across\norganizational trust boundaries is often limited by complex data-sharing\npolicies that prohibit centralized data storage, therefore, inhibit the fast\nand effective setup and scaling of RAG solutions. In this paper, we introduce\nConfidential Computing (CC) techniques as a solution for secure Federated\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\nacross a decentralized network of data providers by ensuring context\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\nMIRAGE benchmarking dataset."
                },
                "authors": [
                    {
                        "name": "Parker Addison"
                    },
                    {
                        "name": "Minh-Tuan H. Nguyen"
                    },
                    {
                        "name": "Tomislav Medan"
                    },
                    {
                        "name": "Mohammad T. Manzari"
                    },
                    {
                        "name": "Brendan McElrone"
                    },
                    {
                        "name": "Laksh Lalwani"
                    },
                    {
                        "name": "Aboli More"
                    },
                    {
                        "name": "Smita Sharma"
                    },
                    {
                        "name": "Holger R. Roth"
                    },
                    {
                        "name": "Isaac Yang"
                    },
                    {
                        "name": "Chester Chen"
                    },
                    {
                        "name": "Daguang Xu"
                    },
                    {
                        "name": "Yan Cheng"
                    },
                    {
                        "name": "Andrew Feng"
                    },
                    {
                        "name": "Ziyue Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Xu"
                },
                "author": "Ziyue Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10370v2",
                "updated": "2024-12-17T18:35:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    35,
                    27,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-14T18:56:40Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    18,
                    56,
                    40,
                    4,
                    166,
                    0
                ],
                "title": "Let's Get to the Point: LLM-Supported Planning, Drafting, and Revising\n  of Research-Paper Blog Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Get to the Point: LLM-Supported Planning, Drafting, and Revising\n  of Research-Paper Blog Posts"
                },
                "summary": "Research-paper blog posts help scientists to disseminate their work to a\nlarger audience, but translating scientific long documents into long-form\nsummaries like blog posts raises unique challenges: 1) planning what paper\ncontent to include in the blog post, 2) drafting the selected content in\nsections amenable to a paper blog post, and 3) revising the blog post to be\nscientifically accurate but also concise, easy to understand, and engaging. Can\nwe harness the power of large language models (LLMs) to assist researchers with\nthese challenges? To investigate this question, we developed Papers-to-Posts,\nan LLM-powered tool that implements a new Plan-Draft-Revise workflow for\nmixed-initiative long-form paper summarization. An LLM-generated paper outline\nwith pre-selected yet adjustable bullet points helps users to plan what\ninformation to include. Meanwhile, customizable LLM instructions support\ndrafting the text with a suitable structure and revising the text to have an\nappropriate tone. Through two studies, we compared Papers-to-Posts to a strong\nbaseline tool that provides an LLM-generated draft and access to free-form LLM\nprompting, and we found that Papers-to-Posts improved researchers' editing\npower. In a within-subjects lab study (N=20 participants), Papers-to-Posts led\nparticipants to make significantly more change to initial LLM drafts within a\nfixed amount of time and to be significantly more satisfied with their final\nblog post, without increasing cognitive load. Furthermore, in a\nbetween-subjects deployment study (N=37 blog posts, 26 participants),\nPapers-to-Posts led participants to make more change to initial LLM drafts\nwithin a given amount of time as well as writing actions, without decreasing\nsatisfaction with the final blog posts or increasing cognitive load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research-paper blog posts help scientists to disseminate their work to a\nlarger audience, but translating scientific long documents into long-form\nsummaries like blog posts raises unique challenges: 1) planning what paper\ncontent to include in the blog post, 2) drafting the selected content in\nsections amenable to a paper blog post, and 3) revising the blog post to be\nscientifically accurate but also concise, easy to understand, and engaging. Can\nwe harness the power of large language models (LLMs) to assist researchers with\nthese challenges? To investigate this question, we developed Papers-to-Posts,\nan LLM-powered tool that implements a new Plan-Draft-Revise workflow for\nmixed-initiative long-form paper summarization. An LLM-generated paper outline\nwith pre-selected yet adjustable bullet points helps users to plan what\ninformation to include. Meanwhile, customizable LLM instructions support\ndrafting the text with a suitable structure and revising the text to have an\nappropriate tone. Through two studies, we compared Papers-to-Posts to a strong\nbaseline tool that provides an LLM-generated draft and access to free-form LLM\nprompting, and we found that Papers-to-Posts improved researchers' editing\npower. In a within-subjects lab study (N=20 participants), Papers-to-Posts led\nparticipants to make significantly more change to initial LLM drafts within a\nfixed amount of time and to be significantly more satisfied with their final\nblog post, without increasing cognitive load. Furthermore, in a\nbetween-subjects deployment study (N=37 blog posts, 26 participants),\nPapers-to-Posts led participants to make more change to initial LLM drafts\nwithin a given amount of time as well as writing actions, without decreasing\nsatisfaction with the final blog posts or increasing cognitive load."
                },
                "authors": [
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Daniel S. Weld"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Jonathan Bragg"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bragg"
                },
                "author": "Jonathan Bragg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13156v1",
                "updated": "2024-12-17T18:30:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    30,
                    22,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:30:22Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    30,
                    22,
                    1,
                    352,
                    0
                ],
                "title": "S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical\n  Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical\n  Imaging"
                },
                "summary": "Robustness and generalizability in medical image segmentation are often\nhindered by scarcity and limited diversity of training data, which stands in\ncontrast to the variability encountered during inference. While conventional\nstrategies -- such as domain-specific augmentation, specialized architectures,\nand tailored training procedures -- can alleviate these issues, they depend on\nthe availability and reliability of domain knowledge. When such knowledge is\nunavailable, misleading, or improperly applied, performance may deteriorate. In\nresponse, we introduce a novel, domain-agnostic, add-on, and data-driven\nstrategy inspired by image stacking in image denoising. Termed ``semantic\nstacking,'' our method estimates a denoised semantic representation that\ncomplements the conventional segmentation loss during training. This method\ndoes not depend on domain-specific assumptions, making it broadly applicable\nacross diverse image modalities, model architectures, and augmentation\ntechniques. Through extensive experiments, we validate the superiority of our\napproach in improving segmentation performance under diverse conditions. Code\nis available at https://github.com/ymp5078/Semantic-Stacking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness and generalizability in medical image segmentation are often\nhindered by scarcity and limited diversity of training data, which stands in\ncontrast to the variability encountered during inference. While conventional\nstrategies -- such as domain-specific augmentation, specialized architectures,\nand tailored training procedures -- can alleviate these issues, they depend on\nthe availability and reliability of domain knowledge. When such knowledge is\nunavailable, misleading, or improperly applied, performance may deteriorate. In\nresponse, we introduce a novel, domain-agnostic, add-on, and data-driven\nstrategy inspired by image stacking in image denoising. Termed ``semantic\nstacking,'' our method estimates a denoised semantic representation that\ncomplements the conventional segmentation loss during training. This method\ndoes not depend on domain-specific assumptions, making it broadly applicable\nacross diverse image modalities, model architectures, and augmentation\ntechniques. Through extensive experiments, we validate the superiority of our\napproach in improving segmentation performance under diverse conditions. Code\nis available at https://github.com/ymp5078/Semantic-Stacking."
                },
                "authors": [
                    {
                        "name": "Yimu Pan"
                    },
                    {
                        "name": "Sitao Zhang"
                    },
                    {
                        "name": "Alison D. Gernand"
                    },
                    {
                        "name": "Jeffery A. Goldstein"
                    },
                    {
                        "name": "James Z. Wang"
                    }
                ],
                "author_detail": {
                    "name": "James Z. Wang"
                },
                "author": "James Z. Wang",
                "arxiv_comment": "AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10683v2",
                "updated": "2024-12-17T18:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    24,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-14T05:06:38Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    6,
                    38,
                    5,
                    349,
                    0
                ],
                "title": "Adaptive Nonparametric Perturbations of Parametric Bayesian Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Nonparametric Perturbations of Parametric Bayesian Models"
                },
                "summary": "Parametric Bayesian modeling offers a powerful and flexible toolbox for\nscientific data analysis. Yet the model, however detailed, may still be wrong,\nand this can make inferences untrustworthy. In this paper we study\nnonparametrically perturbed parametric (NPP) Bayesian models, in which a\nparametric Bayesian model is relaxed via a distortion of its likelihood. We\nanalyze the properties of NPP models when the target of inference is the true\ndata distribution or some functional of it, such as in causal inference. We\nshow that NPP models can offer the robustness of nonparametric models while\nretaining the data efficiency of parametric models, achieving fast convergence\nwhen the parametric model is close to true. To efficiently analyze data with an\nNPP model, we develop a generalized Bayes procedure to approximate its\nposterior. We demonstrate our method by estimating causal effects of gene\nexpression from single cell RNA sequencing data. NPP modeling offers an\nefficient approach to robust Bayesian inference and can be used to robustify\nany parametric Bayesian model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric Bayesian modeling offers a powerful and flexible toolbox for\nscientific data analysis. Yet the model, however detailed, may still be wrong,\nand this can make inferences untrustworthy. In this paper we study\nnonparametrically perturbed parametric (NPP) Bayesian models, in which a\nparametric Bayesian model is relaxed via a distortion of its likelihood. We\nanalyze the properties of NPP models when the target of inference is the true\ndata distribution or some functional of it, such as in causal inference. We\nshow that NPP models can offer the robustness of nonparametric models while\nretaining the data efficiency of parametric models, achieving fast convergence\nwhen the parametric model is close to true. To efficiently analyze data with an\nNPP model, we develop a generalized Bayes procedure to approximate its\nposterior. We demonstrate our method by estimating causal effects of gene\nexpression from single cell RNA sequencing data. NPP modeling offers an\nefficient approach to robust Bayesian inference and can be used to robustify\nany parametric Bayesian model."
                },
                "authors": [
                    {
                        "name": "Bohan Wu"
                    },
                    {
                        "name": "Eli N. Weinstein"
                    },
                    {
                        "name": "Sohrab Salehi"
                    },
                    {
                        "name": "Yixin Wang"
                    },
                    {
                        "name": "David M. Blei"
                    }
                ],
                "author_detail": {
                    "name": "David M. Blei"
                },
                "author": "David M. Blei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13152v1",
                "updated": "2024-12-17T18:23:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    23,
                    33,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:23:33Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    23,
                    33,
                    1,
                    352,
                    0
                ],
                "title": "Continuous Patient Monitoring with AI: Real-Time Analysis of Video in\n  Hospital Care Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Patient Monitoring with AI: Real-Time Analysis of Video in\n  Hospital Care Settings"
                },
                "summary": "This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions."
                },
                "authors": [
                    {
                        "name": "Paolo Gabriel"
                    },
                    {
                        "name": "Peter Rehani"
                    },
                    {
                        "name": "Tyler Troy"
                    },
                    {
                        "name": "Tiffany Wyatt"
                    },
                    {
                        "name": "Michael Choma"
                    },
                    {
                        "name": "Narinder Singh"
                    }
                ],
                "author_detail": {
                    "name": "Narinder Singh"
                },
                "author": "Narinder Singh",
                "arxiv_comment": "21 pages, 9 figures, 3 tables, submitted to Frontiers in Imaging >\n  Imaging Applications > (Research Topic) Deep Learning for Medical Imaging\n  Applications for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13148v1",
                "updated": "2024-12-17T18:13:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:13:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training\n  With Significant Memory Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training\n  With Significant Memory Reduction"
                },
                "summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen."
                },
                "authors": [
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v1",
                "updated": "2024-12-17T18:12:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10400v2",
                "updated": "2024-12-17T18:05:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    5,
                    11,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-05T16:10:42Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    10,
                    42,
                    3,
                    340,
                    0
                ],
                "title": "Reinforcement Learning Enhanced LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Enhanced LLMs: A Survey"
                },
                "summary": "This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}."
                },
                "authors": [
                    {
                        "name": "Shuhe Wang"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Eduard Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Hovy"
                },
                "author": "Eduard Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13474v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13474v3",
                "updated": "2024-12-17T17:45:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    45,
                    7,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-20T13:05:07Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    13,
                    5,
                    7,
                    4,
                    264,
                    0
                ],
                "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models"
                },
                "summary": "Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization."
                },
                "authors": [
                    {
                        "name": "Anmol Mekala"
                    },
                    {
                        "name": "Vineeth Dorna"
                    },
                    {
                        "name": "Shreya Dubey"
                    },
                    {
                        "name": "Abhishek Lalwani"
                    },
                    {
                        "name": "David Koleczek"
                    },
                    {
                        "name": "Mukund Rungta"
                    },
                    {
                        "name": "Sadid Hasan"
                    },
                    {
                        "name": "Elita Lobo"
                    }
                ],
                "author_detail": {
                    "name": "Elita Lobo"
                },
                "author": "Elita Lobo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13474v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13474v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16383v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16383v4",
                "updated": "2024-12-17T17:42:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    42,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-24T18:35:09Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    35,
                    9,
                    1,
                    268,
                    0
                ],
                "title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation"
                },
                "summary": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings."
                },
                "authors": [
                    {
                        "name": "Ioannis Panagiotopoulos"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16383v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16383v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04049v3",
                "updated": "2024-12-17T17:17:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    17,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-06T14:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    51,
                    55,
                    1,
                    37,
                    0
                ],
                "title": "Systematic Biases in LLM Simulations of Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Biases in LLM Simulations of Debates"
                },
                "summary": "The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations."
                },
                "authors": [
                    {
                        "name": "Amir Taubenfeld"
                    },
                    {
                        "name": "Yaniv Dover"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Ariel Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goldstein"
                },
                "author": "Ariel Goldstein",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.04049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a conference paper at EMNLP 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13103v1",
                "updated": "2024-12-17T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    17,
                    3,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    17,
                    3,
                    1,
                    352,
                    0
                ],
                "title": "AI PERSONA: Towards Life-long Personalization of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI PERSONA: Towards Life-long Personalization of LLMs"
                },
                "summary": "In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems."
                },
                "authors": [
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Ruoyu Fang"
                    },
                    {
                        "name": "Huilin Wang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13102v1",
                "updated": "2024-12-17T17:15:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    15,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T17:15:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    15,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark"
                },
                "summary": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench."
                },
                "authors": [
                    {
                        "name": "Jianlyu Chen"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Hao Liao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v4",
                "updated": "2024-12-17T17:06:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    6,
                    1,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts."
                },
                "authors": [
                    {
                        "name": "Rainer Muehlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "33 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13091v1",
                "updated": "2024-12-17T17:01:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    1,
                    15,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T17:01:15Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    1,
                    15,
                    1,
                    352,
                    0
                ],
                "title": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests"
                },
                "summary": "As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development."
                },
                "authors": [
                    {
                        "name": "Jon Saad-Falcon"
                    },
                    {
                        "name": "Rajan Vivek"
                    },
                    {
                        "name": "William Berrios"
                    },
                    {
                        "name": "Nandita Shankar Naik"
                    },
                    {
                        "name": "Matija Franklin"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Shikib Mehri"
                    }
                ],
                "author_detail": {
                    "name": "Shikib Mehri"
                },
                "author": "Shikib Mehri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09739v2",
                "updated": "2024-12-17T16:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    52,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-15T14:10:01Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    10,
                    1,
                    6,
                    259,
                    0
                ],
                "title": "PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution"
                },
                "summary": "The rapid advancement of customized Large Language Models (LLMs) offers\nconsiderable convenience. However, it also intensifies concerns regarding the\nprotection of copyright/confidential information. With the extensive adoption\nof private LLMs, safeguarding model copyright and ensuring data privacy have\nbecome critical. Text watermarking has emerged as a viable solution for\ndetecting AI-generated content and protecting models. However, existing methods\nfall short in providing individualized watermarks for each user, a critical\nfeature for enhancing accountability and traceability. In this paper, we\nintroduce PersonaMark, a novel personalized text watermarking scheme designed\nto protect LLMs' copyrights and bolster accountability. PersonaMark leverages\nsentence structure as a subtle carrier of watermark information and optimizes\nthe generation process to maintain the natural output of the model. By\nemploying a personalized hashing function, unique watermarks are embedded for\neach user, enabling high-quality text generation without compromising the\nmodel's performance. This approach is both time-efficient and scalable, capable\nof handling large numbers of users through a multi-user hashing mechanism. To\nthe best of our knowledge, this is a pioneer study to explore personalized\nwatermarking in LLMs. We conduct extensive evaluations across four LLMs,\nanalyzing various metrics such as perplexity, sentiment, alignment, and\nreadability. The results validate that PersonaMark preserves text quality,\nensures unbiased watermark insertion, and offers robust watermark detection\ncapabilities, all while maintaining the model's behavior with minimal\ndisruption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of customized Large Language Models (LLMs) offers\nconsiderable convenience. However, it also intensifies concerns regarding the\nprotection of copyright/confidential information. With the extensive adoption\nof private LLMs, safeguarding model copyright and ensuring data privacy have\nbecome critical. Text watermarking has emerged as a viable solution for\ndetecting AI-generated content and protecting models. However, existing methods\nfall short in providing individualized watermarks for each user, a critical\nfeature for enhancing accountability and traceability. In this paper, we\nintroduce PersonaMark, a novel personalized text watermarking scheme designed\nto protect LLMs' copyrights and bolster accountability. PersonaMark leverages\nsentence structure as a subtle carrier of watermark information and optimizes\nthe generation process to maintain the natural output of the model. By\nemploying a personalized hashing function, unique watermarks are embedded for\neach user, enabling high-quality text generation without compromising the\nmodel's performance. This approach is both time-efficient and scalable, capable\nof handling large numbers of users through a multi-user hashing mechanism. To\nthe best of our knowledge, this is a pioneer study to explore personalized\nwatermarking in LLMs. We conduct extensive evaluations across four LLMs,\nanalyzing various metrics such as perplexity, sentiment, alignment, and\nreadability. The results validate that PersonaMark preserves text quality,\nensures unbiased watermark insertion, and offers robust watermark detection\ncapabilities, all while maintaining the model's behavior with minimal\ndisruption."
                },
                "authors": [
                    {
                        "name": "Yuehan Zhang"
                    },
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Yinpeng Liu"
                    },
                    {
                        "name": "Yongqiang Ma"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Liu"
                },
                "author": "Jiawei Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04095v2",
                "updated": "2024-12-17T16:51:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    51,
                    14,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-05T09:30:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    9,
                    30,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "Sharp finite statistics for quantum key distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp finite statistics for quantum key distribution"
                },
                "summary": "The performance of quantum key distribution (QKD) heavily depends on\nstatistical inference. For a broad class of protocols, the central statistical\ntask is a random sampling problem, customarily addressed using exponential tail\nbounds on the hypergeometric distribution. Here we devise a strikingly simple\nexponential bound for this task, of unprecedented tightness among QKD security\nanalyses. As a by-product, confidence intervals for the average of\nnon-identical Bernoulli parameters follow too. These naturally fit in\nstatistical analyses of decoy-state QKD and also outperform standard tools.\nLastly, we show that, in a vast parameter regime, the use of tail bounds is not\nenforced because the cumulative mass function of the hypergeometric\ndistribution is accurately computable. This sharply decreases the minimum block\nsizes necessary for QKD, and reveals the tightness of our simple analytical\nbounds when moderate-to-large blocks are considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of quantum key distribution (QKD) heavily depends on\nstatistical inference. For a broad class of protocols, the central statistical\ntask is a random sampling problem, customarily addressed using exponential tail\nbounds on the hypergeometric distribution. Here we devise a strikingly simple\nexponential bound for this task, of unprecedented tightness among QKD security\nanalyses. As a by-product, confidence intervals for the average of\nnon-identical Bernoulli parameters follow too. These naturally fit in\nstatistical analyses of decoy-state QKD and also outperform standard tools.\nLastly, we show that, in a vast parameter regime, the use of tail bounds is not\nenforced because the cumulative mass function of the hypergeometric\ndistribution is accurately computable. This sharply decreases the minimum block\nsizes necessary for QKD, and reveals the tightness of our simple analytical\nbounds when moderate-to-large blocks are considered."
                },
                "authors": [
                    {
                        "name": "Vaisakh Mannalath"
                    },
                    {
                        "name": "Vctor Zapatero"
                    },
                    {
                        "name": "Marcos Curty"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Curty"
                },
                "author": "Marcos Curty",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06527v2",
                "updated": "2024-12-17T16:44:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    44,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-12T23:19:02Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    23,
                    19,
                    2,
                    0,
                    225,
                    0
                ],
                "title": "Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings."
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Xiao Tang"
                    },
                    {
                        "name": "Abdallah El Ali"
                    },
                    {
                        "name": "Zhuying Li"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Jan de Wit"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Jos A. Bosch"
                    }
                ],
                "author_detail": {
                    "name": "Jos A. Bosch"
                },
                "author": "Jos A. Bosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13074v1",
                "updated": "2024-12-17T16:41:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    41,
                    53,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T16:41:53Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    41,
                    53,
                    1,
                    352,
                    0
                ],
                "title": "Predicting Change, Not States: An Alternate Framework for Neural PDE\n  Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Change, Not States: An Alternate Framework for Neural PDE\n  Surrogates"
                },
                "summary": "Neural surrogates for partial differential equations (PDEs) have become\npopular due to their potential to quickly simulate physics. With a few\nexceptions, neural surrogates generally treat the forward evolution of\ntime-dependent PDEs as a black box by directly predicting the next state. While\nthis is a natural and easy framework for applying neural surrogates, it can be\nan over-simplified and rigid framework for predicting physics. In this work, we\npropose an alternative framework in which neural solvers predict the temporal\nderivative and an ODE integrator forwards the solution in time, which has\nlittle overhead and is broadly applicable across model architectures and PDEs.\nWe find that by simply changing the training target and introducing numerical\nintegration during inference, neural surrogates can gain accuracy and\nstability. Predicting temporal derivatives also allows models to not be\nconstrained to a specific temporal discretization, allowing for flexible\ntime-stepping during inference or training on higher-resolution PDE data.\nLastly, we investigate why this new framework can be beneficial and in what\nsituations does it work well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural surrogates for partial differential equations (PDEs) have become\npopular due to their potential to quickly simulate physics. With a few\nexceptions, neural surrogates generally treat the forward evolution of\ntime-dependent PDEs as a black box by directly predicting the next state. While\nthis is a natural and easy framework for applying neural surrogates, it can be\nan over-simplified and rigid framework for predicting physics. In this work, we\npropose an alternative framework in which neural solvers predict the temporal\nderivative and an ODE integrator forwards the solution in time, which has\nlittle overhead and is broadly applicable across model architectures and PDEs.\nWe find that by simply changing the training target and introducing numerical\nintegration during inference, neural surrogates can gain accuracy and\nstability. Predicting temporal derivatives also allows models to not be\nconstrained to a specific temporal discretization, allowing for flexible\ntime-stepping during inference or training on higher-resolution PDE data.\nLastly, we investigate why this new framework can be beneficial and in what\nsituations does it work well."
                },
                "authors": [
                    {
                        "name": "Anthony Zhou"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11017v2",
                "updated": "2024-12-17T16:27:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    27,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-15T02:10:18Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    10,
                    18,
                    6,
                    350,
                    0
                ],
                "title": "On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning"
                },
                "summary": "Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method."
                },
                "authors": [
                    {
                        "name": "Pengfei Fang"
                    },
                    {
                        "name": "Yongchun Qin"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08495v2",
                "updated": "2024-12-17T16:21:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    21,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-16T02:33:55Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    2,
                    33,
                    55,
                    4,
                    229,
                    0
                ],
                "title": "FunEditor: Achieving Complex Image Edits via Function Aggregation with\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FunEditor: Achieving Complex Image Edits via Function Aggregation with\n  Diffusion Models"
                },
                "summary": "Diffusion models have demonstrated outstanding performance in generative\ntasks, making them ideal candidates for image editing. Recent studies highlight\ntheir ability to apply desired edits effectively by following textual\ninstructions, yet with two key challenges remaining. First, these models\nstruggle to apply multiple edits simultaneously, resulting in computational\ninefficiencies due to their reliance on sequential processing. Second, relying\non textual prompts to determine the editing region can lead to unintended\nalterations to the image. We introduce FunEditor, an efficient diffusion model\ndesigned to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. Our experiments demonstrate that FunEditor\nsignificantly outperforms recent inference-time optimization methods and\nfine-tuned models, either quantitatively across various metrics or through\nvisual comparisons or both, on complex tasks like object movement and object\npasting. In the meantime, with only 4 steps of inference, FunEditor achieves\n5-24x inference speedups over existing popular methods. The code is available\nat: mhmdsmdi.github.io/funeditor/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated outstanding performance in generative\ntasks, making them ideal candidates for image editing. Recent studies highlight\ntheir ability to apply desired edits effectively by following textual\ninstructions, yet with two key challenges remaining. First, these models\nstruggle to apply multiple edits simultaneously, resulting in computational\ninefficiencies due to their reliance on sequential processing. Second, relying\non textual prompts to determine the editing region can lead to unintended\nalterations to the image. We introduce FunEditor, an efficient diffusion model\ndesigned to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. Our experiments demonstrate that FunEditor\nsignificantly outperforms recent inference-time optimization methods and\nfine-tuned models, either quantitatively across various metrics or through\nvisual comparisons or both, on complex tasks like object movement and object\npasting. In the meantime, with only 4 steps of inference, FunEditor achieves\n5-24x inference speedups over existing popular methods. The code is available\nat: mhmdsmdi.github.io/funeditor/."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Samadi"
                    },
                    {
                        "name": "Fred X. Han"
                    },
                    {
                        "name": "Mohammad Salameh"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Fengyu Sun"
                    },
                    {
                        "name": "Chunhua Zhou"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02738v3",
                "updated": "2024-12-17T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    10,
                    26,
                    1,
                    352,
                    0
                ],
                "published": "2024-03-05T07:47:34Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    7,
                    47,
                    34,
                    1,
                    65,
                    0
                ],
                "title": "Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment"
                },
                "summary": "Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs."
                },
                "authors": [
                    {
                        "name": "Congzhi Zhang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v4",
                "updated": "2024-12-17T16:03:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    3,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum"
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20587v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20587v5",
                "updated": "2024-12-17T15:59:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    59,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2023-10-31T16:24:17Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    16,
                    24,
                    17,
                    1,
                    304,
                    0
                ],
                "title": "Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning"
                },
                "summary": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\ncloses the gap between value-based offline RL methods and decision transformers\nin dense-reward tasks. In particular, our method demonstrates superior\nperformance in scenarios with limited data samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\ncloses the gap between value-based offline RL methods and decision transformers\nin dense-reward tasks. In particular, our method demonstrates superior\nperformance in scenarios with limited data samples."
                },
                "authors": [
                    {
                        "name": "Ruizhe Shi"
                    },
                    {
                        "name": "Yuyao Liu"
                    },
                    {
                        "name": "Yanjie Ze"
                    },
                    {
                        "name": "Simon S. Du"
                    },
                    {
                        "name": "Huazhe Xu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhe Xu"
                },
                "author": "Huazhe Xu",
                "arxiv_comment": "Format adjustment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20587v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20587v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10582v2",
                "updated": "2024-12-17T15:56:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    56,
                    50,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T21:48:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    21,
                    48,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "WHAT-IF: Exploring Branching Narratives by Meta-Prompting Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHAT-IF: Exploring Branching Narratives by Meta-Prompting Large Language\n  Models"
                },
                "summary": "WHAT-IF -- Writing a Hero's Alternate Timeline through Interactive Fiction --\nis a system that uses zero-shot meta-prompting to create branching narratives\nfrom a prewritten story. Played as an interactive fiction (IF) game, WHAT-IF\nlets the player choose between decisions that the large language model (LLM)\nGPT-4 generates as possible branches in the story. Starting with an existing\nlinear plot as input, a branch is created at each key decision taken by the\nmain character. By meta-prompting the LLM to consider the major plot points\nfrom the story, the system produces coherent and well-structured alternate\nstorylines. WHAT-IF stores the branching plot tree in a graph which helps it to\nboth keep track of the story for prompting and maintain the structure for the\nfinal IF system. A video demo of our system can be found here:\nhttps://youtu.be/8vBqjqtupcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHAT-IF -- Writing a Hero's Alternate Timeline through Interactive Fiction --\nis a system that uses zero-shot meta-prompting to create branching narratives\nfrom a prewritten story. Played as an interactive fiction (IF) game, WHAT-IF\nlets the player choose between decisions that the large language model (LLM)\nGPT-4 generates as possible branches in the story. Starting with an existing\nlinear plot as input, a branch is created at each key decision taken by the\nmain character. By meta-prompting the LLM to consider the major plot points\nfrom the story, the system produces coherent and well-structured alternate\nstorylines. WHAT-IF stores the branching plot tree in a graph which helps it to\nboth keep track of the story for prompting and maintain the structure for the\nfinal IF system. A video demo of our system can be found here:\nhttps://youtu.be/8vBqjqtupcc."
                },
                "authors": [
                    {
                        "name": "Runsheng \"Anson\" Huang"
                    },
                    {
                        "name": "Lara J. Martin"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13024v1",
                "updated": "2024-12-17T15:42:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    42,
                    3,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:42:03Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    42,
                    3,
                    1,
                    352,
                    0
                ],
                "title": "Late-Time Optical and X-ray Emission Evolution of the Oxygen-Rich SN\n  1996cr",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Late-Time Optical and X-ray Emission Evolution of the Oxygen-Rich SN\n  1996cr"
                },
                "summary": "When the ejecta of supernovae interact with the progenitor star's\ncircumstellar environment, a strong shock is driven back into the ejecta,\ncausing the material to become bright optically and in X-rays. Most notably, as\nthe shock traverses the H-rich envelope, it begins to interact with metal rich\nmaterial. Thus, continued monitoring of bright and nearby supernovae provides\nvaluable clues about both the progenitor structure and its pre-supernova\nevolution. Here we present late-time, multi-epoch optical and Chandra} X-ray\nspectra of the core-collapse supernova SN 1996cr. Magellan IMACS optical\nspectra taken in July 2017 and August 2021 show a very different spectrum from\nthat seen in 2006 with broad, double-peaked optical emission lines of oxygen,\nargon, and sulfur with expansion velocities of $\\pm 4500$ km s$^{-1}$.\nRed-shifted emission components are considerably fainter compared to the\nblue-shifted components, presumably due to internal extinction from dust in the\nsupernova ejecta. Broad $\\pm 2400$ km s$^{-1}$ H$\\alpha$ is also seen which we\ninfer is shocked progenitor pre-SN mass-loss, H-rich material. Chandra data\nindicate a slow but steady decline in overall X-ray luminosity, suggesting that\nthe forward shock has broken through any circumstellar shell or torus which is\ninferred from prior deep Chandra ACIS-S/HETG observations. The X-ray properties\nare consistent with what is expected from a shock breaking out into a lower\ndensity environment. Though originally identified as a SN IIn, based upon late\ntime optical emission line spectra, we argue that the SN 1996cr progenitor was\npartially or highly stripped, suggesting a SN IIb/Ib.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the ejecta of supernovae interact with the progenitor star's\ncircumstellar environment, a strong shock is driven back into the ejecta,\ncausing the material to become bright optically and in X-rays. Most notably, as\nthe shock traverses the H-rich envelope, it begins to interact with metal rich\nmaterial. Thus, continued monitoring of bright and nearby supernovae provides\nvaluable clues about both the progenitor structure and its pre-supernova\nevolution. Here we present late-time, multi-epoch optical and Chandra} X-ray\nspectra of the core-collapse supernova SN 1996cr. Magellan IMACS optical\nspectra taken in July 2017 and August 2021 show a very different spectrum from\nthat seen in 2006 with broad, double-peaked optical emission lines of oxygen,\nargon, and sulfur with expansion velocities of $\\pm 4500$ km s$^{-1}$.\nRed-shifted emission components are considerably fainter compared to the\nblue-shifted components, presumably due to internal extinction from dust in the\nsupernova ejecta. Broad $\\pm 2400$ km s$^{-1}$ H$\\alpha$ is also seen which we\ninfer is shocked progenitor pre-SN mass-loss, H-rich material. Chandra data\nindicate a slow but steady decline in overall X-ray luminosity, suggesting that\nthe forward shock has broken through any circumstellar shell or torus which is\ninferred from prior deep Chandra ACIS-S/HETG observations. The X-ray properties\nare consistent with what is expected from a shock breaking out into a lower\ndensity environment. Though originally identified as a SN IIn, based upon late\ntime optical emission line spectra, we argue that the SN 1996cr progenitor was\npartially or highly stripped, suggesting a SN IIb/Ib."
                },
                "authors": [
                    {
                        "name": "Daniel Patnaude"
                    },
                    {
                        "name": "Kathryn Weil"
                    },
                    {
                        "name": "Robert Fesen"
                    },
                    {
                        "name": "Dan Milisavljevic"
                    },
                    {
                        "name": "Ralph Kraft"
                    }
                ],
                "author_detail": {
                    "name": "Ralph Kraft"
                },
                "author": "Ralph Kraft",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13023v1",
                "updated": "2024-12-17T15:41:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    41,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:41:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    41,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Relational Neurosymbolic Markov Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Neurosymbolic Markov Models"
                },
                "summary": "Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Lennert De Smet"
                    },
                    {
                        "name": "Gabriele Venturato"
                    },
                    {
                        "name": "Luc De Raedt"
                    },
                    {
                        "name": "Giuseppe Marra"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Marra"
                },
                "author": "Giuseppe Marra",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13018v1",
                "updated": "2024-12-17T15:38:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    38,
                    42,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:38:42Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    38,
                    42,
                    1,
                    352,
                    0
                ],
                "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain"
                },
                "summary": "As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}."
                },
                "authors": [
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12957v2",
                "updated": "2024-12-17T15:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    38,
                    23,
                    1,
                    352,
                    0
                ],
                "published": "2024-04-19T15:40:39Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    15,
                    40,
                    39,
                    4,
                    110,
                    0
                ],
                "title": "Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction"
                },
                "summary": "In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE"
                },
                "authors": [
                    {
                        "name": "Qinyuan Wu"
                    },
                    {
                        "name": "Mohammad Aflah Khan"
                    },
                    {
                        "name": "Soumi Das"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Camila Kolling"
                    },
                    {
                        "name": "Till Speicher"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Evimaria Terzi"
                    }
                ],
                "author_detail": {
                    "name": "Evimaria Terzi"
                },
                "author": "Evimaria Terzi",
                "arxiv_doi": "10.1145/3701551.3703562",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703562",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.12957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13013v1",
                "updated": "2024-12-17T15:34:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:34:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "The Emergence of Strategic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Strategic Reasoning of Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are increasingly used for a variety of\ncomplex and critical tasks, it is vital to assess their logical capabilities in\nstrategic environments. This paper examines their ability in strategic\nreasoning -- the process of choosing an optimal course of action by predicting\nand adapting to other agents' behavior. Using six LLMs, we analyze responses\nfrom play in classical games from behavioral economics (p-Beauty Contest, 11-20\nMoney Request Game, and Guessing Game) and evaluate their performance through\nhierarchical models of reasoning (level-$k$ theory and cognitive hierarchy\ntheory). Our findings reveal that while LLMs show understanding of the games,\nthe majority struggle with higher-order strategic reasoning. Although most LLMs\ndid demonstrate learning ability with games involving repeated interactions,\nthey still consistently fall short of the reasoning levels demonstrated by\ntypical behavior from human subjects. The exception to these overall findings\nis with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning\ntasks -- which consistently outperforms other LLMs and human subjects. These\nfindings highlight the challenges and pathways in advancing LLMs toward robust\nstrategic reasoning from the perspective of behavioral economics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly used for a variety of\ncomplex and critical tasks, it is vital to assess their logical capabilities in\nstrategic environments. This paper examines their ability in strategic\nreasoning -- the process of choosing an optimal course of action by predicting\nand adapting to other agents' behavior. Using six LLMs, we analyze responses\nfrom play in classical games from behavioral economics (p-Beauty Contest, 11-20\nMoney Request Game, and Guessing Game) and evaluate their performance through\nhierarchical models of reasoning (level-$k$ theory and cognitive hierarchy\ntheory). Our findings reveal that while LLMs show understanding of the games,\nthe majority struggle with higher-order strategic reasoning. Although most LLMs\ndid demonstrate learning ability with games involving repeated interactions,\nthey still consistently fall short of the reasoning levels demonstrated by\ntypical behavior from human subjects. The exception to these overall findings\nis with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning\ntasks -- which consistently outperforms other LLMs and human subjects. These\nfindings highlight the challenges and pathways in advancing LLMs toward robust\nstrategic reasoning from the perspective of behavioral economics."
                },
                "authors": [
                    {
                        "name": "Dongwoo Lee"
                    },
                    {
                        "name": "Gavin Kader"
                    }
                ],
                "author_detail": {
                    "name": "Gavin Kader"
                },
                "author": "Gavin Kader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12589v2",
                "updated": "2024-12-17T15:28:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    28,
                    10,
                    1,
                    352,
                    0
                ],
                "published": "2024-03-19T09:48:18Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    9,
                    48,
                    18,
                    1,
                    79,
                    0
                ],
                "title": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting"
                },
                "summary": "Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition."
                },
                "authors": [
                    {
                        "name": "Clment Gaspard"
                    },
                    {
                        "name": "Grgoire Passault"
                    },
                    {
                        "name": "Mlodie Daniel"
                    },
                    {
                        "name": "Olivier Ly"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Ly"
                },
                "author": "Olivier Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13006v1",
                "updated": "2024-12-17T15:26:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    26,
                    15,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:26:15Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    26,
                    15,
                    1,
                    352,
                    0
                ],
                "title": "What is YOLOv6? A Deep Insight into the Object Detection Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is YOLOv6? A Deep Insight into the Object Detection Model"
                },
                "summary": "This work explores the YOLOv6 object detection model in depth, concentrating\non its design framework, optimization techniques, and detection capabilities.\nYOLOv6's core elements consist of the EfficientRep Backbone for robust feature\nextraction and the Rep-PAN Neck for seamless feature aggregation, ensuring\nhigh-performance object detection. Evaluated on the COCO dataset, YOLOv6-N\nachieves 37.5\\% AP at 1187 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S reaches\n45.0\\% AP at 484 FPS, outperforming models like PPYOLOE-S, YOLOv5-S, YOLOX-S,\nand YOLOv8-S in the same class. Moreover, YOLOv6-M and YOLOv6-L also show\nbetter accuracy (50.0\\% and 52.8\\%) while maintaining comparable inference\nspeeds to other detectors. With an upgraded backbone and neck structure,\nYOLOv6-L6 delivers cutting-edge accuracy in real-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the YOLOv6 object detection model in depth, concentrating\non its design framework, optimization techniques, and detection capabilities.\nYOLOv6's core elements consist of the EfficientRep Backbone for robust feature\nextraction and the Rep-PAN Neck for seamless feature aggregation, ensuring\nhigh-performance object detection. Evaluated on the COCO dataset, YOLOv6-N\nachieves 37.5\\% AP at 1187 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S reaches\n45.0\\% AP at 484 FPS, outperforming models like PPYOLOE-S, YOLOv5-S, YOLOX-S,\nand YOLOv8-S in the same class. Moreover, YOLOv6-M and YOLOv6-L also show\nbetter accuracy (50.0\\% and 52.8\\%) while maintaining comparable inference\nspeeds to other detectors. With an upgraded backbone and neck structure,\nYOLOv6-L6 delivers cutting-edge accuracy in real-time."
                },
                "authors": [
                    {
                        "name": "Athulya Sundaresan Geetha"
                    }
                ],
                "author_detail": {
                    "name": "Athulya Sundaresan Geetha"
                },
                "author": "Athulya Sundaresan Geetha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12983v1",
                "updated": "2024-12-17T15:02:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    2,
                    33,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:02:33Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    2,
                    33,
                    1,
                    352,
                    0
                ],
                "title": "Exploring natural variation in tendon constitutive parameters via\n  Bayesian data selection and mixed effects models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring natural variation in tendon constitutive parameters via\n  Bayesian data selection and mixed effects models"
                },
                "summary": "Combining microstructural mechanical models with experimental data enhances\nour understanding of the mechanics of soft tissue, such as tendons. In previous\nwork, a Bayesian framework was used to infer constitutive parameters from\nuniaxial stress-strain experiments on horse tendons, specifically the\nsuperficial digital flexor tendon (SDFT) and common digital extensor tendon\n(CDET), on a per-experiment basis. Here, we extend this analysis to investigate\nthe natural variation of these parameters across a population of horses. Using\na Bayesian mixed effects model, we infer population distributions of these\nparameters. Given that the chosen hyperelastic model does not account for\ntendon damage, careful data selection is necessary. Avoiding ad hoc methods, we\nintroduce a hierarchical Bayesian data selection method. This two-stage\napproach selects data per experiment, and integrates data weightings into the\nBayesian mixed effects model. Our results indicate that the CDET is stiffer\nthan the SDFT, likely due to a higher collagen volume fraction. The modes of\nthe parameter distributions yield estimates of the product of the collagen\nvolume fraction and Young's modulus as 811.5 MPa for the SDFT and 1430.2 MPa\nfor the CDET. This suggests that positional tendons have stiffer collagen\nfibrils and/or higher collagen volume density than energy-storing tendons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining microstructural mechanical models with experimental data enhances\nour understanding of the mechanics of soft tissue, such as tendons. In previous\nwork, a Bayesian framework was used to infer constitutive parameters from\nuniaxial stress-strain experiments on horse tendons, specifically the\nsuperficial digital flexor tendon (SDFT) and common digital extensor tendon\n(CDET), on a per-experiment basis. Here, we extend this analysis to investigate\nthe natural variation of these parameters across a population of horses. Using\na Bayesian mixed effects model, we infer population distributions of these\nparameters. Given that the chosen hyperelastic model does not account for\ntendon damage, careful data selection is necessary. Avoiding ad hoc methods, we\nintroduce a hierarchical Bayesian data selection method. This two-stage\napproach selects data per experiment, and integrates data weightings into the\nBayesian mixed effects model. Our results indicate that the CDET is stiffer\nthan the SDFT, likely due to a higher collagen volume fraction. The modes of\nthe parameter distributions yield estimates of the product of the collagen\nvolume fraction and Young's modulus as 811.5 MPa for the SDFT and 1430.2 MPa\nfor the CDET. This suggests that positional tendons have stiffer collagen\nfibrils and/or higher collagen volume density than energy-storing tendons."
                },
                "authors": [
                    {
                        "name": "James Casey"
                    },
                    {
                        "name": "Jessica Forsyth"
                    },
                    {
                        "name": "Timothy Waite"
                    },
                    {
                        "name": "Simon Cotter"
                    },
                    {
                        "name": "Tom Shearer"
                    }
                ],
                "author_detail": {
                    "name": "Tom Shearer"
                },
                "author": "Tom Shearer",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12981v1",
                "updated": "2024-12-17T15:01:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    1,
                    7,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:01:07Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    1,
                    7,
                    1,
                    352,
                    0
                ],
                "title": "Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental\n  Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental\n  Health"
                },
                "summary": "Large language models (LLMs) have shown promising capabilities in healthcare\nanalysis but face several challenges like hallucinations, parroting, and bias\nmanifestation. These challenges are exacerbated in complex, sensitive, and\nlow-resource domains. Therefore, in this work we introduce IC-AnnoMI, an\nexpert-annotated motivational interviewing (MI) dataset built upon AnnoMI by\ngenerating in-context conversational dialogues leveraging LLMs, particularly\nChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues\nand tailored information, taking into account therapy style (empathy,\nreflection), contextual relevance, and false semantic change. Subsequently, the\ndialogues are annotated by experts, strictly adhering to the Motivational\nInterviewing Skills Code (MISC), focusing on both the psychological and\nlinguistic dimensions of MI dialogues. We comprehensively evaluate the\nIC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding\nof domain intricacies by modeling novel classification tasks employing several\nclassical machine learning and current state-of-the-art transformer approaches.\nFinally, we discuss the effects of progressive prompting strategies and the\nimpact of augmented data in mitigating the biases manifested in IC-AnnoM. Our\ncontributions provide the MI community with not only a comprehensive dataset\nbut also valuable insights for using LLMs in empathetic text generation for\nconversational therapy in supervised settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising capabilities in healthcare\nanalysis but face several challenges like hallucinations, parroting, and bias\nmanifestation. These challenges are exacerbated in complex, sensitive, and\nlow-resource domains. Therefore, in this work we introduce IC-AnnoMI, an\nexpert-annotated motivational interviewing (MI) dataset built upon AnnoMI by\ngenerating in-context conversational dialogues leveraging LLMs, particularly\nChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues\nand tailored information, taking into account therapy style (empathy,\nreflection), contextual relevance, and false semantic change. Subsequently, the\ndialogues are annotated by experts, strictly adhering to the Motivational\nInterviewing Skills Code (MISC), focusing on both the psychological and\nlinguistic dimensions of MI dialogues. We comprehensively evaluate the\nIC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding\nof domain intricacies by modeling novel classification tasks employing several\nclassical machine learning and current state-of-the-art transformer approaches.\nFinally, we discuss the effects of progressive prompting strategies and the\nimpact of augmented data in mitigating the biases manifested in IC-AnnoM. Our\ncontributions provide the MI community with not only a comprehensive dataset\nbut also valuable insights for using LLMs in empathetic text generation for\nconversational therapy in supervised settings."
                },
                "authors": [
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Eirini Ntoutsi"
                    },
                    {
                        "name": "Pushpraj Singh Rajawat"
                    },
                    {
                        "name": "Giacomo Medda"
                    },
                    {
                        "name": "Diego Reforgiato Recupero"
                    }
                ],
                "author_detail": {
                    "name": "Diego Reforgiato Recupero"
                },
                "author": "Diego Reforgiato Recupero",
                "arxiv_comment": "International Conference on Natural Language Processing and\n  Artificial Intelligence for Cyber Security (NLPAICS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12971v1",
                "updated": "2024-12-17T14:54:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    54,
                    30,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:54:30Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    54,
                    30,
                    1,
                    352,
                    0
                ],
                "title": "ArchesWeather & ArchesWeatherGen: a deterministic and generative model\n  for efficient ML weather forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArchesWeather & ArchesWeatherGen: a deterministic and generative model\n  for efficient ML weather forecasting"
                },
                "summary": "Weather forecasting plays a vital role in today's society, from agriculture\nand logistics to predicting the output of renewable energies, and preparing for\nextreme weather events. Deep learning weather forecasting models trained with\nthe next state prediction objective on ERA5 have shown great success compared\nto numerical global circulation models. However, for a wide range of\napplications, being able to provide representative samples from the\ndistribution of possible future weather states is critical. In this paper, we\npropose a methodology to leverage deterministic weather models in the design of\nprobabilistic weather models, leading to improved performance and reduced\ncomputing costs. We first introduce \\textbf{ArchesWeather}, a transformer-based\ndeterministic model that improves upon Pangu-Weather by removing\noverrestrictive inductive priors. We then design a probabilistic weather model\ncalled \\textbf{ArchesWeatherGen} based on flow matching, a modern variant of\ndiffusion models, that is trained to project ArchesWeather's predictions to the\ndistribution of ERA5 weather states. ArchesWeatherGen is a true stochastic\nemulator of ERA5 and surpasses IFS ENS and NeuralGCM on all WeatherBench\nheadline variables (except for NeuralGCM's geopotential). Our work also aims to\ndemocratize the use of deterministic and generative machine learning models in\nweather forecasting research, with academic computing resources. All models are\ntrained at 1.5{\\deg} resolution, with a training budget of $\\sim$9 V100 days\nfor ArchesWeather and $\\sim$45 V100 days for ArchesWeatherGen. For inference,\nArchesWeatherGen generates 15-day weather trajectories at a rate of 1 minute\nper ensemble member on a A100 GPU card. To make our work fully reproducible,\nour code and models are open source, including the complete pipeline for data\npreparation, training, and evaluation, at https://github.com/INRIA/geoarches .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weather forecasting plays a vital role in today's society, from agriculture\nand logistics to predicting the output of renewable energies, and preparing for\nextreme weather events. Deep learning weather forecasting models trained with\nthe next state prediction objective on ERA5 have shown great success compared\nto numerical global circulation models. However, for a wide range of\napplications, being able to provide representative samples from the\ndistribution of possible future weather states is critical. In this paper, we\npropose a methodology to leverage deterministic weather models in the design of\nprobabilistic weather models, leading to improved performance and reduced\ncomputing costs. We first introduce \\textbf{ArchesWeather}, a transformer-based\ndeterministic model that improves upon Pangu-Weather by removing\noverrestrictive inductive priors. We then design a probabilistic weather model\ncalled \\textbf{ArchesWeatherGen} based on flow matching, a modern variant of\ndiffusion models, that is trained to project ArchesWeather's predictions to the\ndistribution of ERA5 weather states. ArchesWeatherGen is a true stochastic\nemulator of ERA5 and surpasses IFS ENS and NeuralGCM on all WeatherBench\nheadline variables (except for NeuralGCM's geopotential). Our work also aims to\ndemocratize the use of deterministic and generative machine learning models in\nweather forecasting research, with academic computing resources. All models are\ntrained at 1.5{\\deg} resolution, with a training budget of $\\sim$9 V100 days\nfor ArchesWeather and $\\sim$45 V100 days for ArchesWeatherGen. For inference,\nArchesWeatherGen generates 15-day weather trajectories at a rate of 1 minute\nper ensemble member on a A100 GPU card. To make our work fully reproducible,\nour code and models are open source, including the complete pipeline for data\npreparation, training, and evaluation, at https://github.com/INRIA/geoarches ."
                },
                "authors": [
                    {
                        "name": "Guillaume Couairon"
                    },
                    {
                        "name": "Renu Singh"
                    },
                    {
                        "name": "Anastase Charantonis"
                    },
                    {
                        "name": "Christian Lessig"
                    },
                    {
                        "name": "Claire Monteleoni"
                    }
                ],
                "author_detail": {
                    "name": "Claire Monteleoni"
                },
                "author": "Claire Monteleoni",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12968v1",
                "updated": "2024-12-17T14:53:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    53,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:53:38Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    53,
                    38,
                    1,
                    352,
                    0
                ],
                "title": "On Local Overfitting and Forgetting in Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Local Overfitting and Forgetting in Deep Neural Networks"
                },
                "summary": "The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols."
                },
                "authors": [
                    {
                        "name": "Uri Stern"
                    },
                    {
                        "name": "Tomer Yaacoby"
                    },
                    {
                        "name": "Daphna Weinshall"
                    }
                ],
                "author_detail": {
                    "name": "Daphna Weinshall"
                },
                "author": "Daphna Weinshall",
                "arxiv_comment": "to appear in AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12967v1",
                "updated": "2024-12-17T14:51:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    51,
                    46,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:51:46Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    51,
                    46,
                    1,
                    352,
                    0
                ],
                "title": "Neural Posterior Estimation for Stochastic Epidemic Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Posterior Estimation for Stochastic Epidemic Modeling"
                },
                "summary": "Stochastic infectious disease models capture uncertainty in public health\noutcomes and have become increasingly popular in epidemiological practice.\nHowever, calibrating these models to observed data is challenging with existing\nmethods for parameter estimation. Stochastic epidemic models are nonlinear\ndynamical systems with potentially large latent state spaces, resulting in\ncomputationally intractable likelihood densities. We develop an approach to\ncalibrating complex epidemiological models to high-dimensional data using\nNeural Posterior Estimation, a novel technique for simulation-based inference.\nIn NPE, a neural conditional density estimator trained on simulated data learns\nto \"invert\" a stochastic simulator, returning a parametric approximation to the\nposterior distribution. We introduce a stochastic, discrete-time Susceptible\nInfected (SI) model with heterogeneous transmission for healthcare-associated\ninfections (HAIs). HAIs are a major burden on healthcare systems. They exhibit\nhigh rates of asymptotic carriage, making it difficult to estimate infection\nrates. Through extensive simulation experiments, we show that NPE produces\naccurate posterior estimates of infection rates with greater sample efficiency\ncompared to Approximate Bayesian Computation (ABC). We then use NPE to fit our\nSI model to an outbreak of carbapenem-resistant Klebsiella pneumoniae in a\nlong-term acute care facility, finding evidence of location-based heterogeneity\nin patient-to-patient transmission risk. We argue that our methodology can be\nfruitfully applied to a wide range of mechanistic transmission models and\nproblems in the epidemiology of infectious disease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic infectious disease models capture uncertainty in public health\noutcomes and have become increasingly popular in epidemiological practice.\nHowever, calibrating these models to observed data is challenging with existing\nmethods for parameter estimation. Stochastic epidemic models are nonlinear\ndynamical systems with potentially large latent state spaces, resulting in\ncomputationally intractable likelihood densities. We develop an approach to\ncalibrating complex epidemiological models to high-dimensional data using\nNeural Posterior Estimation, a novel technique for simulation-based inference.\nIn NPE, a neural conditional density estimator trained on simulated data learns\nto \"invert\" a stochastic simulator, returning a parametric approximation to the\nposterior distribution. We introduce a stochastic, discrete-time Susceptible\nInfected (SI) model with heterogeneous transmission for healthcare-associated\ninfections (HAIs). HAIs are a major burden on healthcare systems. They exhibit\nhigh rates of asymptotic carriage, making it difficult to estimate infection\nrates. Through extensive simulation experiments, we show that NPE produces\naccurate posterior estimates of infection rates with greater sample efficiency\ncompared to Approximate Bayesian Computation (ABC). We then use NPE to fit our\nSI model to an outbreak of carbapenem-resistant Klebsiella pneumoniae in a\nlong-term acute care facility, finding evidence of location-based heterogeneity\nin patient-to-patient transmission risk. We argue that our methodology can be\nfruitfully applied to a wide range of mechanistic transmission models and\nproblems in the epidemiology of infectious disease."
                },
                "authors": [
                    {
                        "name": "Prayag Chatha"
                    },
                    {
                        "name": "Fan Bu"
                    },
                    {
                        "name": "Jeffrey Regier"
                    },
                    {
                        "name": "Evan Snitkin"
                    },
                    {
                        "name": "Jon Zelner"
                    }
                ],
                "author_detail": {
                    "name": "Jon Zelner"
                },
                "author": "Jon Zelner",
                "arxiv_comment": "36 pages, 22 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12961v1",
                "updated": "2024-12-17T14:44:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    44,
                    27,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:44:27Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    44,
                    27,
                    1,
                    352,
                    0
                ],
                "title": "Adaptations of AI models for querying the LandMatrix database in natural\n  language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptations of AI models for querying the LandMatrix database in natural\n  language"
                },
                "summary": "The Land Matrix initiative (https://landmatrix.org) and its global\nobservatory aim to provide reliable data on large-scale land acquisitions to\ninform debates and actions in sectors such as agriculture, extraction, or\nenergy in low- and middle-income countries. Although these data are recognized\nin the academic world, they remain underutilized in public policy, mainly due\nto the complexity of access and exploitation, which requires technical\nexpertise and a good understanding of the database schema.\n  The objective of this work is to simplify access to data from different\ndatabase systems. The methods proposed in this article are evaluated using data\nfrom the Land Matrix. This work presents various comparisons of Large Language\nModels (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,\nRAG, Agents) to query different database systems (GraphQL and REST queries).\nThe experiments are reproducible, and a demonstration is available online:\nhttps://github.com/tetis-nlp/landmatrix-graphql-python.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Land Matrix initiative (https://landmatrix.org) and its global\nobservatory aim to provide reliable data on large-scale land acquisitions to\ninform debates and actions in sectors such as agriculture, extraction, or\nenergy in low- and middle-income countries. Although these data are recognized\nin the academic world, they remain underutilized in public policy, mainly due\nto the complexity of access and exploitation, which requires technical\nexpertise and a good understanding of the database schema.\n  The objective of this work is to simplify access to data from different\ndatabase systems. The methods proposed in this article are evaluated using data\nfrom the Land Matrix. This work presents various comparisons of Large Language\nModels (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,\nRAG, Agents) to query different database systems (GraphQL and REST queries).\nThe experiments are reproducible, and a demonstration is available online:\nhttps://github.com/tetis-nlp/landmatrix-graphql-python."
                },
                "authors": [
                    {
                        "name": "Fatiha Ait Kbir"
                    },
                    {
                        "name": "Jrmy Bourgoin"
                    },
                    {
                        "name": "Rmy Decoupes"
                    },
                    {
                        "name": "Marie Gradeler"
                    },
                    {
                        "name": "Roberto Interdonato"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Interdonato"
                },
                "author": "Roberto Interdonato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12956v1",
                "updated": "2024-12-17T14:38:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    38,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:38:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    38,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "SnakModel: Lessons Learned from Training an Open Danish Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnakModel: Lessons Learned from Training an Open Danish Large Language\n  Model"
                },
                "summary": "We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints."
                },
                "authors": [
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Max Mller-Eberstein"
                    },
                    {
                        "name": "Elisa Bassignana"
                    },
                    {
                        "name": "Rob van der Goot"
                    }
                ],
                "author_detail": {
                    "name": "Rob van der Goot"
                },
                "author": "Rob van der Goot",
                "arxiv_comment": "Accepted at NoDaLiDa 2025 (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12951v1",
                "updated": "2024-12-17T14:33:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    33,
                    5,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:33:05Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    33,
                    5,
                    1,
                    352,
                    0
                ],
                "title": "FineGates: LLMs Finetuning with Compression using Stochastic Gates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineGates: LLMs Finetuning with Compression using Stochastic Gates"
                },
                "summary": "Large Language Models (LLMs), with billions of parameters, present\nsignificant challenges for full finetuning due to the high computational\ndemands, memory requirements, and impracticality of many real-world\napplications. When faced with limited computational resources or small\ndatasets, updating all model parameters can often result in overfitting. To\naddress this, lightweight finetuning techniques have been proposed, like\nlearning low-rank adapter layers. These methods aim to train only a few\nadditional parameters combined with the base model, which remains frozen,\nreducing resource usage and mitigating overfitting risks. In this work, we\npropose an adaptor model based on stochastic gates that simultaneously sparsify\nthe frozen base model with task-specific adaptation. Our method comes with a\nsmall number of trainable parameters and allows us to speed up the base model\ninference with competitive accuracy. We evaluate it in additional variants by\nequipping it with additional low-rank parameters and comparing it to several\nrecent baselines. Our results show that the proposed method improves the\nfinetuned model accuracy comparatively to the several baselines and allows the\nremoval of up to 20-40\\% without significant accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with billions of parameters, present\nsignificant challenges for full finetuning due to the high computational\ndemands, memory requirements, and impracticality of many real-world\napplications. When faced with limited computational resources or small\ndatasets, updating all model parameters can often result in overfitting. To\naddress this, lightweight finetuning techniques have been proposed, like\nlearning low-rank adapter layers. These methods aim to train only a few\nadditional parameters combined with the base model, which remains frozen,\nreducing resource usage and mitigating overfitting risks. In this work, we\npropose an adaptor model based on stochastic gates that simultaneously sparsify\nthe frozen base model with task-specific adaptation. Our method comes with a\nsmall number of trainable parameters and allows us to speed up the base model\ninference with competitive accuracy. We evaluate it in additional variants by\nequipping it with additional low-rank parameters and comparing it to several\nrecent baselines. Our results show that the proposed method improves the\nfinetuned model accuracy comparatively to the several baselines and allows the\nremoval of up to 20-40\\% without significant accuracy loss."
                },
                "authors": [
                    {
                        "name": "Jonathan Svirsky"
                    },
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09916v2",
                "updated": "2024-12-17T14:30:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    30,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T07:08:13Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    13,
                    4,
                    348,
                    0
                ],
                "title": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer"
                },
                "summary": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents."
                },
                "authors": [
                    {
                        "name": "Sehyeong Jo"
                    },
                    {
                        "name": "Jungwon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jungwon Seo"
                },
                "author": "Jungwon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11497v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11497v3",
                "updated": "2024-12-17T14:11:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    11,
                    19,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-17T13:01:12Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    1,
                    12,
                    0,
                    169,
                    0
                ],
                "title": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen1.5-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen1.5-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods."
                },
                "authors": [
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "AAAI25 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11497v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11497v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12928v1",
                "updated": "2024-12-17T14:07:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    7,
                    1,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    7,
                    1,
                    1,
                    352,
                    0
                ],
                "title": "Truthful Text Sanitization Guided by Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful Text Sanitization Guided by Inference Attacks"
                },
                "summary": "The purpose of text sanitization is to rewrite those text spans in a document\nthat may directly or indirectly identify an individual, to ensure they no\nlonger disclose personal information. Text sanitization must strike a balance\nbetween preventing the leakage of personal information (privacy protection)\nwhile also retaining as much of the document's original content as possible\n(utility preservation). We present an automated text sanitization strategy\nbased on generalizations, which are more abstract (but still informative) terms\nthat subsume the semantic content of the original text spans. The approach\nrelies on instruction-tuned large language models (LLMs) and is divided into\ntwo stages. The LLM is first applied to obtain truth-preserving replacement\ncandidates and rank them according to their abstraction level. Those candidates\nare then evaluated for their ability to protect privacy by conducting inference\nattacks with the LLM. Finally, the system selects the most informative\nreplacement shown to be resistant to those attacks. As a consequence of this\ntwo-stage process, the chosen replacements effectively balance utility and\nprivacy. We also present novel metrics to automatically evaluate these two\naspects without the need to manually annotate data. Empirical results on the\nText Anonymization Benchmark show that the proposed approach leads to enhanced\nutility, with only a marginal increase in the risk of re-identifying protected\nindividuals compared to fully suppressing the original information.\nFurthermore, the selected replacements are shown to be more truth-preserving\nand abstractive than previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of text sanitization is to rewrite those text spans in a document\nthat may directly or indirectly identify an individual, to ensure they no\nlonger disclose personal information. Text sanitization must strike a balance\nbetween preventing the leakage of personal information (privacy protection)\nwhile also retaining as much of the document's original content as possible\n(utility preservation). We present an automated text sanitization strategy\nbased on generalizations, which are more abstract (but still informative) terms\nthat subsume the semantic content of the original text spans. The approach\nrelies on instruction-tuned large language models (LLMs) and is divided into\ntwo stages. The LLM is first applied to obtain truth-preserving replacement\ncandidates and rank them according to their abstraction level. Those candidates\nare then evaluated for their ability to protect privacy by conducting inference\nattacks with the LLM. Finally, the system selects the most informative\nreplacement shown to be resistant to those attacks. As a consequence of this\ntwo-stage process, the chosen replacements effectively balance utility and\nprivacy. We also present novel metrics to automatically evaluate these two\naspects without the need to manually annotate data. Empirical results on the\nText Anonymization Benchmark show that the proposed approach leads to enhanced\nutility, with only a marginal increase in the risk of re-identifying protected\nindividuals compared to fully suppressing the original information.\nFurthermore, the selected replacements are shown to be more truth-preserving\nand abstractive than previous methods."
                },
                "authors": [
                    {
                        "name": "Ildik Piln"
                    },
                    {
                        "name": "Benet Manzanares-Salor"
                    },
                    {
                        "name": "David Snchez"
                    },
                    {
                        "name": "Pierre Lison"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lison"
                },
                "author": "Pierre Lison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11491v2",
                "updated": "2024-12-17T13:32:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    32,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-21T10:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "SCANS: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious\n  Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCANS: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious\n  Activation Steering"
                },
                "summary": "Safety alignment is indispensable for Large Language Models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is indispensable for Large Language Models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability."
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Extended version of paper accepted to AAAI 2025. 14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12841v2",
                "updated": "2024-12-17T13:31:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    31,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-07-04T12:59:10Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    12,
                    59,
                    10,
                    3,
                    186,
                    0
                ],
                "title": "Black-box Model Ensembling for Textual and Visual Question Answering via\n  Information Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Model Ensembling for Textual and Visual Question Answering via\n  Information Fusion"
                },
                "summary": "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, fine-tuning these models\nis either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient ensemble method that\nlearns to dynamically pick the winner from existing black-box models for\npredictions on both textual and multimodal visual question answering tasks.\nUnlike traditional ensemble models, InfoSel does not rely on prediction\nprobabilities or confidences, which typically are not available in black-box\nmodels. Experimental results on four datasets demonstrate that our approach\nachieves an absolute increase of up to +5.19\\% in the F1-score compared to\nstandalone LLMs using only 1K training instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, fine-tuning these models\nis either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient ensemble method that\nlearns to dynamically pick the winner from existing black-box models for\npredictions on both textual and multimodal visual question answering tasks.\nUnlike traditional ensemble models, InfoSel does not rely on prediction\nprobabilities or confidences, which typically are not available in black-box\nmodels. Experimental results on four datasets demonstrate that our approach\nachieves an absolute increase of up to +5.19\\% in the F1-score compared to\nstandalone LLMs using only 1K training instances."
                },
                "authors": [
                    {
                        "name": "Yuxi Xia"
                    },
                    {
                        "name": "Kilm Zaporojets"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "arxiv_comment": "15 pages, 6 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09861v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09861v3",
                "updated": "2024-12-17T13:26:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    26,
                    37,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-15T10:29:06Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    10,
                    29,
                    6,
                    3,
                    46,
                    0
                ],
                "title": "Testing mirror symmetry in the Universe with LIGO-Virgo black-hole\n  mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing mirror symmetry in the Universe with LIGO-Virgo black-hole\n  mergers"
                },
                "summary": "Certain precessing black-hole mergers produce gravitational waves with net\ncircular polarization, understood as an imbalance between right- and\nleft-handed amplitudes. According to the Cosmological Principle, such emission\nmust average to zero across all binary mergers in our Universe to preserve\nmirror-reflection symmetry at very large scales. We present a new independent\ngravitational-wave test of this hypothesis. Using a novel observable based on\nthe Chern-Pontryagin pseudo-scalar, we measure the emission of net circular\npolarization across 47 black-hole mergers recently analyzed by Islam et. al.\nwith a state-of-the art model for precessing black-hole mergers in General\nRelativity. The average value obtained is consistent with zero. Remarkably,\nhowever, we find that at least $82\\%$ of the analysed sources must have\nproduced net circular polarization. Of these, GW200129 shows strong evidence\nfor mirror asymmetry, with a Bayes Factor of 12.6 or, equivalently, $93.1\\%$\nprobability. We obtain consistent (although stronger) results of $97.5\\%$ and\n$94.3\\%$ respectively using public results on this event from Hannam et. al.\nand performing our own parameter inference. This finding further implies\nevidence of astrophysical sources that can spontaneously emit circularly\npolarized photons by quantum effects. Forthcoming black-hole merger detections\nwill enable stronger constraints on large-scale mirror asymmetry and the\nCosmological Principle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certain precessing black-hole mergers produce gravitational waves with net\ncircular polarization, understood as an imbalance between right- and\nleft-handed amplitudes. According to the Cosmological Principle, such emission\nmust average to zero across all binary mergers in our Universe to preserve\nmirror-reflection symmetry at very large scales. We present a new independent\ngravitational-wave test of this hypothesis. Using a novel observable based on\nthe Chern-Pontryagin pseudo-scalar, we measure the emission of net circular\npolarization across 47 black-hole mergers recently analyzed by Islam et. al.\nwith a state-of-the art model for precessing black-hole mergers in General\nRelativity. The average value obtained is consistent with zero. Remarkably,\nhowever, we find that at least $82\\%$ of the analysed sources must have\nproduced net circular polarization. Of these, GW200129 shows strong evidence\nfor mirror asymmetry, with a Bayes Factor of 12.6 or, equivalently, $93.1\\%$\nprobability. We obtain consistent (although stronger) results of $97.5\\%$ and\n$94.3\\%$ respectively using public results on this event from Hannam et. al.\nand performing our own parameter inference. This finding further implies\nevidence of astrophysical sources that can spontaneously emit circularly\npolarized photons by quantum effects. Forthcoming black-hole merger detections\nwill enable stronger constraints on large-scale mirror asymmetry and the\nCosmological Principle."
                },
                "authors": [
                    {
                        "name": "Juan Caldern Bustillo"
                    },
                    {
                        "name": "Adrian del Rio"
                    },
                    {
                        "name": "Nicolas Sanchis-Gual"
                    },
                    {
                        "name": "Koustav Chandra"
                    },
                    {
                        "name": "Samson H. W. Leong"
                    }
                ],
                "author_detail": {
                    "name": "Samson H. W. Leong"
                },
                "author": "Samson H. W. Leong",
                "arxiv_comment": "10 pages, 6 Figures, 3 Appendixes. Version accepted for publication\n  in Physical Review Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09861v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09861v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12902v1",
                "updated": "2024-12-17T13:26:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    26,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:26:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    26,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "DoPTA: Improving Document Layout Analysis using Patch-Text Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoPTA: Improving Document Layout Analysis using Patch-Text Alignment"
                },
                "summary": "The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks."
                },
                "authors": [
                    {
                        "name": "Nikitha SR"
                    },
                    {
                        "name": "Tarun Ram Menta"
                    },
                    {
                        "name": "Mausoom Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Mausoom Sarkar"
                },
                "author": "Mausoom Sarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12898v1",
                "updated": "2024-12-17T13:21:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    21,
                    26,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:21:26Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    21,
                    26,
                    1,
                    352,
                    0
                ],
                "title": "An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions"
                },
                "summary": "The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches."
                },
                "authors": [
                    {
                        "name": "Shreeyash Gowaikar"
                    },
                    {
                        "name": "Srinivasan Iyengar"
                    },
                    {
                        "name": "Sameer Segal"
                    },
                    {
                        "name": "Shivkumar Kalyanaraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivkumar Kalyanaraman"
                },
                "author": "Shivkumar Kalyanaraman",
                "arxiv_comment": "Accepted at the AAAI'25 Workshop on AI to Accelerate Science and\n  Engineering (AI2ASE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12893v1",
                "updated": "2024-12-17T13:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    19,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    19,
                    38,
                    1,
                    352,
                    0
                ],
                "title": "Question: How do Large Language Models perform on the Question Answering\n  tasks? Answer:",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question: How do Large Language Models perform on the Question Answering\n  tasks? Answer:"
                },
                "summary": "Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets."
                },
                "authors": [
                    {
                        "name": "Kevin Fischer"
                    },
                    {
                        "name": "Darren Frst"
                    },
                    {
                        "name": "Sebastian Steindl"
                    },
                    {
                        "name": "Jakob Lindner"
                    },
                    {
                        "name": "Ulrich Schfer"
                    }
                ],
                "author_detail": {
                    "name": "Ulrich Schfer"
                },
                "author": "Ulrich Schfer",
                "arxiv_comment": "Accepted at SAI Computing Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10033v3",
                "updated": "2024-12-17T13:16:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    16,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-16T06:51:32Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    51,
                    32,
                    0,
                    260,
                    0
                ],
                "title": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs"
                },
                "summary": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Ye Shang"
                    },
                    {
                        "name": "Guolin Xu"
                    },
                    {
                        "name": "Congqing He"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "arxiv_comment": "Accepted to the 6th International Workshop on Automated Program\n  Repair (APR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.03351v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.03351v4",
                "updated": "2024-12-17T13:13:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    13,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2023-08-07T07:06:47Z",
                "published_parsed": [
                    2023,
                    8,
                    7,
                    7,
                    6,
                    47,
                    0,
                    219,
                    0
                ],
                "title": "Theory of Unsupervised Super-Resolution Data Assimilation with\n  Conditional Variational Autoencoders: Estimating Background Covariances via\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Unsupervised Super-Resolution Data Assimilation with\n  Conditional Variational Autoencoders: Estimating Background Covariances via\n  Super-Resolution"
                },
                "summary": "This study proposes a theory of unsupervised super-resolution data\nassimilation (SRDA) using conditional variational autoencoders (CVAEs). The\ntheory is based on an evidence lower bound that leads to an objective function\nfor unsupervised learning. This function can be reduced to the objective\nfunction of a traditional data assimilation method, namely the\nthree-dimensional variational (3D-Var) formalism. Similar to 3D-Var, it is\nessential to use a non-diagonal background error covariance matrix to\nassimilate distant observations. Instead of using such a non-diagonal matrix,\nwe extend the proposed theory by using the non-locality of super-resolution\n(SR). For linear SR, SR operators serve as background error covariance\nmatrices. For nonlinear SR, error backpropagation through SR neural networks\ninduces covariance structures in inference. SRDA can be performed using CVAEs\nbecause the loss function for CVAEs is generally an evidence lower bound. By\nincorporating the SR neural network into the CVAE, the encoder estimates the\nhigh-resolution (HR) analysis from HR observations and low-resolution (LR)\nforecasts. The decoder acts as the observation operator and reconstructs the HR\nobservations from the estimated HR analysis. The effectiveness of SRDA was\nevaluated through numerical experiments using an idealized barotropic ocean jet\nsystem. Compared to inference with an ensemble Kalman filter, SRDA demonstrated\nsuperior accuracy in HR inference. SRDA was also computationally efficient\nbecause it does not require HR numerical integration or ensemble calculations.\nThe findings of this study provide a theoretical basis for integrating SR and\nDA, which will stimulate further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a theory of unsupervised super-resolution data\nassimilation (SRDA) using conditional variational autoencoders (CVAEs). The\ntheory is based on an evidence lower bound that leads to an objective function\nfor unsupervised learning. This function can be reduced to the objective\nfunction of a traditional data assimilation method, namely the\nthree-dimensional variational (3D-Var) formalism. Similar to 3D-Var, it is\nessential to use a non-diagonal background error covariance matrix to\nassimilate distant observations. Instead of using such a non-diagonal matrix,\nwe extend the proposed theory by using the non-locality of super-resolution\n(SR). For linear SR, SR operators serve as background error covariance\nmatrices. For nonlinear SR, error backpropagation through SR neural networks\ninduces covariance structures in inference. SRDA can be performed using CVAEs\nbecause the loss function for CVAEs is generally an evidence lower bound. By\nincorporating the SR neural network into the CVAE, the encoder estimates the\nhigh-resolution (HR) analysis from HR observations and low-resolution (LR)\nforecasts. The decoder acts as the observation operator and reconstructs the HR\nobservations from the estimated HR analysis. The effectiveness of SRDA was\nevaluated through numerical experiments using an idealized barotropic ocean jet\nsystem. Compared to inference with an ensemble Kalman filter, SRDA demonstrated\nsuperior accuracy in HR inference. SRDA was also computationally efficient\nbecause it does not require HR numerical integration or ensemble calculations.\nThe findings of this study provide a theoretical basis for integrating SR and\nDA, which will stimulate further research in this direction."
                },
                "authors": [
                    {
                        "name": "Yuki Yasuda"
                    },
                    {
                        "name": "Ryo Onishi"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Onishi"
                },
                "author": "Ryo Onishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.03351v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.03351v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12883v1",
                "updated": "2024-12-17T13:07:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    7,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:07:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    7,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "A Comparative Study of Pruning Methods in Transformer-based Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Pruning Methods in Transformer-based Time Series\n  Forecasting"
                },
                "summary": "The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings."
                },
                "authors": [
                    {
                        "name": "Nicholas Kiefer"
                    },
                    {
                        "name": "Arvid Weyrauch"
                    },
                    {
                        "name": "Muhammed z"
                    },
                    {
                        "name": "Achim Streit"
                    },
                    {
                        "name": "Markus Gtz"
                    },
                    {
                        "name": "Charlotte Debus"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Debus"
                },
                "author": "Charlotte Debus",
                "arxiv_comment": "16 pages, 5 figures, submitted to ACM Transactions on Intelligent\n  Systems and Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12881v1",
                "updated": "2024-12-17T13:05:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    5,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:05:36Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    5,
                    36,
                    1,
                    352,
                    0
                ],
                "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\n  Verification and Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\n  Verification and Refinement"
                },
                "summary": "Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Tao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhang"
                },
                "author": "Tao Zhang",
                "arxiv_comment": "LLM;RAG;MCTS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12865v1",
                "updated": "2024-12-17T12:49:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    49,
                    14,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:49:14Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    49,
                    14,
                    1,
                    352,
                    0
                ],
                "title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models"
                },
                "summary": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO."
                },
                "authors": [
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Yuzhong Hong"
                    },
                    {
                        "name": "Qiushi Wang"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Yang Song"
                },
                "author": "Yang Song",
                "arxiv_comment": "AAAI2025, 12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15061v2",
                "updated": "2024-12-17T12:45:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    45,
                    20,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-23T02:24:15Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    2,
                    24,
                    15,
                    4,
                    54,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Domain-specific Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Domain-specific Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise."
                },
                "authors": [
                    {
                        "name": "Jiawei Zheng"
                    },
                    {
                        "name": "Hanghai Hong"
                    },
                    {
                        "name": "Feiyan Liu"
                    },
                    {
                        "name": "Xiaoli Wang"
                    },
                    {
                        "name": "Jingsong Su"
                    },
                    {
                        "name": "Yonggui Liang"
                    },
                    {
                        "name": "Shikai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shikai Wu"
                },
                "author": "Shikai Wu",
                "arxiv_comment": "13 pages, 5 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12863v1",
                "updated": "2024-12-17T12:44:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    44,
                    6,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:44:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    44,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "DISC: Plug-and-Play Decoding Intervention with Similarity of Characters\n  for Chinese Spelling Check",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DISC: Plug-and-Play Decoding Intervention with Similarity of Characters\n  for Chinese Spelling Check"
                },
                "summary": "One key characteristic of the Chinese spelling check (CSC) task is that\nincorrect characters are usually similar to the correct ones in either\nphonetics or glyph. To accommodate this, previous works usually leverage\nconfusion sets, which suffer from two problems, i.e., difficulty in determining\nwhich character pairs to include and lack of probabilities to distinguish items\nin the set. In this paper, we propose a light-weight plug-and-play DISC (i.e.,\ndecoding intervention with similarity of characters) module for CSC models.DISC\nmeasures phonetic and glyph similarities between characters and incorporates\nthis similarity information only during the inference phase. This method can be\neasily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and\nReLM, without additional training costs. Experiments on three CSC benchmarks\ndemonstrate that our proposed method significantly improves model performance,\napproaching and even surpassing the current state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key characteristic of the Chinese spelling check (CSC) task is that\nincorrect characters are usually similar to the correct ones in either\nphonetics or glyph. To accommodate this, previous works usually leverage\nconfusion sets, which suffer from two problems, i.e., difficulty in determining\nwhich character pairs to include and lack of probabilities to distinguish items\nin the set. In this paper, we propose a light-weight plug-and-play DISC (i.e.,\ndecoding intervention with similarity of characters) module for CSC models.DISC\nmeasures phonetic and glyph similarities between characters and incorporates\nthis similarity information only during the inference phase. This method can be\neasily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and\nReLM, without additional training costs. Experiments on three CSC benchmarks\ndemonstrate that our proposed method significantly improves model performance,\napproaching and even surpassing the current state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Ziheng Qiao"
                    },
                    {
                        "name": "Houquan Zhou"
                    },
                    {
                        "name": "Yumeng Liu"
                    },
                    {
                        "name": "Zhenghua Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16044v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16044v5",
                "updated": "2024-12-17T12:41:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    41,
                    13,
                    1,
                    352,
                    0
                ],
                "published": "2023-12-26T13:17:06Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    13,
                    17,
                    6,
                    1,
                    360,
                    0
                ],
                "title": "LLMLight: Large Language Models as Traffic Signal Control Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMLight: Large Language Models as Traffic Signal Control Agents"
                },
                "summary": "Traffic Signal Control (TSC) is a crucial component in urban traffic\nmanagement, aiming to optimize road network efficiency and reduce congestion.\nTraditional TSC methods, primarily based on transportation engineering and\nreinforcement learning (RL), often struggle with generalization abilities\nacross varied traffic scenarios and lack interpretability. This paper presents\nLLMLight, a novel framework employing Large Language Models (LLMs) as\ndecision-making agents for TSC. Specifically, the framework begins by\ninstructing the LLM with a knowledgeable prompt detailing real-time traffic\nconditions. Leveraging the advanced generalization capabilities of LLMs,\nLLMLight engages a reasoning and decision-making process akin to human\nintuition for effective traffic control. Moreover, we build LightGPT, a\nspecialized backbone LLM tailored for TSC tasks. By learning nuanced traffic\npatterns and control strategies, LightGPT enhances the LLMLight framework\ncost-effectively. Extensive experiments conducted on ten real-world and\nsynthetic datasets, along with evaluations by fifteen human experts,\ndemonstrate the exceptional effectiveness, generalization ability, and\ninterpretability of LLMLight with LightGPT, outperforming nine baseline methods\nand ten advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Signal Control (TSC) is a crucial component in urban traffic\nmanagement, aiming to optimize road network efficiency and reduce congestion.\nTraditional TSC methods, primarily based on transportation engineering and\nreinforcement learning (RL), often struggle with generalization abilities\nacross varied traffic scenarios and lack interpretability. This paper presents\nLLMLight, a novel framework employing Large Language Models (LLMs) as\ndecision-making agents for TSC. Specifically, the framework begins by\ninstructing the LLM with a knowledgeable prompt detailing real-time traffic\nconditions. Leveraging the advanced generalization capabilities of LLMs,\nLLMLight engages a reasoning and decision-making process akin to human\nintuition for effective traffic control. Moreover, we build LightGPT, a\nspecialized backbone LLM tailored for TSC tasks. By learning nuanced traffic\npatterns and control strategies, LightGPT enhances the LLMLight framework\ncost-effectively. Extensive experiments conducted on ten real-world and\nsynthetic datasets, along with evaluations by fifteen human experts,\ndemonstrate the exceptional effectiveness, generalization ability, and\ninterpretability of LLMLight with LightGPT, outperforming nine baseline methods\nand ten advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Siqi Lai"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16044v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16044v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2105.00879v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2105.00879v4",
                "updated": "2024-12-17T12:40:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    40,
                    40,
                    1,
                    352,
                    0
                ],
                "published": "2021-05-03T14:01:06Z",
                "published_parsed": [
                    2021,
                    5,
                    3,
                    14,
                    1,
                    6,
                    0,
                    123,
                    0
                ],
                "title": "Identification and Estimation of Average Causal Effects in Fixed Effects\n  Logit Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Estimation of Average Causal Effects in Fixed Effects\n  Logit Models"
                },
                "summary": "This paper studies identification and estimation of average causal effects,\nsuch as average marginal or treatment effects, in fixed effects logit models\nwith short panels. Relating the identified set of these effects to an extremal\nmoment problem, we first show how to obtain sharp bounds on such effects\nsimply, without any optimization. We also consider even simpler outer bounds,\nwhich, contrary to the sharp bounds, do not require any first-step\nnonparametric estimators. We build confidence intervals based on these two\napproaches and show their asymptotic validity. Monte Carlo simulations suggest\nthat both approaches work well in practice, the second being typically\ncompetitive in terms of interval length. Finally, we show that our method is\nalso useful to measure treatment effect heterogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies identification and estimation of average causal effects,\nsuch as average marginal or treatment effects, in fixed effects logit models\nwith short panels. Relating the identified set of these effects to an extremal\nmoment problem, we first show how to obtain sharp bounds on such effects\nsimply, without any optimization. We also consider even simpler outer bounds,\nwhich, contrary to the sharp bounds, do not require any first-step\nnonparametric estimators. We build confidence intervals based on these two\napproaches and show their asymptotic validity. Monte Carlo simulations suggest\nthat both approaches work well in practice, the second being typically\ncompetitive in terms of interval length. Finally, we show that our method is\nalso useful to measure treatment effect heterogeneity."
                },
                "authors": [
                    {
                        "name": "Laurent Davezies"
                    },
                    {
                        "name": "Xavier D'Haultfuille"
                    },
                    {
                        "name": "Louise Laage"
                    }
                ],
                "author_detail": {
                    "name": "Louise Laage"
                },
                "author": "Louise Laage",
                "arxiv_comment": "93 pages (online appendix starting at p.46). Major rewriting compared\n  to v3. In particular, addition of a literature review, study of general\n  parameters (not only the AME) in the identification, estimation and inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2105.00879v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2105.00879v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17679v3",
                "updated": "2024-12-17T12:37:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    37,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-11-26T18:44:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    44,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning"
                },
                "summary": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness."
                },
                "authors": [
                    {
                        "name": "Zhu Xu"
                    },
                    {
                        "name": "Zhiqiang Zhao"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Yuchi Liu"
                    },
                    {
                        "name": "Quanwei Shen"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yu Kuang"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Conglin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Conglin Liu"
                },
                "author": "Conglin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12852v1",
                "updated": "2024-12-17T12:26:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    26,
                    14,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:26:14Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    26,
                    14,
                    1,
                    352,
                    0
                ],
                "title": "Selective Shot Learning for Code Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Shot Learning for Code Explanation"
                },
                "summary": "Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task."
                },
                "authors": [
                    {
                        "name": "Paheli Bhattacharya"
                    },
                    {
                        "name": "Rishabh Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Gupta"
                },
                "author": "Rishabh Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12848v1",
                "updated": "2024-12-17T12:22:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    22,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:22:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    22,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models"
                },
                "summary": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09056v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09056v3",
                "updated": "2024-12-17T12:20:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    20,
                    34,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-13T12:43:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    12,
                    43,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive\n  Evaluation Framework with CUDRT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive\n  Evaluation Framework with CUDRT"
                },
                "summary": "The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub."
                },
                "authors": [
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Yanfang Chen"
                    },
                    {
                        "name": "Dinghao Xi"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09056v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09056v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12843v1",
                "updated": "2024-12-17T12:11:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    11,
                    4,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:11:04Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    11,
                    4,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks"
                },
                "summary": "Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU,\nrespectively, with extremely 5.48x lower energy consumption and 1.14x faster\ninference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU,\nrespectively, with extremely 5.48x lower energy consumption and 1.14x faster\ninference speed."
                },
                "authors": [
                    {
                        "name": "Xiaxin Zhu"
                    },
                    {
                        "name": "Fangming Guo"
                    },
                    {
                        "name": "Xianlei Long"
                    },
                    {
                        "name": "Qingyi Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Fuqiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Fuqiang Gu"
                },
                "author": "Fuqiang Gu",
                "arxiv_comment": "Submitted to IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12841v1",
                "updated": "2024-12-17T12:10:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    10,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:10:38Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    10,
                    38,
                    1,
                    352,
                    0
                ],
                "title": "Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs"
                },
                "summary": "Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR."
                },
                "authors": [
                    {
                        "name": "Ruikang Ni"
                    },
                    {
                        "name": "Da Xiao"
                    },
                    {
                        "name": "Qingye Meng"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Shihui Zheng"
                    },
                    {
                        "name": "Hongliang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Hongliang Liang"
                },
                "author": "Hongliang Liang",
                "arxiv_comment": "Accepted to the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12839v1",
                "updated": "2024-12-17T12:05:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    5,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:05:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    5,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed\n  Instructions In A Multi-Modal Jungle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed\n  Instructions In A Multi-Modal Jungle"
                },
                "summary": "In response to the call for agent-based solutions that leverage the\never-increasing capabilities of the deep models' ecosystem, we introduce Hive\n-- a comprehensive solution for selecting appropriate models and subsequently\nplanning a set of atomic actions to satisfy the end-users' instructions. Hive\noperates over sets of models and, upon receiving natural language instructions\n(i.e. user queries), schedules and executes explainable plans of atomic\nactions. These actions can involve one or more of the available models to\nachieve the overall task, while respecting end-users specific constraints.\nNotably, Hive handles tasks that involve multi-modal inputs and outputs,\nenabling it to handle complex, real-world queries. Our system is capable of\nplanning complex chains of actions while guaranteeing explainability, using an\nLLM-based formal logic backbone empowered by PDDL operations. We introduce the\nMuSE benchmark in order to offer a comprehensive evaluation of the multi-modal\ncapabilities of agent systems. Our findings show that our framework redefines\nthe state-of-the-art for task selection, outperforming other competing systems\nthat plan operations across multiple models while offering transparency\nguarantees while fully adhering to user constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the call for agent-based solutions that leverage the\never-increasing capabilities of the deep models' ecosystem, we introduce Hive\n-- a comprehensive solution for selecting appropriate models and subsequently\nplanning a set of atomic actions to satisfy the end-users' instructions. Hive\noperates over sets of models and, upon receiving natural language instructions\n(i.e. user queries), schedules and executes explainable plans of atomic\nactions. These actions can involve one or more of the available models to\nachieve the overall task, while respecting end-users specific constraints.\nNotably, Hive handles tasks that involve multi-modal inputs and outputs,\nenabling it to handle complex, real-world queries. Our system is capable of\nplanning complex chains of actions while guaranteeing explainability, using an\nLLM-based formal logic backbone empowered by PDDL operations. We introduce the\nMuSE benchmark in order to offer a comprehensive evaluation of the multi-modal\ncapabilities of agent systems. Our findings show that our framework redefines\nthe state-of-the-art for task selection, outperforming other competing systems\nthat plan operations across multiple models while offering transparency\nguarantees while fully adhering to user constraints."
                },
                "authors": [
                    {
                        "name": "Kaustubh Vyas"
                    },
                    {
                        "name": "Damien Graux"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Sbastien Montella"
                    },
                    {
                        "name": "Chenxin Diao"
                    },
                    {
                        "name": "Wendi Zhou"
                    },
                    {
                        "name": "Pavlos Vougiouklis"
                    },
                    {
                        "name": "Ruofei Lai"
                    },
                    {
                        "name": "Yang Ren"
                    },
                    {
                        "name": "Keshuang Li"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12837v1",
                "updated": "2024-12-17T12:02:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    2,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:02:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    2,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Scrutinizing the Vulnerability of Decentralized Learning to Membership\n  Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scrutinizing the Vulnerability of Decentralized Learning to Membership\n  Inference Attacks"
                },
                "summary": "The primary promise of decentralized learning is to allow users to engage in\nthe training of machine learning models in a collaborative manner while keeping\ntheir data on their premises and without relying on any central entity.\nHowever, this paradigm necessitates the exchange of model parameters or\ngradients between peers. Such exchanges can be exploited to infer sensitive\ninformation about training data, which is achieved through privacy attacks (e.g\nMembership Inference Attacks -- MIA). In order to devise effective defense\nmechanisms, it is important to understand the factors that increase/reduce the\nvulnerability of a given decentralized learning architecture to MIA. In this\nstudy, we extensively explore the vulnerability to MIA of various decentralized\nlearning architectures by varying the graph structure (e.g number of\nneighbors), the graph dynamics, and the aggregation strategy, across diverse\ndatasets and data distributions. Our key finding, which to the best of our\nknowledge we are the first to report, is that the vulnerability to MIA is\nheavily correlated to (i) the local model mixing strategy performed by each\nnode upon reception of models from neighboring nodes and (ii) the global mixing\nproperties of the communication graph. We illustrate these results\nexperimentally using four datasets and by theoretically analyzing the mixing\nproperties of various decentralized architectures. Our paper draws a set of\nlessons learned for devising decentralized learning systems that reduce by\ndesign the vulnerability to MIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary promise of decentralized learning is to allow users to engage in\nthe training of machine learning models in a collaborative manner while keeping\ntheir data on their premises and without relying on any central entity.\nHowever, this paradigm necessitates the exchange of model parameters or\ngradients between peers. Such exchanges can be exploited to infer sensitive\ninformation about training data, which is achieved through privacy attacks (e.g\nMembership Inference Attacks -- MIA). In order to devise effective defense\nmechanisms, it is important to understand the factors that increase/reduce the\nvulnerability of a given decentralized learning architecture to MIA. In this\nstudy, we extensively explore the vulnerability to MIA of various decentralized\nlearning architectures by varying the graph structure (e.g number of\nneighbors), the graph dynamics, and the aggregation strategy, across diverse\ndatasets and data distributions. Our key finding, which to the best of our\nknowledge we are the first to report, is that the vulnerability to MIA is\nheavily correlated to (i) the local model mixing strategy performed by each\nnode upon reception of models from neighboring nodes and (ii) the global mixing\nproperties of the communication graph. We illustrate these results\nexperimentally using four datasets and by theoretically analyzing the mixing\nproperties of various decentralized architectures. Our paper draws a set of\nlessons learned for devising decentralized learning systems that reduce by\ndesign the vulnerability to MIA."
                },
                "authors": [
                    {
                        "name": "Ousmane Touat"
                    },
                    {
                        "name": "Jezekael Brunon"
                    },
                    {
                        "name": "Yacine Belal"
                    },
                    {
                        "name": "Julien Nicolas"
                    },
                    {
                        "name": "Mohamed Maouche"
                    },
                    {
                        "name": "Csar Sabater"
                    },
                    {
                        "name": "Sonia Ben Mokhtar"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Ben Mokhtar"
                },
                "author": "Sonia Ben Mokhtar",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12833v1",
                "updated": "2024-12-17T11:54:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:54:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering"
                },
                "summary": "Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data."
                },
                "authors": [
                    {
                        "name": "Zheng Cheng"
                    },
                    {
                        "name": "Rendong Wang"
                    },
                    {
                        "name": "Zhicheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Wang"
                },
                "author": "Zhicheng Wang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12832v1",
                "updated": "2024-12-17T11:54:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    16,
                    1,
                    352,
                    0
                ],
                "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction\n  in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction\n  in the Era of Large Language Models"
                },
                "summary": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations."
                },
                "authors": [
                    {
                        "name": "Jinxiang Xie"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xunjian Yin"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Extended version of a paper to appear in AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04468v2",
                "updated": "2024-12-17T11:51:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    51,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-06T12:50:15Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    12,
                    50,
                    15,
                    6,
                    280,
                    0
                ],
                "title": "Revisiting In-context Learning Inference Circuit in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting In-context Learning Inference Circuit in Large Language\n  Models"
                },
                "summary": "In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text\n(demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations similar to the\nquery representation on a task subspace, and copy the searched representations\ninto the query. Then, language model heads capture these copied label\nrepresentations to a certain extent and decode them into predicted labels. The\nproposed inference circuit successfully captured many phenomena observed during\nthe ICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text\n(demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations similar to the\nquery representation on a task subspace, and copy the searched representations\ninto the query. Then, language model heads capture these copied label\nrepresentations to a certain extent and decode them into predicted labels. The\nproposed inference circuit successfully captured many phenomena observed during\nthe ICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit."
                },
                "authors": [
                    {
                        "name": "Hakaze Cho"
                    },
                    {
                        "name": "Mariko Kato"
                    },
                    {
                        "name": "Yoshihiro Sakai"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "37 pages, 41 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12808v1",
                "updated": "2024-12-17T11:25:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    25,
                    55,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:25:55Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    25,
                    55,
                    1,
                    352,
                    0
                ],
                "title": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning"
                },
                "summary": "This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR."
                },
                "authors": [
                    {
                        "name": "Ziqi Qiu"
                    },
                    {
                        "name": "Jianxing Yu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Hanjiang Lai"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Qinliang Su"
                    },
                    {
                        "name": "Jian Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yin"
                },
                "author": "Jian Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11239v2",
                "updated": "2024-12-17T11:14:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    14,
                    52,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-15T16:42:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    16,
                    42,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "Learning Set Functions with Implicit Differentiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Set Functions with Implicit Differentiation"
                },
                "summary": "Ou et al. (2022) introduce the problem of learning set functions from data\ngenerated by a so-called optimal subset oracle. Their approach approximates the\nunderlying utility function with an energy-based model, whose parameters are\nestimated via mean-field variational inference. Ou et al. (2022) show this\nreduces to fixed point iterations; however, as the number of iterations\nincreases, automatic differentiation quickly becomes computationally\nprohibitive due to the size of the Jacobians that are stacked during\nbackpropagation. We address this challenge with implicit differentiation and\nexamine the convergence conditions for the fixed-point iterations. We\nempirically demonstrate the efficiency of our method on synthetic and\nreal-world subset selection applications including product recommendation, set\nanomaly detection and compound selection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ou et al. (2022) introduce the problem of learning set functions from data\ngenerated by a so-called optimal subset oracle. Their approach approximates the\nunderlying utility function with an energy-based model, whose parameters are\nestimated via mean-field variational inference. Ou et al. (2022) show this\nreduces to fixed point iterations; however, as the number of iterations\nincreases, automatic differentiation quickly becomes computationally\nprohibitive due to the size of the Jacobians that are stacked during\nbackpropagation. We address this challenge with implicit differentiation and\nexamine the convergence conditions for the fixed-point iterations. We\nempirically demonstrate the efficiency of our method on synthetic and\nreal-world subset selection applications including product recommendation, set\nanomaly detection and compound selection tasks."
                },
                "authors": [
                    {
                        "name": "Gzde zcan"
                    },
                    {
                        "name": "Chengzhi Shi"
                    },
                    {
                        "name": "Stratis Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Stratis Ioannidis"
                },
                "author": "Stratis Ioannidis",
                "arxiv_comment": "19 pages, 1 figure, extended version of the AAAI 2025 paper with the\n  same title",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12799v1",
                "updated": "2024-12-17T11:02:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    2,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:02:36Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    2,
                    36,
                    1,
                    352,
                    0
                ],
                "title": "RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential\n  Decoder for 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential\n  Decoder for 3D Object Detection"
                },
                "summary": "In radar-camera 3D object detection, the radar point clouds are sparse and\nnoisy, which causes difficulties in fusing camera and radar modalities. To\nsolve this, we introduce a novel query-based detection method named\nRadar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then concatenate them with\nthe image tokens. By doing this, we can fully explore the 3D information of\neach interest region and reduce the interference of empty tokens during the\nfusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes\nbased on the obtained tokens and random initialized queries. To alleviate the\neffect of elevation ambiguity in radar point clouds, we gradually locate the\nposition of the object via a sequential fusion structure. It helps to get more\nprecise and flexible correspondences between tokens and queries. A pruning\ntraining strategy is adopted in the decoder, which can save much time during\ninference and inhibit queries from losing their distinctiveness. Extensive\nexperiments on the large-scale nuScenes dataset prove the superiority of our\nmethod, and we also achieve new state-of-the-art radar-camera 3D detection\nresults. Our implementation is available at https://github.com/liyih/RCTrans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In radar-camera 3D object detection, the radar point clouds are sparse and\nnoisy, which causes difficulties in fusing camera and radar modalities. To\nsolve this, we introduce a novel query-based detection method named\nRadar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then concatenate them with\nthe image tokens. By doing this, we can fully explore the 3D information of\neach interest region and reduce the interference of empty tokens during the\nfusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes\nbased on the obtained tokens and random initialized queries. To alleviate the\neffect of elevation ambiguity in radar point clouds, we gradually locate the\nposition of the object via a sequential fusion structure. It helps to get more\nprecise and flexible correspondences between tokens and queries. A pruning\ntraining strategy is adopted in the decoder, which can save much time during\ninference and inhibit queries from losing their distinctiveness. Extensive\nexperiments on the large-scale nuScenes dataset prove the superiority of our\nmethod, and we also achieve new state-of-the-art radar-camera 3D detection\nresults. Our implementation is available at https://github.com/liyih/RCTrans."
                },
                "authors": [
                    {
                        "name": "Yiheng Li"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12791v1",
                "updated": "2024-12-17T10:52:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    52,
                    50,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:52:50Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    52,
                    50,
                    1,
                    352,
                    0
                ],
                "title": "Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning"
                },
                "summary": "Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods."
                },
                "authors": [
                    {
                        "name": "Shiping Ge"
                    },
                    {
                        "name": "Qiang Chen"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Liu Qin"
                    },
                    {
                        "name": "Ziyao Chen"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12785v1",
                "updated": "2024-12-17T10:44:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    44,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:44:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    44,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference"
                },
                "summary": "Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales."
                },
                "authors": [
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Chengxing Zhou"
                    },
                    {
                        "name": "Zejun Li"
                    },
                    {
                        "name": "Zhihao Fan"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12775v1",
                "updated": "2024-12-17T10:36:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    36,
                    52,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:36:52Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    36,
                    52,
                    1,
                    352,
                    0
                ],
                "title": "RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the service quality of large\nlanguage models by retrieving relevant documents from credible literature and\nintegrating them into the context of the user query. Recently, the rise of the\ncloud RAG service has made it possible for users to query relevant documents\nconveniently. However, directly sending queries to the cloud brings potential\nprivacy leakage. In this paper, we are the first to formally define the\nprivacy-preserving cloud RAG service to protect the user query and propose\nRemoteRAG as a solution regarding privacy, efficiency, and accuracy. For\nprivacy, we introduce $(n,\\epsilon)$-DistanceDP to characterize privacy leakage\nof the user query and the leakage inferred from relevant documents. For\nefficiency, we limit the search range from the total documents to a small\nnumber of selected documents related to a perturbed embedding generated from\n$(n,\\epsilon)$-DistanceDP, so that computation and communication costs required\nfor privacy protection significantly decrease. For accuracy, we ensure that the\nsmall range includes target documents related to the user query with detailed\ntheoretical analysis. Experimental results also demonstrate that RemoteRAG can\nresist existing embedding inversion attack methods while achieving no loss in\nretrieval under various settings. Moreover, RemoteRAG is efficient, incurring\nonly $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$\nGB with the non-optimized privacy-preserving scheme) when retrieving from a\ntotal of $10^6$ documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the service quality of large\nlanguage models by retrieving relevant documents from credible literature and\nintegrating them into the context of the user query. Recently, the rise of the\ncloud RAG service has made it possible for users to query relevant documents\nconveniently. However, directly sending queries to the cloud brings potential\nprivacy leakage. In this paper, we are the first to formally define the\nprivacy-preserving cloud RAG service to protect the user query and propose\nRemoteRAG as a solution regarding privacy, efficiency, and accuracy. For\nprivacy, we introduce $(n,\\epsilon)$-DistanceDP to characterize privacy leakage\nof the user query and the leakage inferred from relevant documents. For\nefficiency, we limit the search range from the total documents to a small\nnumber of selected documents related to a perturbed embedding generated from\n$(n,\\epsilon)$-DistanceDP, so that computation and communication costs required\nfor privacy protection significantly decrease. For accuracy, we ensure that the\nsmall range includes target documents related to the user query with detailed\ntheoretical analysis. Experimental results also demonstrate that RemoteRAG can\nresist existing embedding inversion attack methods while achieving no loss in\nretrieval under various settings. Moreover, RemoteRAG is efficient, incurring\nonly $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$\nGB with the non-optimized privacy-preserving scheme) when retrieving from a\ntotal of $10^6$ documents."
                },
                "authors": [
                    {
                        "name": "Yihang Cheng"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Junyang Wang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Yunhao Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yunhao Yao"
                },
                "author": "Yunhao Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10487v2",
                "updated": "2024-12-17T10:35:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    35,
                    33,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T15:18:39Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    18,
                    39,
                    4,
                    348,
                    0
                ],
                "title": "HyperGraphOS: A Modern Meta-Operating System for the Scientific and\n  Engineering Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperGraphOS: A Modern Meta-Operating System for the Scientific and\n  Engineering Domains"
                },
                "summary": "This paper presents HyperGraphOS, a significant innovation in the domain of\noperating systems, specifically designed to address the needs of scientific and\nengineering domains. This platform aims to combine model-based engineering,\ngraph modeling, data containers, and documents, along with tools for handling\ncomputational elements. HyperGraphOS functions as an Operating System offering\nto users an infinite workspace for creating and managing complex models\nrepresented as graphs with customizable semantics. By leveraging a web-based\narchitecture, it requires only a modern web browser for access, allowing\norganization of knowledge, documents, and content into models represented in a\nnetwork of workspaces. Elements of the workspace are defined in terms of\ndomain-specific languages (DSLs). These DSLs are pivotal for navigating\nworkspaces, generating code, triggering AI components, and organizing\ninformation and processes. The models' dual nature as both visual drawings and\ndata structures allows dynamic modifications and inspections both interactively\nas well as programaticaly. We evaluated HyperGraphOS's efficiency and\napplicability across a large set of diverse domains, including the design and\ndevelopment of a virtual Avatar dialog system, a robotic task planner based on\nlarge language models (LLMs), a new meta-model for feature-based code\ndevelopment and many others. Our findings show that HyperGraphOS offers\nsubstantial benefits in the interaction with a computer as information system,\nas platoform for experiments and data analysis, as streamlined engineering\nprocesses, demonstrating enhanced flexibility in managing data, computation and\ndocuments, showing an innovative approaches to persistent desktop environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents HyperGraphOS, a significant innovation in the domain of\noperating systems, specifically designed to address the needs of scientific and\nengineering domains. This platform aims to combine model-based engineering,\ngraph modeling, data containers, and documents, along with tools for handling\ncomputational elements. HyperGraphOS functions as an Operating System offering\nto users an infinite workspace for creating and managing complex models\nrepresented as graphs with customizable semantics. By leveraging a web-based\narchitecture, it requires only a modern web browser for access, allowing\norganization of knowledge, documents, and content into models represented in a\nnetwork of workspaces. Elements of the workspace are defined in terms of\ndomain-specific languages (DSLs). These DSLs are pivotal for navigating\nworkspaces, generating code, triggering AI components, and organizing\ninformation and processes. The models' dual nature as both visual drawings and\ndata structures allows dynamic modifications and inspections both interactively\nas well as programaticaly. We evaluated HyperGraphOS's efficiency and\napplicability across a large set of diverse domains, including the design and\ndevelopment of a virtual Avatar dialog system, a robotic task planner based on\nlarge language models (LLMs), a new meta-model for feature-based code\ndevelopment and many others. Our findings show that HyperGraphOS offers\nsubstantial benefits in the interaction with a computer as information system,\nas platoform for experiments and data analysis, as streamlined engineering\nprocesses, demonstrating enhanced flexibility in managing data, computation and\ndocuments, showing an innovative approaches to persistent desktop environments."
                },
                "authors": [
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Frank Joublin"
                    }
                ],
                "author_detail": {
                    "name": "Frank Joublin"
                },
                "author": "Frank Joublin",
                "arxiv_comment": "29 Pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12772v1",
                "updated": "2024-12-17T10:33:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:33:36Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    36,
                    1,
                    352,
                    0
                ],
                "title": "Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior"
                },
                "summary": "Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available."
                },
                "authors": [
                    {
                        "name": "Leo Segre"
                    },
                    {
                        "name": "Shai Avidan"
                    }
                ],
                "author_detail": {
                    "name": "Shai Avidan"
                },
                "author": "Shai Avidan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12770v1",
                "updated": "2024-12-17T10:33:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    13,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:33:13Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    13,
                    1,
                    352,
                    0
                ],
                "title": "A Survey on Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sequential Recommendation"
                },
                "summary": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield."
                },
                "authors": [
                    {
                        "name": "Liwei Pan"
                    },
                    {
                        "name": "Weike Pan"
                    },
                    {
                        "name": "Meiyan Wei"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Zhong Ming"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Ming"
                },
                "author": "Zhong Ming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12767v1",
                "updated": "2024-12-17T10:31:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    31,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:31:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    31,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "A Survey of Calibration Process for Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Calibration Process for Black-Box LLMs"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs"
                },
                "authors": [
                    {
                        "name": "Liangru Xie"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jingying Zeng"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Yan Han"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.02029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.02029v2",
                "updated": "2024-12-17T10:25:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    25,
                    14,
                    1,
                    352,
                    0
                ],
                "published": "2022-02-04T09:10:39Z",
                "published_parsed": [
                    2022,
                    2,
                    4,
                    9,
                    10,
                    39,
                    4,
                    35,
                    0
                ],
                "title": "First-order integer-valued autoregressive processes with Generalized\n  Katz innovations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-order integer-valued autoregressive processes with Generalized\n  Katz innovations"
                },
                "summary": "A new integer--valued autoregressive process (INAR) with Generalised\nLagrangian Katz (GLK) innovations is defined. This process family provides a\nflexible modelling framework for count data, allowing for under and\nover--dispersion, asymmetry, and excess of kurtosis and includes standard INAR\nmodels such as Generalized Poisson and Negative Binomial as special cases. We\nshow that the GLK--INAR process is discrete semi--self--decomposable, infinite\ndivisible, stable by aggregation and provides stationarity conditions. Some\nextensions are discussed, such as the Markov--Switching and the zero--inflated\nGLK--INARs. A Bayesian inference framework and an efficient posterior\napproximation procedure are introduced. The proposed models are applied to 130\ntime series from Google Trend, which proxy the worldwide public concern about\nclimate change. New evidence is found of heterogeneity across time, countries\nand keywords in the persistence, uncertainty, and long--run public awareness\nlevel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new integer--valued autoregressive process (INAR) with Generalised\nLagrangian Katz (GLK) innovations is defined. This process family provides a\nflexible modelling framework for count data, allowing for under and\nover--dispersion, asymmetry, and excess of kurtosis and includes standard INAR\nmodels such as Generalized Poisson and Negative Binomial as special cases. We\nshow that the GLK--INAR process is discrete semi--self--decomposable, infinite\ndivisible, stable by aggregation and provides stationarity conditions. Some\nextensions are discussed, such as the Markov--Switching and the zero--inflated\nGLK--INARs. A Bayesian inference framework and an efficient posterior\napproximation procedure are introduced. The proposed models are applied to 130\ntime series from Google Trend, which proxy the worldwide public concern about\nclimate change. New evidence is found of heterogeneity across time, countries\nand keywords in the persistence, uncertainty, and long--run public awareness\nlevel."
                },
                "authors": [
                    {
                        "name": "Ovielt Baltodano Lopez"
                    },
                    {
                        "name": "Federico Bassetti"
                    },
                    {
                        "name": "Giulia Carallo"
                    },
                    {
                        "name": "Roberto Casarin"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Casarin"
                },
                "author": "Roberto Casarin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2202.02029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.02029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12737v1",
                "updated": "2024-12-17T09:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    59,
                    53,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    59,
                    53,
                    1,
                    352,
                    0
                ],
                "title": "PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything\n  Model"
                },
                "summary": "PolSAR data presents unique challenges due to its rich and complex\ncharacteristics. Existing data representations, such as complex-valued data,\npolarimetric features, and amplitude images, are widely used. However, these\nformats often face issues related to usability, interpretability, and data\nintegrity. Most feature extraction networks for PolSAR are small, limiting\ntheir ability to capture features effectively. To address these issues, We\npropose the Polarimetric Scattering Mechanism-Informed SAM (PolSAM), an\nenhanced Segment Anything Model (SAM) that integrates domain-specific\nscattering characteristics and a novel prompt generation strategy. PolSAM\nintroduces Microwave Vision Data (MVD), a lightweight and interpretable data\nrepresentation derived from polarimetric decomposition and semantic\ncorrelations. We propose two key components: the Feature-Level Fusion Prompt\n(FFP), which fuses visual tokens from pseudo-colored SAR images and MVD to\naddress modality incompatibility in the frozen SAM encoder, and the\nSemantic-Level Fusion Prompt (SFP), which refines sparse and dense segmentation\nprompts using semantic information. Experimental results on the PhySAR-Seg\ndatasets demonstrate that PolSAM significantly outperforms existing SAM-based\nand multimodal fusion models, improving segmentation accuracy, reducing data\nstorage, and accelerating inference time. The source code and datasets will be\nmade publicly available at \\url{https://github.com/XAI4SAR/PolSAM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolSAR data presents unique challenges due to its rich and complex\ncharacteristics. Existing data representations, such as complex-valued data,\npolarimetric features, and amplitude images, are widely used. However, these\nformats often face issues related to usability, interpretability, and data\nintegrity. Most feature extraction networks for PolSAR are small, limiting\ntheir ability to capture features effectively. To address these issues, We\npropose the Polarimetric Scattering Mechanism-Informed SAM (PolSAM), an\nenhanced Segment Anything Model (SAM) that integrates domain-specific\nscattering characteristics and a novel prompt generation strategy. PolSAM\nintroduces Microwave Vision Data (MVD), a lightweight and interpretable data\nrepresentation derived from polarimetric decomposition and semantic\ncorrelations. We propose two key components: the Feature-Level Fusion Prompt\n(FFP), which fuses visual tokens from pseudo-colored SAR images and MVD to\naddress modality incompatibility in the frozen SAM encoder, and the\nSemantic-Level Fusion Prompt (SFP), which refines sparse and dense segmentation\nprompts using semantic information. Experimental results on the PhySAR-Seg\ndatasets demonstrate that PolSAM significantly outperforms existing SAM-based\nand multimodal fusion models, improving segmentation accuracy, reducing data\nstorage, and accelerating inference time. The source code and datasets will be\nmade publicly available at \\url{https://github.com/XAI4SAR/PolSAM}."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Zhongling Huang"
                    },
                    {
                        "name": "Shuxin Yang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Xiaolan Qiu"
                    },
                    {
                        "name": "Junwei Han"
                    },
                    {
                        "name": "Dingwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Zhang"
                },
                "author": "Dingwen Zhang",
                "arxiv_comment": "The manuscript is 15 pages long, includes 14 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12732v1",
                "updated": "2024-12-17T09:55:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    55,
                    2,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:55:02Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    55,
                    2,
                    1,
                    352,
                    0
                ],
                "title": "Using LLM-Generated Draft Replies to Support Human Experts in Responding\n  to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of\n  Industrial AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLM-Generated Draft Replies to Support Human Experts in Responding\n  to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of\n  Industrial AI"
                },
                "summary": "The maritime industry requires effective communication among diverse\nstakeholders to address complex, safety-critical challenges. Industrial AI,\nincluding Large Language Models (LLMs), has the potential to augment human\nexperts' workflows in this specialized domain. Our case study investigated the\nutility of LLMs in drafting replies to stakeholder inquiries and supporting\ncase handlers. We conducted a preliminary study (observations and interviews),\na survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding\nSimilarity). We discover that while LLM drafts can streamline workflows, they\noften require significant modifications to meet the specific demands of\nmaritime communications. Though LLMs are not yet mature enough for\nsafety-critical applications without human oversight, they can serve as\nvaluable augmentative tools. Final decision-making thus must remain with human\nexperts. However, by leveraging the strengths of both humans and LLMs,\nfostering human-AI collaboration, industries can increase efficiency while\nmaintaining high standards of quality and precision tailored to each case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maritime industry requires effective communication among diverse\nstakeholders to address complex, safety-critical challenges. Industrial AI,\nincluding Large Language Models (LLMs), has the potential to augment human\nexperts' workflows in this specialized domain. Our case study investigated the\nutility of LLMs in drafting replies to stakeholder inquiries and supporting\ncase handlers. We conducted a preliminary study (observations and interviews),\na survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding\nSimilarity). We discover that while LLM drafts can streamline workflows, they\noften require significant modifications to meet the specific demands of\nmaritime communications. Though LLMs are not yet mature enough for\nsafety-critical applications without human oversight, they can serve as\nvaluable augmentative tools. Final decision-making thus must remain with human\nexperts. However, by leveraging the strengths of both humans and LLMs,\nfostering human-AI collaboration, industries can increase efficiency while\nmaintaining high standards of quality and precision tailored to each case."
                },
                "authors": [
                    {
                        "name": "Tita Alissa Bach"
                    },
                    {
                        "name": "Aleksandar Babic"
                    },
                    {
                        "name": "Narae Park"
                    },
                    {
                        "name": "Tor Sporsem"
                    },
                    {
                        "name": "Rasmus Ulfsnes"
                    },
                    {
                        "name": "Henrik Smith-Meyer"
                    },
                    {
                        "name": "Torkel Skeie"
                    }
                ],
                "author_detail": {
                    "name": "Torkel Skeie"
                },
                "arxiv_affiliation": "DNV, Hvik, Norway",
                "author": "Torkel Skeie",
                "arxiv_comment": "These authors share the first authorship: Tita Alissa Bach (1),\n  Aleksandar Babic (1), Narae Park (1)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.16909v2",
                "updated": "2024-12-17T09:54:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    54,
                    15,
                    1,
                    352,
                    0
                ],
                "published": "2023-03-29T08:06:22Z",
                "published_parsed": [
                    2023,
                    3,
                    29,
                    8,
                    6,
                    22,
                    2,
                    88,
                    0
                ],
                "title": "RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data\n  Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data\n  Lakes"
                },
                "summary": "Can foundation models (such as ChatGPT) clean your data? In this proposal, we\ndemonstrate that indeed ChatGPT can assist in data cleaning by suggesting\ncorrections for specific cells in a data table (scenario 1). However, ChatGPT\nmay struggle with datasets it has never encountered before (e.g., local\nenterprise data) or when the user requires an explanation of the source of the\nsuggested clean values. To address these issues, we developed a retrieval-based\nmethod that complements ChatGPT's power with a user-provided data lake. The\ndata lake is first indexed, we then retrieve the top-k relevant tuples to the\nuser's query tuple and finally leverage ChatGPT to infer the correct value\n(scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally\nhosted model, might not be feasible for privacy reasons. To assist with this\nscenario, we developed a custom RoBERTa-based foundation model that can be\nlocally deployed. By fine-tuning it on a small number of examples, it can\neffectively make value inferences based on the retrieved tuples (scenario 3).\nOur proposed system, RetClean, seamlessly supports all three scenarios and\nprovides a user-friendly GUI that enables the VLDB audience to explore and\nexperiment with the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can foundation models (such as ChatGPT) clean your data? In this proposal, we\ndemonstrate that indeed ChatGPT can assist in data cleaning by suggesting\ncorrections for specific cells in a data table (scenario 1). However, ChatGPT\nmay struggle with datasets it has never encountered before (e.g., local\nenterprise data) or when the user requires an explanation of the source of the\nsuggested clean values. To address these issues, we developed a retrieval-based\nmethod that complements ChatGPT's power with a user-provided data lake. The\ndata lake is first indexed, we then retrieve the top-k relevant tuples to the\nuser's query tuple and finally leverage ChatGPT to infer the correct value\n(scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally\nhosted model, might not be feasible for privacy reasons. To assist with this\nscenario, we developed a custom RoBERTa-based foundation model that can be\nlocally deployed. By fine-tuning it on a small number of examples, it can\neffectively make value inferences based on the retrieved tuples (scenario 3).\nOur proposed system, RetClean, seamlessly supports all three scenarios and\nprovides a user-friendly GUI that enables the VLDB audience to explore and\nexperiment with the system."
                },
                "authors": [
                    {
                        "name": "Zan Ahmad Naeem"
                    },
                    {
                        "name": "Mohammad Shahmeer Ahmad"
                    },
                    {
                        "name": "Mohamed Eltabakh"
                    },
                    {
                        "name": "Mourad Ouzzani"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18264v2",
                "updated": "2024-12-17T09:53:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    53,
                    41,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-28T11:51:56Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    11,
                    51,
                    56,
                    2,
                    59,
                    0
                ],
                "title": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation under\n  Real-World Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation under\n  Real-World Scenario"
                },
                "summary": "It presents significant challenges to generate comprehensive and accurate\nWikipedia articles for newly emerging events under a real-world scenario.\nExisting attempts fall short either by focusing only on short snippets or by\nusing metrics that are insufficient to evaluate real-world scenarios. In this\npaper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,\ndesigned to align with real-world scenarios in both generation and evaluation.\nFor generation, we explore a real-world scenario where structured, full-length\nWikipedia articles with citations are generated for new events using input\ndocuments from web sources. For evaluation, we integrate systematic metrics and\nLLM-based metrics to assess the verifiability, organization, and other aspects\naligned with real-world scenarios. Based on this benchmark, we conduct\nextensive experiments using various models within three commonly used\nframeworks: direct RAG, hierarchical structure-based RAG, and RAG with a\nfine-tuned generation model. Experimental results show that hierarchical-based\nmethods can generate more comprehensive content, while fine-tuned methods\nachieve better verifiability. However, even the best methods still show a\nsignificant gap compared to existing Wikipedia content, indicating that further\nresearch is necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It presents significant challenges to generate comprehensive and accurate\nWikipedia articles for newly emerging events under a real-world scenario.\nExisting attempts fall short either by focusing only on short snippets or by\nusing metrics that are insufficient to evaluate real-world scenarios. In this\npaper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,\ndesigned to align with real-world scenarios in both generation and evaluation.\nFor generation, we explore a real-world scenario where structured, full-length\nWikipedia articles with citations are generated for new events using input\ndocuments from web sources. For evaluation, we integrate systematic metrics and\nLLM-based metrics to assess the verifiability, organization, and other aspects\naligned with real-world scenarios. Based on this benchmark, we conduct\nextensive experiments using various models within three commonly used\nframeworks: direct RAG, hierarchical structure-based RAG, and RAG with a\nfine-tuned generation model. Experimental results show that hierarchical-based\nmethods can generate more comprehensive content, while fine-tuned methods\nachieve better verifiability. However, even the best methods still show a\nsignificant gap compared to existing Wikipedia content, indicating that further\nresearch is necessary."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Eugene J. Yu"
                    },
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Chenhao Xiong"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Han Qian"
                    },
                    {
                        "name": "Mingbo Song"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "COLING 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13362v2",
                "updated": "2024-12-17T09:46:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    46,
                    19,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-19T09:07:31Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    7,
                    31,
                    2,
                    171,
                    0
                ],
                "title": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models"
                },
                "summary": "Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV."
                },
                "authors": [
                    {
                        "name": "Haowen Hou"
                    },
                    {
                        "name": "Peigen Zeng"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "Accepted at COLING 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18263v2",
                "updated": "2024-12-17T09:34:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    34,
                    49,
                    1,
                    352,
                    0
                ],
                "published": "2024-11-27T12:01:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    1,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution"
                },
                "summary": "Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yihong Guo"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12718v1",
                "updated": "2024-12-17T09:33:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    33,
                    6,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:33:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    33,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding"
                },
                "summary": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin."
                },
                "authors": [
                    {
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Multimedia",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11768v2",
                "updated": "2024-12-17T09:30:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    30,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T13:41:37Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    41,
                    37,
                    0,
                    351,
                    0
                ],
                "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No More Adam: Learning Rate Scaling at Initialization is All You Need"
                },
                "summary": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings."
                },
                "authors": [
                    {
                        "name": "Minghao Xu"
                    },
                    {
                        "name": "Lichuan Xiang"
                    },
                    {
                        "name": "Xu Cai"
                    },
                    {
                        "name": "Hongkai Wen"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Wen"
                },
                "author": "Hongkai Wen",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12710v1",
                "updated": "2024-12-17T09:25:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    25,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:25:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    25,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "Enhancing Naturalness in LLM-Generated Utterances through Disfluency\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Naturalness in LLM-Generated Utterances through Disfluency\n  Insertion"
                },
                "summary": "Disfluencies are a natural feature of spontaneous human speech but are\ntypically absent from the outputs of Large Language Models (LLMs). This absence\ncan diminish the perceived naturalness of synthesized speech, which is an\nimportant criteria when building conversational agents that aim to mimick human\nbehaviours. We show how the insertion of disfluencies can alleviate this\nshortcoming. The proposed approach involves (1) fine-tuning an LLM with\nLow-Rank Adaptation (LoRA) to incorporate various types of disfluencies into\nLLM-generated utterances and (2) synthesizing those utterances using a\ntext-to-speech model that supports the generation of speech phenomena such as\ndisfluencies. We evaluated the quality of the generated speech across two\nmetrics: intelligibility and perceived spontaneity. We demonstrate through a\nuser study that the insertion of disfluencies significantly increase the\nperceived spontaneity of the generated speech. This increase came, however,\nalong with a slight reduction in intelligibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disfluencies are a natural feature of spontaneous human speech but are\ntypically absent from the outputs of Large Language Models (LLMs). This absence\ncan diminish the perceived naturalness of synthesized speech, which is an\nimportant criteria when building conversational agents that aim to mimick human\nbehaviours. We show how the insertion of disfluencies can alleviate this\nshortcoming. The proposed approach involves (1) fine-tuning an LLM with\nLow-Rank Adaptation (LoRA) to incorporate various types of disfluencies into\nLLM-generated utterances and (2) synthesizing those utterances using a\ntext-to-speech model that supports the generation of speech phenomena such as\ndisfluencies. We evaluated the quality of the generated speech across two\nmetrics: intelligibility and perceived spontaneity. We demonstrate through a\nuser study that the insertion of disfluencies significantly increase the\nperceived spontaneity of the generated speech. This increase came, however,\nalong with a slight reduction in intelligibility."
                },
                "authors": [
                    {
                        "name": "Syed Zohaib Hassan"
                    },
                    {
                        "name": "Pierre Lison"
                    },
                    {
                        "name": "Pl Halvorsen"
                    }
                ],
                "author_detail": {
                    "name": "Pl Halvorsen"
                },
                "author": "Pl Halvorsen",
                "arxiv_comment": "4 pages short paper, references and appendix are additional",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12701v1",
                "updated": "2024-12-17T09:16:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    54,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:16:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Trigger$^3$: Refining Query Correction via Adaptive Model Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trigger$^3$: Refining Query Correction via Adaptive Model Selector"
                },
                "summary": "In search scenarios, user experience can be hindered by erroneous queries due\nto typos, voice errors, or knowledge gaps. Therefore, query correction is\ncrucial for search engines. Current correction models, usually small models\ntrained on specific data, often struggle with queries beyond their training\nscope or those requiring contextual understanding. While the advent of Large\nLanguage Models (LLMs) offers a potential solution, they are still limited by\ntheir pre-training data and inference cost, particularly for complex queries,\nmaking them not always effective for query correction. To tackle these, we\npropose Trigger$^3$, a large-small model collaboration framework that\nintegrates the traditional correction model and LLM for query correction,\ncapable of adaptively choosing the appropriate correction method based on the\nquery and the correction results from the traditional correction model and LLM.\nTrigger$^3$ first employs a correction trigger to filter out correct queries.\nIncorrect queries are then corrected by the traditional correction model. If\nthis fails, an LLM trigger is activated to call the LLM for correction.\nFinally, for queries that no model can correct, a fallback trigger decides to\nreturn the original query. Extensive experiments demonstrate Trigger$^3$\noutperforms correction baselines while maintaining efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In search scenarios, user experience can be hindered by erroneous queries due\nto typos, voice errors, or knowledge gaps. Therefore, query correction is\ncrucial for search engines. Current correction models, usually small models\ntrained on specific data, often struggle with queries beyond their training\nscope or those requiring contextual understanding. While the advent of Large\nLanguage Models (LLMs) offers a potential solution, they are still limited by\ntheir pre-training data and inference cost, particularly for complex queries,\nmaking them not always effective for query correction. To tackle these, we\npropose Trigger$^3$, a large-small model collaboration framework that\nintegrates the traditional correction model and LLM for query correction,\ncapable of adaptively choosing the appropriate correction method based on the\nquery and the correction results from the traditional correction model and LLM.\nTrigger$^3$ first employs a correction trigger to filter out correct queries.\nIncorrect queries are then corrected by the traditional correction model. If\nthis fails, an LLM trigger is activated to call the LLM for correction.\nFinally, for queries that no model can correct, a fallback trigger decides to\nreturn the original query. Extensive experiments demonstrate Trigger$^3$\noutperforms correction baselines while maintaining efficiency."
                },
                "authors": [
                    {
                        "name": "Kepu Zhang"
                    },
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12688v1",
                "updated": "2024-12-17T09:08:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    52,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:08:52Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    52,
                    1,
                    352,
                    0
                ],
                "title": "UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation\n  Benchmarks with Unified Entrez Gene Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation\n  Benchmarks with Unified Entrez Gene Identifiers"
                },
                "summary": "Gene studies are crucial for fields such as protein structure prediction,\ndrug discovery, and cancer genomics, yet they face challenges in fully\nutilizing the vast and diverse information available. Gene studies require\nclean, factual datasets to ensure reliable results. Ontology graphs, neatly\norganized domain terminology graphs, provide ideal sources for domain facts.\nHowever, available gene ontology annotations are currently distributed across\nvarious databases without unified identifiers for genes and gene products. To\naddress these challenges, we introduce Unified Entrez Gene Identifier Dataset\nand Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale\npublic Gene Ontology Annotations (GOA) from various databases using unique gene\nidentifiers. UniEntrezDB includes a pre-training dataset and four downstream\ntasks designed to comprehensively evaluate gene embedding performance from\ngene, protein, and cell levels, ultimately enhancing the reliability and\napplicability of LLMs in gene research and other professional settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene studies are crucial for fields such as protein structure prediction,\ndrug discovery, and cancer genomics, yet they face challenges in fully\nutilizing the vast and diverse information available. Gene studies require\nclean, factual datasets to ensure reliable results. Ontology graphs, neatly\norganized domain terminology graphs, provide ideal sources for domain facts.\nHowever, available gene ontology annotations are currently distributed across\nvarious databases without unified identifiers for genes and gene products. To\naddress these challenges, we introduce Unified Entrez Gene Identifier Dataset\nand Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale\npublic Gene Ontology Annotations (GOA) from various databases using unique gene\nidentifiers. UniEntrezDB includes a pre-training dataset and four downstream\ntasks designed to comprehensively evaluate gene embedding performance from\ngene, protein, and cell levels, ultimately enhancing the reliability and\napplicability of LLMs in gene research and other professional settings."
                },
                "authors": [
                    {
                        "name": "Yuwei Miao"
                    },
                    {
                        "name": "Yuzhi Guo"
                    },
                    {
                        "name": "Hehuan Ma"
                    },
                    {
                        "name": "Jingquan Yan"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Weizhi An"
                    },
                    {
                        "name": "Jean Gao"
                    },
                    {
                        "name": "Junzhou Huang"
                    }
                ],
                "author_detail": {
                    "name": "Junzhou Huang"
                },
                "author": "Junzhou Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.13178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v1",
                "updated": "2024-12-17T18:55:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "21 pages, 14 tables, 7 figures, submitted to ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06215v2",
                "updated": "2024-12-17T18:54:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    54,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-08T17:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    20,
                    37,
                    1,
                    282,
                    0
                ],
                "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback"
                },
                "summary": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project Page: https://DataEnvGym.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13175v1",
                "updated": "2024-12-17T18:54:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    54,
                    1,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:54:01Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    54,
                    1,
                    1,
                    352,
                    0
                ],
                "title": "DnDScore: Decontextualization and Decomposition for Factuality\n  Verification in Long-Form Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DnDScore: Decontextualization and Decomposition for Factuality\n  Verification in Long-Form Text Generation"
                },
                "summary": "The decompose-then-verify strategy for verification of Large Language Model\n(LLM) generations decomposes claims that are then independently verified.\nDecontextualization augments text (claims) to ensure it can be verified outside\nof the original context, enabling reliable verification. While decomposition\nand decontextualization have been explored independently, their interactions in\na complete system have not been investigated. Their conflicting purposes can\ncreate tensions: decomposition isolates atomic facts while decontextualization\ninserts relevant information. Furthermore, a decontextualized subclaim presents\na challenge to the verification step: what part of the augmented text should be\nverified as it now contains multiple atomic facts? We conduct an evaluation of\ndifferent decomposition, decontextualization, and verification strategies and\nfind that the choice of strategy matters in the resulting factuality scores.\nAdditionally, we introduce DnDScore, a decontextualization aware verification\nmethod which validates subclaims in the context of contextual information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The decompose-then-verify strategy for verification of Large Language Model\n(LLM) generations decomposes claims that are then independently verified.\nDecontextualization augments text (claims) to ensure it can be verified outside\nof the original context, enabling reliable verification. While decomposition\nand decontextualization have been explored independently, their interactions in\na complete system have not been investigated. Their conflicting purposes can\ncreate tensions: decomposition isolates atomic facts while decontextualization\ninserts relevant information. Furthermore, a decontextualized subclaim presents\na challenge to the verification step: what part of the augmented text should be\nverified as it now contains multiple atomic facts? We conduct an evaluation of\ndifferent decomposition, decontextualization, and verification strategies and\nfind that the choice of strategy matters in the resulting factuality scores.\nAdditionally, we introduce DnDScore, a decontextualization aware verification\nmethod which validates subclaims in the context of contextual information."
                },
                "authors": [
                    {
                        "name": "Miriam Wanner"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13169v1",
                "updated": "2024-12-17T18:46:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    46,
                    32,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:46:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    46,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study"
                },
                "summary": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness."
                },
                "authors": [
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Berk Yoztyurk"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Markus Herklotz"
                    },
                    {
                        "name": "Frauke Kreuter"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Matthias Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Assenmacher"
                },
                "author": "Matthias Assenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13163v1",
                "updated": "2024-12-17T18:42:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    42,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:42:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    42,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System"
                },
                "summary": "Organizations seeking to utilize Large Language Models (LLMs) for knowledge\nquerying and analysis often encounter challenges in maintaining an LLM\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\nsolution for organizations looking to overcome the challenges of maintaining\nproprietary models and to help reduce LLM hallucinations in their query\nresponses. However, RAG comes with its own issues regarding scaling data\npipelines across tiered-access and disparate data sources. In many scenarios,\nit is necessary to query beyond a single data silo to provide richer and more\nrelevant context for an LLM. Analyzing data sources within and across\norganizational trust boundaries is often limited by complex data-sharing\npolicies that prohibit centralized data storage, therefore, inhibit the fast\nand effective setup and scaling of RAG solutions. In this paper, we introduce\nConfidential Computing (CC) techniques as a solution for secure Federated\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\nacross a decentralized network of data providers by ensuring context\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\nMIRAGE benchmarking dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations seeking to utilize Large Language Models (LLMs) for knowledge\nquerying and analysis often encounter challenges in maintaining an LLM\nfine-tuned on targeted, up-to-date information that keeps answers relevant and\ngrounded. Retrieval Augmented Generation (RAG) has quickly become a feasible\nsolution for organizations looking to overcome the challenges of maintaining\nproprietary models and to help reduce LLM hallucinations in their query\nresponses. However, RAG comes with its own issues regarding scaling data\npipelines across tiered-access and disparate data sources. In many scenarios,\nit is necessary to query beyond a single data silo to provide richer and more\nrelevant context for an LLM. Analyzing data sources within and across\norganizational trust boundaries is often limited by complex data-sharing\npolicies that prohibit centralized data storage, therefore, inhibit the fast\nand effective setup and scaling of RAG solutions. In this paper, we introduce\nConfidential Computing (CC) techniques as a solution for secure Federated\nRetrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG\nsystem (C-FedRAG) enables secure connection and scaling of a RAG workflows\nacross a decentralized network of data providers by ensuring context\nconfidentiality. We also demonstrate how to implement a C-FedRAG system using\nthe NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and\nMIRAGE benchmarking dataset."
                },
                "authors": [
                    {
                        "name": "Parker Addison"
                    },
                    {
                        "name": "Minh-Tuan H. Nguyen"
                    },
                    {
                        "name": "Tomislav Medan"
                    },
                    {
                        "name": "Mohammad T. Manzari"
                    },
                    {
                        "name": "Brendan McElrone"
                    },
                    {
                        "name": "Laksh Lalwani"
                    },
                    {
                        "name": "Aboli More"
                    },
                    {
                        "name": "Smita Sharma"
                    },
                    {
                        "name": "Holger R. Roth"
                    },
                    {
                        "name": "Isaac Yang"
                    },
                    {
                        "name": "Chester Chen"
                    },
                    {
                        "name": "Daguang Xu"
                    },
                    {
                        "name": "Yan Cheng"
                    },
                    {
                        "name": "Andrew Feng"
                    },
                    {
                        "name": "Ziyue Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Xu"
                },
                "author": "Ziyue Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10370v2",
                "updated": "2024-12-17T18:35:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    35,
                    27,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-14T18:56:40Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    18,
                    56,
                    40,
                    4,
                    166,
                    0
                ],
                "title": "Let's Get to the Point: LLM-Supported Planning, Drafting, and Revising\n  of Research-Paper Blog Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Get to the Point: LLM-Supported Planning, Drafting, and Revising\n  of Research-Paper Blog Posts"
                },
                "summary": "Research-paper blog posts help scientists to disseminate their work to a\nlarger audience, but translating scientific long documents into long-form\nsummaries like blog posts raises unique challenges: 1) planning what paper\ncontent to include in the blog post, 2) drafting the selected content in\nsections amenable to a paper blog post, and 3) revising the blog post to be\nscientifically accurate but also concise, easy to understand, and engaging. Can\nwe harness the power of large language models (LLMs) to assist researchers with\nthese challenges? To investigate this question, we developed Papers-to-Posts,\nan LLM-powered tool that implements a new Plan-Draft-Revise workflow for\nmixed-initiative long-form paper summarization. An LLM-generated paper outline\nwith pre-selected yet adjustable bullet points helps users to plan what\ninformation to include. Meanwhile, customizable LLM instructions support\ndrafting the text with a suitable structure and revising the text to have an\nappropriate tone. Through two studies, we compared Papers-to-Posts to a strong\nbaseline tool that provides an LLM-generated draft and access to free-form LLM\nprompting, and we found that Papers-to-Posts improved researchers' editing\npower. In a within-subjects lab study (N=20 participants), Papers-to-Posts led\nparticipants to make significantly more change to initial LLM drafts within a\nfixed amount of time and to be significantly more satisfied with their final\nblog post, without increasing cognitive load. Furthermore, in a\nbetween-subjects deployment study (N=37 blog posts, 26 participants),\nPapers-to-Posts led participants to make more change to initial LLM drafts\nwithin a given amount of time as well as writing actions, without decreasing\nsatisfaction with the final blog posts or increasing cognitive load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research-paper blog posts help scientists to disseminate their work to a\nlarger audience, but translating scientific long documents into long-form\nsummaries like blog posts raises unique challenges: 1) planning what paper\ncontent to include in the blog post, 2) drafting the selected content in\nsections amenable to a paper blog post, and 3) revising the blog post to be\nscientifically accurate but also concise, easy to understand, and engaging. Can\nwe harness the power of large language models (LLMs) to assist researchers with\nthese challenges? To investigate this question, we developed Papers-to-Posts,\nan LLM-powered tool that implements a new Plan-Draft-Revise workflow for\nmixed-initiative long-form paper summarization. An LLM-generated paper outline\nwith pre-selected yet adjustable bullet points helps users to plan what\ninformation to include. Meanwhile, customizable LLM instructions support\ndrafting the text with a suitable structure and revising the text to have an\nappropriate tone. Through two studies, we compared Papers-to-Posts to a strong\nbaseline tool that provides an LLM-generated draft and access to free-form LLM\nprompting, and we found that Papers-to-Posts improved researchers' editing\npower. In a within-subjects lab study (N=20 participants), Papers-to-Posts led\nparticipants to make significantly more change to initial LLM drafts within a\nfixed amount of time and to be significantly more satisfied with their final\nblog post, without increasing cognitive load. Furthermore, in a\nbetween-subjects deployment study (N=37 blog posts, 26 participants),\nPapers-to-Posts led participants to make more change to initial LLM drafts\nwithin a given amount of time as well as writing actions, without decreasing\nsatisfaction with the final blog posts or increasing cognitive load."
                },
                "authors": [
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Daniel S. Weld"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Jonathan Bragg"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bragg"
                },
                "author": "Jonathan Bragg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13148v1",
                "updated": "2024-12-17T18:13:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:13:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    13,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training\n  With Significant Memory Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training\n  With Significant Memory Reduction"
                },
                "summary": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they maintain additional moving\naverage states throughout training, which results in memory requirements\nseveral times greater than the model. This overhead imposes constraints on\nscalability and computational efficiency. On the other hand, while stochastic\ngradient descent (SGD) is optimal in terms of memory efficiency, their\ncapability in LLM training is limited (Zhao et al., 2024b).\n  To address this dilemma, we show that pre-processing SGD is sufficient to\nreach Adam-level performance on LLMs. Specifically, we propose to preprocess\nthe instantaneous stochastic gradients with two simple operators:\n$\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$\nstabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the\nlocal curvature of the loss landscape, respectively. This results in SWAN (SGD\nwith Whitening And Normalization), a stochastic optimizer that eliminates the\nneed to store any accumulative state variables. Empirically, SWAN has the same\nmemory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end\nmemory compared to Adam. In language modeling tasks, SWAN demonstrates the same\nor even a substantial improvement over Adam. Specifically, when pre-training\nthe LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by\nreaching the same evaluation perplexity in less than half tokens seen."
                },
                "authors": [
                    {
                        "name": "Chao Ma"
                    },
                    {
                        "name": "Wenbo Gong"
                    },
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Edward Meeds"
                    }
                ],
                "author_detail": {
                    "name": "Edward Meeds"
                },
                "author": "Edward Meeds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v1",
                "updated": "2024-12-17T18:12:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10400v2",
                "updated": "2024-12-17T18:05:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    5,
                    11,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-05T16:10:42Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    10,
                    42,
                    3,
                    340,
                    0
                ],
                "title": "Reinforcement Learning Enhanced LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Enhanced LLMs: A Survey"
                },
                "summary": "This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper surveys research in the rapidly growing field of enhancing large\nlanguage models (LLMs) with reinforcement learning (RL), a technique that\nenables LLMs to improve their performance by receiving feedback in the form of\nrewards based on the quality of their outputs, allowing them to generate more\naccurate, coherent, and contextually appropriate responses. In this work, we\nmake a systematic review of the most up-to-date state of knowledge on\nRL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing\nresearch in this field, helping researchers understand the current challenges\nand advancements. Specifically, we (1) detail the basics of RL; (2) introduce\npopular RL-enhanced LLMs; (3) review researches on two widely-used reward\nmodel-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)\nand Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct\nPreference Optimization (DPO), a set of methods that bypass the reward model to\ndirectly use human preference data for aligning LLM outputs with human\nexpectations. We will also point out current challenges and deficiencies of\nexisting methods and suggest some avenues for further improvements. Project\npage of this work can be found at:\n\\url{https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey}."
                },
                "authors": [
                    {
                        "name": "Shuhe Wang"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Eduard Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Hovy"
                },
                "author": "Eduard Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13474v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13474v3",
                "updated": "2024-12-17T17:45:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    45,
                    7,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-20T13:05:07Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    13,
                    5,
                    7,
                    4,
                    264,
                    0
                ],
                "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models"
                },
                "summary": "Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization."
                },
                "authors": [
                    {
                        "name": "Anmol Mekala"
                    },
                    {
                        "name": "Vineeth Dorna"
                    },
                    {
                        "name": "Shreya Dubey"
                    },
                    {
                        "name": "Abhishek Lalwani"
                    },
                    {
                        "name": "David Koleczek"
                    },
                    {
                        "name": "Mukund Rungta"
                    },
                    {
                        "name": "Sadid Hasan"
                    },
                    {
                        "name": "Elita Lobo"
                    }
                ],
                "author_detail": {
                    "name": "Elita Lobo"
                },
                "author": "Elita Lobo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13474v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13474v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16383v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16383v4",
                "updated": "2024-12-17T17:42:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    42,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-24T18:35:09Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    18,
                    35,
                    9,
                    1,
                    268,
                    0
                ],
                "title": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation"
                },
                "summary": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings."
                },
                "authors": [
                    {
                        "name": "Ioannis Panagiotopoulos"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16383v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16383v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08359v2",
                "updated": "2024-12-17T17:31:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    31,
                    46,
                    1,
                    352,
                    0
                ],
                "published": "2024-05-14T06:55:16Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    6,
                    55,
                    16,
                    1,
                    135,
                    0
                ],
                "title": "GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for\n  Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for\n  Autonomous Vehicles"
                },
                "summary": "Autonomous Vehicles (AVs) heavily rely on sensors and communication networks\nlike Global Positioning System (GPS) to navigate autonomously. Prior research\nhas indicated that networks like GPS are vulnerable to cyber-attacks such as\nspoofing and jamming, thus posing serious risks like navigation errors and\nsystem failures. These threats are expected to intensify with the widespread\ndeployment of AVs, making it crucial to detect and mitigate such attacks. This\npaper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly-based\nintrusion detection framework to detect GPS spoofing attacks on AVs. The\nframework uses a novel physics-based vehicle behavior model where a GPS\nnavigation model is integrated into the conventional dynamic bicycle model for\naccurate AV behavior representation. Temporal features derived from this\nbehavior model are analyzed using machine learning to detect normal and\nabnormal navigation behaviors. The performance of the GPS-IDS framework is\nevaluated on the AV-GPS-Dataset -- a GPS security dataset for AVs comprising\nreal-world data collected using an AV testbed, and simulated data representing\nurban traffic environments. To the best of our knowledge, this dataset is the\nfirst of its kind and has been publicly released for the global research\ncommunity to address such security challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) heavily rely on sensors and communication networks\nlike Global Positioning System (GPS) to navigate autonomously. Prior research\nhas indicated that networks like GPS are vulnerable to cyber-attacks such as\nspoofing and jamming, thus posing serious risks like navigation errors and\nsystem failures. These threats are expected to intensify with the widespread\ndeployment of AVs, making it crucial to detect and mitigate such attacks. This\npaper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly-based\nintrusion detection framework to detect GPS spoofing attacks on AVs. The\nframework uses a novel physics-based vehicle behavior model where a GPS\nnavigation model is integrated into the conventional dynamic bicycle model for\naccurate AV behavior representation. Temporal features derived from this\nbehavior model are analyzed using machine learning to detect normal and\nabnormal navigation behaviors. The performance of the GPS-IDS framework is\nevaluated on the AV-GPS-Dataset -- a GPS security dataset for AVs comprising\nreal-world data collected using an AV testbed, and simulated data representing\nurban traffic environments. To the best of our knowledge, this dataset is the\nfirst of its kind and has been publicly released for the global research\ncommunity to address such security challenges."
                },
                "authors": [
                    {
                        "name": "Murad Mehrab Abrar"
                    },
                    {
                        "name": "Amal Youssef"
                    },
                    {
                        "name": "Raian Islam"
                    },
                    {
                        "name": "Shalaka Satam"
                    },
                    {
                        "name": "Banafsheh Saber Latibari"
                    },
                    {
                        "name": "Salim Hariri"
                    },
                    {
                        "name": "Sicong Shao"
                    },
                    {
                        "name": "Soheil Salehi"
                    },
                    {
                        "name": "Pratik Satam"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Satam"
                },
                "author": "Pratik Satam",
                "arxiv_comment": "Article under review at IEEE Transactions on Dependable and Secure\n  Computing. For associated AV-GPS-Dataset, see\n  https://github.com/mehrab-abrar/AV-GPS-Dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04049v3",
                "updated": "2024-12-17T17:17:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    17,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-06T14:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    51,
                    55,
                    1,
                    37,
                    0
                ],
                "title": "Systematic Biases in LLM Simulations of Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Biases in LLM Simulations of Debates"
                },
                "summary": "The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs), has opened exciting\npossibilities for constructing computational simulations designed to replicate\nhuman behavior accurately. Current research suggests that LLM-based agents\nbecome increasingly human-like in their performance, sparking interest in using\nthese AI agents as substitutes for human participants in behavioral studies.\nHowever, LLMs are complex statistical learners without straightforward\ndeductive rules, making them prone to unexpected behaviors. Hence, it is\ncrucial to study and pinpoint the key behavioral distinctions between humans\nand LLM-based agents. In this study, we highlight the limitations of LLMs in\nsimulating human interactions, particularly focusing on LLMs' ability to\nsimulate political debates on topics that are important aspects of people's\nday-to-day lives and decision-making processes. Our findings indicate a\ntendency for LLM agents to conform to the model's inherent social biases\ndespite being directed to debate from certain political perspectives. This\ntendency results in behavioral patterns that seem to deviate from\nwell-established social dynamics among humans. We reinforce these observations\nusing an automatic self-fine-tuning method, which enables us to manipulate the\nbiases within the LLM and demonstrate that agents subsequently align with the\naltered biases. These results underscore the need for further research to\ndevelop methods that help agents overcome these biases, a critical step toward\ncreating more realistic simulations."
                },
                "authors": [
                    {
                        "name": "Amir Taubenfeld"
                    },
                    {
                        "name": "Yaniv Dover"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Ariel Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goldstein"
                },
                "author": "Ariel Goldstein",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.04049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a conference paper at EMNLP 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13103v1",
                "updated": "2024-12-17T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    17,
                    3,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    17,
                    3,
                    1,
                    352,
                    0
                ],
                "title": "AI PERSONA: Towards Life-long Personalization of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI PERSONA: Towards Life-long Personalization of LLMs"
                },
                "summary": "In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems."
                },
                "authors": [
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Ruoyu Fang"
                    },
                    {
                        "name": "Huilin Wang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13102v1",
                "updated": "2024-12-17T17:15:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    15,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T17:15:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    15,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark"
                },
                "summary": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench."
                },
                "authors": [
                    {
                        "name": "Jianlyu Chen"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Hao Liao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v4",
                "updated": "2024-12-17T17:06:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    6,
                    1,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Study in German language.] This study examines the AI-powered grading tool\n\"AI Grading Assistant\" by the German company Fobizz, designed to support\nteachers in evaluating and providing feedback on student assignments. Against\nthe societal backdrop of an overburdened education system and rising\nexpectations for artificial intelligence as a solution to these challenges, the\ninvestigation evaluates the tool's functional suitability through two test\nseries. The results reveal significant shortcomings: The tool's numerical\ngrades and qualitative feedback are often random and do not improve even when\nits suggestions are incorporated. The highest ratings are achievable only with\ntexts generated by ChatGPT. False claims and nonsensical submissions frequently\ngo undetected, while the implementation of some grading criteria is unreliable\nand opaque. Since these deficiencies stem from the inherent limitations of\nlarge language models (LLMs), fundamental improvements to this or similar tools\nare not immediately foreseeable. The study critiques the broader trend of\nadopting AI as a quick fix for systemic problems in education, concluding that\nFobizz's marketing of the tool as an objective and time-saving solution is\nmisleading and irresponsible. Finally, the study calls for systematic\nevaluation and subject-specific pedagogical scrutiny of the use of AI tools in\neducational contexts."
                },
                "authors": [
                    {
                        "name": "Rainer Muehlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "33 pages, in German language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13091v1",
                "updated": "2024-12-17T17:01:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    1,
                    15,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T17:01:15Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    17,
                    1,
                    15,
                    1,
                    352,
                    0
                ],
                "title": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests"
                },
                "summary": "As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development."
                },
                "authors": [
                    {
                        "name": "Jon Saad-Falcon"
                    },
                    {
                        "name": "Rajan Vivek"
                    },
                    {
                        "name": "William Berrios"
                    },
                    {
                        "name": "Nandita Shankar Naik"
                    },
                    {
                        "name": "Matija Franklin"
                    },
                    {
                        "name": "Bertie Vidgen"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Shikib Mehri"
                    }
                ],
                "author_detail": {
                    "name": "Shikib Mehri"
                },
                "author": "Shikib Mehri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09739v2",
                "updated": "2024-12-17T16:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    52,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-15T14:10:01Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    10,
                    1,
                    6,
                    259,
                    0
                ],
                "title": "PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution"
                },
                "summary": "The rapid advancement of customized Large Language Models (LLMs) offers\nconsiderable convenience. However, it also intensifies concerns regarding the\nprotection of copyright/confidential information. With the extensive adoption\nof private LLMs, safeguarding model copyright and ensuring data privacy have\nbecome critical. Text watermarking has emerged as a viable solution for\ndetecting AI-generated content and protecting models. However, existing methods\nfall short in providing individualized watermarks for each user, a critical\nfeature for enhancing accountability and traceability. In this paper, we\nintroduce PersonaMark, a novel personalized text watermarking scheme designed\nto protect LLMs' copyrights and bolster accountability. PersonaMark leverages\nsentence structure as a subtle carrier of watermark information and optimizes\nthe generation process to maintain the natural output of the model. By\nemploying a personalized hashing function, unique watermarks are embedded for\neach user, enabling high-quality text generation without compromising the\nmodel's performance. This approach is both time-efficient and scalable, capable\nof handling large numbers of users through a multi-user hashing mechanism. To\nthe best of our knowledge, this is a pioneer study to explore personalized\nwatermarking in LLMs. We conduct extensive evaluations across four LLMs,\nanalyzing various metrics such as perplexity, sentiment, alignment, and\nreadability. The results validate that PersonaMark preserves text quality,\nensures unbiased watermark insertion, and offers robust watermark detection\ncapabilities, all while maintaining the model's behavior with minimal\ndisruption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of customized Large Language Models (LLMs) offers\nconsiderable convenience. However, it also intensifies concerns regarding the\nprotection of copyright/confidential information. With the extensive adoption\nof private LLMs, safeguarding model copyright and ensuring data privacy have\nbecome critical. Text watermarking has emerged as a viable solution for\ndetecting AI-generated content and protecting models. However, existing methods\nfall short in providing individualized watermarks for each user, a critical\nfeature for enhancing accountability and traceability. In this paper, we\nintroduce PersonaMark, a novel personalized text watermarking scheme designed\nto protect LLMs' copyrights and bolster accountability. PersonaMark leverages\nsentence structure as a subtle carrier of watermark information and optimizes\nthe generation process to maintain the natural output of the model. By\nemploying a personalized hashing function, unique watermarks are embedded for\neach user, enabling high-quality text generation without compromising the\nmodel's performance. This approach is both time-efficient and scalable, capable\nof handling large numbers of users through a multi-user hashing mechanism. To\nthe best of our knowledge, this is a pioneer study to explore personalized\nwatermarking in LLMs. We conduct extensive evaluations across four LLMs,\nanalyzing various metrics such as perplexity, sentiment, alignment, and\nreadability. The results validate that PersonaMark preserves text quality,\nensures unbiased watermark insertion, and offers robust watermark detection\ncapabilities, all while maintaining the model's behavior with minimal\ndisruption."
                },
                "authors": [
                    {
                        "name": "Yuehan Zhang"
                    },
                    {
                        "name": "Peizhuo Lv"
                    },
                    {
                        "name": "Yinpeng Liu"
                    },
                    {
                        "name": "Yongqiang Ma"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Liu"
                },
                "author": "Jiawei Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06527v2",
                "updated": "2024-12-17T16:44:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    44,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-12T23:19:02Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    23,
                    19,
                    2,
                    0,
                    225,
                    0
                ],
                "title": "Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings."
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Xiao Tang"
                    },
                    {
                        "name": "Abdallah El Ali"
                    },
                    {
                        "name": "Zhuying Li"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Jan de Wit"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Jos A. Bosch"
                    }
                ],
                "author_detail": {
                    "name": "Jos A. Bosch"
                },
                "author": "Jos A. Bosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02738v3",
                "updated": "2024-12-17T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    10,
                    26,
                    1,
                    352,
                    0
                ],
                "published": "2024-03-05T07:47:34Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    7,
                    47,
                    34,
                    1,
                    65,
                    0
                ],
                "title": "Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment"
                },
                "summary": "Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs."
                },
                "authors": [
                    {
                        "name": "Congzhi Zhang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v4",
                "updated": "2024-12-17T16:03:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    16,
                    3,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of high-quality long-context\nsummarization datasets has hindered further advancements in this area. To\naddress this, we introduce CNNSum, a multi-scale long-context summarization\nbenchmark based on Chinese novels, featuring human-driven annotations, which\ncomprises four subsets totaling 695 samples, with lengths ranging from 16k to\n128k. We evaluate numerous LLMs and conduct detailed case analyses.\nFurthermore, we conduct extensive fine-tuning experiments to explore and\nimprove long-context summarization. In our study: (1) Advanced LLMs like GPT-4o\nmay still generate subjective commentary, leading to vague summaries. (2)\nCurrently, long-context summarization mainly relies on memory ability afforded\nby longer context lengths. The advantages of Large LLMs are hard to utilize,\nthus small LLMs are the most cost-effective. (3) Different prompt templates\npaired with various version models may cause large performance gaps. In further\nfine-tuning, these can be mitigated, and the Base version models perform\nbetter. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential;\nusing short-context data can significantly improve long-context summarization\nperformance. However, further applying other interpolation methods requires\ncareful selection. (5) CNNSum provides more reliable and insightful evaluation\nresults than other benchmarks. We release CNNSum to advance future research in\nthis field. https://github.com/CxsGhost/CNNSum"
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20587v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20587v5",
                "updated": "2024-12-17T15:59:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    59,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2023-10-31T16:24:17Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    16,
                    24,
                    17,
                    1,
                    304,
                    0
                ],
                "title": "Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning"
                },
                "summary": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\ncloses the gap between value-based offline RL methods and decision transformers\nin dense-reward tasks. In particular, our method demonstrates superior\nperformance in scenarios with limited data samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\ncloses the gap between value-based offline RL methods and decision transformers\nin dense-reward tasks. In particular, our method demonstrates superior\nperformance in scenarios with limited data samples."
                },
                "authors": [
                    {
                        "name": "Ruizhe Shi"
                    },
                    {
                        "name": "Yuyao Liu"
                    },
                    {
                        "name": "Yanjie Ze"
                    },
                    {
                        "name": "Simon S. Du"
                    },
                    {
                        "name": "Huazhe Xu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhe Xu"
                },
                "author": "Huazhe Xu",
                "arxiv_comment": "Format adjustment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20587v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20587v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10582v2",
                "updated": "2024-12-17T15:56:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    56,
                    50,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T21:48:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    21,
                    48,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "WHAT-IF: Exploring Branching Narratives by Meta-Prompting Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHAT-IF: Exploring Branching Narratives by Meta-Prompting Large Language\n  Models"
                },
                "summary": "WHAT-IF -- Writing a Hero's Alternate Timeline through Interactive Fiction --\nis a system that uses zero-shot meta-prompting to create branching narratives\nfrom a prewritten story. Played as an interactive fiction (IF) game, WHAT-IF\nlets the player choose between decisions that the large language model (LLM)\nGPT-4 generates as possible branches in the story. Starting with an existing\nlinear plot as input, a branch is created at each key decision taken by the\nmain character. By meta-prompting the LLM to consider the major plot points\nfrom the story, the system produces coherent and well-structured alternate\nstorylines. WHAT-IF stores the branching plot tree in a graph which helps it to\nboth keep track of the story for prompting and maintain the structure for the\nfinal IF system. A video demo of our system can be found here:\nhttps://youtu.be/8vBqjqtupcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WHAT-IF -- Writing a Hero's Alternate Timeline through Interactive Fiction --\nis a system that uses zero-shot meta-prompting to create branching narratives\nfrom a prewritten story. Played as an interactive fiction (IF) game, WHAT-IF\nlets the player choose between decisions that the large language model (LLM)\nGPT-4 generates as possible branches in the story. Starting with an existing\nlinear plot as input, a branch is created at each key decision taken by the\nmain character. By meta-prompting the LLM to consider the major plot points\nfrom the story, the system produces coherent and well-structured alternate\nstorylines. WHAT-IF stores the branching plot tree in a graph which helps it to\nboth keep track of the story for prompting and maintain the structure for the\nfinal IF system. A video demo of our system can be found here:\nhttps://youtu.be/8vBqjqtupcc."
                },
                "authors": [
                    {
                        "name": "Runsheng \"Anson\" Huang"
                    },
                    {
                        "name": "Lara J. Martin"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13023v1",
                "updated": "2024-12-17T15:41:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    41,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:41:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    41,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Relational Neurosymbolic Markov Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Neurosymbolic Markov Models"
                },
                "summary": "Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential problems are ubiquitous in AI, such as in reinforcement learning\nor natural language processing. State-of-the-art deep sequential models, like\ntransformers, excel in these settings but fail to guarantee the satisfaction of\nconstraints necessary for trustworthy deployment. In contrast, neurosymbolic AI\n(NeSy) provides a sound formalism to enforce constraints in deep probabilistic\nmodels but scales exponentially on sequential problems. To overcome these\nlimitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a\nnew class of end-to-end differentiable sequential models that integrate and\nprovably satisfy relational logical constraints. We propose a strategy for\ninference and learning that scales on sequential settings, and that combines\napproximate Bayesian inference, automated reasoning, and gradient estimation.\nOur experiments show that NeSy-MMs can solve problems beyond the current\nstate-of-the-art in neurosymbolic AI and still provide strong guarantees with\nrespect to desired properties. Moreover, we show that our models are more\ninterpretable and that constraints can be adapted at test time to\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Lennert De Smet"
                    },
                    {
                        "name": "Gabriele Venturato"
                    },
                    {
                        "name": "Luc De Raedt"
                    },
                    {
                        "name": "Giuseppe Marra"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Marra"
                },
                "author": "Giuseppe Marra",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13021v1",
                "updated": "2024-12-17T15:41:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    41,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:41:36Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    41,
                    36,
                    1,
                    352,
                    0
                ],
                "title": "Queries, Representation & Detection: The Next 100 Model Fingerprinting\n  Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queries, Representation & Detection: The Next 100 Model Fingerprinting\n  Schemes"
                },
                "summary": "The deployment of machine learning models in operational contexts represents\na significant investment for any organisation. Consequently, the risk of these\nmodels being misappropriated by competitors needs to be addressed. In recent\nyears, numerous proposals have been put forth to detect instances of model\nstealing. However, these proposals operate under implicit and disparate data\nand model access assumptions; as a consequence, it remains unclear how they can\nbe effectively compared to one another. Our evaluation shows that a simple\nbaseline that we introduce performs on par with existing state-of-the-art\nfingerprints, which, on the other hand, are much more complex. To uncover the\nreasons behind this intriguing result, this paper introduces a systematic\napproach to both the creation of model fingerprinting schemes and their\nevaluation benchmarks. By dividing model fingerprinting into three core\ncomponents -- Query, Representation and Detection (QuRD) -- we are able to\nidentify $\\sim100$ previously unexplored QuRD combinations and gain insights\ninto their performance. Finally, we introduce a set of metrics to compare and\nguide the creation of more representative model stealing detection benchmarks.\nOur approach reveals the need for more challenging benchmarks and a sound\ncomparison with baselines. To foster the creation of new fingerprinting schemes\nand benchmarks, we open-source our fingerprinting toolbox.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of machine learning models in operational contexts represents\na significant investment for any organisation. Consequently, the risk of these\nmodels being misappropriated by competitors needs to be addressed. In recent\nyears, numerous proposals have been put forth to detect instances of model\nstealing. However, these proposals operate under implicit and disparate data\nand model access assumptions; as a consequence, it remains unclear how they can\nbe effectively compared to one another. Our evaluation shows that a simple\nbaseline that we introduce performs on par with existing state-of-the-art\nfingerprints, which, on the other hand, are much more complex. To uncover the\nreasons behind this intriguing result, this paper introduces a systematic\napproach to both the creation of model fingerprinting schemes and their\nevaluation benchmarks. By dividing model fingerprinting into three core\ncomponents -- Query, Representation and Detection (QuRD) -- we are able to\nidentify $\\sim100$ previously unexplored QuRD combinations and gain insights\ninto their performance. Finally, we introduce a set of metrics to compare and\nguide the creation of more representative model stealing detection benchmarks.\nOur approach reveals the need for more challenging benchmarks and a sound\ncomparison with baselines. To foster the creation of new fingerprinting schemes\nand benchmarks, we open-source our fingerprinting toolbox."
                },
                "authors": [
                    {
                        "name": "Augustin Godinot"
                    },
                    {
                        "name": "Erwan Le Merrer"
                    },
                    {
                        "name": "Camilla Penzo"
                    },
                    {
                        "name": "Franois Taani"
                    },
                    {
                        "name": "Gilles Trdan"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Trdan"
                },
                "author": "Gilles Trdan",
                "arxiv_comment": "Accepted to AAAI2025 Main Technical Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13018v1",
                "updated": "2024-12-17T15:38:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    38,
                    42,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:38:42Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    38,
                    42,
                    1,
                    352,
                    0
                ],
                "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain"
                },
                "summary": "As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}."
                },
                "authors": [
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Jiejun Tan"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12957v2",
                "updated": "2024-12-17T15:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    38,
                    23,
                    1,
                    352,
                    0
                ],
                "published": "2024-04-19T15:40:39Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    15,
                    40,
                    39,
                    4,
                    110,
                    0
                ],
                "title": "Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction"
                },
                "summary": "In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE"
                },
                "authors": [
                    {
                        "name": "Qinyuan Wu"
                    },
                    {
                        "name": "Mohammad Aflah Khan"
                    },
                    {
                        "name": "Soumi Das"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Camila Kolling"
                    },
                    {
                        "name": "Till Speicher"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Evimaria Terzi"
                    }
                ],
                "author_detail": {
                    "name": "Evimaria Terzi"
                },
                "author": "Evimaria Terzi",
                "arxiv_doi": "10.1145/3701551.3703562",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703562",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.12957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13013v1",
                "updated": "2024-12-17T15:34:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:34:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "The Emergence of Strategic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Strategic Reasoning of Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are increasingly used for a variety of\ncomplex and critical tasks, it is vital to assess their logical capabilities in\nstrategic environments. This paper examines their ability in strategic\nreasoning -- the process of choosing an optimal course of action by predicting\nand adapting to other agents' behavior. Using six LLMs, we analyze responses\nfrom play in classical games from behavioral economics (p-Beauty Contest, 11-20\nMoney Request Game, and Guessing Game) and evaluate their performance through\nhierarchical models of reasoning (level-$k$ theory and cognitive hierarchy\ntheory). Our findings reveal that while LLMs show understanding of the games,\nthe majority struggle with higher-order strategic reasoning. Although most LLMs\ndid demonstrate learning ability with games involving repeated interactions,\nthey still consistently fall short of the reasoning levels demonstrated by\ntypical behavior from human subjects. The exception to these overall findings\nis with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning\ntasks -- which consistently outperforms other LLMs and human subjects. These\nfindings highlight the challenges and pathways in advancing LLMs toward robust\nstrategic reasoning from the perspective of behavioral economics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly used for a variety of\ncomplex and critical tasks, it is vital to assess their logical capabilities in\nstrategic environments. This paper examines their ability in strategic\nreasoning -- the process of choosing an optimal course of action by predicting\nand adapting to other agents' behavior. Using six LLMs, we analyze responses\nfrom play in classical games from behavioral economics (p-Beauty Contest, 11-20\nMoney Request Game, and Guessing Game) and evaluate their performance through\nhierarchical models of reasoning (level-$k$ theory and cognitive hierarchy\ntheory). Our findings reveal that while LLMs show understanding of the games,\nthe majority struggle with higher-order strategic reasoning. Although most LLMs\ndid demonstrate learning ability with games involving repeated interactions,\nthey still consistently fall short of the reasoning levels demonstrated by\ntypical behavior from human subjects. The exception to these overall findings\nis with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning\ntasks -- which consistently outperforms other LLMs and human subjects. These\nfindings highlight the challenges and pathways in advancing LLMs toward robust\nstrategic reasoning from the perspective of behavioral economics."
                },
                "authors": [
                    {
                        "name": "Dongwoo Lee"
                    },
                    {
                        "name": "Gavin Kader"
                    }
                ],
                "author_detail": {
                    "name": "Gavin Kader"
                },
                "author": "Gavin Kader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12589v2",
                "updated": "2024-12-17T15:28:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    28,
                    10,
                    1,
                    352,
                    0
                ],
                "published": "2024-03-19T09:48:18Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    9,
                    48,
                    18,
                    1,
                    79,
                    0
                ],
                "title": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting"
                },
                "summary": "Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition."
                },
                "authors": [
                    {
                        "name": "Clment Gaspard"
                    },
                    {
                        "name": "Grgoire Passault"
                    },
                    {
                        "name": "Mlodie Daniel"
                    },
                    {
                        "name": "Olivier Ly"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Ly"
                },
                "author": "Olivier Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13391v3",
                "updated": "2024-12-17T15:12:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    12,
                    2,
                    1,
                    352,
                    0
                ],
                "published": "2024-01-24T11:41:30Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    11,
                    41,
                    30,
                    2,
                    24,
                    0
                ],
                "title": "Reranking individuals: The effect of fair classification within-groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking individuals: The effect of fair classification within-groups"
                },
                "summary": "Artificial Intelligence (AI) finds widespread application across various\ndomains, but it sparks concerns about fairness in its deployment. The\nprevailing discourse in classification often emphasizes outcome-based metrics\ncomparing sensitive subgroups without a nuanced consideration of the\ndifferential impacts within subgroups. Bias mitigation techniques not only\naffect the ranking of pairs of instances across sensitive groups, but often\nalso significantly affect the ranking of instances within these groups. Such\nchanges are hard to explain and raise concerns regarding the validity of the\nintervention. Unfortunately, these effects remain under the radar in the\naccuracy-fairness evaluation framework that is usually applied. Additionally,\nwe illustrate the effect of several popular bias mitigation methods, and how\ntheir output often does not reflect real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) finds widespread application across various\ndomains, but it sparks concerns about fairness in its deployment. The\nprevailing discourse in classification often emphasizes outcome-based metrics\ncomparing sensitive subgroups without a nuanced consideration of the\ndifferential impacts within subgroups. Bias mitigation techniques not only\naffect the ranking of pairs of instances across sensitive groups, but often\nalso significantly affect the ranking of instances within these groups. Such\nchanges are hard to explain and raise concerns regarding the validity of the\nintervention. Unfortunately, these effects remain under the radar in the\naccuracy-fairness evaluation framework that is usually applied. Additionally,\nwe illustrate the effect of several popular bias mitigation methods, and how\ntheir output often does not reflect real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Sofie Goethals"
                    },
                    {
                        "name": "Marco Favier"
                    },
                    {
                        "name": "Toon Calders"
                    }
                ],
                "author_detail": {
                    "name": "Toon Calders"
                },
                "author": "Toon Calders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12981v1",
                "updated": "2024-12-17T15:01:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    1,
                    7,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T15:01:07Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    1,
                    7,
                    1,
                    352,
                    0
                ],
                "title": "Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental\n  Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental\n  Health"
                },
                "summary": "Large language models (LLMs) have shown promising capabilities in healthcare\nanalysis but face several challenges like hallucinations, parroting, and bias\nmanifestation. These challenges are exacerbated in complex, sensitive, and\nlow-resource domains. Therefore, in this work we introduce IC-AnnoMI, an\nexpert-annotated motivational interviewing (MI) dataset built upon AnnoMI by\ngenerating in-context conversational dialogues leveraging LLMs, particularly\nChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues\nand tailored information, taking into account therapy style (empathy,\nreflection), contextual relevance, and false semantic change. Subsequently, the\ndialogues are annotated by experts, strictly adhering to the Motivational\nInterviewing Skills Code (MISC), focusing on both the psychological and\nlinguistic dimensions of MI dialogues. We comprehensively evaluate the\nIC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding\nof domain intricacies by modeling novel classification tasks employing several\nclassical machine learning and current state-of-the-art transformer approaches.\nFinally, we discuss the effects of progressive prompting strategies and the\nimpact of augmented data in mitigating the biases manifested in IC-AnnoM. Our\ncontributions provide the MI community with not only a comprehensive dataset\nbut also valuable insights for using LLMs in empathetic text generation for\nconversational therapy in supervised settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising capabilities in healthcare\nanalysis but face several challenges like hallucinations, parroting, and bias\nmanifestation. These challenges are exacerbated in complex, sensitive, and\nlow-resource domains. Therefore, in this work we introduce IC-AnnoMI, an\nexpert-annotated motivational interviewing (MI) dataset built upon AnnoMI by\ngenerating in-context conversational dialogues leveraging LLMs, particularly\nChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues\nand tailored information, taking into account therapy style (empathy,\nreflection), contextual relevance, and false semantic change. Subsequently, the\ndialogues are annotated by experts, strictly adhering to the Motivational\nInterviewing Skills Code (MISC), focusing on both the psychological and\nlinguistic dimensions of MI dialogues. We comprehensively evaluate the\nIC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding\nof domain intricacies by modeling novel classification tasks employing several\nclassical machine learning and current state-of-the-art transformer approaches.\nFinally, we discuss the effects of progressive prompting strategies and the\nimpact of augmented data in mitigating the biases manifested in IC-AnnoM. Our\ncontributions provide the MI community with not only a comprehensive dataset\nbut also valuable insights for using LLMs in empathetic text generation for\nconversational therapy in supervised settings."
                },
                "authors": [
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Eirini Ntoutsi"
                    },
                    {
                        "name": "Pushpraj Singh Rajawat"
                    },
                    {
                        "name": "Giacomo Medda"
                    },
                    {
                        "name": "Diego Reforgiato Recupero"
                    }
                ],
                "author_detail": {
                    "name": "Diego Reforgiato Recupero"
                },
                "author": "Diego Reforgiato Recupero",
                "arxiv_comment": "International Conference on Natural Language Processing and\n  Artificial Intelligence for Cyber Security (NLPAICS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12969v1",
                "updated": "2024-12-17T14:53:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    53,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:53:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    53,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "System-Level Experimental Evaluation of Reconfigurable Intelligent\n  Surfaces for NextG Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System-Level Experimental Evaluation of Reconfigurable Intelligent\n  Surfaces for NextG Communication Systems"
                },
                "summary": "Reconfigurable Intelligent Surfaces (RISs) are a promising technique for\nenhancing the performance of Next Generation (NextG) wireless communication\nsystems in terms of both spectral and energy efficiency, as well as resource\nutilization. However, current RIS research has primarily focused on theoretical\nmodeling and Physical (PHY) layer considerations only. Full protocol stack\nemulation and accurate modeling of the propagation characteristics of the\nwireless channel are necessary for studying the benefits introduced by RIS\ntechnology across various spectrum bands and use-cases. In this paper, we\npropose, for the first time: (i) accurate PHY layer RIS-enabled channel\nmodeling through Geometry-Based Stochastic Models (GBSMs), leveraging the QUAsi\nDeterministic RadIo channel GenerAtor (QuaDRiGa) open-source statistical\nray-tracer; (ii) optimized resource allocation with RISs by comprehensively\nstudying energy efficiency and power control on different portions of the\nspectrum through a single-leader multiple-followers Stackelberg game\ntheoretical approach; (iii) full-stack emulation and performance evaluation of\nRIS-assisted channels with SCOPE/srsRAN for Enhanced Mobile Broadband (eMBB)\nand Ultra Reliable and Low Latency Communications (URLLC) applications in the\nworlds largest emulator of wireless systems with hardware-in-the-loop, namely\nColosseum. Our findings indicate (i) the significant power savings in terms of\nenergy efficiency achieved with RIS-assisted topologies, especially in the\nmillimeter wave (mmWave) band; and (ii) the benefits introduced for Sub-6 GHz\nband User Equipments (UEs), where the deployment of a relatively small RIS\n(e.g., in the order of 100 RIS elements) can result in decreased levels of\nlatency for URLLC services in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces (RISs) are a promising technique for\nenhancing the performance of Next Generation (NextG) wireless communication\nsystems in terms of both spectral and energy efficiency, as well as resource\nutilization. However, current RIS research has primarily focused on theoretical\nmodeling and Physical (PHY) layer considerations only. Full protocol stack\nemulation and accurate modeling of the propagation characteristics of the\nwireless channel are necessary for studying the benefits introduced by RIS\ntechnology across various spectrum bands and use-cases. In this paper, we\npropose, for the first time: (i) accurate PHY layer RIS-enabled channel\nmodeling through Geometry-Based Stochastic Models (GBSMs), leveraging the QUAsi\nDeterministic RadIo channel GenerAtor (QuaDRiGa) open-source statistical\nray-tracer; (ii) optimized resource allocation with RISs by comprehensively\nstudying energy efficiency and power control on different portions of the\nspectrum through a single-leader multiple-followers Stackelberg game\ntheoretical approach; (iii) full-stack emulation and performance evaluation of\nRIS-assisted channels with SCOPE/srsRAN for Enhanced Mobile Broadband (eMBB)\nand Ultra Reliable and Low Latency Communications (URLLC) applications in the\nworlds largest emulator of wireless systems with hardware-in-the-loop, namely\nColosseum. Our findings indicate (i) the significant power savings in terms of\nenergy efficiency achieved with RIS-assisted topologies, especially in the\nmillimeter wave (mmWave) band; and (ii) the benefits introduced for Sub-6 GHz\nband User Equipments (UEs), where the deployment of a relatively small RIS\n(e.g., in the order of 100 RIS elements) can result in decreased levels of\nlatency for URLLC services in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Maria Tsampazi"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12961v1",
                "updated": "2024-12-17T14:44:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    44,
                    27,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:44:27Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    44,
                    27,
                    1,
                    352,
                    0
                ],
                "title": "Adaptations of AI models for querying the LandMatrix database in natural\n  language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptations of AI models for querying the LandMatrix database in natural\n  language"
                },
                "summary": "The Land Matrix initiative (https://landmatrix.org) and its global\nobservatory aim to provide reliable data on large-scale land acquisitions to\ninform debates and actions in sectors such as agriculture, extraction, or\nenergy in low- and middle-income countries. Although these data are recognized\nin the academic world, they remain underutilized in public policy, mainly due\nto the complexity of access and exploitation, which requires technical\nexpertise and a good understanding of the database schema.\n  The objective of this work is to simplify access to data from different\ndatabase systems. The methods proposed in this article are evaluated using data\nfrom the Land Matrix. This work presents various comparisons of Large Language\nModels (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,\nRAG, Agents) to query different database systems (GraphQL and REST queries).\nThe experiments are reproducible, and a demonstration is available online:\nhttps://github.com/tetis-nlp/landmatrix-graphql-python.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Land Matrix initiative (https://landmatrix.org) and its global\nobservatory aim to provide reliable data on large-scale land acquisitions to\ninform debates and actions in sectors such as agriculture, extraction, or\nenergy in low- and middle-income countries. Although these data are recognized\nin the academic world, they remain underutilized in public policy, mainly due\nto the complexity of access and exploitation, which requires technical\nexpertise and a good understanding of the database schema.\n  The objective of this work is to simplify access to data from different\ndatabase systems. The methods proposed in this article are evaluated using data\nfrom the Land Matrix. This work presents various comparisons of Large Language\nModels (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,\nRAG, Agents) to query different database systems (GraphQL and REST queries).\nThe experiments are reproducible, and a demonstration is available online:\nhttps://github.com/tetis-nlp/landmatrix-graphql-python."
                },
                "authors": [
                    {
                        "name": "Fatiha Ait Kbir"
                    },
                    {
                        "name": "Jrmy Bourgoin"
                    },
                    {
                        "name": "Rmy Decoupes"
                    },
                    {
                        "name": "Marie Gradeler"
                    },
                    {
                        "name": "Roberto Interdonato"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Interdonato"
                },
                "author": "Roberto Interdonato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12956v1",
                "updated": "2024-12-17T14:38:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    38,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:38:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    38,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "SnakModel: Lessons Learned from Training an Open Danish Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnakModel: Lessons Learned from Training an Open Danish Large Language\n  Model"
                },
                "summary": "We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints."
                },
                "authors": [
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Max Mller-Eberstein"
                    },
                    {
                        "name": "Elisa Bassignana"
                    },
                    {
                        "name": "Rob van der Goot"
                    }
                ],
                "author_detail": {
                    "name": "Rob van der Goot"
                },
                "author": "Rob van der Goot",
                "arxiv_comment": "Accepted at NoDaLiDa 2025 (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12951v1",
                "updated": "2024-12-17T14:33:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    33,
                    5,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:33:05Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    33,
                    5,
                    1,
                    352,
                    0
                ],
                "title": "FineGates: LLMs Finetuning with Compression using Stochastic Gates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineGates: LLMs Finetuning with Compression using Stochastic Gates"
                },
                "summary": "Large Language Models (LLMs), with billions of parameters, present\nsignificant challenges for full finetuning due to the high computational\ndemands, memory requirements, and impracticality of many real-world\napplications. When faced with limited computational resources or small\ndatasets, updating all model parameters can often result in overfitting. To\naddress this, lightweight finetuning techniques have been proposed, like\nlearning low-rank adapter layers. These methods aim to train only a few\nadditional parameters combined with the base model, which remains frozen,\nreducing resource usage and mitigating overfitting risks. In this work, we\npropose an adaptor model based on stochastic gates that simultaneously sparsify\nthe frozen base model with task-specific adaptation. Our method comes with a\nsmall number of trainable parameters and allows us to speed up the base model\ninference with competitive accuracy. We evaluate it in additional variants by\nequipping it with additional low-rank parameters and comparing it to several\nrecent baselines. Our results show that the proposed method improves the\nfinetuned model accuracy comparatively to the several baselines and allows the\nremoval of up to 20-40\\% without significant accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with billions of parameters, present\nsignificant challenges for full finetuning due to the high computational\ndemands, memory requirements, and impracticality of many real-world\napplications. When faced with limited computational resources or small\ndatasets, updating all model parameters can often result in overfitting. To\naddress this, lightweight finetuning techniques have been proposed, like\nlearning low-rank adapter layers. These methods aim to train only a few\nadditional parameters combined with the base model, which remains frozen,\nreducing resource usage and mitigating overfitting risks. In this work, we\npropose an adaptor model based on stochastic gates that simultaneously sparsify\nthe frozen base model with task-specific adaptation. Our method comes with a\nsmall number of trainable parameters and allows us to speed up the base model\ninference with competitive accuracy. We evaluate it in additional variants by\nequipping it with additional low-rank parameters and comparing it to several\nrecent baselines. Our results show that the proposed method improves the\nfinetuned model accuracy comparatively to the several baselines and allows the\nremoval of up to 20-40\\% without significant accuracy loss."
                },
                "authors": [
                    {
                        "name": "Jonathan Svirsky"
                    },
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09916v2",
                "updated": "2024-12-17T14:30:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    30,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T07:08:13Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    13,
                    4,
                    348,
                    0
                ],
                "title": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer"
                },
                "summary": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents."
                },
                "authors": [
                    {
                        "name": "Sehyeong Jo"
                    },
                    {
                        "name": "Jungwon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jungwon Seo"
                },
                "author": "Jungwon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11497v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11497v3",
                "updated": "2024-12-17T14:11:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    11,
                    19,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-17T13:01:12Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    1,
                    12,
                    0,
                    169,
                    0
                ],
                "title": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen1.5-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen1.5-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods."
                },
                "authors": [
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "AAAI25 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11497v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11497v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12928v1",
                "updated": "2024-12-17T14:07:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    7,
                    1,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    7,
                    1,
                    1,
                    352,
                    0
                ],
                "title": "Truthful Text Sanitization Guided by Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful Text Sanitization Guided by Inference Attacks"
                },
                "summary": "The purpose of text sanitization is to rewrite those text spans in a document\nthat may directly or indirectly identify an individual, to ensure they no\nlonger disclose personal information. Text sanitization must strike a balance\nbetween preventing the leakage of personal information (privacy protection)\nwhile also retaining as much of the document's original content as possible\n(utility preservation). We present an automated text sanitization strategy\nbased on generalizations, which are more abstract (but still informative) terms\nthat subsume the semantic content of the original text spans. The approach\nrelies on instruction-tuned large language models (LLMs) and is divided into\ntwo stages. The LLM is first applied to obtain truth-preserving replacement\ncandidates and rank them according to their abstraction level. Those candidates\nare then evaluated for their ability to protect privacy by conducting inference\nattacks with the LLM. Finally, the system selects the most informative\nreplacement shown to be resistant to those attacks. As a consequence of this\ntwo-stage process, the chosen replacements effectively balance utility and\nprivacy. We also present novel metrics to automatically evaluate these two\naspects without the need to manually annotate data. Empirical results on the\nText Anonymization Benchmark show that the proposed approach leads to enhanced\nutility, with only a marginal increase in the risk of re-identifying protected\nindividuals compared to fully suppressing the original information.\nFurthermore, the selected replacements are shown to be more truth-preserving\nand abstractive than previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of text sanitization is to rewrite those text spans in a document\nthat may directly or indirectly identify an individual, to ensure they no\nlonger disclose personal information. Text sanitization must strike a balance\nbetween preventing the leakage of personal information (privacy protection)\nwhile also retaining as much of the document's original content as possible\n(utility preservation). We present an automated text sanitization strategy\nbased on generalizations, which are more abstract (but still informative) terms\nthat subsume the semantic content of the original text spans. The approach\nrelies on instruction-tuned large language models (LLMs) and is divided into\ntwo stages. The LLM is first applied to obtain truth-preserving replacement\ncandidates and rank them according to their abstraction level. Those candidates\nare then evaluated for their ability to protect privacy by conducting inference\nattacks with the LLM. Finally, the system selects the most informative\nreplacement shown to be resistant to those attacks. As a consequence of this\ntwo-stage process, the chosen replacements effectively balance utility and\nprivacy. We also present novel metrics to automatically evaluate these two\naspects without the need to manually annotate data. Empirical results on the\nText Anonymization Benchmark show that the proposed approach leads to enhanced\nutility, with only a marginal increase in the risk of re-identifying protected\nindividuals compared to fully suppressing the original information.\nFurthermore, the selected replacements are shown to be more truth-preserving\nand abstractive than previous methods."
                },
                "authors": [
                    {
                        "name": "Ildik Piln"
                    },
                    {
                        "name": "Benet Manzanares-Salor"
                    },
                    {
                        "name": "David Snchez"
                    },
                    {
                        "name": "Pierre Lison"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lison"
                },
                "author": "Pierre Lison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11491v2",
                "updated": "2024-12-17T13:32:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    32,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-21T10:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    1,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "SCANS: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious\n  Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCANS: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious\n  Activation Steering"
                },
                "summary": "Safety alignment is indispensable for Large Language Models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is indispensable for Large Language Models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability."
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Extended version of paper accepted to AAAI 2025. 14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12841v2",
                "updated": "2024-12-17T13:31:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    31,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-07-04T12:59:10Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    12,
                    59,
                    10,
                    3,
                    186,
                    0
                ],
                "title": "Black-box Model Ensembling for Textual and Visual Question Answering via\n  Information Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Model Ensembling for Textual and Visual Question Answering via\n  Information Fusion"
                },
                "summary": "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, fine-tuning these models\nis either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient ensemble method that\nlearns to dynamically pick the winner from existing black-box models for\npredictions on both textual and multimodal visual question answering tasks.\nUnlike traditional ensemble models, InfoSel does not rely on prediction\nprobabilities or confidences, which typically are not available in black-box\nmodels. Experimental results on four datasets demonstrate that our approach\nachieves an absolute increase of up to +5.19\\% in the F1-score compared to\nstandalone LLMs using only 1K training instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A diverse range of large language models (LLMs), e.g., ChatGPT, and visual\nquestion answering (VQA) models, e.g., BLIP, have been developed for solving\ntextual and visual question answering tasks. However, fine-tuning these models\nis either difficult, as it requires access via APIs, rendering them as\nblack-boxes, or costly due to the need of tuning a large number of parameters.\nTo address this, we introduce InfoSel, a data-efficient ensemble method that\nlearns to dynamically pick the winner from existing black-box models for\npredictions on both textual and multimodal visual question answering tasks.\nUnlike traditional ensemble models, InfoSel does not rely on prediction\nprobabilities or confidences, which typically are not available in black-box\nmodels. Experimental results on four datasets demonstrate that our approach\nachieves an absolute increase of up to +5.19\\% in the F1-score compared to\nstandalone LLMs using only 1K training instances."
                },
                "authors": [
                    {
                        "name": "Yuxi Xia"
                    },
                    {
                        "name": "Kilm Zaporojets"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "arxiv_comment": "15 pages, 6 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12898v1",
                "updated": "2024-12-17T13:21:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    21,
                    26,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:21:26Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    21,
                    26,
                    1,
                    352,
                    0
                ],
                "title": "An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions"
                },
                "summary": "The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches."
                },
                "authors": [
                    {
                        "name": "Shreeyash Gowaikar"
                    },
                    {
                        "name": "Srinivasan Iyengar"
                    },
                    {
                        "name": "Sameer Segal"
                    },
                    {
                        "name": "Shivkumar Kalyanaraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivkumar Kalyanaraman"
                },
                "author": "Shivkumar Kalyanaraman",
                "arxiv_comment": "Accepted at the AAAI'25 Workshop on AI to Accelerate Science and\n  Engineering (AI2ASE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12894v1",
                "updated": "2024-12-17T13:19:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    19,
                    55,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:19:55Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    19,
                    55,
                    1,
                    352,
                    0
                ],
                "title": "Design of Restricted Normalizing Flow towards Arbitrary Stochastic\n  Policy with Computational Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design of Restricted Normalizing Flow towards Arbitrary Stochastic\n  Policy with Computational Efficiency"
                },
                "summary": "This paper proposes a new design method for a stochastic control policy using\na normalizing flow (NF). In reinforcement learning (RL), the policy is usually\nmodeled as a distribution model with trainable parameters. When this\nparameterization has less expressiveness, it would fail to acquiring the\noptimal policy. A mixture model has capability of a universal approximation,\nbut it with too much redundancy increases the computational cost, which can\nbecome a bottleneck when considering the use of real-time robot control. As\nanother approach, NF, which is with additional parameters for invertible\ntransformation from a simple stochastic model as a base, is expected to exert\nhigh expressiveness and lower computational cost. However, NF cannot compute\nits mean analytically due to complexity of the invertible transformation, and\nit lacks reliability because it retains stochastic behaviors after deployment\nfor robot controller. This paper therefore designs a restricted NF (RNF) that\nachieves an analytic mean by appropriately restricting the invertible\ntransformation. In addition, the expressiveness impaired by this restriction is\nregained using bimodal student-t distribution as its base, so-called Bit-RNF.\nIn RL benchmarks, Bit-RNF policy outperformed the previous models. Finally, a\nreal robot experiment demonstrated the applicability of Bit-RNF policy to real\nworld. The attached video is uploaded on youtube: https://youtu.be/R_GJVZDW9bk",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a new design method for a stochastic control policy using\na normalizing flow (NF). In reinforcement learning (RL), the policy is usually\nmodeled as a distribution model with trainable parameters. When this\nparameterization has less expressiveness, it would fail to acquiring the\noptimal policy. A mixture model has capability of a universal approximation,\nbut it with too much redundancy increases the computational cost, which can\nbecome a bottleneck when considering the use of real-time robot control. As\nanother approach, NF, which is with additional parameters for invertible\ntransformation from a simple stochastic model as a base, is expected to exert\nhigh expressiveness and lower computational cost. However, NF cannot compute\nits mean analytically due to complexity of the invertible transformation, and\nit lacks reliability because it retains stochastic behaviors after deployment\nfor robot controller. This paper therefore designs a restricted NF (RNF) that\nachieves an analytic mean by appropriately restricting the invertible\ntransformation. In addition, the expressiveness impaired by this restriction is\nregained using bimodal student-t distribution as its base, so-called Bit-RNF.\nIn RL benchmarks, Bit-RNF policy outperformed the previous models. Finally, a\nreal robot experiment demonstrated the applicability of Bit-RNF policy to real\nworld. The attached video is uploaded on youtube: https://youtu.be/R_GJVZDW9bk"
                },
                "authors": [
                    {
                        "name": "Taisuke Kobayashi"
                    },
                    {
                        "name": "Takumi Aotani"
                    }
                ],
                "author_detail": {
                    "name": "Takumi Aotani"
                },
                "author": "Takumi Aotani",
                "arxiv_doi": "10.1080/01691864.2023.2208634",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/01691864.2023.2208634",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 13 figures",
                "arxiv_journal_ref": "Advanced Robotics, 2023",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12893v1",
                "updated": "2024-12-17T13:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    19,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    19,
                    38,
                    1,
                    352,
                    0
                ],
                "title": "Question: How do Large Language Models perform on the Question Answering\n  tasks? Answer:",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question: How do Large Language Models perform on the Question Answering\n  tasks? Answer:"
                },
                "summary": "Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets."
                },
                "authors": [
                    {
                        "name": "Kevin Fischer"
                    },
                    {
                        "name": "Darren Frst"
                    },
                    {
                        "name": "Sebastian Steindl"
                    },
                    {
                        "name": "Jakob Lindner"
                    },
                    {
                        "name": "Ulrich Schfer"
                    }
                ],
                "author_detail": {
                    "name": "Ulrich Schfer"
                },
                "author": "Ulrich Schfer",
                "arxiv_comment": "Accepted at SAI Computing Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10033v3",
                "updated": "2024-12-17T13:16:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    16,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-09-16T06:51:32Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    51,
                    32,
                    0,
                    260,
                    0
                ],
                "title": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs"
                },
                "summary": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Ye Shang"
                    },
                    {
                        "name": "Guolin Xu"
                    },
                    {
                        "name": "Congqing He"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "arxiv_comment": "Accepted to the 6th International Workshop on Automated Program\n  Repair (APR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12883v1",
                "updated": "2024-12-17T13:07:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    7,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:07:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    7,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "A Comparative Study of Pruning Methods in Transformer-based Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Pruning Methods in Transformer-based Time Series\n  Forecasting"
                },
                "summary": "The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings."
                },
                "authors": [
                    {
                        "name": "Nicholas Kiefer"
                    },
                    {
                        "name": "Arvid Weyrauch"
                    },
                    {
                        "name": "Muhammed z"
                    },
                    {
                        "name": "Achim Streit"
                    },
                    {
                        "name": "Markus Gtz"
                    },
                    {
                        "name": "Charlotte Debus"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Debus"
                },
                "author": "Charlotte Debus",
                "arxiv_comment": "16 pages, 5 figures, submitted to ACM Transactions on Intelligent\n  Systems and Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12881v1",
                "updated": "2024-12-17T13:05:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    5,
                    36,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T13:05:36Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    13,
                    5,
                    36,
                    1,
                    352,
                    0
                ],
                "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\n  Verification and Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\n  Verification and Refinement"
                },
                "summary": "Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods."
                },
                "authors": [
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Tao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhang"
                },
                "author": "Tao Zhang",
                "arxiv_comment": "LLM;RAG;MCTS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12865v1",
                "updated": "2024-12-17T12:49:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    49,
                    14,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:49:14Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    49,
                    14,
                    1,
                    352,
                    0
                ],
                "title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models"
                },
                "summary": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO."
                },
                "authors": [
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Yuzhong Hong"
                    },
                    {
                        "name": "Qiushi Wang"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Yang Song"
                },
                "author": "Yang Song",
                "arxiv_comment": "AAAI2025, 12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15061v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15061v2",
                "updated": "2024-12-17T12:45:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    45,
                    20,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-23T02:24:15Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    2,
                    24,
                    15,
                    4,
                    54,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Domain-specific Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Domain-specific Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise."
                },
                "authors": [
                    {
                        "name": "Jiawei Zheng"
                    },
                    {
                        "name": "Hanghai Hong"
                    },
                    {
                        "name": "Feiyan Liu"
                    },
                    {
                        "name": "Xiaoli Wang"
                    },
                    {
                        "name": "Jingsong Su"
                    },
                    {
                        "name": "Yonggui Liang"
                    },
                    {
                        "name": "Shikai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shikai Wu"
                },
                "author": "Shikai Wu",
                "arxiv_comment": "13 pages, 5 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15061v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15061v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16044v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16044v5",
                "updated": "2024-12-17T12:41:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    41,
                    13,
                    1,
                    352,
                    0
                ],
                "published": "2023-12-26T13:17:06Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    13,
                    17,
                    6,
                    1,
                    360,
                    0
                ],
                "title": "LLMLight: Large Language Models as Traffic Signal Control Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMLight: Large Language Models as Traffic Signal Control Agents"
                },
                "summary": "Traffic Signal Control (TSC) is a crucial component in urban traffic\nmanagement, aiming to optimize road network efficiency and reduce congestion.\nTraditional TSC methods, primarily based on transportation engineering and\nreinforcement learning (RL), often struggle with generalization abilities\nacross varied traffic scenarios and lack interpretability. This paper presents\nLLMLight, a novel framework employing Large Language Models (LLMs) as\ndecision-making agents for TSC. Specifically, the framework begins by\ninstructing the LLM with a knowledgeable prompt detailing real-time traffic\nconditions. Leveraging the advanced generalization capabilities of LLMs,\nLLMLight engages a reasoning and decision-making process akin to human\nintuition for effective traffic control. Moreover, we build LightGPT, a\nspecialized backbone LLM tailored for TSC tasks. By learning nuanced traffic\npatterns and control strategies, LightGPT enhances the LLMLight framework\ncost-effectively. Extensive experiments conducted on ten real-world and\nsynthetic datasets, along with evaluations by fifteen human experts,\ndemonstrate the exceptional effectiveness, generalization ability, and\ninterpretability of LLMLight with LightGPT, outperforming nine baseline methods\nand ten advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Signal Control (TSC) is a crucial component in urban traffic\nmanagement, aiming to optimize road network efficiency and reduce congestion.\nTraditional TSC methods, primarily based on transportation engineering and\nreinforcement learning (RL), often struggle with generalization abilities\nacross varied traffic scenarios and lack interpretability. This paper presents\nLLMLight, a novel framework employing Large Language Models (LLMs) as\ndecision-making agents for TSC. Specifically, the framework begins by\ninstructing the LLM with a knowledgeable prompt detailing real-time traffic\nconditions. Leveraging the advanced generalization capabilities of LLMs,\nLLMLight engages a reasoning and decision-making process akin to human\nintuition for effective traffic control. Moreover, we build LightGPT, a\nspecialized backbone LLM tailored for TSC tasks. By learning nuanced traffic\npatterns and control strategies, LightGPT enhances the LLMLight framework\ncost-effectively. Extensive experiments conducted on ten real-world and\nsynthetic datasets, along with evaluations by fifteen human experts,\ndemonstrate the exceptional effectiveness, generalization ability, and\ninterpretability of LLMLight with LightGPT, outperforming nine baseline methods\nand ten advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Siqi Lai"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16044v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16044v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12858v1",
                "updated": "2024-12-17T12:38:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    38,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:38:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    38,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Speech Command Recognition Leveraging Spiking Neural Network\n  and Curriculum Learning-based Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Speech Command Recognition Leveraging Spiking Neural Network\n  and Curriculum Learning-based Knowledge Distillation"
                },
                "summary": "The intrinsic dynamics and event-driven nature of spiking neural networks\n(SNNs) make them excel in processing temporal information by naturally\nutilizing embedded time sequences as time steps. Recent studies adopting this\napproach have demonstrated SNNs' effectiveness in speech command recognition,\nachieving high performance by employing large time steps for long time\nsequences. However, the large time steps lead to increased deployment burdens\nfor edge computing applications. Thus, it is important to balance high\nperformance and low energy consumption when detecting temporal patterns in edge\ndevices. Our solution comprises two key components. 1). We propose a\nhigh-performance fully spike-driven framework termed SpikeSCR, characterized by\na global-local hybrid structure for efficient representation learning, which\nexhibits long-term learning capabilities with extended time steps. 2). To\nfurther fully embrace low energy consumption, we propose an effective knowledge\ndistillation method based on curriculum learning (KDCL), where valuable\nrepresentations learned from the easy curriculum are progressively transferred\nto the hard curriculum with minor loss, striking a trade-off between power\nefficiency and high performance. We evaluate our method on three benchmark\ndatasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands\n(SSC), and the Google Speech Commands (GSC) V2. Our experimental results\ndemonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods\nacross these three datasets with the same time steps. Furthermore, by executing\nKDCL, we reduce the number of time steps by 60% and decrease energy consumption\nby 54.8% while maintaining comparable performance to recent SOTA results.\nTherefore, this work offers valuable insights for tackling temporal processing\nchallenges with long time sequences in edge neuromorphic computing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intrinsic dynamics and event-driven nature of spiking neural networks\n(SNNs) make them excel in processing temporal information by naturally\nutilizing embedded time sequences as time steps. Recent studies adopting this\napproach have demonstrated SNNs' effectiveness in speech command recognition,\nachieving high performance by employing large time steps for long time\nsequences. However, the large time steps lead to increased deployment burdens\nfor edge computing applications. Thus, it is important to balance high\nperformance and low energy consumption when detecting temporal patterns in edge\ndevices. Our solution comprises two key components. 1). We propose a\nhigh-performance fully spike-driven framework termed SpikeSCR, characterized by\na global-local hybrid structure for efficient representation learning, which\nexhibits long-term learning capabilities with extended time steps. 2). To\nfurther fully embrace low energy consumption, we propose an effective knowledge\ndistillation method based on curriculum learning (KDCL), where valuable\nrepresentations learned from the easy curriculum are progressively transferred\nto the hard curriculum with minor loss, striking a trade-off between power\nefficiency and high performance. We evaluate our method on three benchmark\ndatasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands\n(SSC), and the Google Speech Commands (GSC) V2. Our experimental results\ndemonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods\nacross these three datasets with the same time steps. Furthermore, by executing\nKDCL, we reduce the number of time steps by 60% and decrease energy consumption\nby 54.8% while maintaining comparable performance to recent SOTA results.\nTherefore, this work offers valuable insights for tackling temporal processing\nchallenges with long time sequences in edge neuromorphic computing systems."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Liutao Yu"
                    },
                    {
                        "name": "Liwei Huang"
                    },
                    {
                        "name": "Chenlin Zhou"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Zhenxi Song"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhengyu Ma"
                    },
                    {
                        "name": "Zhiguo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguo Zhang"
                },
                "author": "Zhiguo Zhang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17679v3",
                "updated": "2024-12-17T12:37:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    37,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-11-26T18:44:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    44,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning"
                },
                "summary": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness."
                },
                "authors": [
                    {
                        "name": "Zhu Xu"
                    },
                    {
                        "name": "Zhiqiang Zhao"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Yuchi Liu"
                    },
                    {
                        "name": "Quanwei Shen"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yu Kuang"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Conglin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Conglin Liu"
                },
                "author": "Conglin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12852v1",
                "updated": "2024-12-17T12:26:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    26,
                    14,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:26:14Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    26,
                    14,
                    1,
                    352,
                    0
                ],
                "title": "Selective Shot Learning for Code Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Shot Learning for Code Explanation"
                },
                "summary": "Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task."
                },
                "authors": [
                    {
                        "name": "Paheli Bhattacharya"
                    },
                    {
                        "name": "Rishabh Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Gupta"
                },
                "author": "Rishabh Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12848v1",
                "updated": "2024-12-17T12:22:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    22,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:22:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    22,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models"
                },
                "summary": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09056v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09056v3",
                "updated": "2024-12-17T12:20:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    20,
                    34,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-13T12:43:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    12,
                    43,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive\n  Evaluation Framework with CUDRT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive\n  Evaluation Framework with CUDRT"
                },
                "summary": "The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub."
                },
                "authors": [
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Yanfang Chen"
                    },
                    {
                        "name": "Dinghao Xi"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09056v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09056v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12841v1",
                "updated": "2024-12-17T12:10:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    10,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:10:38Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    10,
                    38,
                    1,
                    352,
                    0
                ],
                "title": "Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs"
                },
                "summary": "Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR."
                },
                "authors": [
                    {
                        "name": "Ruikang Ni"
                    },
                    {
                        "name": "Da Xiao"
                    },
                    {
                        "name": "Qingye Meng"
                    },
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Shihui Zheng"
                    },
                    {
                        "name": "Hongliang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Hongliang Liang"
                },
                "author": "Hongliang Liang",
                "arxiv_comment": "Accepted to the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12839v1",
                "updated": "2024-12-17T12:05:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    5,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T12:05:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    5,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed\n  Instructions In A Multi-Modal Jungle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed\n  Instructions In A Multi-Modal Jungle"
                },
                "summary": "In response to the call for agent-based solutions that leverage the\never-increasing capabilities of the deep models' ecosystem, we introduce Hive\n-- a comprehensive solution for selecting appropriate models and subsequently\nplanning a set of atomic actions to satisfy the end-users' instructions. Hive\noperates over sets of models and, upon receiving natural language instructions\n(i.e. user queries), schedules and executes explainable plans of atomic\nactions. These actions can involve one or more of the available models to\nachieve the overall task, while respecting end-users specific constraints.\nNotably, Hive handles tasks that involve multi-modal inputs and outputs,\nenabling it to handle complex, real-world queries. Our system is capable of\nplanning complex chains of actions while guaranteeing explainability, using an\nLLM-based formal logic backbone empowered by PDDL operations. We introduce the\nMuSE benchmark in order to offer a comprehensive evaluation of the multi-modal\ncapabilities of agent systems. Our findings show that our framework redefines\nthe state-of-the-art for task selection, outperforming other competing systems\nthat plan operations across multiple models while offering transparency\nguarantees while fully adhering to user constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the call for agent-based solutions that leverage the\never-increasing capabilities of the deep models' ecosystem, we introduce Hive\n-- a comprehensive solution for selecting appropriate models and subsequently\nplanning a set of atomic actions to satisfy the end-users' instructions. Hive\noperates over sets of models and, upon receiving natural language instructions\n(i.e. user queries), schedules and executes explainable plans of atomic\nactions. These actions can involve one or more of the available models to\nachieve the overall task, while respecting end-users specific constraints.\nNotably, Hive handles tasks that involve multi-modal inputs and outputs,\nenabling it to handle complex, real-world queries. Our system is capable of\nplanning complex chains of actions while guaranteeing explainability, using an\nLLM-based formal logic backbone empowered by PDDL operations. We introduce the\nMuSE benchmark in order to offer a comprehensive evaluation of the multi-modal\ncapabilities of agent systems. Our findings show that our framework redefines\nthe state-of-the-art for task selection, outperforming other competing systems\nthat plan operations across multiple models while offering transparency\nguarantees while fully adhering to user constraints."
                },
                "authors": [
                    {
                        "name": "Kaustubh Vyas"
                    },
                    {
                        "name": "Damien Graux"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Sbastien Montella"
                    },
                    {
                        "name": "Chenxin Diao"
                    },
                    {
                        "name": "Wendi Zhou"
                    },
                    {
                        "name": "Pavlos Vougiouklis"
                    },
                    {
                        "name": "Ruofei Lai"
                    },
                    {
                        "name": "Yang Ren"
                    },
                    {
                        "name": "Keshuang Li"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11764v2",
                "updated": "2024-12-17T12:04:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    4,
                    49,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T13:31:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    31,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study"
                },
                "summary": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/."
                },
                "authors": [
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yuqing Xie"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yinuo Chen"
                    },
                    {
                        "name": "Shu'ang Yu"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Shilong Ji"
                    },
                    {
                        "name": "Mo Mu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "The first two authors contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12833v1",
                "updated": "2024-12-17T11:54:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:54:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering"
                },
                "summary": "Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data."
                },
                "authors": [
                    {
                        "name": "Zheng Cheng"
                    },
                    {
                        "name": "Rendong Wang"
                    },
                    {
                        "name": "Zhicheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Wang"
                },
                "author": "Zhicheng Wang",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12832v1",
                "updated": "2024-12-17T11:54:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    54,
                    16,
                    1,
                    352,
                    0
                ],
                "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction\n  in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction\n  in the Era of Large Language Models"
                },
                "summary": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations."
                },
                "authors": [
                    {
                        "name": "Jinxiang Xie"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xunjian Yin"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Extended version of a paper to appear in AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10080v2",
                "updated": "2024-12-17T11:45:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    45,
                    55,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T12:15:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    15,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "Africanus IV. The Stimela2 framework: scalable and reproducible\n  workflows, from local to cloud compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Africanus IV. The Stimela2 framework: scalable and reproducible\n  workflows, from local to cloud compute"
                },
                "summary": "Stimela2 is a new-generation framework for developing data reduction\nworkflows. It is designed for radio astronomy data but can be adapted for other\ndata processing applications. Stimela2 aims at the middle ground between ease\nof development, human readability, and enabling robust, scalable and\nreproducible workflows. It represents workflows by linear, concise and\nintuitive YAML-format \"recipes\". Atomic data reduction tasks (binary\nexecutables, Python functions and code, and CASA tasks) are described by\nYAML-format \"cab definitions\" detailing each task's \"schema\" (inputs and\noutputs). Stimela2 provides a rich syntax for chaining tasks together, and\nencourages a high degree of modularity: recipes may be nested into other\nrecipes, and configuration is cleanly separated from recipe logic. Tasks can be\nexecuted natively or in isolated environments using containerization\ntechnologies such as Apptainer. The container images are open-source and\nmaintained through a companion package called cult-cargo. This enables the\ndevelopment of system-agnostic and fully reproducible workflows. Stimela2\nfacilitates the deployment of scalable, distributed workflows by interfacing\nwith the Slurm scheduler and the Kubernetes API. The latter allows workflows to\nbe readily deployed in the cloud. Previous papers in this series used Stimela2\nas the underlying technology to run workflows on the AWS cloud.\n  This paper presents an overview of Stimela2's design, architecture and use in\nthe radio astronomy context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stimela2 is a new-generation framework for developing data reduction\nworkflows. It is designed for radio astronomy data but can be adapted for other\ndata processing applications. Stimela2 aims at the middle ground between ease\nof development, human readability, and enabling robust, scalable and\nreproducible workflows. It represents workflows by linear, concise and\nintuitive YAML-format \"recipes\". Atomic data reduction tasks (binary\nexecutables, Python functions and code, and CASA tasks) are described by\nYAML-format \"cab definitions\" detailing each task's \"schema\" (inputs and\noutputs). Stimela2 provides a rich syntax for chaining tasks together, and\nencourages a high degree of modularity: recipes may be nested into other\nrecipes, and configuration is cleanly separated from recipe logic. Tasks can be\nexecuted natively or in isolated environments using containerization\ntechnologies such as Apptainer. The container images are open-source and\nmaintained through a companion package called cult-cargo. This enables the\ndevelopment of system-agnostic and fully reproducible workflows. Stimela2\nfacilitates the deployment of scalable, distributed workflows by interfacing\nwith the Slurm scheduler and the Kubernetes API. The latter allows workflows to\nbe readily deployed in the cloud. Previous papers in this series used Stimela2\nas the underlying technology to run workflows on the AWS cloud.\n  This paper presents an overview of Stimela2's design, architecture and use in\nthe radio astronomy context."
                },
                "authors": [
                    {
                        "name": "Oleg M. Smirnov"
                    },
                    {
                        "name": "Sphesihle Makhathini"
                    },
                    {
                        "name": "Jonathan S. Kenyon"
                    },
                    {
                        "name": "Hertzog L. Bester"
                    },
                    {
                        "name": "Simon J. Perkins"
                    },
                    {
                        "name": "Athanaseus J. T. Ramaila"
                    },
                    {
                        "name": "Benjamin V. Hugo"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin V. Hugo"
                },
                "author": "Benjamin V. Hugo",
                "arxiv_comment": "26 pages, submitted to Astronomy & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12785v1",
                "updated": "2024-12-17T10:44:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    44,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:44:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    44,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference"
                },
                "summary": "Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales."
                },
                "authors": [
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Chengxing Zhou"
                    },
                    {
                        "name": "Zejun Li"
                    },
                    {
                        "name": "Zhihao Fan"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12775v1",
                "updated": "2024-12-17T10:36:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    36,
                    52,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:36:52Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    36,
                    52,
                    1,
                    352,
                    0
                ],
                "title": "RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the service quality of large\nlanguage models by retrieving relevant documents from credible literature and\nintegrating them into the context of the user query. Recently, the rise of the\ncloud RAG service has made it possible for users to query relevant documents\nconveniently. However, directly sending queries to the cloud brings potential\nprivacy leakage. In this paper, we are the first to formally define the\nprivacy-preserving cloud RAG service to protect the user query and propose\nRemoteRAG as a solution regarding privacy, efficiency, and accuracy. For\nprivacy, we introduce $(n,\\epsilon)$-DistanceDP to characterize privacy leakage\nof the user query and the leakage inferred from relevant documents. For\nefficiency, we limit the search range from the total documents to a small\nnumber of selected documents related to a perturbed embedding generated from\n$(n,\\epsilon)$-DistanceDP, so that computation and communication costs required\nfor privacy protection significantly decrease. For accuracy, we ensure that the\nsmall range includes target documents related to the user query with detailed\ntheoretical analysis. Experimental results also demonstrate that RemoteRAG can\nresist existing embedding inversion attack methods while achieving no loss in\nretrieval under various settings. Moreover, RemoteRAG is efficient, incurring\nonly $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$\nGB with the non-optimized privacy-preserving scheme) when retrieving from a\ntotal of $10^6$ documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the service quality of large\nlanguage models by retrieving relevant documents from credible literature and\nintegrating them into the context of the user query. Recently, the rise of the\ncloud RAG service has made it possible for users to query relevant documents\nconveniently. However, directly sending queries to the cloud brings potential\nprivacy leakage. In this paper, we are the first to formally define the\nprivacy-preserving cloud RAG service to protect the user query and propose\nRemoteRAG as a solution regarding privacy, efficiency, and accuracy. For\nprivacy, we introduce $(n,\\epsilon)$-DistanceDP to characterize privacy leakage\nof the user query and the leakage inferred from relevant documents. For\nefficiency, we limit the search range from the total documents to a small\nnumber of selected documents related to a perturbed embedding generated from\n$(n,\\epsilon)$-DistanceDP, so that computation and communication costs required\nfor privacy protection significantly decrease. For accuracy, we ensure that the\nsmall range includes target documents related to the user query with detailed\ntheoretical analysis. Experimental results also demonstrate that RemoteRAG can\nresist existing embedding inversion attack methods while achieving no loss in\nretrieval under various settings. Moreover, RemoteRAG is efficient, incurring\nonly $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$\nGB with the non-optimized privacy-preserving scheme) when retrieving from a\ntotal of $10^6$ documents."
                },
                "authors": [
                    {
                        "name": "Yihang Cheng"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Junyang Wang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Yunhao Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yunhao Yao"
                },
                "author": "Yunhao Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10487v2",
                "updated": "2024-12-17T10:35:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    35,
                    33,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-13T15:18:39Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    18,
                    39,
                    4,
                    348,
                    0
                ],
                "title": "HyperGraphOS: A Modern Meta-Operating System for the Scientific and\n  Engineering Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperGraphOS: A Modern Meta-Operating System for the Scientific and\n  Engineering Domains"
                },
                "summary": "This paper presents HyperGraphOS, a significant innovation in the domain of\noperating systems, specifically designed to address the needs of scientific and\nengineering domains. This platform aims to combine model-based engineering,\ngraph modeling, data containers, and documents, along with tools for handling\ncomputational elements. HyperGraphOS functions as an Operating System offering\nto users an infinite workspace for creating and managing complex models\nrepresented as graphs with customizable semantics. By leveraging a web-based\narchitecture, it requires only a modern web browser for access, allowing\norganization of knowledge, documents, and content into models represented in a\nnetwork of workspaces. Elements of the workspace are defined in terms of\ndomain-specific languages (DSLs). These DSLs are pivotal for navigating\nworkspaces, generating code, triggering AI components, and organizing\ninformation and processes. The models' dual nature as both visual drawings and\ndata structures allows dynamic modifications and inspections both interactively\nas well as programaticaly. We evaluated HyperGraphOS's efficiency and\napplicability across a large set of diverse domains, including the design and\ndevelopment of a virtual Avatar dialog system, a robotic task planner based on\nlarge language models (LLMs), a new meta-model for feature-based code\ndevelopment and many others. Our findings show that HyperGraphOS offers\nsubstantial benefits in the interaction with a computer as information system,\nas platoform for experiments and data analysis, as streamlined engineering\nprocesses, demonstrating enhanced flexibility in managing data, computation and\ndocuments, showing an innovative approaches to persistent desktop environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents HyperGraphOS, a significant innovation in the domain of\noperating systems, specifically designed to address the needs of scientific and\nengineering domains. This platform aims to combine model-based engineering,\ngraph modeling, data containers, and documents, along with tools for handling\ncomputational elements. HyperGraphOS functions as an Operating System offering\nto users an infinite workspace for creating and managing complex models\nrepresented as graphs with customizable semantics. By leveraging a web-based\narchitecture, it requires only a modern web browser for access, allowing\norganization of knowledge, documents, and content into models represented in a\nnetwork of workspaces. Elements of the workspace are defined in terms of\ndomain-specific languages (DSLs). These DSLs are pivotal for navigating\nworkspaces, generating code, triggering AI components, and organizing\ninformation and processes. The models' dual nature as both visual drawings and\ndata structures allows dynamic modifications and inspections both interactively\nas well as programaticaly. We evaluated HyperGraphOS's efficiency and\napplicability across a large set of diverse domains, including the design and\ndevelopment of a virtual Avatar dialog system, a robotic task planner based on\nlarge language models (LLMs), a new meta-model for feature-based code\ndevelopment and many others. Our findings show that HyperGraphOS offers\nsubstantial benefits in the interaction with a computer as information system,\nas platoform for experiments and data analysis, as streamlined engineering\nprocesses, demonstrating enhanced flexibility in managing data, computation and\ndocuments, showing an innovative approaches to persistent desktop environments."
                },
                "authors": [
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Frank Joublin"
                    }
                ],
                "author_detail": {
                    "name": "Frank Joublin"
                },
                "author": "Frank Joublin",
                "arxiv_comment": "29 Pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12770v1",
                "updated": "2024-12-17T10:33:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    13,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:33:13Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    33,
                    13,
                    1,
                    352,
                    0
                ],
                "title": "A Survey on Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sequential Recommendation"
                },
                "summary": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from most conventional recommendation problems, sequential\nrecommendation focuses on learning users' preferences by exploiting the\ninternal order and dependency among the interacted items, which has received\nsignificant attention from both researchers and practitioners. In recent years,\nwe have witnessed great progress and achievements in this field, necessitating\na new survey. In this survey, we study the SR problem from a new perspective\n(i.e., the construction of an item's properties), and summarize the most recent\ntechniques used in sequential recommendation such as pure ID-based SR, SR with\nside information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR\nand data-augmented SR. Moreover, we introduce some frontier research topics in\nsequential recommendation, e.g., open-domain SR, data-centric SR, could-edge\ncollaborative SR, continuous SR, SR for good, and explainable SR. We believe\nthat our survey could be served as a valuable roadmap for readers in this\nfield."
                },
                "authors": [
                    {
                        "name": "Liwei Pan"
                    },
                    {
                        "name": "Weike Pan"
                    },
                    {
                        "name": "Meiyan Wei"
                    },
                    {
                        "name": "Hongzhi Yin"
                    },
                    {
                        "name": "Zhong Ming"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Ming"
                },
                "author": "Zhong Ming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12767v1",
                "updated": "2024-12-17T10:31:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    31,
                    21,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T10:31:21Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    31,
                    21,
                    1,
                    352,
                    0
                ],
                "title": "A Survey of Calibration Process for Black-Box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Calibration Process for Black-Box LLMs"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs"
                },
                "authors": [
                    {
                        "name": "Liangru Xie"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jingying Zeng"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Yan Han"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12732v1",
                "updated": "2024-12-17T09:55:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    55,
                    2,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:55:02Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    55,
                    2,
                    1,
                    352,
                    0
                ],
                "title": "Using LLM-Generated Draft Replies to Support Human Experts in Responding\n  to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of\n  Industrial AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLM-Generated Draft Replies to Support Human Experts in Responding\n  to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of\n  Industrial AI"
                },
                "summary": "The maritime industry requires effective communication among diverse\nstakeholders to address complex, safety-critical challenges. Industrial AI,\nincluding Large Language Models (LLMs), has the potential to augment human\nexperts' workflows in this specialized domain. Our case study investigated the\nutility of LLMs in drafting replies to stakeholder inquiries and supporting\ncase handlers. We conducted a preliminary study (observations and interviews),\na survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding\nSimilarity). We discover that while LLM drafts can streamline workflows, they\noften require significant modifications to meet the specific demands of\nmaritime communications. Though LLMs are not yet mature enough for\nsafety-critical applications without human oversight, they can serve as\nvaluable augmentative tools. Final decision-making thus must remain with human\nexperts. However, by leveraging the strengths of both humans and LLMs,\nfostering human-AI collaboration, industries can increase efficiency while\nmaintaining high standards of quality and precision tailored to each case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maritime industry requires effective communication among diverse\nstakeholders to address complex, safety-critical challenges. Industrial AI,\nincluding Large Language Models (LLMs), has the potential to augment human\nexperts' workflows in this specialized domain. Our case study investigated the\nutility of LLMs in drafting replies to stakeholder inquiries and supporting\ncase handlers. We conducted a preliminary study (observations and interviews),\na survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding\nSimilarity). We discover that while LLM drafts can streamline workflows, they\noften require significant modifications to meet the specific demands of\nmaritime communications. Though LLMs are not yet mature enough for\nsafety-critical applications without human oversight, they can serve as\nvaluable augmentative tools. Final decision-making thus must remain with human\nexperts. However, by leveraging the strengths of both humans and LLMs,\nfostering human-AI collaboration, industries can increase efficiency while\nmaintaining high standards of quality and precision tailored to each case."
                },
                "authors": [
                    {
                        "name": "Tita Alissa Bach"
                    },
                    {
                        "name": "Aleksandar Babic"
                    },
                    {
                        "name": "Narae Park"
                    },
                    {
                        "name": "Tor Sporsem"
                    },
                    {
                        "name": "Rasmus Ulfsnes"
                    },
                    {
                        "name": "Henrik Smith-Meyer"
                    },
                    {
                        "name": "Torkel Skeie"
                    }
                ],
                "author_detail": {
                    "name": "Torkel Skeie"
                },
                "arxiv_affiliation": "DNV, Hvik, Norway",
                "author": "Torkel Skeie",
                "arxiv_comment": "These authors share the first authorship: Tita Alissa Bach (1),\n  Aleksandar Babic (1), Narae Park (1)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18264v2",
                "updated": "2024-12-17T09:53:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    53,
                    41,
                    1,
                    352,
                    0
                ],
                "published": "2024-02-28T11:51:56Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    11,
                    51,
                    56,
                    2,
                    59,
                    0
                ],
                "title": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation under\n  Real-World Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WIKIGENBENCH: Exploring Full-length Wikipedia Generation under\n  Real-World Scenario"
                },
                "summary": "It presents significant challenges to generate comprehensive and accurate\nWikipedia articles for newly emerging events under a real-world scenario.\nExisting attempts fall short either by focusing only on short snippets or by\nusing metrics that are insufficient to evaluate real-world scenarios. In this\npaper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,\ndesigned to align with real-world scenarios in both generation and evaluation.\nFor generation, we explore a real-world scenario where structured, full-length\nWikipedia articles with citations are generated for new events using input\ndocuments from web sources. For evaluation, we integrate systematic metrics and\nLLM-based metrics to assess the verifiability, organization, and other aspects\naligned with real-world scenarios. Based on this benchmark, we conduct\nextensive experiments using various models within three commonly used\nframeworks: direct RAG, hierarchical structure-based RAG, and RAG with a\nfine-tuned generation model. Experimental results show that hierarchical-based\nmethods can generate more comprehensive content, while fine-tuned methods\nachieve better verifiability. However, even the best methods still show a\nsignificant gap compared to existing Wikipedia content, indicating that further\nresearch is necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It presents significant challenges to generate comprehensive and accurate\nWikipedia articles for newly emerging events under a real-world scenario.\nExisting attempts fall short either by focusing only on short snippets or by\nusing metrics that are insufficient to evaluate real-world scenarios. In this\npaper, we construct WIKIGENBENCH, a new benchmark consisting of 1,320 entries,\ndesigned to align with real-world scenarios in both generation and evaluation.\nFor generation, we explore a real-world scenario where structured, full-length\nWikipedia articles with citations are generated for new events using input\ndocuments from web sources. For evaluation, we integrate systematic metrics and\nLLM-based metrics to assess the verifiability, organization, and other aspects\naligned with real-world scenarios. Based on this benchmark, we conduct\nextensive experiments using various models within three commonly used\nframeworks: direct RAG, hierarchical structure-based RAG, and RAG with a\nfine-tuned generation model. Experimental results show that hierarchical-based\nmethods can generate more comprehensive content, while fine-tuned methods\nachieve better verifiability. However, even the best methods still show a\nsignificant gap compared to existing Wikipedia content, indicating that further\nresearch is necessary."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Eugene J. Yu"
                    },
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Chenhao Xiong"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Han Qian"
                    },
                    {
                        "name": "Mingbo Song"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "COLING 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12718v1",
                "updated": "2024-12-17T09:33:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    33,
                    6,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:33:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    33,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding"
                },
                "summary": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin."
                },
                "authors": [
                    {
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Multimedia",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11768v2",
                "updated": "2024-12-17T09:30:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    30,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T13:41:37Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    41,
                    37,
                    0,
                    351,
                    0
                ],
                "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No More Adam: Learning Rate Scaling at Initialization is All You Need"
                },
                "summary": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings."
                },
                "authors": [
                    {
                        "name": "Minghao Xu"
                    },
                    {
                        "name": "Lichuan Xiang"
                    },
                    {
                        "name": "Xu Cai"
                    },
                    {
                        "name": "Hongkai Wen"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Wen"
                },
                "author": "Hongkai Wen",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12710v1",
                "updated": "2024-12-17T09:25:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    25,
                    44,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:25:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    25,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "Enhancing Naturalness in LLM-Generated Utterances through Disfluency\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Naturalness in LLM-Generated Utterances through Disfluency\n  Insertion"
                },
                "summary": "Disfluencies are a natural feature of spontaneous human speech but are\ntypically absent from the outputs of Large Language Models (LLMs). This absence\ncan diminish the perceived naturalness of synthesized speech, which is an\nimportant criteria when building conversational agents that aim to mimick human\nbehaviours. We show how the insertion of disfluencies can alleviate this\nshortcoming. The proposed approach involves (1) fine-tuning an LLM with\nLow-Rank Adaptation (LoRA) to incorporate various types of disfluencies into\nLLM-generated utterances and (2) synthesizing those utterances using a\ntext-to-speech model that supports the generation of speech phenomena such as\ndisfluencies. We evaluated the quality of the generated speech across two\nmetrics: intelligibility and perceived spontaneity. We demonstrate through a\nuser study that the insertion of disfluencies significantly increase the\nperceived spontaneity of the generated speech. This increase came, however,\nalong with a slight reduction in intelligibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disfluencies are a natural feature of spontaneous human speech but are\ntypically absent from the outputs of Large Language Models (LLMs). This absence\ncan diminish the perceived naturalness of synthesized speech, which is an\nimportant criteria when building conversational agents that aim to mimick human\nbehaviours. We show how the insertion of disfluencies can alleviate this\nshortcoming. The proposed approach involves (1) fine-tuning an LLM with\nLow-Rank Adaptation (LoRA) to incorporate various types of disfluencies into\nLLM-generated utterances and (2) synthesizing those utterances using a\ntext-to-speech model that supports the generation of speech phenomena such as\ndisfluencies. We evaluated the quality of the generated speech across two\nmetrics: intelligibility and perceived spontaneity. We demonstrate through a\nuser study that the insertion of disfluencies significantly increase the\nperceived spontaneity of the generated speech. This increase came, however,\nalong with a slight reduction in intelligibility."
                },
                "authors": [
                    {
                        "name": "Syed Zohaib Hassan"
                    },
                    {
                        "name": "Pierre Lison"
                    },
                    {
                        "name": "Pl Halvorsen"
                    }
                ],
                "author_detail": {
                    "name": "Pl Halvorsen"
                },
                "author": "Pl Halvorsen",
                "arxiv_comment": "4 pages short paper, references and appendix are additional",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12701v1",
                "updated": "2024-12-17T09:16:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    54,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:16:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Trigger$^3$: Refining Query Correction via Adaptive Model Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trigger$^3$: Refining Query Correction via Adaptive Model Selector"
                },
                "summary": "In search scenarios, user experience can be hindered by erroneous queries due\nto typos, voice errors, or knowledge gaps. Therefore, query correction is\ncrucial for search engines. Current correction models, usually small models\ntrained on specific data, often struggle with queries beyond their training\nscope or those requiring contextual understanding. While the advent of Large\nLanguage Models (LLMs) offers a potential solution, they are still limited by\ntheir pre-training data and inference cost, particularly for complex queries,\nmaking them not always effective for query correction. To tackle these, we\npropose Trigger$^3$, a large-small model collaboration framework that\nintegrates the traditional correction model and LLM for query correction,\ncapable of adaptively choosing the appropriate correction method based on the\nquery and the correction results from the traditional correction model and LLM.\nTrigger$^3$ first employs a correction trigger to filter out correct queries.\nIncorrect queries are then corrected by the traditional correction model. If\nthis fails, an LLM trigger is activated to call the LLM for correction.\nFinally, for queries that no model can correct, a fallback trigger decides to\nreturn the original query. Extensive experiments demonstrate Trigger$^3$\noutperforms correction baselines while maintaining efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In search scenarios, user experience can be hindered by erroneous queries due\nto typos, voice errors, or knowledge gaps. Therefore, query correction is\ncrucial for search engines. Current correction models, usually small models\ntrained on specific data, often struggle with queries beyond their training\nscope or those requiring contextual understanding. While the advent of Large\nLanguage Models (LLMs) offers a potential solution, they are still limited by\ntheir pre-training data and inference cost, particularly for complex queries,\nmaking them not always effective for query correction. To tackle these, we\npropose Trigger$^3$, a large-small model collaboration framework that\nintegrates the traditional correction model and LLM for query correction,\ncapable of adaptively choosing the appropriate correction method based on the\nquery and the correction results from the traditional correction model and LLM.\nTrigger$^3$ first employs a correction trigger to filter out correct queries.\nIncorrect queries are then corrected by the traditional correction model. If\nthis fails, an LLM trigger is activated to call the LLM for correction.\nFinally, for queries that no model can correct, a fallback trigger decides to\nreturn the original query. Extensive experiments demonstrate Trigger$^3$\noutperforms correction baselines while maintaining efficiency."
                },
                "authors": [
                    {
                        "name": "Kepu Zhang"
                    },
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12698v1",
                "updated": "2024-12-17T09:16:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    28,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:16:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling"
                },
                "summary": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations."
                },
                "authors": [
                    {
                        "name": "Allen Lei"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jianfei Yang"
                    },
                    {
                        "name": "Shenghai Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shenghai Yuan"
                },
                "author": "Shenghai Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12688v1",
                "updated": "2024-12-17T09:08:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    52,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:08:52Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    52,
                    1,
                    352,
                    0
                ],
                "title": "UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation\n  Benchmarks with Unified Entrez Gene Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation\n  Benchmarks with Unified Entrez Gene Identifiers"
                },
                "summary": "Gene studies are crucial for fields such as protein structure prediction,\ndrug discovery, and cancer genomics, yet they face challenges in fully\nutilizing the vast and diverse information available. Gene studies require\nclean, factual datasets to ensure reliable results. Ontology graphs, neatly\norganized domain terminology graphs, provide ideal sources for domain facts.\nHowever, available gene ontology annotations are currently distributed across\nvarious databases without unified identifiers for genes and gene products. To\naddress these challenges, we introduce Unified Entrez Gene Identifier Dataset\nand Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale\npublic Gene Ontology Annotations (GOA) from various databases using unique gene\nidentifiers. UniEntrezDB includes a pre-training dataset and four downstream\ntasks designed to comprehensively evaluate gene embedding performance from\ngene, protein, and cell levels, ultimately enhancing the reliability and\napplicability of LLMs in gene research and other professional settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene studies are crucial for fields such as protein structure prediction,\ndrug discovery, and cancer genomics, yet they face challenges in fully\nutilizing the vast and diverse information available. Gene studies require\nclean, factual datasets to ensure reliable results. Ontology graphs, neatly\norganized domain terminology graphs, provide ideal sources for domain facts.\nHowever, available gene ontology annotations are currently distributed across\nvarious databases without unified identifiers for genes and gene products. To\naddress these challenges, we introduce Unified Entrez Gene Identifier Dataset\nand Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale\npublic Gene Ontology Annotations (GOA) from various databases using unique gene\nidentifiers. UniEntrezDB includes a pre-training dataset and four downstream\ntasks designed to comprehensively evaluate gene embedding performance from\ngene, protein, and cell levels, ultimately enhancing the reliability and\napplicability of LLMs in gene research and other professional settings."
                },
                "authors": [
                    {
                        "name": "Yuwei Miao"
                    },
                    {
                        "name": "Yuzhi Guo"
                    },
                    {
                        "name": "Hehuan Ma"
                    },
                    {
                        "name": "Jingquan Yan"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Weizhi An"
                    },
                    {
                        "name": "Jean Gao"
                    },
                    {
                        "name": "Junzhou Huang"
                    }
                ],
                "author_detail": {
                    "name": "Junzhou Huang"
                },
                "author": "Junzhou Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12687v1",
                "updated": "2024-12-17T09:08:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    18,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models"
                },
                "summary": "This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware HLM (U-HLM), wherein the SLM\nlocally measures its output uncertainty, and skips both uplink transmissions\nand LLM operations for tokens that are likely to be accepted. This\nopportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputation by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware HLM (U-HLM), wherein the SLM\nlocally measures its output uncertainty, and skips both uplink transmissions\nand LLM operations for tokens that are likely to be accepted. This\nopportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputation by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping."
                },
                "authors": [
                    {
                        "name": "Seungeun Oh"
                    },
                    {
                        "name": "Jinhyuk Kim"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Seung-Woo Ko"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    },
                    {
                        "name": "Seong-Lyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Lyun Kim"
                },
                "author": "Seong-Lyun Kim",
                "arxiv_comment": "6 pages, 6 figures; This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12686v1",
                "updated": "2024-12-17T09:05:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    5,
                    30,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    5,
                    30,
                    1,
                    352,
                    0
                ],
                "title": "XTransplant: A Probe into the Upper Bound Performance of Multilingual\n  Capability and Culture Adaptability in LLMs via Mutual Cross-lingual\n  Feed-forward Transplantation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XTransplant: A Probe into the Upper Bound Performance of Multilingual\n  Capability and Culture Adaptability in LLMs via Mutual Cross-lingual\n  Feed-forward Transplantation"
                },
                "summary": "Current large language models (LLMs) often exhibit imbalances in multilingual\ncapabilities and cultural adaptability, largely due to their English-centric\npretraining data. To address this imbalance, we propose a probing method named\nXTransplant that explores cross-lingual latent interactions via cross-lingual\nfeed-forward transplantation during inference stage, with the hope of enabling\nthe model to leverage the strengths of both English and non-English languages.\nThrough extensive pilot experiments, we empirically prove that both the\nmultilingual capabilities and cultural adaptability of LLMs hold the potential\nto be significantly improved by XTransplant, respectively from En -> non-En and\nnon-En -> En, highlighting the underutilization of current LLMs' multilingual\npotential. And the patterns observed in these pilot experiments further\nmotivate an offline scaling inference strategy, which demonstrates consistent\nperformance improvements in multilingual and culture-aware tasks, sometimes\neven surpassing multilingual supervised fine-tuning. And we do hope our further\nanalysis and discussion could help gain deeper insights into XTransplant\nmechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) often exhibit imbalances in multilingual\ncapabilities and cultural adaptability, largely due to their English-centric\npretraining data. To address this imbalance, we propose a probing method named\nXTransplant that explores cross-lingual latent interactions via cross-lingual\nfeed-forward transplantation during inference stage, with the hope of enabling\nthe model to leverage the strengths of both English and non-English languages.\nThrough extensive pilot experiments, we empirically prove that both the\nmultilingual capabilities and cultural adaptability of LLMs hold the potential\nto be significantly improved by XTransplant, respectively from En -> non-En and\nnon-En -> En, highlighting the underutilization of current LLMs' multilingual\npotential. And the patterns observed in these pilot experiments further\nmotivate an offline scaling inference strategy, which demonstrates consistent\nperformance improvements in multilingual and culture-aware tasks, sometimes\neven surpassing multilingual supervised fine-tuning. And we do hope our further\nanalysis and discussion could help gain deeper insights into XTransplant\nmechanism."
                },
                "authors": [
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Yichong Huang"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Zhirui Zhang"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Xiaohui Yan"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12679v1",
                "updated": "2024-12-17T08:47:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    47,
                    41,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T08:47:41Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    47,
                    41,
                    1,
                    352,
                    0
                ],
                "title": "Detecting Document-level Paraphrased Machine Generated Content:\n  Mimicking Human Writing Style and Involving Discourse Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Document-level Paraphrased Machine Generated Content:\n  Mimicking Human Writing Style and Involving Discourse Features"
                },
                "summary": "The availability of high-quality APIs for Large Language Models (LLMs) has\nfacilitated the widespread creation of Machine-Generated Content (MGC), posing\nchallenges such as academic plagiarism and the spread of misinformation.\nExisting MGC detectors often focus solely on surface-level information,\noverlooking implicit and structural features. This makes them susceptible to\ndeception by surface-level sentence patterns, particularly for longer texts and\nin texts that have been subsequently paraphrased.\n  To overcome these challenges, we introduce novel methodologies and datasets.\nBesides the publicly available dataset Plagbench, we developed the paraphrased\nLong-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts\n(paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by\nextending artifacts from their original versions. To address the challenge of\ndetecting highly similar paraphrased texts, we propose MhBART, an\nencoder-decoder model designed to emulate human writing style while\nincorporating a novel difference score mechanism. This model outperforms strong\nclassifier baselines and identifies deceptive sentence patterns. To better\ncapture the structure of longer texts at document level, we propose\nDTransformer, a model that integrates discourse analysis through PDTB\npreprocessing to encode structural features. It results in substantial\nperformance gains across both datasets -- 15.5\\% absolute improvement on\nparaLFQA, 4\\% absolute improvement on paraWP, and 1.5\\% absolute improvement on\nM4 compared to SOTA approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of high-quality APIs for Large Language Models (LLMs) has\nfacilitated the widespread creation of Machine-Generated Content (MGC), posing\nchallenges such as academic plagiarism and the spread of misinformation.\nExisting MGC detectors often focus solely on surface-level information,\noverlooking implicit and structural features. This makes them susceptible to\ndeception by surface-level sentence patterns, particularly for longer texts and\nin texts that have been subsequently paraphrased.\n  To overcome these challenges, we introduce novel methodologies and datasets.\nBesides the publicly available dataset Plagbench, we developed the paraphrased\nLong-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts\n(paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by\nextending artifacts from their original versions. To address the challenge of\ndetecting highly similar paraphrased texts, we propose MhBART, an\nencoder-decoder model designed to emulate human writing style while\nincorporating a novel difference score mechanism. This model outperforms strong\nclassifier baselines and identifies deceptive sentence patterns. To better\ncapture the structure of longer texts at document level, we propose\nDTransformer, a model that integrates discourse analysis through PDTB\npreprocessing to encode structural features. It results in substantial\nperformance gains across both datasets -- 15.5\\% absolute improvement on\nparaLFQA, 4\\% absolute improvement on paraWP, and 1.5\\% absolute improvement on\nM4 compared to SOTA approaches."
                },
                "authors": [
                    {
                        "name": "Yupei Li"
                    },
                    {
                        "name": "Manuel Milling"
                    },
                    {
                        "name": "Lucia Specia"
                    },
                    {
                        "name": "Bjrn W. Schuller"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn W. Schuller"
                },
                "author": "Bjrn W. Schuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12674v1",
                "updated": "2024-12-17T08:44:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    44,
                    0,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T08:44:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    44,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "Train More Parameters But Mind Their Placement: Insights into Language\n  Adaptation with PEFT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train More Parameters But Mind Their Placement: Insights into Language\n  Adaptation with PEFT"
                },
                "summary": "Smaller LLMs still face significant challenges even in medium-resourced\nlanguages, particularly when it comes to language-specific knowledge -- a\nproblem not easily resolved with machine-translated data. In this case study on\nIcelandic, we aim to enhance the generation performance of an LLM by\nspecialising it using unstructured text corpora. A key focus is on preventing\ninterference with the models' capabilities of handling longer context during\nthis adaptation. Through ablation studies using various parameter-efficient\nfine-tuning (PEFT) methods and setups, we find that increasing the number of\ntrainable parameters leads to better and more robust language adaptation. LoRAs\nplaced in the feed-forward layers and bottleneck adapters show promising\nresults with sufficient parameters, while prefix tuning and (IA)3 are not\nsuitable. Although improvements are consistent in 0-shot summarisation, some\nadapted models struggle with longer context lengths, an issue that can be\nmitigated by adapting only the final layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smaller LLMs still face significant challenges even in medium-resourced\nlanguages, particularly when it comes to language-specific knowledge -- a\nproblem not easily resolved with machine-translated data. In this case study on\nIcelandic, we aim to enhance the generation performance of an LLM by\nspecialising it using unstructured text corpora. A key focus is on preventing\ninterference with the models' capabilities of handling longer context during\nthis adaptation. Through ablation studies using various parameter-efficient\nfine-tuning (PEFT) methods and setups, we find that increasing the number of\ntrainable parameters leads to better and more robust language adaptation. LoRAs\nplaced in the feed-forward layers and bottleneck adapters show promising\nresults with sufficient parameters, while prefix tuning and (IA)3 are not\nsuitable. Although improvements are consistent in 0-shot summarisation, some\nadapted models struggle with longer context lengths, an issue that can be\nmitigated by adapting only the final layers."
                },
                "authors": [
                    {
                        "name": "Jenny Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Jenny Kunz"
                },
                "author": "Jenny Kunz",
                "arxiv_comment": "To appear at NoDaLiDa 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04662v2",
                "updated": "2024-12-17T08:37:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    37,
                    34,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-06T02:13:15Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    13,
                    15,
                    1,
                    219,
                    0
                ],
                "title": "Citekit: A Modular Toolkit for Large Language Model Citation Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citekit: A Modular Toolkit for Large Language Model Citation Generation"
                },
                "summary": "Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit."
                },
                "authors": [
                    {
                        "name": "Jiajun Shen"
                    },
                    {
                        "name": "Tong Zhou"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "7 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12653v1",
                "updated": "2024-12-17T08:20:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    20,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T08:20:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    20,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "Predicting User Behavior in Smart Spaces with LLM-Enhanced Logs and\n  Personalized Prompts (Data Description)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting User Behavior in Smart Spaces with LLM-Enhanced Logs and\n  Personalized Prompts (Data Description)"
                },
                "summary": "Enhancing the intelligence of smart systems, such as smart home, and smart\nvehicle, and smart grids, critically depends on developing sophisticated\nplanning capabilities that can anticipate the next desired function based on\nhistorical interactions. While existing methods view user behaviors as\nsequential data and apply models like RNNs and Transformers to predict future\nactions, they often fail to incorporate domain knowledge and capture\npersonalized user preferences. In this paper, we propose a novel approach that\nincorporates LLM-enhanced logs and personalized prompts. Our approach first\nconstructs a graph that captures individual behavior preferences derived from\ntheir interaction histories. This graph effectively transforms into a soft\ncontinuous prompt that precedes the sequence of user behaviors. Then our\napproach leverages the vast general knowledge and robust reasoning capabilities\nof a pretrained LLM to enrich the oversimplified and incomplete log records. By\nenhancing these logs semantically, our approach better understands the user's\nactions and intentions, especially for those rare events in the dataset. We\nevaluate the method across four real-world datasets from both smart vehicle and\nsmart home settings. The findings validate the effectiveness of our\nLLM-enhanced description and personalized prompt, shedding light on potential\nways to advance the intelligence of smart space. Note: While this manuscript\nprovides description of the data, we are \\textbf{not} permitted to make these\ndatasets publicly available due to restrictions imposed by the data provider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the intelligence of smart systems, such as smart home, and smart\nvehicle, and smart grids, critically depends on developing sophisticated\nplanning capabilities that can anticipate the next desired function based on\nhistorical interactions. While existing methods view user behaviors as\nsequential data and apply models like RNNs and Transformers to predict future\nactions, they often fail to incorporate domain knowledge and capture\npersonalized user preferences. In this paper, we propose a novel approach that\nincorporates LLM-enhanced logs and personalized prompts. Our approach first\nconstructs a graph that captures individual behavior preferences derived from\ntheir interaction histories. This graph effectively transforms into a soft\ncontinuous prompt that precedes the sequence of user behaviors. Then our\napproach leverages the vast general knowledge and robust reasoning capabilities\nof a pretrained LLM to enrich the oversimplified and incomplete log records. By\nenhancing these logs semantically, our approach better understands the user's\nactions and intentions, especially for those rare events in the dataset. We\nevaluate the method across four real-world datasets from both smart vehicle and\nsmart home settings. The findings validate the effectiveness of our\nLLM-enhanced description and personalized prompt, shedding light on potential\nways to advance the intelligence of smart space. Note: While this manuscript\nprovides description of the data, we are \\textbf{not} permitted to make these\ndatasets publicly available due to restrictions imposed by the data provider."
                },
                "authors": [
                    {
                        "name": "Yunpeng Song"
                    }
                ],
                "author_detail": {
                    "name": "Yunpeng Song"
                },
                "author": "Yunpeng Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12643v1",
                "updated": "2024-12-17T08:07:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    7,
                    16,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T08:07:16Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    7,
                    16,
                    1,
                    352,
                    0
                ],
                "title": "LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering"
                },
                "summary": "Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm, which may hinder the advancement of the LLM-based KGQA model. To deal\nwith the issue, we propose a novel LLM-based Discriminative Reasoning (LDR)\nmethod to explicitly model the subgraph retrieval and answer inference process.\nBy adopting discriminative strategies, the proposed LDR method not only\nenhances the capability of LLMs to retrieve question-related subgraphs but also\nalleviates the issue of ungrounded reasoning brought by the generative paradigm\nof LLMs. Experimental results show that the proposed approach outperforms\nmultiple strong comparison methods, along with achieving state-of-the-art\nperformance on two widely used WebQSP and CWQ benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm, which may hinder the advancement of the LLM-based KGQA model. To deal\nwith the issue, we propose a novel LLM-based Discriminative Reasoning (LDR)\nmethod to explicitly model the subgraph retrieval and answer inference process.\nBy adopting discriminative strategies, the proposed LDR method not only\nenhances the capability of LLMs to retrieve question-related subgraphs but also\nalleviates the issue of ungrounded reasoning brought by the generative paradigm\nof LLMs. Experimental results show that the proposed approach outperforms\nmultiple strong comparison methods, along with achieving state-of-the-art\nperformance on two widely used WebQSP and CWQ benchmarks."
                },
                "authors": [
                    {
                        "name": "Mufan Xu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v4",
                "updated": "2024-12-17T08:03:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    3,
                    10,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12639v1",
                "updated": "2024-12-17T08:02:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    2,
                    8,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T08:02:08Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    2,
                    8,
                    1,
                    352,
                    0
                ],
                "title": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree"
                },
                "summary": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers."
                },
                "authors": [
                    {
                        "name": "Xiangxiang Gao"
                    },
                    {
                        "name": "Weisheng Xie"
                    },
                    {
                        "name": "Yiwei Xiang"
                    },
                    {
                        "name": "Feng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ji"
                },
                "author": "Feng Ji",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12632v1",
                "updated": "2024-12-17T07:49:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    49,
                    49,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T07:49:49Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    49,
                    49,
                    1,
                    352,
                    0
                ],
                "title": "What External Knowledge is Preferred by LLMs? Characterizing and\n  Exploring Chain of Evidence in Imperfect Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What External Knowledge is Preferred by LLMs? Characterizing and\n  Exploring Chain of Evidence in Imperfect Context"
                },
                "summary": "Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nexplore LLMs' preferences from their effectiveness, faithfulness and\nrobustness, as well as CoE's usability in a naive Retrieval-Augmented\nGeneration (RAG) case. The evaluation on five LLMs reveals that CoE enhances\nLLMs through more accurate generation, stronger answer faithfulness, better\nrobustness against knowledge conflict, and improved performance in a popular\nRAG case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nexplore LLMs' preferences from their effectiveness, faithfulness and\nrobustness, as well as CoE's usability in a naive Retrieval-Augmented\nGeneration (RAG) case. The evaluation on five LLMs reveals that CoE enhances\nLLMs through more accurate generation, stronger answer faithfulness, better\nrobustness against knowledge conflict, and improved performance in a popular\nRAG case."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12621v1",
                "updated": "2024-12-17T07:33:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    33,
                    41,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T07:33:41Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    33,
                    41,
                    1,
                    352,
                    0
                ],
                "title": "Jailbreaking? One Step Is Enough!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking? One Step Is Enough!"
                },
                "summary": "Large language models (LLMs) excel in various tasks but remain vulnerable to\njailbreak attacks, where adversaries manipulate prompts to generate harmful\noutputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs.\nHowever, current jailbreak methods and the target model's defenses are engaged\nin an independent and adversarial process, resulting in the need for frequent\nattack iterations and redesigning attacks for different models. To address\nthese gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that\ndisguises the attack intention as the \"defense\". intention against harmful\ncontent. Specifically, REDA starts from the target response, guiding the model\nto embed harmful content within its defensive measures, thereby relegating\nharmful content to a secondary role and making the model believe it is\nperforming a defensive task. The attacking model considers that it is guiding\nthe target model to deal with harmful content, while the target model thinks it\nis performing a defensive task, creating an illusion of cooperation between the\ntwo. Additionally, to enhance the model's confidence and guidance in\n\"defensive\" intentions, we adopt in-context learning (ICL) with a small number\nof attack examples and construct a corresponding dataset of attack examples.\nExtensive evaluations demonstrate that the REDA method enables cross-model\nattacks without the need to redesign attack strategies for different models,\nenables successful jailbreak in one iteration, and outperforms existing methods\non both open-source and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in various tasks but remain vulnerable to\njailbreak attacks, where adversaries manipulate prompts to generate harmful\noutputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs.\nHowever, current jailbreak methods and the target model's defenses are engaged\nin an independent and adversarial process, resulting in the need for frequent\nattack iterations and redesigning attacks for different models. To address\nthese gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that\ndisguises the attack intention as the \"defense\". intention against harmful\ncontent. Specifically, REDA starts from the target response, guiding the model\nto embed harmful content within its defensive measures, thereby relegating\nharmful content to a secondary role and making the model believe it is\nperforming a defensive task. The attacking model considers that it is guiding\nthe target model to deal with harmful content, while the target model thinks it\nis performing a defensive task, creating an illusion of cooperation between the\ntwo. Additionally, to enhance the model's confidence and guidance in\n\"defensive\" intentions, we adopt in-context learning (ICL) with a small number\nof attack examples and construct a corresponding dataset of attack examples.\nExtensive evaluations demonstrate that the REDA method enables cross-model\nattacks without the need to redesign attack strategies for different models,\nenables successful jailbreak in one iteration, and outperforms existing methods\non both open-source and closed-source models."
                },
                "authors": [
                    {
                        "name": "Weixiong Zheng"
                    },
                    {
                        "name": "Peijian Zeng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Hongyan Wu"
                    },
                    {
                        "name": "Nankai Lin"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Aimin Yang"
                    },
                    {
                        "name": "Yongmei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yongmei Zhou"
                },
                "author": "Yongmei Zhou",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08072v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08072v3",
                "updated": "2024-12-17T07:30:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    30,
                    54,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-15T10:44:38Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    44,
                    38,
                    3,
                    228,
                    0
                ],
                "title": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative\n  Self-Enhancement Paradigm"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, however,\nthe common learning paradigm treats LLMs as passive information repositories,\nneglecting their potential for active learning and alignment. Some approaches\ntrain LLMs using their own generated synthetic data, exploring the possibility\nof active alignment. However, there is still a huge gap between these one-time\nalignment methods and the continuous automatic alignment of humans. In this\npaper, we introduce \\textbf{I-SHEEP}, an \\textbf{I}terative\n\\textbf{S}elf-En\\textbf{H}anc\\textbf{E}m\\textbf{E}nt \\textbf{P}aradigm.This\nhuman-like paradigm enables LLMs to \\textbf{continuously self-align from\nscratch with nothing}. Compared to the one-time alignment method Dromedary\n\\cite{sun2023principledriven}, which refers to the first iteration in this\npaper, I-SHEEP can significantly enhance capacities on both Qwen and Llama\nmodels. I-SHEEP achieves a maximum relative improvement of 78.2\\% in the Alpaca\nEval, 24.0\\% in the MT Bench, and an absolute increase of 8.88\\% in the IFEval\naccuracy over subsequent iterations in Qwen-1.5 72B model. Additionally,\nI-SHEEP surpasses the base model in various standard benchmark generation\ntasks, achieving an average improvement of 24.77\\% in code generation tasks,\n12.04\\% in TrivialQA, and 20.29\\% in SQuAD. We also provide new insights based\non the experiment results. Our codes, datasets, and models are available at\n\\textbf{https://anonymous.4open.science/r/I-SHEEP}."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Zhenzhu Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08072v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05026v2",
                "updated": "2024-12-17T07:26:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    26,
                    39,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-30T09:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    35,
                    35,
                    2,
                    304,
                    0
                ],
                "title": "Deep Learning and Machine Learning -- Natural Language Processing: From\n  Theory to Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning and Machine Learning -- Natural Language Processing: From\n  Theory to Application"
                },
                "summary": "With a focus on natural language processing (NLP) and the role of large\nlanguage models (LLMs), we explore the intersection of machine learning, deep\nlearning, and artificial intelligence. As artificial intelligence continues to\nrevolutionize fields from healthcare to finance, NLP techniques such as\ntokenization, text classification, and entity recognition are essential for\nprocessing and understanding human language. This paper discusses advanced data\npreprocessing techniques and the use of frameworks like Hugging Face for\nimplementing transformer-based models. Additionally, it highlights challenges\nsuch as handling multilingual data, reducing bias, and ensuring model\nrobustness. By addressing key aspects of data processing and model fine-tuning,\nthis work aims to provide insights into deploying effective and ethically sound\nAI solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a focus on natural language processing (NLP) and the role of large\nlanguage models (LLMs), we explore the intersection of machine learning, deep\nlearning, and artificial intelligence. As artificial intelligence continues to\nrevolutionize fields from healthcare to finance, NLP techniques such as\ntokenization, text classification, and entity recognition are essential for\nprocessing and understanding human language. This paper discusses advanced data\npreprocessing techniques and the use of frameworks like Hugging Face for\nimplementing transformer-based models. Additionally, it highlights challenges\nsuch as handling multilingual data, reducing bias, and ensuring model\nrobustness. By addressing key aspects of data processing and model fine-tuning,\nthis work aims to provide insights into deploying effective and ethically sound\nAI solutions."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xuanhe Pan"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Jinlang Wang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Yizhu Wen"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jintao Ren"
                    },
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Weiche Hsieh"
                    },
                    {
                        "name": "Lawrence K. Q. Yan"
                    },
                    {
                        "name": "Chia Xin Liang"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Hong-Ming Tseng"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "252 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12612v1",
                "updated": "2024-12-17T07:21:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    21,
                    25,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T07:21:25Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    21,
                    25,
                    1,
                    352,
                    0
                ],
                "title": "SynthCypher: A Fully Synthetic Data Generation Framework for\n  Text-to-Cypher Querying in Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynthCypher: A Fully Synthetic Data Generation Framework for\n  Text-to-Cypher Querying in Knowledge Graphs"
                },
                "summary": "Cypher, the query language for Neo4j graph databases, plays a critical role\nin enabling graph-based analytics and data exploration. While substantial\nresearch has been dedicated to natural language to SQL query generation\n(Text2SQL), the analogous problem for graph databases referred to as\nText2Cypher remains underexplored. In this work, we introduce SynthCypher, a\nfully synthetic and automated data generation pipeline designed to address this\ngap. SynthCypher employs a novel LLMSupervised Generation-Verification\nframework, ensuring syntactically and semantically correct Cypher queries\nacross diverse domains and query complexities. Using this pipeline, we create\nSynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher\ninstances. Fine-tuning open-source large language models (LLMs), including\nLLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant\nperformance improvements of up to 40% on the Text2Cypher test set and 30% on\nthe SPIDER benchmark adapted for graph databases. This work demonstrates that\nhigh-quality synthetic data can effectively advance the state-of-the-art in\nText2Cypher tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cypher, the query language for Neo4j graph databases, plays a critical role\nin enabling graph-based analytics and data exploration. While substantial\nresearch has been dedicated to natural language to SQL query generation\n(Text2SQL), the analogous problem for graph databases referred to as\nText2Cypher remains underexplored. In this work, we introduce SynthCypher, a\nfully synthetic and automated data generation pipeline designed to address this\ngap. SynthCypher employs a novel LLMSupervised Generation-Verification\nframework, ensuring syntactically and semantically correct Cypher queries\nacross diverse domains and query complexities. Using this pipeline, we create\nSynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher\ninstances. Fine-tuning open-source large language models (LLMs), including\nLLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant\nperformance improvements of up to 40% on the Text2Cypher test set and 30% on\nthe SPIDER benchmark adapted for graph databases. This work demonstrates that\nhigh-quality synthetic data can effectively advance the state-of-the-art in\nText2Cypher tasks."
                },
                "authors": [
                    {
                        "name": "Aman Tiwari"
                    },
                    {
                        "name": "Shiva Krishna Reddy Malay"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    }
                ],
                "author_detail": {
                    "name": "Sathwik Tejaswi Madhusudhan"
                },
                "author": "Sathwik Tejaswi Madhusudhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16822v2",
                "updated": "2024-12-17T07:20:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    20,
                    33,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-22T08:48:52Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    48,
                    52,
                    1,
                    296,
                    0
                ],
                "title": "Can Large Language Models Act as Ensembler for Multi-GNNs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Act as Ensembler for Multi-GNNs?"
                },
                "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://anonymous.4open.science/r/EnsemGNN-E267/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://anonymous.4open.science/r/EnsemGNN-E267/."
                },
                "authors": [
                    {
                        "name": "Hanqi Duan"
                    },
                    {
                        "name": "Yao Cheng"
                    },
                    {
                        "name": "Jianxiang Yu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18200v2",
                "updated": "2024-12-17T07:18:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    18,
                    53,
                    1,
                    352,
                    0
                ],
                "published": "2024-06-26T09:33:41Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    9,
                    33,
                    41,
                    2,
                    178,
                    0
                ],
                "title": "SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative\n  Decoding"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable emergent abilities across\nvarious tasks, yet fall short of complex reasoning and planning tasks. The\ntree-search-based reasoning methods address this by surpassing the capabilities\nof chain-of-thought prompting, encouraging exploration of intermediate steps.\nHowever, such methods introduce significant inference latency due to the\nsystematic exploration and evaluation of multiple thought paths. This paper\nintroduces SeeD, a novel and efficient inference framework to optimize runtime\nspeed and GPU memory management concurrently. By employing a scheduled\nspeculative execution, SeeD efficiently handles multiple iterations for the\nthought generation and the state evaluation, leveraging a rounds-scheduled\nstrategy to manage draft model dispatching. Extensive experimental evaluations\non three reasoning datasets demonstrate superior speedup performance of SeeD,\nproviding a viable path for batched inference in training-free speculative\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable emergent abilities across\nvarious tasks, yet fall short of complex reasoning and planning tasks. The\ntree-search-based reasoning methods address this by surpassing the capabilities\nof chain-of-thought prompting, encouraging exploration of intermediate steps.\nHowever, such methods introduce significant inference latency due to the\nsystematic exploration and evaluation of multiple thought paths. This paper\nintroduces SeeD, a novel and efficient inference framework to optimize runtime\nspeed and GPU memory management concurrently. By employing a scheduled\nspeculative execution, SeeD efficiently handles multiple iterations for the\nthought generation and the state evaluation, leveraging a rounds-scheduled\nstrategy to manage draft model dispatching. Extensive experimental evaluations\non three reasoning datasets demonstrate superior speedup performance of SeeD,\nproviding a viable path for batched inference in training-free speculative\ndecoding."
                },
                "authors": [
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Congzhi Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by COLING2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00222v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00222v5",
                "updated": "2024-12-17T07:15:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    15,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-08-30T19:26:15Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    26,
                    15,
                    4,
                    243,
                    0
                ],
                "title": "Can Large Language Models Address Open-Target Stance Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Address Open-Target Stance Detection?"
                },
                "summary": "Stance detection (SD) identifies the text position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs) from\nGPT, Gemini, Llama, and Mistral families, comparing their performance to the\nonly existing work, Target-Stance Extraction (TSE), which benefits from\npredefined targets. Unlike TSE, OTSD removes the dependency of a predefined\nlist, making target generation and evaluation more challenging. We also provide\na metric for evaluating target quality that correlates well with human\njudgment. Our experiments reveal that LLMs outperform TSE in target generation,\nboth when the real target is explicitly and not explicitly mentioned in the\ntext. Similarly, LLMs overall surpass TSE in stance detection for both explicit\nand non-explicit cases. However, LLMs struggle in both target generation and\nstance detection when the target is not explicit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection (SD) identifies the text position towards a target,\ntypically labeled as favor, against, or none. We introduce Open-Target Stance\nDetection (OTSD), the most realistic task where targets are neither seen during\ntraining nor provided as input. We evaluate Large Language Models (LLMs) from\nGPT, Gemini, Llama, and Mistral families, comparing their performance to the\nonly existing work, Target-Stance Extraction (TSE), which benefits from\npredefined targets. Unlike TSE, OTSD removes the dependency of a predefined\nlist, making target generation and evaluation more challenging. We also provide\na metric for evaluating target quality that correlates well with human\njudgment. Our experiments reveal that LLMs outperform TSE in target generation,\nboth when the real target is explicitly and not explicitly mentioned in the\ntext. Similarly, LLMs overall surpass TSE in stance detection for both explicit\nand non-explicit cases. However, LLMs struggle in both target generation and\nstance detection when the target is not explicit."
                },
                "authors": [
                    {
                        "name": "Abu Ubaida Akash"
                    },
                    {
                        "name": "Ahmed Fahmy"
                    },
                    {
                        "name": "Amine Trabelsi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Trabelsi"
                },
                "author": "Amine Trabelsi",
                "arxiv_comment": "13 pages; currently under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00222v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00222v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12609v1",
                "updated": "2024-12-17T07:14:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    14,
                    3,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T07:14:03Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    14,
                    3,
                    1,
                    352,
                    0
                ],
                "title": "MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program\n  Fine-tuning"
                },
                "summary": "Program-of-Thought (PoT), which aims to use programming language instead of\nnatural language as an intermediate step in reasoning, is an important way for\nLLMs to solve mathematical problems. Since different programming languages\nexcel in different areas, it is natural to use the most suitable language for\nsolving specific problems. However, current PoT research only focuses on single\nlanguage PoT, ignoring the differences between different programming languages.\nTherefore, this paper proposes an multilingual program reasoning method,\nMultiLingPoT. This method allows the model to answer questions using multiple\nprogramming languages by fine-tuning on multilingual data. Additionally, prior\nand posterior hybrid methods are used to help the model select the most\nsuitable language for each problem. Our experimental results show that the\ntraining of MultiLingPoT improves each program's mathematical reasoning by\nabout 2.5\\%. Moreover, with proper mixing, the performance of MultiLingPoT can\nbe further improved, achieving a 6\\% increase compared to the single-language\nPoT with the data augmentation.Resources of this paper can be found at\nhttps://github.com/Nianqi-Li/MultiLingPoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program-of-Thought (PoT), which aims to use programming language instead of\nnatural language as an intermediate step in reasoning, is an important way for\nLLMs to solve mathematical problems. Since different programming languages\nexcel in different areas, it is natural to use the most suitable language for\nsolving specific problems. However, current PoT research only focuses on single\nlanguage PoT, ignoring the differences between different programming languages.\nTherefore, this paper proposes an multilingual program reasoning method,\nMultiLingPoT. This method allows the model to answer questions using multiple\nprogramming languages by fine-tuning on multilingual data. Additionally, prior\nand posterior hybrid methods are used to help the model select the most\nsuitable language for each problem. Our experimental results show that the\ntraining of MultiLingPoT improves each program's mathematical reasoning by\nabout 2.5\\%. Moreover, with proper mixing, the performance of MultiLingPoT can\nbe further improved, achieving a 6\\% increase compared to the single-language\nPoT with the data augmentation.Resources of this paper can be found at\nhttps://github.com/Nianqi-Li/MultiLingPoT."
                },
                "authors": [
                    {
                        "name": "Nianqi Li"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20962v3",
                "updated": "2024-12-17T07:13:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    13,
                    38,
                    1,
                    352,
                    0
                ],
                "published": "2024-07-30T16:43:24Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    16,
                    43,
                    24,
                    1,
                    212,
                    0
                ],
                "title": "MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions"
                },
                "summary": "Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training."
                },
                "authors": [
                    {
                        "name": "Xiaowei Chi"
                    },
                    {
                        "name": "Yatian Wang"
                    },
                    {
                        "name": "Aosong Cheng"
                    },
                    {
                        "name": "Pengjun Fang"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Xingqun Qi"
                    },
                    {
                        "name": "Jiahao Pan"
                    },
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Mengfei Li"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yanbing Jiang"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Wenhan Luo"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "arxiv_comment": "15 Pages. Dataset report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12602v1",
                "updated": "2024-12-17T06:59:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    59,
                    29,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T06:59:29Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    59,
                    29,
                    1,
                    352,
                    0
                ],
                "title": "Don't Yell at Your Robot: Physical Correction as the Collaborative\n  Interface for Language Model Powered Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Yell at Your Robot: Physical Correction as the Collaborative\n  Interface for Language Model Powered Robots"
                },
                "summary": "We present a novel approach for enhancing human-robot collaboration using\nphysical interactions for real-time error correction of large language model\n(LLM) powered robots. Unlike other methods that rely on verbal or text\ncommands, the robot leverages an LLM to proactively executes 6 DoF linear\nDynamical System (DS) commands using a description of the scene in natural\nlanguage. During motion, a human can provide physical corrections, used to\nre-estimate the desired intention, also parameterized by linear DS. This\ncorrected DS can be converted to natural language and used as part of the\nprompt to improve future LLM interactions. We provide proof-of-concept result\nin a hybrid real+sim experiment, showcasing physical interaction as a new\npossibility for LLM powered human-robot interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for enhancing human-robot collaboration using\nphysical interactions for real-time error correction of large language model\n(LLM) powered robots. Unlike other methods that rely on verbal or text\ncommands, the robot leverages an LLM to proactively executes 6 DoF linear\nDynamical System (DS) commands using a description of the scene in natural\nlanguage. During motion, a human can provide physical corrections, used to\nre-estimate the desired intention, also parameterized by linear DS. This\ncorrected DS can be converted to natural language and used as part of the\nprompt to improve future LLM interactions. We provide proof-of-concept result\nin a hybrid real+sim experiment, showcasing physical interaction as a new\npossibility for LLM powered human-robot interface."
                },
                "authors": [
                    {
                        "name": "Chuye Zhang"
                    },
                    {
                        "name": "Yifei Simon Shao"
                    },
                    {
                        "name": "Harshil Parekh"
                    },
                    {
                        "name": "Junyao Shi"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    },
                    {
                        "name": "Vijay Kumar"
                    },
                    {
                        "name": "Nadia Figueroa"
                    }
                ],
                "author_detail": {
                    "name": "Nadia Figueroa"
                },
                "author": "Nadia Figueroa",
                "arxiv_comment": "7 pages, 3 figures; Generative Modeling meets HRI - RSS'24 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12608v2",
                "updated": "2024-12-17T06:55:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    55,
                    0,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T14:24:55Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    55,
                    2,
                    290,
                    0
                ],
                "title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning"
                },
                "summary": "Large language models (LLMs) have shown increasing competence in solving\nmathematical reasoning problems. However, many open-source LLMs still struggle\nwith errors in calculation and semantic understanding during intermediate\nreasoning steps. In this work, we introduce Prove, a simple yet effective\nframework that leverages translated programs derived from natural language\nsolutions as a verification mechanism to filter out potentially incorrect\nreasoning paths before aggregating final answers. Unlike vanilla majority\nvoting, our approach filters out solutions whose corresponding program output\nis inconsistent with the generated solution, aggregating only those that pass\nverification. We conducted extensive experiments using 13 open-source LLMs from\nvarious model families and sizes, ranging from 0.5B to 13B parameters, across\neight mathematical benchmarks. Our results show that Prove consistently\noutperforms vanilla majority voting as a heuristic for solving mathematical\nreasoning tasks across all model sizes and datasets, achieving improvements of\nup to 18% on GSM8K and 8% on MATH-500. Our codes are available at\nhttps://github.com/declare-lab/prove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown increasing competence in solving\nmathematical reasoning problems. However, many open-source LLMs still struggle\nwith errors in calculation and semantic understanding during intermediate\nreasoning steps. In this work, we introduce Prove, a simple yet effective\nframework that leverages translated programs derived from natural language\nsolutions as a verification mechanism to filter out potentially incorrect\nreasoning paths before aggregating final answers. Unlike vanilla majority\nvoting, our approach filters out solutions whose corresponding program output\nis inconsistent with the generated solution, aggregating only those that pass\nverification. We conducted extensive experiments using 13 open-source LLMs from\nvarious model families and sizes, ranging from 0.5B to 13B parameters, across\neight mathematical benchmarks. Our results show that Prove consistently\noutperforms vanilla majority voting as a heuristic for solving mathematical\nreasoning tasks across all model sizes and datasets, achieving improvements of\nup to 18% on GSM8K and 8% on MATH-500. Our codes are available at\nhttps://github.com/declare-lab/prove."
                },
                "authors": [
                    {
                        "name": "Vernon Y. H. Toh"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16657v2",
                "updated": "2024-12-17T06:52:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    52,
                    46,
                    1,
                    352,
                    0
                ],
                "published": "2024-11-25T18:41:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation"
                },
                "summary": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples."
                },
                "authors": [
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project website: https://zunwang1.github.io/DreamRunner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12593v1",
                "updated": "2024-12-17T06:50:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    50,
                    15,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T06:50:15Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    50,
                    15,
                    1,
                    352,
                    0
                ],
                "title": "Asymmetric protocols for mode pairing quantum key distribution with\n  finite-key analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymmetric protocols for mode pairing quantum key distribution with\n  finite-key analysis"
                },
                "summary": "The mode pairing quantum key distribution (MP-QKD) protocol has attracted\nconsiderable attention for its capability to ensure high secure key rates over\nlong distances without requiring global phase locking. However, ensuring\nsymmetric channels for the MP-QKD protocol is challenging in practical quantum\ncommunication networks. Previous studies on the asymmetric MP-QKD protocol have\nrelied on ideal decoy state assumptions and infinite-key analysis, which are\nunattainable for real-world deployment. In this paper, we conduct a security\nanalysis of asymmetric MP-QKD protocol with the finite-key analysis, where we\ndiscard the previously impractical assumptions made in the decoy-state method.\nCombined with statistical fluctuation analysis, we globally optimized the 12\nindependent parameters in the asymmetric MP-QKD protocol by employing our\nmodified particle swarm optimization. The simulation results demonstrate that\nour work can achieve significantly enhanced secure key rates and transmission\ndistances compared to the original strategy with adding extra attenuation. We\nfurther investigate the relationship between the intensities and probabilities\nof signal, decoy, and vacuum states with transmission distance, facilitating\nits more efficient deployment in future quantum networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mode pairing quantum key distribution (MP-QKD) protocol has attracted\nconsiderable attention for its capability to ensure high secure key rates over\nlong distances without requiring global phase locking. However, ensuring\nsymmetric channels for the MP-QKD protocol is challenging in practical quantum\ncommunication networks. Previous studies on the asymmetric MP-QKD protocol have\nrelied on ideal decoy state assumptions and infinite-key analysis, which are\nunattainable for real-world deployment. In this paper, we conduct a security\nanalysis of asymmetric MP-QKD protocol with the finite-key analysis, where we\ndiscard the previously impractical assumptions made in the decoy-state method.\nCombined with statistical fluctuation analysis, we globally optimized the 12\nindependent parameters in the asymmetric MP-QKD protocol by employing our\nmodified particle swarm optimization. The simulation results demonstrate that\nour work can achieve significantly enhanced secure key rates and transmission\ndistances compared to the original strategy with adding extra attenuation. We\nfurther investigate the relationship between the intensities and probabilities\nof signal, decoy, and vacuum states with transmission distance, facilitating\nits more efficient deployment in future quantum networks."
                },
                "authors": [
                    {
                        "name": "Zhenhua Li"
                    },
                    {
                        "name": "Tianqi Dou"
                    },
                    {
                        "name": "Yuheng Xie"
                    },
                    {
                        "name": "Weiwen Kong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haiqiang Ma"
                    },
                    {
                        "name": "Jianjun Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Tang"
                },
                "author": "Jianjun Tang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12591v1",
                "updated": "2024-12-17T06:48:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    48,
                    24,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T06:48:24Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    48,
                    24,
                    1,
                    352,
                    0
                ],
                "title": "LLMs are Also Effective Embedding Models: An In-depth Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Also Effective Embedding Models: An In-depth Overview"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods, such as handling longer texts, and multilingual and cross-modal data.\nFurthermore, we discuss factors affecting choices of embedding models, such as\nperformance/efficiency comparisons, dense vs sparse embeddings, pooling\nstrategies, and scaling law. Lastly, the survey highlights the limitations and\nchallenges in adapting LLMs for embeddings, including cross-task embedding\nquality, trade-offs between efficiency and accuracy, low-resource,\nlong-context, data bias, robustness, etc. This survey serves as a valuable\nresource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods, such as handling longer texts, and multilingual and cross-modal data.\nFurthermore, we discuss factors affecting choices of embedding models, such as\nperformance/efficiency comparisons, dense vs sparse embeddings, pooling\nstrategies, and scaling law. Lastly, the survey highlights the limitations and\nchallenges in adapting LLMs for embeddings, including cross-task embedding\nquality, trade-offs between efficiency and accuracy, low-resource,\nlong-context, data bias, robustness, etc. This survey serves as a valuable\nresource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models."
                },
                "authors": [
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Junshuo Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Shuai Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Ma"
                },
                "author": "Shuai Ma",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v4",
                "updated": "2024-12-17T06:35:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    35,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]