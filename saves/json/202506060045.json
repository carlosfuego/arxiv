[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.04225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04225v1",
                "updated": "2025-06-04T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation"
                },
                "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."
                },
                "authors": [
                    {
                        "name": "Tianyu Huang"
                    },
                    {
                        "name": "Wangguandong Zheng"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Junta Wu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04213v2",
                "updated": "2025-06-05T03:35:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    35,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    57,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers"
                },
                "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."
                },
                "authors": [
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Quande Liu"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04108v2",
                "updated": "2025-06-05T05:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    39,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T16:01:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    1,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Rectified Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Sparse Attention"
                },
                "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Yizhao Gao"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03854v1",
                "updated": "2025-06-04T11:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:37:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks"
                },
                "summary": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases."
                },
                "authors": [
                    {
                        "name": "Emmanouil Anagnostakis"
                    },
                    {
                        "name": "Polyvios Pratikakis"
                    }
                ],
                "author_detail": {
                    "name": "Polyvios Pratikakis"
                },
                "author": "Polyvios Pratikakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03762v1",
                "updated": "2025-06-04T09:25:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T09:25:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Yifeng Gu"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Jianxiu Jin"
                    },
                    {
                        "name": "Kailing Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03700v1",
                "updated": "2025-06-04T08:32:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T08:32:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding."
                },
                "authors": [
                    {
                        "name": "Zhepei Wei"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Meng"
                },
                "author": "Yu Meng",
                "arxiv_comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01969v2",
                "updated": "2025-06-04T03:20:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    3,
                    20,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-13T17:45:34Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    45,
                    34,
                    1,
                    133,
                    0
                ],
                "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs"
                },
                "summary": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP."
                },
                "authors": [
                    {
                        "name": "Pengcuo Dege"
                    },
                    {
                        "name": "Qiuming Luo"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Chang Kong"
                    }
                ],
                "author_detail": {
                    "name": "Chang Kong"
                },
                "author": "Chang Kong",
                "arxiv_comment": "15 pages, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v1",
                "updated": "2025-06-03T18:35:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings.APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings.APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03275v1",
                "updated": "2025-06-03T18:03:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T18:03:32Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas"
                },
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact."
                },
                "authors": [
                    {
                        "name": "Austin Silveria"
                    },
                    {
                        "name": "Soham V. Govande"
                    },
                    {
                        "name": "Daniel Y. Fu"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Y. Fu"
                },
                "author": "Daniel Y. Fu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v1",
                "updated": "2025-06-03T13:19:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02671v1",
                "updated": "2025-06-03T09:16:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T09:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet"
                },
                "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jiazhen Huang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Xianghua Fu"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v1",
                "updated": "2025-06-03T08:51:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02523v1",
                "updated": "2025-06-03T06:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T06:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators."
                },
                "authors": [
                    {
                        "name": "Robin Geens"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v3",
                "updated": "2025-06-03T06:43:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    43,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02488v2",
                "updated": "2025-06-04T23:47:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    47,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T06:02:50Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Hongtao Huang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "arxiv_comment": "This paper was intended to be a v2 version of my previous paper\n  (arXiv:2409.17566), but it was submitted as a new paper by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v2",
                "updated": "2025-06-03T03:32:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    32,
                    10,
                    1,
                    154,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning\n  Models Acceleration"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v3",
                "updated": "2025-06-03T01:55:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    55,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v2",
                "updated": "2025-06-03T01:51:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    51,
                    37,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3716368.3735166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3716368.3735166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 14 figures",
                "arxiv_journal_ref": "Great Lakes Symposium on VLSI 2025 (GLSVLSI '25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v2",
                "updated": "2025-06-02T19:27:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    19,
                    27,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01928v1",
                "updated": "2025-06-02T17:47:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v8",
                "updated": "2025-06-02T17:46:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    46,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01880v1",
                "updated": "2025-06-02T17:09:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:09:59Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    9,
                    59,
                    0,
                    153,
                    0
                ],
                "title": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning"
                },
                "summary": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto."
                },
                "authors": [
                    {
                        "name": "Djamel Rassem Lamouri"
                    },
                    {
                        "name": "Iheb Nassim Aouadj"
                    },
                    {
                        "name": "Smail Kourta"
                    },
                    {
                        "name": "Riyadh Baghdadi"
                    }
                ],
                "author_detail": {
                    "name": "Riyadh Baghdadi"
                },
                "author": "Riyadh Baghdadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01827v1",
                "updated": "2025-06-02T16:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T16:12:22Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    16,
                    12,
                    22,
                    0,
                    153,
                    0
                ],
                "title": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts"
                },
                "summary": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations."
                },
                "authors": [
                    {
                        "name": "Spencer Banasik"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Banasik"
                },
                "author": "Spencer Banasik",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01643v1",
                "updated": "2025-06-02T13:16:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T13:16:14Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    16,
                    14,
                    0,
                    153,
                    0
                ],
                "title": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low Power Monolithic Active Pixel Sensor Prototype for the STCF Inner\n  Tracker"
                },
                "summary": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Tau-Charm Facility (STCF) is a proposed $e^+e^-$ collider with a\npeak luminosity 100 times higher than that of the present tau-charm factory.\nThe inner tracker (ITK) of STCF should feature a low material budget and high\nreadout speed. Under these requirements, the monolithic active pixel sensor\n(MAPS) is considered as a promising candidate for the ITK. To minimize the\npower consumption of MAPS (for low material budget), larger-size sensors are\nproposed to reduce the scale of the readout circuitry while preserving the\nrequired position resolution. Multiple sensors with varying dimensions and\nstructures were designed and integrated in several prototype chips for\nperformance comparison, fabricated in a 180~nm CIS process. The in-pixel\nreadout circuit can also provide time of arrival (ToA) and time-over-threshold\n(ToT) of the hit signal, with a least significant bit (LSB) of 50 ns. The\nperipheral readout circuit performs operations including timestamp correction,\ndata aggregation, caching, framing, 8b/10b encoding, and serialization.\nAccording to simulation, the power consumption for a full-scale chip is about\n55.7 mW/cm2. Preliminary measurements have been conducted on the prototype\nchips."
                },
                "authors": [
                    {
                        "name": "Dongwei Xuan"
                    },
                    {
                        "name": "Ruiyang Zhang"
                    },
                    {
                        "name": "Jiajun Qin"
                    },
                    {
                        "name": "Hao Han"
                    },
                    {
                        "name": "Xinyu Bin"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Jianbei Liu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Anqing Wang"
                    },
                    {
                        "name": "Aodong Song"
                    },
                    {
                        "name": "Xiangming Sun"
                    },
                    {
                        "name": "Le Xiao"
                    },
                    {
                        "name": "Lailin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Lailin Xu"
                },
                "author": "Lailin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v6",
                "updated": "2025-06-02T11:46:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    11,
                    46,
                    43,
                    0,
                    153,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v3",
                "updated": "2025-06-02T02:08:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    2,
                    8,
                    6,
                    0,
                    153,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v2",
                "updated": "2025-06-02T01:08:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    1,
                    8,
                    24,
                    0,
                    153,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams"
                },
                "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01215v1",
                "updated": "2025-06-01T23:49:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T23:49:14Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in\n  Transformers"
                },
                "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance."
                },
                "authors": [
                    {
                        "name": "Woomin Song"
                    },
                    {
                        "name": "Sai Muralidhar Jayanthi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kanthashree Mysore Sathyendra"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Shubham Katiyar"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    }
                ],
                "author_detail": {
                    "name": "Sravan Babu Bodapati"
                },
                "author": "Sravan Babu Bodapati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01151v1",
                "updated": "2025-06-01T20:05:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "published": "2025-06-01T20:05:30Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    20,
                    5,
                    30,
                    6,
                    152,
                    0
                ],
                "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron."
                },
                "authors": [
                    {
                        "name": "Xintong Sun"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Minghao Tian"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "arxiv_comment": "ICML2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18458v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18458v3",
                "updated": "2025-06-01T16:00:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    16,
                    0,
                    34,
                    6,
                    152,
                    0
                ],
                "published": "2025-05-24T01:57:12Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    1,
                    57,
                    12,
                    5,
                    144,
                    0
                ],
                "title": "A Survey of LLM $\\times$ DATA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM $\\times$ DATA"
                },
                "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Junxuan He"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Haoyu Zhao"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18458v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18458v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v2",
                "updated": "2025-06-01T10:36:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    1,
                    10,
                    36,
                    7,
                    6,
                    152,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00839v2",
                "updated": "2025-06-04T18:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    10,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-01T05:04:56Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "title": "Neural Path Guiding with Distribution Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Path Guiding with Distribution Factorization"
                },
                "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport."
                },
                "authors": [
                    {
                        "name": "Pedro Figueiredo"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Nima Khademi Kalantari"
                    }
                ],
                "author_detail": {
                    "name": "Nima Khademi Kalantari"
                },
                "author": "Nima Khademi Kalantari",
                "arxiv_comment": "11 pages, 11 figures. Accepted to EGSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v1",
                "updated": "2025-05-31T23:16:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17491v2",
                "updated": "2025-05-31T23:01:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    1,
                    0,
                    5,
                    151,
                    0
                ],
                "published": "2024-07-04T02:35:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    2,
                    35,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Adaptation of Foundation Models with Black-Box Visual Prompting"
                },
                "summary": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a surge of large-scale pre-trained models, parameter-efficient transfer\nlearning (PETL) of large models has garnered significant attention. While\npromising, they commonly rely on two optimistic assumptions: 1) full access to\nthe parameters of a PTM, and 2) sufficient memory capacity to cache all\nintermediate activations for gradient computation. However, in most real-world\napplications, PTMs serve as black-box APIs or proprietary software without full\nparameter accessibility. Besides, it is hard to meet a large memory requirement\nfor modern PTMs. This work proposes black-box visual prompting (BlackVIP),\nwhich efficiently adapts the PTMs without knowledge of their architectures or\nparameters. BlackVIP has two components: 1) Coordinator and 2) simultaneous\nperturbation stochastic approximation with gradient correction (SPSA-GC). The\nCoordinator designs input-dependent visual prompts, which allow the target PTM\nto adapt in the wild. SPSA-GC efficiently estimates the gradient of PTM to\nupdate Coordinator. Besides, we introduce a variant, BlackVIP-SE, which\nsignificantly reduces the runtime and computational cost of BlackVIP. Extensive\nexperiments on 19 datasets demonstrate that BlackVIPs enable robust adaptation\nto diverse domains and tasks with minimal memory requirements. We further\nprovide a theoretical analysis on the generalization of visual prompting\nmethods by presenting their connection to the certified robustness of\nrandomized smoothing, and presenting an empirical support for improved\nrobustness."
                },
                "authors": [
                    {
                        "name": "Changdae Oh"
                    },
                    {
                        "name": "Gyeongdeok Seo"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Hosik Choi"
                    },
                    {
                        "name": "Jiyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    }
                ],
                "author_detail": {
                    "name": "Kyungwoo Song"
                },
                "author": "Kyungwoo Song",
                "arxiv_comment": "Extended work from the CVPR'23 paper: arxiv:2303.14773; This paper\n  has been submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v2",
                "updated": "2025-05-31T22:12:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    22,
                    12,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v6",
                "updated": "2025-05-31T17:58:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    17,
                    58,
                    24,
                    5,
                    151,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v2",
                "updated": "2025-05-31T13:43:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    13,
                    43,
                    36,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02038v1",
                "updated": "2025-05-31T06:58:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:58:52Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    58,
                    52,
                    5,
                    151,
                    0
                ],
                "title": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain Powered Edge Intelligence for U-Healthcare in Privacy\n  Critical and Time Sensitive Environment"
                },
                "summary": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Intelligence (EI) serves as a critical enabler for privacy-preserving\nsystems by providing AI-empowered computation and distributed caching services\nat the edge, thereby minimizing latency and enhancing data privacy. The\nintegration of blockchain technology further augments EI frameworks by ensuring\ntransactional transparency, auditability, and system-wide reliability through a\ndecentralized network model. However, the operational architecture of such\nsystems introduces inherent vulnerabilities, particularly due to the extensive\ndata interactions between edge gateways (EGs) and the distributed nature of\ninformation storage during service provisioning. To address these challenges,\nwe propose an autonomous computing model along with its interaction topologies\ntailored for privacy-critical and time-sensitive health applications. The\nsystem supports continuous monitoring, real-time alert notifications, disease\ndetection, and robust data processing and aggregation. It also includes a data\ntransaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a\nresource-efficient one-dimensional convolutional neural network (1D-CNN) is\nproposed for the multiclass classification of arrhythmia, enabling accurate and\nreal-time analysis of constrained EGs. Furthermore, a secure access scheme is\ndefined to manage both off-chain and on-chain data sharing and storage. To\nvalidate the proposed model, comprehensive security, performance, and cost\nanalyses are conducted, demonstrating the efficiency and reliability of the\nfine-grained access control scheme."
                },
                "authors": [
                    {
                        "name": "Anum Nawaz"
                    },
                    {
                        "name": "Hafiz Humza Mahmood Ramzan"
                    },
                    {
                        "name": "Xianjia Yu"
                    },
                    {
                        "name": "Zhuo Zou"
                    },
                    {
                        "name": "Tomi Westerlund"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Westerlund"
                },
                "author": "Tomi Westerlund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00420v1",
                "updated": "2025-05-31T06:50:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:50:05Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    50,
                    5,
                    5,
                    151,
                    0
                ],
                "title": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Spatiotemporal Correlation Anomaly Detection Method that\n  Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor\n  Networks"
                },
                "summary": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods."
                },
                "authors": [
                    {
                        "name": "Miao Ye"
                    },
                    {
                        "name": "Suxiao Wang"
                    },
                    {
                        "name": "Jiaguang Han"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Xiaoli Wang"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Peng Wen"
                    },
                    {
                        "name": "Jing Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jing Cui"
                },
                "author": "Jing Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v1",
                "updated": "2025-05-31T06:10:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v4",
                "updated": "2025-05-31T04:45:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    45,
                    23,
                    5,
                    151,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we theoretically analyze the\ninherent correlation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is generally more important than\nvalue cache for quantization error reduction. We further propose a simple yet\neffective framework KVTuner to adaptively search for the optimal\nhardware-friendly layer-wise KV quantization precision pairs for coarse-grained\nKV cache with multi-objective optimization and directly utilize the offline\nsearched configurations during online inference. To reduce the computational\ncost of offline calibration, we utilize the intra-layer KV precision pair\npruning and inter-layer clustering to reduce the search space. Experimental\nresults show that we can achieve nearly lossless 3.25-bit mixed precision KV\ncache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for\nsensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The\nmaximum inference throughput can be improved by 21.25\\% compared with KIVI-KV8\nquantization over various context lengths. Our code and searched configurations\nare available at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Accepted by ICML25. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v1",
                "updated": "2025-05-31T04:27:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Deep-Learning-Driven Prefetching for Far Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-Learning-Driven Prefetching for Far Memory"
                },
                "summary": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v1",
                "updated": "2025-05-31T00:52:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v1",
                "updated": "2025-05-30T15:42:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24643v1",
                "updated": "2025-05-30T14:29:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T14:29:55Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    29,
                    55,
                    4,
                    150,
                    0
                ],
                "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based\n  Pairwise Ranking with Batching and Caching"
                },
                "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."
                },
                "authors": [
                    {
                        "name": "Juan Wisznia"
                    },
                    {
                        "name": "Cecilia Bolaos"
                    },
                    {
                        "name": "Juan Tollo"
                    },
                    {
                        "name": "Giovanni Marraffini"
                    },
                    {
                        "name": "Agustn Gianolini"
                    },
                    {
                        "name": "Noe Hsueh"
                    },
                    {
                        "name": "Luciano Del Corro"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Del Corro"
                },
                "author": "Luciano Del Corro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v2",
                "updated": "2025-05-30T11:43:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    11,
                    43,
                    48,
                    4,
                    150,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v2",
                "updated": "2025-06-05T02:27:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    2,
                    27,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24221v1",
                "updated": "2025-05-30T05:17:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T05:17:44Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    5,
                    17,
                    44,
                    4,
                    150,
                    0
                ],
                "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management"
                },
                "summary": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Wenzhe Zhu"
                    },
                    {
                        "name": "Yongkun Li"
                    },
                    {
                        "name": "Yinlong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinlong Xu"
                },
                "author": "Yinlong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24095v1",
                "updated": "2025-05-30T00:46:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v3",
                "updated": "2025-05-30T00:36:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    36,
                    37,
                    4,
                    150,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23970v1",
                "updated": "2025-05-29T19:52:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T19:52:44Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    19,
                    52,
                    44,
                    3,
                    149,
                    0
                ],
                "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving"
                },
                "summary": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low."
                },
                "authors": [
                    {
                        "name": "Yuyang Tian"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Yi Ding"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23938v1",
                "updated": "2025-05-29T18:41:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T18:41:13Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    18,
                    41,
                    13,
                    3,
                    149,
                    0
                ],
                "title": "Digital Forensic Investigation of the ChatGPT Windows Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Forensic Investigation of the ChatGPT Windows Application"
                },
                "summary": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics."
                },
                "authors": [
                    {
                        "name": "Malithi Wanniarachchi Kankanamge"
                    },
                    {
                        "name": "Nick McKenna"
                    },
                    {
                        "name": "Santiago Carmona"
                    },
                    {
                        "name": "Syed Mhamudul Hasan"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v1",
                "updated": "2025-05-29T17:12:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23520v1",
                "updated": "2025-05-29T14:59:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T14:59:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    14,
                    59,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorAttention: Difference-Aware Sparse Attention with Stripe\n  Granularity"
                },
                "summary": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Zhang"
                },
                "author": "Yiming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v1",
                "updated": "2025-05-29T13:05:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21889v2",
                "updated": "2025-05-29T12:59:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    12,
                    59,
                    26,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-28T02:07:03Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    2,
                    7,
                    3,
                    2,
                    148,
                    0
                ],
                "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV\n  Cache Reuse"
                },
                "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Cheater Lin"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Xianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianwei Zhang"
                },
                "author": "Xianwei Zhang",
                "arxiv_comment": "31st International European Conference on Parallel and Distributed\n  Computing (Euro-Par 2025 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23351v1",
                "updated": "2025-05-29T11:16:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T11:16:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    16,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores"
                },
                "summary": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods."
                },
                "authors": [
                    {
                        "name": "Sudam M. Wasala"
                    },
                    {
                        "name": "Jurre Wolff"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Anuj Pathania"
                    },
                    {
                        "name": "Clemens Grelck"
                    },
                    {
                        "name": "Andy D. Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Andy D. Pimentel"
                },
                "author": "Andy D. Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23275v1",
                "updated": "2025-05-29T09:23:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    23,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic\n  Perception"
                },
                "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v2",
                "updated": "2025-05-29T09:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    18,
                    35,
                    3,
                    149,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23258v1",
                "updated": "2025-05-29T09:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-29T09:06:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    6,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization\n  Under Dynamic Workloads for Securities Trading System"
                },
                "summary": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading."
                },
                "authors": [
                    {
                        "name": "Haojie Jia"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "9 pages, In Proceedings of IEEE ICCCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v4",
                "updated": "2025-05-29T09:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    1,
                    23,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor Product Attention Transformer,(T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines, including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at the decoding stage enable processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew C Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C Yao"
                },
                "author": "Andrew C Yao",
                "arxiv_comment": "52 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v2",
                "updated": "2025-05-29T03:11:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    3,
                    11,
                    10,
                    3,
                    149,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21070v2",
                "updated": "2025-05-29T01:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    34,
                    8,
                    3,
                    149,
                    0
                ],
                "published": "2025-05-27T11:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    11,
                    55,
                    22,
                    1,
                    147,
                    0
                ],
                "title": "Minute-Long Videos with Dual Parallelisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minute-Long Videos with Dual Parallelisms"
                },
                "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22927v1",
                "updated": "2025-05-28T22:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    59,
                    24,
                    2,
                    148,
                    0
                ],
                "title": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wideband Glide-Symmetric Slow-Wave Structure for Millimeter-Wave Sheet\n  Beam TWTs"
                },
                "summary": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a slow-wave structure (SWS) for a millimeter-wave sheet-beam\ntraveling-wave tube (TWT) with wide bandwidth. The wideband and stable\noperation is enabled through the topological properties associated with\nglide-symmetry that close the bandgap at the $3\\pi$-point and also make the\non-axis interaction impedance negligible for the backward wave. This space\nharmonic structure is designed to operate in the $V$-band over 55-68 GHz with\nsynchronism to a 5.2 kV, 11 mA sheet electron beam that will be produced by a\ndiamond field-emitter array."
                },
                "authors": [
                    {
                        "name": "Robert Marosi"
                    },
                    {
                        "name": "Muhammed Zuboraj"
                    },
                    {
                        "name": "Filippo Capolino"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Capolino"
                },
                "author": "Filippo Capolino",
                "arxiv_comment": "8 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v1",
                "updated": "2025-05-28T22:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v6",
                "updated": "2025-05-28T18:58:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    58,
                    29,
                    2,
                    148,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v1",
                "updated": "2025-05-28T17:39:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22425v1",
                "updated": "2025-05-28T14:52:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T14:52:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    14,
                    52,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Scaling Reasoning without Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reasoning without Attention"
                },
                "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."
                },
                "authors": [
                    {
                        "name": "Xueliang Zhao"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v4",
                "updated": "2025-05-28T12:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    7,
                    57,
                    2,
                    148,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility\n  and Speedup"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v1",
                "updated": "2025-05-28T09:20:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21919v1",
                "updated": "2025-05-28T03:05:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-28T03:05:55Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    3,
                    5,
                    55,
                    2,
                    148,
                    0
                ],
                "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in\n  LLM Inference"
                },
                "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
                },
                "authors": [
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eun Kyung Lee"
                },
                "author": "Eun Kyung Lee",
                "arxiv_comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v2",
                "updated": "2025-05-28T01:38:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    1,
                    38,
                    7,
                    2,
                    148,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v3",
                "updated": "2025-05-28T00:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    28,
                    0,
                    43,
                    47,
                    2,
                    148,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21669v1",
                "updated": "2025-05-27T18:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T18:47:34Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    47,
                    34,
                    1,
                    147,
                    0
                ],
                "title": "Improved Prefetching Techniques for Linked Data Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Prefetching Techniques for Linked Data Structures"
                },
                "summary": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ever-increasing main memory stall times, we need novel techniques to\nreduce effective memory access latencies. Prefetching has been shown to be an\neffective solution, especially with contiguous data structures that follow the\ntraditional principles of spatial and temporal locality. However, on linked\ndata structures$-$made up of many nodes linked together with pointers$-$typical\nprefetchers struggle, failing to predict accesses as elements are arbitrarily\nscattered throughout memory and access patters are arbitrarily complex and\nhence difficult to predict. To remedy these issues, we introduce\n$\\textit{Linkey}$, a novel prefetcher that utilizes hints from the\nprogrammer/compiler to cache layout information and accurately prefetch linked\ndata structures. $\\textit{Linkey}$ obtains substantial performance improvements\nover a striding baseline. We achieve a geomean 13% reduction in miss rate with\na maximum improvement of 58.8%, and a 65.4% geomean increase in accuracy, with\nmany benchmarks improving from 0%. On benchmarks where $\\textit{Linkey}$ is\napplicable, we observe a geomean IPC improvement of 1.40%, up to 12.1%."
                },
                "authors": [
                    {
                        "name": "Nikola Vuk Maruszewski"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Vuk Maruszewski"
                },
                "author": "Nikola Vuk Maruszewski",
                "arxiv_comment": "73 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.5.3; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21487v1",
                "updated": "2025-05-27T17:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    54,
                    7,
                    1,
                    147,
                    0
                ],
                "title": "Hardware-Efficient Attention for Fast Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Efficient Attention for Fast Decoding"
                },
                "summary": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
                },
                "authors": [
                    {
                        "name": "Ted Zadouri"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "37 pages, 15 figures, 45 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v1",
                "updated": "2025-05-27T17:39:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Language Model Inference via Efficient KV Caching\n  and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21259v1",
                "updated": "2025-05-27T14:39:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T14:39:28Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    14,
                    39,
                    28,
                    1,
                    147,
                    0
                ],
                "title": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Geometry-Based Performance Evaluation for LEO\n  Satellite-Assisted Space Caching"
                },
                "summary": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve the Internet of Things (IoT) vision,Mobile Edge Computing (MEC) is\na promising technology aimed at providing low-latency computing services to\nuser equipment (UE). However, terrestrial MEC network struggles to provide\nservice to UEs in remote and maritime region. Low Earth Orbit (LEO) satellite\nnetworks have the potential to overcome geographical restrictions and provide\nseamless global coverage for UEs. In this paper, we provide the first attempt\nto use stochastic geometry to investigate the performance of implementing space\ncaching with LEO satellites (SATs) in the MEC network. We study a LEO\nsatellite-assisted space caching MEC network, and LEO SATs can be equipped with\nservers to enable space caching, with the advantage of seamless coverage to\nassist terrestrial CSs for serving UEs in remote or maritime reigon. Using\nstochastic geometry and queuing theory, we establish an analytical framework\nfor this MEC network. Meanwhile, we develop association strategies for UEs to\nconnect with LEO SATs or CSs and utilize stochastic geometry to derive uplink\nand downlink coverage probabilities, considering the diversity of task and\nservice types. On this basis, we employ the queuing theory to calculate the\naverage delay to evaluate the system performance. Through Monte Carlo\nsimulations and numerical results, the system performance is evaluated. The\nresults show the potential of SAT spatial caching in improving the performance\nof the MEC network. Additionally, our results reveal useful insights such as\nthe significant impact of the altitude and number of LEO SATs on the average\ndelay of the network, providing helpful system-level recommendations for the\ndesign and configuration of the space-caching MEC network."
                },
                "authors": [
                    {
                        "name": "Chunyi Ma"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Jianhua Yang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Kishk"
                },
                "author": "Mustafa A. Kishk",
                "arxiv_doi": "10.1109/JIOT.2025.3574814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3574814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 12 figures, be accepted by IEEE IoTJ",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v3",
                "updated": "2025-05-27T12:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    5,
                    4,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but are slow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "SEA-2025 version. 18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v3",
                "updated": "2025-05-27T09:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    24,
                    50,
                    1,
                    147,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Caching for Serving Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show great capabilities in a wide range of\napplications, but serving them efficiently becomes increasingly challenging as\nrequests (prompts) become more complex. Context caching improves serving\nperformance by reusing Key-Value (KV) vectors, the intermediate representations\nof tokens that are repeated across requests. However, existing context caching\nrequires exact prefix matches across requests, limiting reuse cases in settings\nsuch as few-shot learning and retrieval-augmented generation, where immutable\ncontent (e.g., documents) remains unchanged across requests but is preceded by\nvarying prefixes. Position-Independent Caching (PIC) addresses this issue by\nenabling modular reuse of the KV vectors regardless of prefixes. We formalize\nPIC and advance prior work by introducing EPIC, a serving system incorporating\nour new LegoLink algorithm, which mitigates the inappropriate \"attention sink\"\neffect at every document beginning, to maintain accuracy with minimal\ncomputation. Experiments show that EPIC achieves up to 8x improvements in\nTime-To-First-Token (TTFT) and 7x throughput gains over existing systems, with\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v1",
                "updated": "2025-05-27T06:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models, reducing latency across\nall stages. To improve draft accuracy and speed, we propose Cross-model\nRetrieval, a novel KV cache update strategy that uses the target model's\nattention scores to dynamically select relevant context for the draft model.\nExtensive evaluations on three long-context understanding datasets show that\nSpecExtend accelerates standard tree-based speculative decoding by up to 2.22x\nfor inputs up to 16K tokens, providing an effective solution for speculative\ndecoding of long sequences. The code is available at\nhttps://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "8 pages, 3 figures. Under review at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v3",
                "updated": "2025-05-27T04:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    4,
                    15,
                    22,
                    1,
                    147,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached LLM-generated responses for semantically\nsimilar prompts to reduce inference latency and cost. They embed cached prompts\nand store them alongside their response in a vector database. Embedding\nsimilarity metrics assign a numerical score to quantify the similarity between\na request and its nearest neighbor prompt from the cache. Existing systems use\nthe same static similarity threshold across all requests to determine whether\ntwo prompts can share similar responses. However, we observe that static\nthresholds do not give formal correctness guarantees, can result in unexpected\nerror rates, and lead to suboptimal cache hit rates. This paper proposes\nvCache, the first verified semantic cache with user-defined error rate\nguarantees. It employs an online learning algorithm to estimate an optimal\nthreshold for each cached prompt, enabling reliable cache responses without\nadditional training. Our experiments show that vCache consistently meets the\nspecified error bounds while outperforming state-of-the-art static-threshold\nand fine-tuned embedding baselines. We release the vCache implementation and\nbenchmarks to support future research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19586v2",
                "updated": "2025-05-27T03:16:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    16,
                    32,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-26T07:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    0,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV\n  Cache Optimization"
                },
                "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Bowen Shen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v4",
                "updated": "2025-05-27T03:08:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    8,
                    57,
                    1,
                    147,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20600v1",
                "updated": "2025-05-27T00:36:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "published": "2025-05-27T00:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    0,
                    36,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware\n  Caching and Scheduling"
                },
                "summary": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative image editing using diffusion models has become a prevalent\napplication in today's AI cloud services. In production environments, image\nediting typically involves a mask that specifies the regions of an image\ntemplate to be edited. The use of masks provides direct control over the\nediting process and introduces sparsity in the model inference. In this paper,\nwe present InstGenIE, a system that efficiently serves image editing requests.\nThe key insight behind InstGenIE is that image editing only modifies the masked\nregions of image templates while preserving the original content in the\nunmasked areas. Driven by this insight, InstGenIE judiciously skips redundant\ncomputations associated with the unmasked areas by reusing cached intermediate\nactivations from previous inferences. To mitigate the high cache loading\noverhead, InstGenIE employs a bubble-free pipeline scheme that overlaps\ncomputation with cache loading. Additionally, to reduce queuing latency in\nonline serving while improving the GPU utilization, InstGenIE proposes a novel\ncontinuous batching strategy for diffusion model serving, allowing newly\narrived requests to join the running batch in just one step of denoising\ncomputation, without waiting for the entire batch to complete. As heterogeneous\nmasks induce imbalanced loads, InstGenIE also develops a load balancing\nstrategy that takes into account the loads of both computation and cache\nloading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving\nsystems for image editing, achieving up to 3x higher throughput and reducing\naverage request latency by up to 14.7x while ensuring image quality."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Tianyu Feng"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Guoxuan Zhu"
                    },
                    {
                        "name": "Xiu Lin"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20438v1",
                "updated": "2025-05-26T18:34:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T18:34:07Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    34,
                    7,
                    0,
                    146,
                    0
                ],
                "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAMburger: Accelerating LLM Inference via Token Smashing"
                },
                "summary": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design."
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Ce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ce Zhang"
                },
                "author": "Ce Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v5",
                "updated": "2025-05-26T16:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    16,
                    43,
                    0,
                    146,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v2",
                "updated": "2025-05-26T13:20:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    20,
                    45,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAP: Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Jiaheng Dai"
                    },
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Tianqi Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v1",
                "updated": "2025-05-26T12:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_comment": "Accepted for publication in 2025 IEEE 18th International Conference\n  on Cloud Computing (CLOUD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v1",
                "updated": "2025-05-26T11:35:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://anonymous.4open.science/r/HIT_model-5C23."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v2",
                "updated": "2025-05-26T10:07:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    7,
                    5,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xinyu Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17062v3",
                "updated": "2025-05-26T08:39:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    39,
                    26,
                    0,
                    146,
                    0
                ],
                "published": "2024-05-27T11:31:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    31,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniICL: An Efficient Unified Framework Unifying Compression, Selection,\n  and Generation"
                },
                "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Qi Lv"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Tianxiang Wu"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v2",
                "updated": "2025-05-26T07:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    30,
                    17,
                    0,
                    146,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs\nthat consist of large number of devices. Unfortunately, communication between\nthese devices incurs significant overhead, increasing the inference latency and\ncost while limiting the scalability. Prior work addressed this issue by\noverlapping communication with compute, but has severe limitations due to the\ndata dependencies between these operations. In this paper, we propose PRESERVE,\na novel framework that prefetches model weights and KV-cache from off-chip HBM\nmemory to the on-chip cache of AI accelerators during the communication\noperations, which offers various advantages and performance improvements\ncompared to prior methods.\n  Through extensive experiments conducted on commercial AI accelerators, we\ndemonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source\nLLMs. Additionally, we perform a design space exploration that identifies the\noptimal hardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yzgler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19602v1",
                "updated": "2025-05-26T07:11:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T07:11:42Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    11,
                    42,
                    0,
                    146,
                    0
                ],
                "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression"
                },
                "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."
                },
                "authors": [
                    {
                        "name": "Kunjun Li"
                    },
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Cheng-Yen Yang"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Jenq-Neng Hwang"
                },
                "author": "Jenq-Neng Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v1",
                "updated": "2025-05-26T05:58:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v2",
                "updated": "2025-05-26T05:56:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    56,
                    51,
                    0,
                    146,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v2",
                "updated": "2025-05-26T05:28:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    28,
                    49,
                    0,
                    146,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v2",
                "updated": "2025-05-25T13:03:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    13,
                    3,
                    54,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "arxiv_comment": "Accepted at Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19089v1",
                "updated": "2025-05-25T10:57:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "published": "2025-05-25T10:57:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    10,
                    57,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation"
                },
                "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.04228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04228v1",
                "updated": "2025-06-04T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    58,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    58,
                    2,
                    155,
                    0
                ],
                "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerFlow: A Unified Model for Layer-aware Video Generation"
                },
                "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers."
                },
                "authors": [
                    {
                        "name": "Sihui Ji"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yuanpeng Tu"
                    },
                    {
                        "name": "Yiyang Wang"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "Project Page: https://sihuiji.github.io/LayerFlow-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04225v1",
                "updated": "2025-06-04T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation"
                },
                "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."
                },
                "authors": [
                    {
                        "name": "Tianyu Huang"
                    },
                    {
                        "name": "Wangguandong Zheng"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Junta Wu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04211v1",
                "updated": "2025-06-04T17:56:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    56,
                    46,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:56:46Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    56,
                    46,
                    2,
                    155,
                    0
                ],
                "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object\n  Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object\n  Detector"
                },
                "summary": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher."
                },
                "authors": [
                    {
                        "name": "Boyong He"
                    },
                    {
                        "name": "Yuxiang Ji"
                    },
                    {
                        "name": "Zhuoyue Tan"
                    },
                    {
                        "name": "Liaoni Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liaoni Wu"
                },
                "author": "Liaoni Wu",
                "arxiv_comment": "MM2024 poster, with appendix and codes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04210v1",
                "updated": "2025-06-04T17:55:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    55,
                    9,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:55:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    55,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "Does Thinking More always Help? Understanding Test-Time Scaling in\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Thinking More always Help? Understanding Test-Time Scaling in\n  Reasoning Models"
                },
                "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models."
                },
                "authors": [
                    {
                        "name": "Soumya Suvra Ghosal"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Avinash Reddy"
                    },
                    {
                        "name": "Yifu Lu"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Dinesh Manocha"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Mohammad Ghavamzadeh"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04209v1",
                "updated": "2025-06-04T17:51:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    51,
                    56,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:51:56Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    51,
                    56,
                    2,
                    155,
                    0
                ],
                "title": "Language-Image Alignment with Fixed Text Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Image Alignment with Fixed Text Encoders"
                },
                "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations."
                },
                "authors": [
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yi Ma"
                },
                "author": "Yi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04205v1",
                "updated": "2025-06-04T17:49:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    49,
                    10,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:49:10Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    49,
                    10,
                    2,
                    155,
                    0
                ],
                "title": "EPiC: Towards Lossless Speedup for Reasoning Training through\n  Edge-Preserving CoT Condensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPiC: Towards Lossless Speedup for Reasoning Training through\n  Edge-Preserving CoT Condensation"
                },
                "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation."
                },
                "authors": [
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Hadi Reisizadeh"
                    },
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Nathalie Baracaldo"
                    },
                    {
                        "name": "Mingyi Hong"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04203v1",
                "updated": "2025-06-04T17:48:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:48:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "Cascadia: A Cascade Serving System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascadia: A Cascade Serving System for Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality answers. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this\nlatency-quality trade-off using model cascades, which route simpler queries to\nsmaller models and more complex ones to larger models. However, enabling\nefficient cascade serving remains challenging. Current frameworks lack\neffective mechanisms for handling (i) the huge and varying resource demands of\ndifferent LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the\nco-optimization of system deployment and routing strategy. Motivated by these\nobservations, we introduce Cascadia, a novel cascade serving framework designed\nexplicitly to schedule request routing and deploy model cascades for fast,\nquality-preserving LLM serving. Cascadia employs a bi-level optimization\nmethod: at the inner level, it uses a mixed-integer linear program to select\nresource allocations and parallelism strategies based on LLM information and\nworkload characteristics; at the outer level, it applies a weighted Tchebycheff\nalgorithm to iteratively co-optimize the routing strategy and the system\ndeployment produced by the inner level. Our extensive evaluation on diverse\nworkload traces and different model cascades (DeepSeek and the Llama series)\ndemonstrates that Cascadia significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to\n4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher\nthroughput while maintaining target answer quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality answers. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this\nlatency-quality trade-off using model cascades, which route simpler queries to\nsmaller models and more complex ones to larger models. However, enabling\nefficient cascade serving remains challenging. Current frameworks lack\neffective mechanisms for handling (i) the huge and varying resource demands of\ndifferent LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the\nco-optimization of system deployment and routing strategy. Motivated by these\nobservations, we introduce Cascadia, a novel cascade serving framework designed\nexplicitly to schedule request routing and deploy model cascades for fast,\nquality-preserving LLM serving. Cascadia employs a bi-level optimization\nmethod: at the inner level, it uses a mixed-integer linear program to select\nresource allocations and parallelism strategies based on LLM information and\nworkload characteristics; at the outer level, it applies a weighted Tchebycheff\nalgorithm to iteratively co-optimize the routing strategy and the system\ndeployment produced by the inner level. Our extensive evaluation on diverse\nworkload traces and different model cascades (DeepSeek and the Llama series)\ndemonstrates that Cascadia significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to\n4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher\nthroughput while maintaining target answer quality."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Stephan Rabanser"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04202v1",
                "updated": "2025-06-04T17:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    16,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracLLM: A Generic Framework for Attributing Long Context LLMs"
                },
                "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM."
                },
                "authors": [
                    {
                        "name": "Yanting Wang"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Jinyuan Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Jia"
                },
                "author": "Jinyuan Jia",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04201v1",
                "updated": "2025-06-04T17:46:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    46,
                    2,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:46:02Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    46,
                    2,
                    2,
                    155,
                    0
                ],
                "title": "Analyzing Line-of-sight selection biases in galaxy-scale strong lensing\n  with external convergence and shear",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Line-of-sight selection biases in galaxy-scale strong lensing\n  with external convergence and shear"
                },
                "summary": "The upcoming Vera Rubin Observatory Legacy Survey of Space and Time (LSST)\nwill dramatically increase the number of strong gravitational lensing systems,\nrequiring precise modeling of line-of-sight (LOS) effects to mitigate biases in\nlensing observations and cosmological inferences. We develop a method to\nconstruct joint distributions of external convergence ($\\kappa_{\\mathrm{ext}}$)\nand shear ($\\gamma_{\\mathrm{ext}}$) for strong lensing LOS by aggregating\nlarge-scale structure simulations with high-resolution halo renderings and\nnon-linear correction. Our approach captures both smooth background matter and\nperturbations from halos, enabling accurate modeling of LOS effects. We apply\nnon-linear LOS corrections to $\\kappa_{\\mathrm{ext}}$ and\n$\\gamma_{\\mathrm{ext}}$ that address the non-additive lensing effects caused by\nobjects along the LOS in strong lensing. We find that, with a minimum image\nseparation of $1.0^{\\prime\\prime}$, non-linear LOS correction due to the\npresence of a dominant deflector slightly increases the ratio of quadruple to\ndouble lenses; this non-linear LOS correction also introduces systematic biases\nof $\\sim 0.1\\%$ for galaxy-AGN lenses in the inferred Hubble constant ($H_0$)\nif not accounted for. We also observe a $0.66\\%$ bias for galaxy-galaxy lenses\non $H_0$, and even larger biases up to $1.02\\%$ for galaxy-AGN systems if LOS\neffects are not accounted for. These results highlight the importance of LOS\nfor precision cosmology. The publicly available code and datasets provide tools\nfor incorporating LOS effects in future analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The upcoming Vera Rubin Observatory Legacy Survey of Space and Time (LSST)\nwill dramatically increase the number of strong gravitational lensing systems,\nrequiring precise modeling of line-of-sight (LOS) effects to mitigate biases in\nlensing observations and cosmological inferences. We develop a method to\nconstruct joint distributions of external convergence ($\\kappa_{\\mathrm{ext}}$)\nand shear ($\\gamma_{\\mathrm{ext}}$) for strong lensing LOS by aggregating\nlarge-scale structure simulations with high-resolution halo renderings and\nnon-linear correction. Our approach captures both smooth background matter and\nperturbations from halos, enabling accurate modeling of LOS effects. We apply\nnon-linear LOS corrections to $\\kappa_{\\mathrm{ext}}$ and\n$\\gamma_{\\mathrm{ext}}$ that address the non-additive lensing effects caused by\nobjects along the LOS in strong lensing. We find that, with a minimum image\nseparation of $1.0^{\\prime\\prime}$, non-linear LOS correction due to the\npresence of a dominant deflector slightly increases the ratio of quadruple to\ndouble lenses; this non-linear LOS correction also introduces systematic biases\nof $\\sim 0.1\\%$ for galaxy-AGN lenses in the inferred Hubble constant ($H_0$)\nif not accounted for. We also observe a $0.66\\%$ bias for galaxy-galaxy lenses\non $H_0$, and even larger biases up to $1.02\\%$ for galaxy-AGN systems if LOS\neffects are not accounted for. These results highlight the importance of LOS\nfor precision cosmology. The publicly available code and datasets provide tools\nfor incorporating LOS effects in future analyses."
                },
                "authors": [
                    {
                        "name": "Xianzhe TZ Tang"
                    },
                    {
                        "name": "Simon Birrer"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "Narayan Khadka"
                    },
                    {
                        "name": "the LSST Strong Gravitational Lensing Science Collaboration"
                    },
                    {
                        "name": "the LSST Dark Energy Science Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "the LSST Dark Energy Science Collaboration"
                },
                "author": "the LSST Dark Energy Science Collaboration",
                "arxiv_comment": "27 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04194v1",
                "updated": "2025-06-04T17:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    40,
                    55,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:40:55Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    40,
                    55,
                    2,
                    155,
                    0
                ],
                "title": "What Makes Treatment Effects Identifiable? Characterizations and\n  Estimators Beyond Unconfoundedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Makes Treatment Effects Identifiable? Characterizations and\n  Estimators Beyond Unconfoundedness"
                },
                "summary": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms."
                },
                "authors": [
                    {
                        "name": "Yang Cai"
                    },
                    {
                        "name": "Alkis Kalavasis"
                    },
                    {
                        "name": "Katerina Mamali"
                    },
                    {
                        "name": "Anay Mehrotra"
                    },
                    {
                        "name": "Manolis Zampetakis"
                    }
                ],
                "author_detail": {
                    "name": "Manolis Zampetakis"
                },
                "author": "Manolis Zampetakis",
                "arxiv_comment": "Accepted for presentation at the 38th Conference on Learning Theory\n  (COLT) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04185v1",
                "updated": "2025-06-04T17:29:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    29,
                    22,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:29:22Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    29,
                    22,
                    2,
                    155,
                    0
                ],
                "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Dingling Xu"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Limin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Limin Liu"
                },
                "author": "Limin Liu",
                "arxiv_comment": "16 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13865v2",
                "updated": "2025-06-04T17:29:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    29,
                    13,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-27T17:58:31Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    58,
                    31,
                    3,
                    86,
                    0
                ],
                "title": "A Survey on (M)LLM-Based GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on (M)LLM-Based GUI Agents"
                },
                "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation."
                },
                "authors": [
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Siqi Chen"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Zeqi Tan"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04182v1",
                "updated": "2025-06-04T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    28,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    28,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "Long or short CoT? Investigating Instance-level Switch of Large\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long or short CoT? Investigating Instance-level Switch of Large\n  Reasoning Models"
                },
                "summary": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone."
                },
                "authors": [
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Changyi Xiao"
                    },
                    {
                        "name": "Yixin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Cao"
                },
                "author": "Yixin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04180v1",
                "updated": "2025-06-04T17:27:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    27,
                    42,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:27:42Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    27,
                    42,
                    2,
                    155,
                    0
                ],
                "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models"
                },
                "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16733v2",
                "updated": "2025-06-04T17:26:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    26,
                    55,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-23T22:14:42Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    22,
                    14,
                    42,
                    6,
                    54,
                    0
                ],
                "title": "Coreset Selection via LLM-based Concept Bottlenecks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreset Selection via LLM-based Concept Bottlenecks"
                },
                "summary": "Coreset Selection (CS) aims to identify a subset of the training dataset that\nachieves model performance comparable to using the entire dataset. Many\nstate-of-the-art CS methods select coresets using scores whose computation\nrequires training the downstream model on the entire dataset first and\nrecording changes in the model's behavior on samples as it trains (training\ndynamics). These scores are inefficient to compute and hard to interpret, as\nthey do not indicate whether a sample is difficult to learn in general or only\nfor a specific downstream model. Our work addresses these challenges by\nproposing a score that computes a sample's difficulty using\nhuman-understandable textual attributes (concepts) independent of any\ndownstream model. Specifically, we measure the alignment between a sample's\nvisual features and concept bottlenecks, derived via large language models, by\ntraining a linear concept bottleneck layer and computing the sample's\ndifficulty score using it.We then use stratified sampling based on this score\nto generate a coreset of the dataset.Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10/100,\nand ImageNet-1K, we show that our coresets outperform random subsets, even at\nhigh pruning rates, and achieve model performance comparable to or better than\ncoresets found by training dynamics-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreset Selection (CS) aims to identify a subset of the training dataset that\nachieves model performance comparable to using the entire dataset. Many\nstate-of-the-art CS methods select coresets using scores whose computation\nrequires training the downstream model on the entire dataset first and\nrecording changes in the model's behavior on samples as it trains (training\ndynamics). These scores are inefficient to compute and hard to interpret, as\nthey do not indicate whether a sample is difficult to learn in general or only\nfor a specific downstream model. Our work addresses these challenges by\nproposing a score that computes a sample's difficulty using\nhuman-understandable textual attributes (concepts) independent of any\ndownstream model. Specifically, we measure the alignment between a sample's\nvisual features and concept bottlenecks, derived via large language models, by\ntraining a linear concept bottleneck layer and computing the sample's\ndifficulty score using it.We then use stratified sampling based on this score\nto generate a coreset of the dataset.Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10/100,\nand ImageNet-1K, we show that our coresets outperform random subsets, even at\nhigh pruning rates, and achieve model performance comparable to or better than\ncoresets found by training dynamics-based methods."
                },
                "authors": [
                    {
                        "name": "Akshay Mehra"
                    },
                    {
                        "name": "Trisha Mittal"
                    },
                    {
                        "name": "Subhadra Gopalakrishnan"
                    },
                    {
                        "name": "Joshua Kimball"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Kimball"
                },
                "author": "Joshua Kimball",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04179v1",
                "updated": "2025-06-04T17:26:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    26,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:26:31Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    26,
                    31,
                    2,
                    155,
                    0
                ],
                "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\n  Module Decoupling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\n  Module Decoupling"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT."
                },
                "authors": [
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Zhiwei Fei"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03023v2",
                "updated": "2025-06-04T17:22:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    22,
                    25,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T15:59:29Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    15,
                    59,
                    29,
                    1,
                    154,
                    0
                ],
                "title": "TDCOSMO 2025: Cosmological constraints from strong lensing time delays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDCOSMO 2025: Cosmological constraints from strong lensing time delays"
                },
                "summary": "We present cosmological constraints from 8 strongly lensed quasars\n(hereafter, the TDCOSMO-2025 sample). Building on previous work, our analysis\nincorporated new deflector stellar velocity dispersions measured from spectra\nobtained with the James Webb Space Telescope (JWST), the Keck Telescopes, and\nthe Very Large Telescope (VLT), utilizing improved methods. We used integrated\nJWST stellar kinematics for 5 lenses, VLT-MUSE for 2, and resolved kinematics\nfrom Keck and JWST for RX J1131-1231. We also considered two samples of\nnon-time-delay lenses: 11 from the Sloan Lens ACS (SLACS) sample with Keck-KCWI\nresolved kinematics; and 4 from the Strong Lenses in the Legacy Survey (SL2S)\nsample. We improved our analysis of line-of-sight effects, the surface\nbrightness profile of the lens galaxies, and orbital anisotropy, and corrected\nfor projection effects in the dynamics. Our uncertainties are maximally\nconservative by accounting for the mass-sheet degeneracy in the deflectors'\nmass density profiles. The analysis was blinded to prevent experimenter bias.\nOur primary result is based on the TDCOSMO-2025 sample, in combination with\n$\\Omega_{\\rm m}$ constraints from the Pantheon+ Type Ia supernovae (SN)\ndataset. In the flat $\\Lambda$ Cold Dark Matter (CDM), we find\n$H_0=72.1^{+4.0}_{-3.7}$ km s$^{-1}$ Mpc$^{-1}$. The SLACS and SL2S samples are\nin excellent agreement with the TDCOSMO-2025 sample, improving the precision on\n$H_0$ in flat $\\Lambda$CDM to 4.4%. Using the Dark Energy Survey SN Year-5\ndataset (DES-SN5YR) or DESI-DR2 baryonic acoustic oscillations (BAO)\nlikelihoods instead of Pantheon+ yields very similar results. We also present\nconstraints in the open $\\Lambda$CDM, $w$CDM, $w_0w_a$CDM, and $w_{\\phi}$CDM\ncosmologies. The TDCOSMO $H_0$ inference is robust and consistent across all\npresented cosmological models, and our cosmological constraints in them agree\nwith those from the BAO and SN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present cosmological constraints from 8 strongly lensed quasars\n(hereafter, the TDCOSMO-2025 sample). Building on previous work, our analysis\nincorporated new deflector stellar velocity dispersions measured from spectra\nobtained with the James Webb Space Telescope (JWST), the Keck Telescopes, and\nthe Very Large Telescope (VLT), utilizing improved methods. We used integrated\nJWST stellar kinematics for 5 lenses, VLT-MUSE for 2, and resolved kinematics\nfrom Keck and JWST for RX J1131-1231. We also considered two samples of\nnon-time-delay lenses: 11 from the Sloan Lens ACS (SLACS) sample with Keck-KCWI\nresolved kinematics; and 4 from the Strong Lenses in the Legacy Survey (SL2S)\nsample. We improved our analysis of line-of-sight effects, the surface\nbrightness profile of the lens galaxies, and orbital anisotropy, and corrected\nfor projection effects in the dynamics. Our uncertainties are maximally\nconservative by accounting for the mass-sheet degeneracy in the deflectors'\nmass density profiles. The analysis was blinded to prevent experimenter bias.\nOur primary result is based on the TDCOSMO-2025 sample, in combination with\n$\\Omega_{\\rm m}$ constraints from the Pantheon+ Type Ia supernovae (SN)\ndataset. In the flat $\\Lambda$ Cold Dark Matter (CDM), we find\n$H_0=72.1^{+4.0}_{-3.7}$ km s$^{-1}$ Mpc$^{-1}$. The SLACS and SL2S samples are\nin excellent agreement with the TDCOSMO-2025 sample, improving the precision on\n$H_0$ in flat $\\Lambda$CDM to 4.4%. Using the Dark Energy Survey SN Year-5\ndataset (DES-SN5YR) or DESI-DR2 baryonic acoustic oscillations (BAO)\nlikelihoods instead of Pantheon+ yields very similar results. We also present\nconstraints in the open $\\Lambda$CDM, $w$CDM, $w_0w_a$CDM, and $w_{\\phi}$CDM\ncosmologies. The TDCOSMO $H_0$ inference is robust and consistent across all\npresented cosmological models, and our cosmological constraints in them agree\nwith those from the BAO and SN."
                },
                "authors": [
                    {
                        "name": "TDCOSMO Collaboration"
                    },
                    {
                        "name": "Simon Birrer"
                    },
                    {
                        "name": "Elizabeth J. Buckley-Geer"
                    },
                    {
                        "name": "Michele Cappellari"
                    },
                    {
                        "name": "Frdric Courbin"
                    },
                    {
                        "name": "Frdric Dux"
                    },
                    {
                        "name": "Christopher D. Fassnacht"
                    },
                    {
                        "name": "Joshua A. Frieman"
                    },
                    {
                        "name": "Aymeric Galan"
                    },
                    {
                        "name": "Daniel Gilman"
                    },
                    {
                        "name": "Xiang-Yu Huang"
                    },
                    {
                        "name": "Shawn Knabel"
                    },
                    {
                        "name": "Danial Langeroodi"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Martin Millon"
                    },
                    {
                        "name": "Takahiro Morishita"
                    },
                    {
                        "name": "Veronica Motta"
                    },
                    {
                        "name": "Pritom Mozumdar"
                    },
                    {
                        "name": "Eric Paic"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "William Sheu"
                    },
                    {
                        "name": "Dominique Sluse"
                    },
                    {
                        "name": "Alessandro Sonnenfeld"
                    },
                    {
                        "name": "Chiara Spiniello"
                    },
                    {
                        "name": "Massimo Stiavelli"
                    },
                    {
                        "name": "Sherry H. Suyu"
                    },
                    {
                        "name": "Chin Yi Tan"
                    },
                    {
                        "name": "Tommaso Treu"
                    },
                    {
                        "name": "Lyne Van de Vyvere"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Patrick Wells"
                    },
                    {
                        "name": "Devon M. Williams"
                    },
                    {
                        "name": "Kenneth C. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth C. Wong"
                },
                "author": "Kenneth C. Wong",
                "arxiv_comment": "34 pages, 17 figures, 8 tables, (this version: fixed author names). A\n  virtual talk on the presented results will be given in the CosmoVerse Seminar\n  on June 12, 2025, at 4pm UK time, details:\n  https://cosmoversetensions.eu/for-scientists/cosmoverse-seminars/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04174v1",
                "updated": "2025-06-04T17:17:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    17,
                    57,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:17:57Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    17,
                    57,
                    2,
                    155,
                    0
                ],
                "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D\n  Gaussian Splatting"
                },
                "summary": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene\nrepresentation and novel view synthesis due to its efficient rendering\ncapabilities. However, 3DGS demands relatively significant GPU memory, limiting\nits use on devices with restricted computational resources. Previous approaches\nhave focused on pruning less important Gaussians, effectively compressing 3DGS\nbut often requiring a fine-tuning stage and lacking adaptability for the\nspecific memory needs of different devices. In this work, we present an elastic\ninference method for 3DGS. Given an input for the desired model size, our\nmethod selects and transforms a subset of Gaussians, achieving substantial\nrendering performance without additional fine-tuning. We introduce a tiny\nlearnable module that controls Gaussian selection based on the input\npercentage, along with a transformation module that adjusts the selected\nGaussians to complement the performance of the reduced model. Comprehensive\nexperiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the\neffectiveness of our approach. Code is available at https://flexgs.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene\nrepresentation and novel view synthesis due to its efficient rendering\ncapabilities. However, 3DGS demands relatively significant GPU memory, limiting\nits use on devices with restricted computational resources. Previous approaches\nhave focused on pruning less important Gaussians, effectively compressing 3DGS\nbut often requiring a fine-tuning stage and lacking adaptability for the\nspecific memory needs of different devices. In this work, we present an elastic\ninference method for 3DGS. Given an input for the desired model size, our\nmethod selects and transforms a subset of Gaussians, achieving substantial\nrendering performance without additional fine-tuning. We introduce a tiny\nlearnable module that controls Gaussian selection based on the input\npercentage, along with a transformation module that adjusts the selected\nGaussians to complement the performance of the reduced model. Comprehensive\nexperiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the\neffectiveness of our approach. Code is available at https://flexgs.github.io."
                },
                "authors": [
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Yuehao Wang"
                    },
                    {
                        "name": "Chenxin Li"
                    },
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "Wuyang Li"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "CVPR 2025; Project Page: https://flexgs.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14522v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14522v3",
                "updated": "2025-06-05T06:39:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    6,
                    39,
                    10,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-20T07:39:00Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    39,
                    0,
                    6,
                    110,
                    0
                ],
                "title": "Biased by Design: Leveraging Inherent AI Biases to Enhance Critical\n  Thinking of News Readers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased by Design: Leveraging Inherent AI Biases to Enhance Critical\n  Thinking of News Readers"
                },
                "summary": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection."
                },
                "authors": [
                    {
                        "name": "Liudmila Zavolokina"
                    },
                    {
                        "name": "Kilian Sprenkamp"
                    },
                    {
                        "name": "Zoya Katashinskaya"
                    },
                    {
                        "name": "Daniel Gordon Jones"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gordon Jones"
                },
                "author": "Daniel Gordon Jones",
                "arxiv_comment": "European Conference on Information Systems (ECIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14522v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14522v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04172v1",
                "updated": "2025-06-04T17:15:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    15,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:15:19Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    15,
                    19,
                    2,
                    155,
                    0
                ],
                "title": "Does Prompt Design Impact Quality of Data Imputation by LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Prompt Design Impact Quality of Data Imputation by LLMs?"
                },
                "summary": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation."
                },
                "authors": [
                    {
                        "name": "Shreenidhi Srinivasan"
                    },
                    {
                        "name": "Lydia Manikonda"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Manikonda"
                },
                "author": "Lydia Manikonda",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04171v1",
                "updated": "2025-06-04T17:12:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    12,
                    37,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:12:37Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    12,
                    37,
                    2,
                    155,
                    0
                ],
                "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard\n  Constraints"
                },
                "summary": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential."
                },
                "authors": [
                    {
                        "name": "Utkarsh Utkarsh"
                    },
                    {
                        "name": "Pengfei Cai"
                    },
                    {
                        "name": "Alan Edelman"
                    },
                    {
                        "name": "Rafael Gomez-Bombarelli"
                    },
                    {
                        "name": "Christopher Vincent Rackauckas"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Vincent Rackauckas"
                },
                "author": "Christopher Vincent Rackauckas",
                "arxiv_comment": "27 pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16748v2",
                "updated": "2025-06-04T17:05:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    5,
                    12,
                    2,
                    155,
                    0
                ],
                "published": "2025-01-28T06:58:25Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    58,
                    25,
                    1,
                    28,
                    0
                ],
                "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04167v1",
                "updated": "2025-06-04T17:04:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    48,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:04:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity\n  on Human-LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity\n  on Human-LLM Collaboration"
                },
                "summary": "AI-based interactive assistants are advancing human-augmenting technology,\nyet their effects on users' mental and physiological states remain\nunder-explored. We address this gap by analyzing how Copilot for Microsoft\nWord, a LLM-based assistant, impacts users. Using tasks ranging from objective\n(SAT reading comprehension) to subjective (personal reflection), and with\nmeasurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we\nmeasure Copilot's effects on users. We also evaluate users' performance with\nand without Copilot across tasks. In objective tasks, participants reported a\nreduction of workload and an increase in enjoyment, which was paired with\nobjective performance increases. Participants reported reduced workload and\nincreased enjoyment with no change in performance in a creative poetry writing\ntask. However, no benefits due to Copilot use were reported in a highly\nsubjective self-reflection task. Although no physiological changes were\nrecorded due to Copilot use, task-dependent differences in prefrontal cortex\nactivation offer complementary insights into the cognitive processes associated\nwith successful and unsuccessful human-AI collaboration. These findings suggest\nthat AI assistants' effectiveness varies with task type-particularly showing\ndecreased usefulness in tasks that engage episodic memory-and presents a\nbrain-network based hypothesis of human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based interactive assistants are advancing human-augmenting technology,\nyet their effects on users' mental and physiological states remain\nunder-explored. We address this gap by analyzing how Copilot for Microsoft\nWord, a LLM-based assistant, impacts users. Using tasks ranging from objective\n(SAT reading comprehension) to subjective (personal reflection), and with\nmeasurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we\nmeasure Copilot's effects on users. We also evaluate users' performance with\nand without Copilot across tasks. In objective tasks, participants reported a\nreduction of workload and an increase in enjoyment, which was paired with\nobjective performance increases. Participants reported reduced workload and\nincreased enjoyment with no change in performance in a creative poetry writing\ntask. However, no benefits due to Copilot use were reported in a highly\nsubjective self-reflection task. Although no physiological changes were\nrecorded due to Copilot use, task-dependent differences in prefrontal cortex\nactivation offer complementary insights into the cognitive processes associated\nwith successful and unsuccessful human-AI collaboration. These findings suggest\nthat AI assistants' effectiveness varies with task type-particularly showing\ndecreased usefulness in tasks that engage episodic memory-and presents a\nbrain-network based hypothesis of human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Matthew Russell"
                    },
                    {
                        "name": "Aman Shah"
                    },
                    {
                        "name": "Giles Blaney"
                    },
                    {
                        "name": "Judith Amores"
                    },
                    {
                        "name": "Mary Czerwinski"
                    },
                    {
                        "name": "Robert J. K. Jacob"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. K. Jacob"
                },
                "author": "Robert J. K. Jacob",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04166v1",
                "updated": "2025-06-04T17:04:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    34,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:04:34Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    34,
                    2,
                    155,
                    0
                ],
                "title": "N$^2$: A Unified Python Package and Test Bench for Nearest\n  Neighbor-Based Matrix Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N$^2$: A Unified Python Package and Test Bench for Nearest\n  Neighbor-Based Matrix Completion"
                },
                "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings."
                },
                "authors": [
                    {
                        "name": "Caleb Chin"
                    },
                    {
                        "name": "Aashish Khubchandani"
                    },
                    {
                        "name": "Harshvardhan Maskara"
                    },
                    {
                        "name": "Kyuseong Choi"
                    },
                    {
                        "name": "Jacob Feitelberg"
                    },
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Manit Paul"
                    },
                    {
                        "name": "Tathagata Sadhukhan"
                    },
                    {
                        "name": "Anish Agarwal"
                    },
                    {
                        "name": "Raaz Dwivedi"
                    }
                ],
                "author_detail": {
                    "name": "Raaz Dwivedi"
                },
                "author": "Raaz Dwivedi",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04161v1",
                "updated": "2025-06-04T17:00:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    0,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:00:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    0,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing"
                },
                "summary": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%."
                },
                "authors": [
                    {
                        "name": "Parsa Alian"
                    },
                    {
                        "name": "Martin Tang"
                    },
                    {
                        "name": "Ali Mesbah"
                    }
                ],
                "author_detail": {
                    "name": "Ali Mesbah"
                },
                "author": "Ali Mesbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00335v2",
                "updated": "2025-06-04T17:00:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    0,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-31T01:23:39Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    1,
                    23,
                    39,
                    5,
                    151,
                    0
                ],
                "title": "Recover Experimental Data with Selection Bias using Counterfactual Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recover Experimental Data with Selection Bias using Counterfactual Logic"
                },
                "summary": "Selection bias, arising from the systematic inclusion or exclusion of certain\nsamples, poses a significant challenge to the validity of causal inference.\nWhile Bareinboim et al. introduced methods for recovering unbiased\nobservational and interventional distributions from biased data using partial\nexternal information, the complexity of the backdoor adjustment and the\nmethod's strong reliance on observational data limit its applicability in many\npractical settings. In this paper, we formally discover the recoverability of\n$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly\nconstructing counterfactual worlds via Structural Causal Models (SCMs), we\nanalyze how selection mechanisms in the observational world propagate to the\ncounterfactual domain. We derive a complete set of graphical and theoretical\ncriteria to determine that the experimental distribution remain unaffected by\nselection bias. Furthermore, we propose principled methods for leveraging\npartially unbiased observational data to recover $P(Y^*_{x^*})$ from biased\nexperimental datasets. Simulation studies replicating realistic research\nscenarios demonstrate the practical utility of our approach, offering concrete\nguidance for mitigating selection bias in applied causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selection bias, arising from the systematic inclusion or exclusion of certain\nsamples, poses a significant challenge to the validity of causal inference.\nWhile Bareinboim et al. introduced methods for recovering unbiased\nobservational and interventional distributions from biased data using partial\nexternal information, the complexity of the backdoor adjustment and the\nmethod's strong reliance on observational data limit its applicability in many\npractical settings. In this paper, we formally discover the recoverability of\n$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly\nconstructing counterfactual worlds via Structural Causal Models (SCMs), we\nanalyze how selection mechanisms in the observational world propagate to the\ncounterfactual domain. We derive a complete set of graphical and theoretical\ncriteria to determine that the experimental distribution remain unaffected by\nselection bias. Furthermore, we propose principled methods for leveraging\npartially unbiased observational data to recover $P(Y^*_{x^*})$ from biased\nexperimental datasets. Simulation studies replicating realistic research\nscenarios demonstrate the practical utility of our approach, offering concrete\nguidance for mitigating selection bias in applied causal inference."
                },
                "authors": [
                    {
                        "name": "Jingyang He"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02101v2",
                "updated": "2025-06-04T16:55:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    55,
                    35,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-03T22:36:22Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    22,
                    36,
                    22,
                    0,
                    62,
                    0
                ],
                "title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion\n  Models for Domain-Generalized Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Diffusion Detector: Mining Robust Features from Diffusion\n  Models for Domain-Generalized Detection"
                },
                "summary": "Domain generalization (DG) for object detection aims to enhance detectors'\nperformance in unseen scenarios. This task remains challenging due to complex\nvariations in real-world applications. Recently, diffusion models have\ndemonstrated remarkable capabilities in diverse scene generation, which\ninspires us to explore their potential for improving DG tasks. Instead of\ngenerating images, our method extracts multi-step intermediate features during\nthe diffusion process to obtain domain-invariant features for generalized\ndetection. Furthermore, we propose an efficient knowledge transfer framework\nthat enables detectors to inherit the generalization capabilities of diffusion\nmodels through feature and object-level alignment, without increasing inference\ntime. We conduct extensive experiments on six challenging DG benchmarks. The\nresults demonstrate that our method achieves substantial improvements of 14.0%\nmAP over existing DG approaches across different domains and corruption types.\nNotably, our method even outperforms most domain adaptation methods without\naccessing any target domain data. Moreover, the diffusion-guided detectors show\nconsistent improvements of 15.9% mAP on average compared to the baseline. Our\nwork aims to present an effective approach for domain-generalized detection and\nprovide potential insights for robust visual recognition in real-world\nscenarios. The code is available at\nhttps://github.com/heboyong/Generalized-Diffusion-Detector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalization (DG) for object detection aims to enhance detectors'\nperformance in unseen scenarios. This task remains challenging due to complex\nvariations in real-world applications. Recently, diffusion models have\ndemonstrated remarkable capabilities in diverse scene generation, which\ninspires us to explore their potential for improving DG tasks. Instead of\ngenerating images, our method extracts multi-step intermediate features during\nthe diffusion process to obtain domain-invariant features for generalized\ndetection. Furthermore, we propose an efficient knowledge transfer framework\nthat enables detectors to inherit the generalization capabilities of diffusion\nmodels through feature and object-level alignment, without increasing inference\ntime. We conduct extensive experiments on six challenging DG benchmarks. The\nresults demonstrate that our method achieves substantial improvements of 14.0%\nmAP over existing DG approaches across different domains and corruption types.\nNotably, our method even outperforms most domain adaptation methods without\naccessing any target domain data. Moreover, the diffusion-guided detectors show\nconsistent improvements of 15.9% mAP on average compared to the baseline. Our\nwork aims to present an effective approach for domain-generalized detection and\nprovide potential insights for robust visual recognition in real-world\nscenarios. The code is available at\nhttps://github.com/heboyong/Generalized-Diffusion-Detector."
                },
                "authors": [
                    {
                        "name": "Boyong He"
                    },
                    {
                        "name": "Yuxiang Ji"
                    },
                    {
                        "name": "Qianwen Ye"
                    },
                    {
                        "name": "Zhuoyue Tan"
                    },
                    {
                        "name": "Liaoni Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liaoni Wu"
                },
                "author": "Liaoni Wu",
                "arxiv_comment": "CVPR2025 camera-ready version with supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04156v1",
                "updated": "2025-06-04T16:55:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    55,
                    8,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:55:08Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    55,
                    8,
                    2,
                    155,
                    0
                ],
                "title": "A Dataset for Addressing Patient's Information Needs related to Clinical\n  Course of Hospitalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset for Addressing Patient's Information Needs related to Clinical\n  Course of Hospitalization"
                },
                "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts."
                },
                "authors": [
                    {
                        "name": "Sarvesh Soni"
                    },
                    {
                        "name": "Dina Demner-Fushman"
                    }
                ],
                "author_detail": {
                    "name": "Dina Demner-Fushman"
                },
                "author": "Dina Demner-Fushman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14748v2",
                "updated": "2025-06-04T16:49:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    49,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-20T17:19:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    19,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of Topic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of Topic Models"
                },
                "summary": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints."
                },
                "authors": [
                    {
                        "name": "Zongxia Li"
                    },
                    {
                        "name": "Lorena Calvo-Bartolom"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Alden Dima"
                    },
                    {
                        "name": "Juan Francisco Fung"
                    },
                    {
                        "name": "Jordan Boyd-Graber"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Boyd-Graber"
                },
                "author": "Jordan Boyd-Graber",
                "arxiv_comment": "22 Pages. LLM for Data Exploration and content analysis, Topic\n  Models. 63rd Annual Meeting of the Association for Computational Linguistics\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13656v2",
                "updated": "2025-06-04T16:39:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    39,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-19T12:07:53Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    7,
                    53,
                    2,
                    50,
                    0
                ],
                "title": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models"
                },
                "summary": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis."
                },
                "authors": [
                    {
                        "name": "Liyang He"
                    },
                    {
                        "name": "Chenglong Liu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Shulan Ruan"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04142v1",
                "updated": "2025-06-04T16:33:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    33,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:33:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    33,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis"
                },
                "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation"
                },
                "authors": [
                    {
                        "name": "Kejian Zhu"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04141v1",
                "updated": "2025-06-04T16:33:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    33,
                    41,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:33:41Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    33,
                    41,
                    2,
                    155,
                    0
                ],
                "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos"
                },
                "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Kejian Zhu"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Hongbang Yuan"
                    },
                    {
                        "name": "Jiachun Li"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Project Page: https://mmr-v.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04140v1",
                "updated": "2025-06-04T16:31:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    31,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:31:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    31,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Quantifying Query Fairness Under Unawareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Query Fairness Under Unawareness"
                },
                "summary": "Traditional ranking algorithms are designed to retrieve the most relevant\nitems for a user's query, but they often inherit biases from data that can\nunfairly disadvantage vulnerable groups. Fairness in information access systems\n(IAS) is typically assessed by comparing the distribution of groups in a\nranking to a target distribution, such as the overall group distribution in the\ndataset. These fairness metrics depend on knowing the true group labels for\neach item. However, when groups are defined by demographic or sensitive\nattributes, these labels are often unknown, leading to a setting known as\n\"fairness under unawareness\". To address this, group membership can be inferred\nusing machine-learned classifiers, and group prevalence is estimated by\ncounting the predicted labels. Unfortunately, such an estimation is known to be\nunreliable under dataset shift, compromising the accuracy of fairness\nevaluations. In this paper, we introduce a robust fairness estimator based on\nquantification that effectively handles multiple sensitive attributes beyond\nbinary classifications. Our method outperforms existing baselines across\nvarious sensitive attributes and, to the best of our knowledge, is the first to\nestablish a reliable protocol for measuring fairness under unawareness across\nmultiple queries and groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional ranking algorithms are designed to retrieve the most relevant\nitems for a user's query, but they often inherit biases from data that can\nunfairly disadvantage vulnerable groups. Fairness in information access systems\n(IAS) is typically assessed by comparing the distribution of groups in a\nranking to a target distribution, such as the overall group distribution in the\ndataset. These fairness metrics depend on knowing the true group labels for\neach item. However, when groups are defined by demographic or sensitive\nattributes, these labels are often unknown, leading to a setting known as\n\"fairness under unawareness\". To address this, group membership can be inferred\nusing machine-learned classifiers, and group prevalence is estimated by\ncounting the predicted labels. Unfortunately, such an estimation is known to be\nunreliable under dataset shift, compromising the accuracy of fairness\nevaluations. In this paper, we introduce a robust fairness estimator based on\nquantification that effectively handles multiple sensitive attributes beyond\nbinary classifications. Our method outperforms existing baselines across\nvarious sensitive attributes and, to the best of our knowledge, is the first to\nestablish a reliable protocol for measuring fairness under unawareness across\nmultiple queries and groups."
                },
                "authors": [
                    {
                        "name": "Thomas Jaenich"
                    },
                    {
                        "name": "Alejandro Moreo"
                    },
                    {
                        "name": "Alessandro Fabris"
                    },
                    {
                        "name": "Graham McDonald"
                    },
                    {
                        "name": "Andrea Esuli"
                    },
                    {
                        "name": "Iadh Ounis"
                    },
                    {
                        "name": "Fabrizio Sebastiani"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Sebastiani"
                },
                "author": "Fabrizio Sebastiani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04139v1",
                "updated": "2025-06-04T16:31:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    31,
                    37,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:31:37Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    31,
                    37,
                    2,
                    155,
                    0
                ],
                "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in\n  Low-Resource Flemish?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in\n  Low-Resource Flemish?"
                },
                "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research."
                },
                "authors": [
                    {
                        "name": "Ratna Kandala"
                    },
                    {
                        "name": "Katie Hoemann"
                    }
                ],
                "author_detail": {
                    "name": "Katie Hoemann"
                },
                "author": "Katie Hoemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10981v2",
                "updated": "2025-06-04T16:27:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    27,
                    57,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-16T08:28:57Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    28,
                    57,
                    4,
                    136,
                    0
                ],
                "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A\n  Perspective of Probability Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A\n  Perspective of Probability Theory"
                },
                "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a probabilistic method to efficiently predict scaling\nperformance and identify the best prompting strategy under large sampling\ntimes, eliminating the need for resource-intensive inference processes in\npractical applications. Furthermore, we introduce two ways derived from our\ntheoretical analysis to significantly improve the scaling performance. We hope\nthat our research can promote to re-examine the role of complicated prompting,\nunleash the potential of simple prompting strategies, and provide new insights\nfor enhancing test-time scaling performance. Code is available at\nhttps://github.com/MraDonkey/rethinking_prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a probabilistic method to efficiently predict scaling\nperformance and identify the best prompting strategy under large sampling\ntimes, eliminating the need for resource-intensive inference processes in\npractical applications. Furthermore, we introduce two ways derived from our\ntheoretical analysis to significantly improve the scaling performance. We hope\nthat our research can promote to re-examine the role of complicated prompting,\nunleash the potential of simple prompting strategies, and provide new insights\nfor enhancing test-time scaling performance. Code is available at\nhttps://github.com/MraDonkey/rethinking_prompting."
                },
                "authors": [
                    {
                        "name": "Yexiang Liu"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Zhi Fang"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "ACL 2025 Main, 33 pages, 51 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01747v2",
                "updated": "2025-06-04T16:26:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    58,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-04T02:08:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSaur: Large Language Agents Beyond Predefined Actions"
                },
                "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Nedim Lipka"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04133v1",
                "updated": "2025-06-04T16:26:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    11,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:26:11Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    11,
                    2,
                    155,
                    0
                ],
                "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems"
                },
                "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Emmanouilidis"
                },
                "author": "Christos Emmanouilidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23899v2",
                "updated": "2025-06-04T16:23:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    23,
                    54,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-31T09:48:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    48,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset"
                },
                "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code are available at\nhttps://github.com/RubriksCube/rubriks_cube.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code are available at\nhttps://github.com/RubriksCube/rubriks_cube."
                },
                "authors": [
                    {
                        "name": "Diana Galvan-Sosa"
                    },
                    {
                        "name": "Gabrielle Gaudeau"
                    },
                    {
                        "name": "Pride Kavumba"
                    },
                    {
                        "name": "Yunmeng Li"
                    },
                    {
                        "name": "Hongyi gu"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Paula Buttery"
                    }
                ],
                "author_detail": {
                    "name": "Paula Buttery"
                },
                "author": "Paula Buttery",
                "arxiv_comment": "10 main pages (24 appendix pages), 9 figures, accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19176v2",
                "updated": "2025-06-04T16:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    16,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-25T14:48:49Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    14,
                    48,
                    49,
                    6,
                    145,
                    0
                ],
                "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."
                },
                "authors": [
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Xun Deng"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04118v1",
                "updated": "2025-06-04T16:12:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    12,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:12:26Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    12,
                    26,
                    2,
                    155,
                    0
                ],
                "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs"
                },
                "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI ."
                },
                "authors": [
                    {
                        "name": "Jonathan Geuter"
                    },
                    {
                        "name": "Youssef Mroueh"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    }
                ],
                "author_detail": {
                    "name": "David Alvarez-Melis"
                },
                "author": "David Alvarez-Melis",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04116v1",
                "updated": "2025-06-04T16:09:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    9,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:09:19Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    9,
                    19,
                    2,
                    155,
                    0
                ],
                "title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency\n  Enhancement Framework for 4D MRI imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency\n  Enhancement Framework for 4D MRI imaging"
                },
                "summary": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency."
                },
                "authors": [
                    {
                        "name": "Xuanru Zhou"
                    },
                    {
                        "name": "Jiarun Liu"
                    },
                    {
                        "name": "Shoujun Yu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Shanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Wang"
                },
                "author": "Shanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v4",
                "updated": "2025-06-04T16:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    2,
                    28,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate significant persuasive capabilities\nin one-on-one interactions, but their influence within social networks, where\ninterconnected users and complex opinion dynamics pose unique challenges,\nremains underexplored. This paper addresses the research question: \\emph{Can\nLLMs generate meaningful content that maximizes user engagement on social\nnetworks?}\n  To answer this, we propose a pipeline using reinforcement learning with\nsimulated feedback, where the network's response to LLM-generated content\n(i.e., the reward) is simulated through a formal engagement model. This\napproach bypasses the temporal cost and complexity of live experiments,\nenabling an efficient feedback loop between the LLM and the network under\nstudy. It also allows to control over endogenous factors such as the LLM's\nposition within the social network and the distribution of opinions on a given\ntopic. Our approach is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. Such flexibility makes it suitable for\nmore complex engagement tasks and interventions in computational social\nscience.\n  Using our framework, we analyze the performance of LLMs in generating social\nengagement under different conditions, showcasing their full potential in this\ntask. The experimental code is publicly available at\nhttps://github.com/mminici/Engagement-Driven-Content-Generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate significant persuasive capabilities\nin one-on-one interactions, but their influence within social networks, where\ninterconnected users and complex opinion dynamics pose unique challenges,\nremains underexplored. This paper addresses the research question: \\emph{Can\nLLMs generate meaningful content that maximizes user engagement on social\nnetworks?}\n  To answer this, we propose a pipeline using reinforcement learning with\nsimulated feedback, where the network's response to LLM-generated content\n(i.e., the reward) is simulated through a formal engagement model. This\napproach bypasses the temporal cost and complexity of live experiments,\nenabling an efficient feedback loop between the LLM and the network under\nstudy. It also allows to control over endogenous factors such as the LLM's\nposition within the social network and the distribution of opinions on a given\ntopic. Our approach is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. Such flexibility makes it suitable for\nmore complex engagement tasks and interventions in computational social\nscience.\n  Using our framework, we analyze the performance of LLMs in generating social\nengagement under different conditions, showcasing their full potential in this\ntask. The experimental code is publicly available at\nhttps://github.com/mminici/Engagement-Driven-Content-Generation."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04108v2",
                "updated": "2025-06-05T05:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    39,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T16:01:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    1,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Rectified Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Sparse Attention"
                },
                "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Yizhao Gao"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05239v2",
                "updated": "2025-06-04T15:59:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    59,
                    6,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-07T23:28:23Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    23,
                    28,
                    23,
                    3,
                    312,
                    0
                ],
                "title": "ZipNN: Lossless Compression for AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipNN: Lossless Compression for AI Models"
                },
                "summary": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face."
                },
                "authors": [
                    {
                        "name": "Moshik Hershcovitch"
                    },
                    {
                        "name": "Andrew Wood"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Guy Girmonsky"
                    },
                    {
                        "name": "Roy Leibovitz"
                    },
                    {
                        "name": "Ilias Ennmouri"
                    },
                    {
                        "name": "Michal Malka"
                    },
                    {
                        "name": "Peter Chin"
                    },
                    {
                        "name": "Swaminathan Sundararaman"
                    },
                    {
                        "name": "Danny Harnik"
                    }
                ],
                "author_detail": {
                    "name": "Danny Harnik"
                },
                "author": "Danny Harnik",
                "arxiv_comment": "IEEE Cloud. arXiv admin note: text overlap with arXiv:2404.15198",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04089v1",
                "updated": "2025-06-04T15:47:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    47,
                    7,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:47:07Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    47,
                    7,
                    2,
                    155,
                    0
                ],
                "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment"
                },
                "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ivanova"
                    },
                    {
                        "name": "Eva Bakaeva"
                    },
                    {
                        "name": "Zoya Volovikova"
                    },
                    {
                        "name": "Alexey K. Kovalev"
                    },
                    {
                        "name": "Aleksandr I. Panov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr I. Panov"
                },
                "author": "Aleksandr I. Panov",
                "arxiv_comment": "ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04088v1",
                "updated": "2025-06-04T15:46:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    46,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:46:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    46,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "Multimodal Tabular Reasoning with Privileged Structured Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Tabular Reasoning with Privileged Structured Information"
                },
                "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets."
                },
                "authors": [
                    {
                        "name": "Jun-Peng Jiang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04082v1",
                "updated": "2025-06-04T15:44:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    44,
                    32,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:44:32Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    44,
                    32,
                    2,
                    155,
                    0
                ],
                "title": "Adaptive tuning of Hamiltonian Monte Carlo methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive tuning of Hamiltonian Monte Carlo methods"
                },
                "summary": "With the recently increased interest in probabilistic models, the efficiency\nof an underlying sampler becomes a crucial consideration. A Hamiltonian Monte\nCarlo (HMC) sampler is one popular option for models of this kind. Performance\nof HMC, however, strongly relies on a choice of parameters associated with an\nintegration method for Hamiltonian equations, which up to date remains mainly\nheuristic or introduce time complexity. We propose a novel computationally\ninexpensive and flexible approach (we call it Adaptive Tuning or ATune) that,\nby analyzing the data generated during a burning stage of an HMC simulation,\ndetects a system specific splitting integrator with a set of reliable HMC\nhyperparameters, including their credible randomization intervals, to be\nreadily used in a production simulation. The method automatically eliminates\nthose values of simulation parameters which could cause undesired extreme\nscenarios, such as resonance artifacts, low accuracy or poor sampling. The new\napproach is implemented in the in-house software package \\textsf{HaiCS}, with\nno computational overheads introduced in a production simulation, and can be\neasily incorporated in any package for Bayesian inference with HMC. The tests\non popular statistical models using original HMC and generalized Hamiltonian\nMonte Carlo (GHMC) reveal the superiority of adaptively tuned methods in terms\nof stability, performance and accuracy over conventional HMC tuned\nheuristically and coupled with the well-established integrators. We also claim\nthat the generalized formulation of HMC, i.e. GHMC, is preferable for achieving\nhigh sampling performance. The efficiency of the new methodology is assessed in\ncomparison with state-of-the-art samplers, e.g. the No-U-Turn-Sampler (NUTS),\nin real-world applications, such as endocrine therapy resistance in cancer,\nmodeling of cell-cell adhesion dynamics and influenza epidemic outbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recently increased interest in probabilistic models, the efficiency\nof an underlying sampler becomes a crucial consideration. A Hamiltonian Monte\nCarlo (HMC) sampler is one popular option for models of this kind. Performance\nof HMC, however, strongly relies on a choice of parameters associated with an\nintegration method for Hamiltonian equations, which up to date remains mainly\nheuristic or introduce time complexity. We propose a novel computationally\ninexpensive and flexible approach (we call it Adaptive Tuning or ATune) that,\nby analyzing the data generated during a burning stage of an HMC simulation,\ndetects a system specific splitting integrator with a set of reliable HMC\nhyperparameters, including their credible randomization intervals, to be\nreadily used in a production simulation. The method automatically eliminates\nthose values of simulation parameters which could cause undesired extreme\nscenarios, such as resonance artifacts, low accuracy or poor sampling. The new\napproach is implemented in the in-house software package \\textsf{HaiCS}, with\nno computational overheads introduced in a production simulation, and can be\neasily incorporated in any package for Bayesian inference with HMC. The tests\non popular statistical models using original HMC and generalized Hamiltonian\nMonte Carlo (GHMC) reveal the superiority of adaptively tuned methods in terms\nof stability, performance and accuracy over conventional HMC tuned\nheuristically and coupled with the well-established integrators. We also claim\nthat the generalized formulation of HMC, i.e. GHMC, is preferable for achieving\nhigh sampling performance. The efficiency of the new methodology is assessed in\ncomparison with state-of-the-art samplers, e.g. the No-U-Turn-Sampler (NUTS),\nin real-world applications, such as endocrine therapy resistance in cancer,\nmodeling of cell-cell adhesion dynamics and influenza epidemic outbreak."
                },
                "authors": [
                    {
                        "name": "Elena Akhmatskaya"
                    },
                    {
                        "name": "Lorenzo Nagar"
                    },
                    {
                        "name": "Jose Antonio Carrillo"
                    },
                    {
                        "name": "Leonardo Gavira Balmacz"
                    },
                    {
                        "name": "Hristo Inouzhe"
                    },
                    {
                        "name": "Martn Parga Pazos"
                    },
                    {
                        "name": "Mara Xos Rodrguez lvarez"
                    }
                ],
                "author_detail": {
                    "name": "Mara Xos Rodrguez lvarez"
                },
                "author": "Mara Xos Rodrguez lvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04079v1",
                "updated": "2025-06-04T15:43:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    31,
                    2,
                    155,
                    0
                ],
                "title": "EuroLLM-9B: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuroLLM-9B: Technical Report"
                },
                "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset."
                },
                "authors": [
                    {
                        "name": "Pedro Henrique Martins"
                    },
                    {
                        "name": "Joo Alves"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "Mateusz Klimaszewski"
                    },
                    {
                        "name": "Duarte M. Alves"
                    },
                    {
                        "name": "Jos Pombal"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Franois Yvon"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Jos G. C. de Souza"
                    },
                    {
                        "name": "Alexandra Birch"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "56 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09019v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09019v3",
                "updated": "2025-06-04T15:43:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    27,
                    2,
                    155,
                    0
                ],
                "published": "2024-09-13T17:47:20Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    17,
                    47,
                    20,
                    4,
                    257,
                    0
                ],
                "title": "Nonequilibrium Phenomenology of Identified Particle Spectra in Heavy-Ion\n  Collisions at LHC Energies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium Phenomenology of Identified Particle Spectra in Heavy-Ion\n  Collisions at LHC Energies"
                },
                "summary": "We employ the Zubarev approach of the non-equilibrium statistical operator to\ninvestigate the enhancement of the low-$p_T$ region of pion spectra,\nintroducing an effective pion chemical potential to describe the overpopulation\nof low-energy pion states. We test a corresponding freeze-out approach by\nanalyzing the transverse-momentum spectra of identified particles measured\nrecently with high precision by the ALICE Collaboration in Pb+Pb collisions at\nCERN LHC. A blast-wave model and a blast-wave-based particle generator, coupled\nto a hadronic transport model, are utilized. Bayesian inference methods are\napplied to extract the most probable sets of thermodynamic parameters at the\nchemical freeze-out hypersurface. Both models for the overpopulated pion\nstates, the hadronic transport model and the thermal model with a nonzero pion\nchemical potential, provide a satisfactory description of the observed pion\nspectra. However, both approaches contain approximations which can be improved\nwithin a systematic nonequilibrium approach. We demonstrate that the\nintroduction of a nonequilibrium pion chemical potential offers an efficient\nalternative to the conventional explanation of the low-$p_T$ enhancement,\ntypically attributed to resonance decays with subsequent thermalization. A\nsimilar discussion holds also for the kaon spectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ the Zubarev approach of the non-equilibrium statistical operator to\ninvestigate the enhancement of the low-$p_T$ region of pion spectra,\nintroducing an effective pion chemical potential to describe the overpopulation\nof low-energy pion states. We test a corresponding freeze-out approach by\nanalyzing the transverse-momentum spectra of identified particles measured\nrecently with high precision by the ALICE Collaboration in Pb+Pb collisions at\nCERN LHC. A blast-wave model and a blast-wave-based particle generator, coupled\nto a hadronic transport model, are utilized. Bayesian inference methods are\napplied to extract the most probable sets of thermodynamic parameters at the\nchemical freeze-out hypersurface. Both models for the overpopulated pion\nstates, the hadronic transport model and the thermal model with a nonzero pion\nchemical potential, provide a satisfactory description of the observed pion\nspectra. However, both approaches contain approximations which can be improved\nwithin a systematic nonequilibrium approach. We demonstrate that the\nintroduction of a nonequilibrium pion chemical potential offers an efficient\nalternative to the conventional explanation of the low-$p_T$ enhancement,\ntypically attributed to resonance decays with subsequent thermalization. A\nsimilar discussion holds also for the kaon spectra."
                },
                "authors": [
                    {
                        "name": "Oleksandr Vitiuk"
                    },
                    {
                        "name": "David Blaschke"
                    },
                    {
                        "name": "Benjamin Dnigus"
                    },
                    {
                        "name": "Gerd Rpke"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Rpke"
                },
                "author": "Gerd Rpke",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09019v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09019v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04078v1",
                "updated": "2025-06-04T15:43:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    14,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:43:14Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    14,
                    2,
                    155,
                    0
                ],
                "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation"
                },
                "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Binze Hu"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Chenhao Huang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04077v1",
                "updated": "2025-06-04T15:42:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    42,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:42:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    42,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions"
                },
                "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information."
                },
                "authors": [
                    {
                        "name": "Chung-Chun Wang"
                    },
                    {
                        "name": "Jhen-Ke Lin"
                    },
                    {
                        "name": "Hao-Chien Lu"
                    },
                    {
                        "name": "Hong-Yun Lin"
                    },
                    {
                        "name": "Berlin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Berlin Chen"
                },
                "author": "Berlin Chen",
                "arxiv_comment": "submitted to the ISCA SLaTE-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04076v1",
                "updated": "2025-06-04T15:41:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    41,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:41:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    41,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End\n  Verbatim Transcription Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End\n  Verbatim Transcription Systems"
                },
                "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription."
                },
                "authors": [
                    {
                        "name": "Jhen-Ke Lin"
                    },
                    {
                        "name": "Hao-Chien Lu"
                    },
                    {
                        "name": "Chung-Chun Wang"
                    },
                    {
                        "name": "Hong-Yun Lin"
                    },
                    {
                        "name": "Berlin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Berlin Chen"
                },
                "author": "Berlin Chen",
                "arxiv_comment": "submitted to the ISCA SLaTE-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04072v1",
                "updated": "2025-06-04T15:38:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    38,
                    21,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:38:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    38,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Controlling Difficulty of Generated Text for AI-Assisted Language\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Difficulty of Generated Text for AI-Assisted Language\n  Learning"
                },
                "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset."
                },
                "authors": [
                    {
                        "name": "Meiqing Jin"
                    },
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Submitted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14194v3",
                "updated": "2025-06-04T15:35:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    35,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-04-19T06:12:33Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    12,
                    33,
                    5,
                    109,
                    0
                ],
                "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models"
                },
                "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater."
                },
                "authors": [
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Yinfan Wang"
                    },
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Xingjian Wei"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04070v1",
                "updated": "2025-06-04T15:34:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    34,
                    33,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:34:33Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    34,
                    33,
                    2,
                    155,
                    0
                ],
                "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\n  Impaired via GRPO with LLM-as-Follower Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\n  Impaired via GRPO with LLM-as-Follower Reward"
                },
                "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17169v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17169v4",
                "updated": "2025-06-04T15:32:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    32,
                    37,
                    2,
                    155,
                    0
                ],
                "published": "2024-09-17T22:40:54Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    40,
                    54,
                    1,
                    261,
                    0
                ],
                "title": "REAL: Response Embedding-based Alignment for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAL: Response Embedding-based Alignment for LLMs"
                },
                "summary": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\n(DPO) rely on pairs of AI-generated responses ranked according to human\nannotation. The response pair annotation process might bring human bias.\nBuilding a correct preference dataset is the costly part of the alignment\npipeline. To improve annotation efficiency and quality in the LLMs alignment,\nwe propose REAL: Response Embedding-based Alignment for LLMs, a strategy for\nconstructing a high-quality training dataset that focuses on acquiring the less\nambiguous preference pairs for labeling out of a set of response candidates.\nOur selection process is based on the similarity of embedding responses\nindependently of prompts, which guarantees the selection process in an\noff-policy setting, avoiding adaptively measuring the similarity during the\ntraining. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF\nbenchmarks indicate that choosing dissimilar response pairs enhances the direct\nalignment of LLMs while reducing inherited labeling errors. The model aligned\nwith dissimilar response pairs obtained a better margin and win rate on the\ndialogue task. Our findings suggest that focusing on distinct pairs can reduce\nthe label error and improve LLM alignment efficiency, saving up to $65\\%$ of\nannotators' work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\n(DPO) rely on pairs of AI-generated responses ranked according to human\nannotation. The response pair annotation process might bring human bias.\nBuilding a correct preference dataset is the costly part of the alignment\npipeline. To improve annotation efficiency and quality in the LLMs alignment,\nwe propose REAL: Response Embedding-based Alignment for LLMs, a strategy for\nconstructing a high-quality training dataset that focuses on acquiring the less\nambiguous preference pairs for labeling out of a set of response candidates.\nOur selection process is based on the similarity of embedding responses\nindependently of prompts, which guarantees the selection process in an\noff-policy setting, avoiding adaptively measuring the similarity during the\ntraining. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF\nbenchmarks indicate that choosing dissimilar response pairs enhances the direct\nalignment of LLMs while reducing inherited labeling errors. The model aligned\nwith dissimilar response pairs obtained a better margin and win rate on the\ndialogue task. Our findings suggest that focusing on distinct pairs can reduce\nthe label error and improve LLM alignment efficiency, saving up to $65\\%$ of\nannotators' work."
                },
                "authors": [
                    {
                        "name": "Honggen Zhang"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Igor Molybog"
                    },
                    {
                        "name": "June Zhang"
                    }
                ],
                "author_detail": {
                    "name": "June Zhang"
                },
                "author": "June Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17169v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17169v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04065v1",
                "updated": "2025-06-04T15:31:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    31,
                    46,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:31:46Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    31,
                    46,
                    2,
                    155,
                    0
                ],
                "title": "Progressive Mastery: Customized Curriculum Learning with Guided\n  Prompting for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Mastery: Customized Curriculum Learning with Guided\n  Prompting for Mathematical Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance."
                },
                "authors": [
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Qi Qian"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Zisu Huang"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "LI Miao"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Zhenghua Wang"
                    },
                    {
                        "name": "Zhibo Xu"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Tianlong Li"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04063v1",
                "updated": "2025-06-04T15:26:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    26,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:26:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    26,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "Crowd-SFT: Crowdsourcing for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowd-SFT: Crowdsourcing for LLM Alignment"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation."
                },
                "authors": [
                    {
                        "name": "Alex Sotiropoulos"
                    },
                    {
                        "name": "Sulyab Thottungal Valapu"
                    },
                    {
                        "name": "Linus Lei"
                    },
                    {
                        "name": "Jared Coleman"
                    },
                    {
                        "name": "Bhaskar Krishnamachari"
                    }
                ],
                "author_detail": {
                    "name": "Bhaskar Krishnamachari"
                },
                "author": "Bhaskar Krishnamachari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12134v3",
                "updated": "2025-06-04T15:22:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    22,
                    3,
                    2,
                    155,
                    0
                ],
                "published": "2024-05-20T15:53:50Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    15,
                    53,
                    50,
                    0,
                    141,
                    0
                ],
                "title": "Two-dimensional signal-dependent parabolic-elliptic Keller-Segel system\n  and its mean-field derivation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional signal-dependent parabolic-elliptic Keller-Segel system\n  and its mean-field derivation"
                },
                "summary": "In this paper, the well-posedness of two-dimensional signal-dependent\nKeller-Segel system and its mean-field derivation from a interacting particle\nsystem on the whole space are investigated. The signal dependence effect is\nreflected by the fact that the diffusion coefficient in the particle system\ndepends non-linearly on the interactions between the individuals. Therefore,\nthe mathematical challenge in studying the well-posedness of this system lies\nin the possible degeneracy and the aggregation effect when the concentration of\nsignal becomes unbounded. The well-established method on bounded domains, to\nobtain the appropriate estimates for the signal concentration, is invalid for\nthe whole space case. Motivated by the entropy minimization method and Onofri's\ninequality, which has been successfully applied for the parabolic-parabolic\nKeller-Segel system, we establish a complete entropy estimate benefited from\nthe linear diffusion term, which plays an important role in obtaining the L^p\nestimates for the solution. Furthermore, the upper bound for the concentration\nof signal is obtained. Based on the estimates we obtained for the density of\nbacteria, the rigorous mean-field derivation is proved by introducing an\nintermediate particle system with a mollified interaction potential with\nlogarithmic scaling. By using this mollification, we obtain the convergence of\nthe particle trajectories in expectation, which implies the weak propagation of\nchaos. Additionally, under a regularity assumption of the initial data, we\ninfer higher regularity for the solutions, which allows us to use the relative\nentropy method to derive the strong L^1 convergence for the propagation of\nchaos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, the well-posedness of two-dimensional signal-dependent\nKeller-Segel system and its mean-field derivation from a interacting particle\nsystem on the whole space are investigated. The signal dependence effect is\nreflected by the fact that the diffusion coefficient in the particle system\ndepends non-linearly on the interactions between the individuals. Therefore,\nthe mathematical challenge in studying the well-posedness of this system lies\nin the possible degeneracy and the aggregation effect when the concentration of\nsignal becomes unbounded. The well-established method on bounded domains, to\nobtain the appropriate estimates for the signal concentration, is invalid for\nthe whole space case. Motivated by the entropy minimization method and Onofri's\ninequality, which has been successfully applied for the parabolic-parabolic\nKeller-Segel system, we establish a complete entropy estimate benefited from\nthe linear diffusion term, which plays an important role in obtaining the L^p\nestimates for the solution. Furthermore, the upper bound for the concentration\nof signal is obtained. Based on the estimates we obtained for the density of\nbacteria, the rigorous mean-field derivation is proved by introducing an\nintermediate particle system with a mollified interaction potential with\nlogarithmic scaling. By using this mollification, we obtain the convergence of\nthe particle trajectories in expectation, which implies the weak propagation of\nchaos. Additionally, under a regularity assumption of the initial data, we\ninfer higher regularity for the solutions, which allows us to use the relative\nentropy method to derive the strong L^1 convergence for the propagation of\nchaos."
                },
                "authors": [
                    {
                        "name": "Lukas Bol"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yue Li"
                    }
                ],
                "author_detail": {
                    "name": "Yue Li"
                },
                "author": "Yue Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08001v2",
                "updated": "2025-06-04T15:19:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    19,
                    15,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-11T22:48:49Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    22,
                    48,
                    49,
                    1,
                    42,
                    0
                ],
                "title": "Unveiling Client Privacy Leakage from Public Dataset Usage in Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Client Privacy Leakage from Public Dataset Usage in Federated\n  Distillation"
                },
                "summary": "Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems."
                },
                "authors": [
                    {
                        "name": "Haonan Shi"
                    },
                    {
                        "name": "Tu Ouyang"
                    },
                    {
                        "name": "An Wang"
                    }
                ],
                "author_detail": {
                    "name": "An Wang"
                },
                "author": "An Wang",
                "arxiv_comment": "To appear in Proceedings of Privacy Enhancing Technologies 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04054v1",
                "updated": "2025-06-04T15:19:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    19,
                    11,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:19:11Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    19,
                    11,
                    2,
                    155,
                    0
                ],
                "title": "Video Deblurring with Deconvolution and Aggregation Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Deblurring with Deconvolution and Aggregation Networks"
                },
                "summary": "In contrast to single-image deblurring, video deblurring has the advantage\nthat neighbor frames can be utilized to deblur a target frame. However,\nexisting video deblurring algorithms often fail to properly employ the neighbor\nframes, resulting in sub-optimal performance. In this paper, we propose a\ndeconvolution and aggregation network (DAN) for video deblurring that utilizes\nthe information of neighbor frames well. In DAN, both deconvolution and\naggregation strategies are achieved through three sub-networks: the\npreprocessing network (PPN) and the alignment-based deconvolution network\n(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for\nthe aggregation scheme. In the deconvolution part, blurry inputs are first\npreprocessed by the PPN with non-local operations. Then, the output frames from\nthe PPN are deblurred by the ABDN based on the frame alignment. In the FAN,\nthese deblurred frames from the deconvolution part are combined into a latent\nframe according to reliability maps which infer pixel-wise sharpness. The\nproper combination of three sub-networks can achieve favorable performance on\nvideo deblurring by using the neighbor frames suitably. In experiments, the\nproposed DAN was demonstrated to be superior to existing state-of-the-art\nmethods through both quantitative and qualitative evaluations on the public\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contrast to single-image deblurring, video deblurring has the advantage\nthat neighbor frames can be utilized to deblur a target frame. However,\nexisting video deblurring algorithms often fail to properly employ the neighbor\nframes, resulting in sub-optimal performance. In this paper, we propose a\ndeconvolution and aggregation network (DAN) for video deblurring that utilizes\nthe information of neighbor frames well. In DAN, both deconvolution and\naggregation strategies are achieved through three sub-networks: the\npreprocessing network (PPN) and the alignment-based deconvolution network\n(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for\nthe aggregation scheme. In the deconvolution part, blurry inputs are first\npreprocessed by the PPN with non-local operations. Then, the output frames from\nthe PPN are deblurred by the ABDN based on the frame alignment. In the FAN,\nthese deblurred frames from the deconvolution part are combined into a latent\nframe according to reliability maps which infer pixel-wise sharpness. The\nproper combination of three sub-networks can achieve favorable performance on\nvideo deblurring by using the neighbor frames suitably. In experiments, the\nproposed DAN was demonstrated to be superior to existing state-of-the-art\nmethods through both quantitative and qualitative evaluations on the public\ndatasets."
                },
                "authors": [
                    {
                        "name": "Giyong Choi"
                    },
                    {
                        "name": "HyunWook Park"
                    }
                ],
                "author_detail": {
                    "name": "HyunWook Park"
                },
                "author": "HyunWook Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04051v1",
                "updated": "2025-06-04T15:16:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    16,
                    21,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:16:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    16,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through\n  Capability-Aligned Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Accuracy, Less Talk (HALT): Reliable LLMs through\n  Capability-Aligned Finetuning"
                },
                "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning."
                },
                "authors": [
                    {
                        "name": "Tim Franzmeyer"
                    },
                    {
                        "name": "Archie Sravankumar"
                    },
                    {
                        "name": "Lijuan Liu"
                    },
                    {
                        "name": "Yuning Mao"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Sinong Wang"
                    },
                    {
                        "name": "Jakob N. Foerster"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Madian Khabsa"
                    }
                ],
                "author_detail": {
                    "name": "Madian Khabsa"
                },
                "author": "Madian Khabsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23276v2",
                "updated": "2025-06-04T15:16:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    16,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-29T09:24:00Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    24,
                    0,
                    3,
                    149,
                    0
                ],
                "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large\n  Language Models Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large\n  Language Models Text"
                },
                "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts."
                },
                "authors": [
                    {
                        "name": "Maged S. Al-Shaibani"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04050v1",
                "updated": "2025-06-04T15:15:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    15,
                    42,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:15:42Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    15,
                    42,
                    2,
                    155,
                    0
                ],
                "title": "Explainability-Based Token Replacement on LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability-Based Token Replacement on LLM-Generated Text"
                },
                "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    },
                    {
                        "name": "Daniel L. Oberski"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    }
                ],
                "author_detail": {
                    "name": "Ayoub Bagheri"
                },
                "author": "Ayoub Bagheri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v7",
                "updated": "2025-06-04T15:11:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    11,
                    0,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper, we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two\nnovel approaches to repair unwanted false mappings caused by Phase 2 text\npreprocessing. One is an ad hoc logic-based repair approach that employs an\nontology-specific check to find common words that cause false mappings. These\nwords are stored in a reserved word set and applied before the text\npreprocessing. By leveraging the power of large language models (LLMs), we also\npropose a post hoc LLM-based repair approach. This approach utilises the strong\nbackground knowledge provided by LLMs to repair non-existent and\ncounter-intuitive false mappings after the text preprocessing. It also\novercomes the tendency towards unstable true mappings by injecting the classic\ntext preprocessing pipeline via function calling. The experimental results show\nthat these two approaches can improve the matching correctness and the overall\nmatching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper, we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two\nnovel approaches to repair unwanted false mappings caused by Phase 2 text\npreprocessing. One is an ad hoc logic-based repair approach that employs an\nontology-specific check to find common words that cause false mappings. These\nwords are stored in a reserved word set and applied before the text\npreprocessing. By leveraging the power of large language models (LLMs), we also\npropose a post hoc LLM-based repair approach. This approach utilises the strong\nbackground knowledge provided by LLMs to repair non-existent and\ncounter-intuitive false mappings after the text preprocessing. It also\novercomes the tendency towards unstable true mappings by injecting the classic\ntext preprocessing pipeline via function calling. The experimental results show\nthat these two approaches can improve the matching correctness and the overall\nmatching performance."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "13 pages, 14 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04044v1",
                "updated": "2025-06-04T15:10:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    10,
                    9,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:10:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    10,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based\n  Unlearning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based\n  Unlearning for LLMs"
                },
                "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task."
                },
                "authors": [
                    {
                        "name": "Aleksey Kudelya"
                    },
                    {
                        "name": "Alexander Shirnin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Shirnin"
                },
                "author": "Alexander Shirnin",
                "arxiv_comment": "Accepted to SemEval-2025, an ACL 2025 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04043v1",
                "updated": "2025-06-04T15:09:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    9,
                    20,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:09:20Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    9,
                    20,
                    2,
                    155,
                    0
                ],
                "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of\n  Persona-Guided LLMs for Countering Hate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of\n  Persona-Guided LLMs for Countering Hate"
                },
                "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness."
                },
                "authors": [
                    {
                        "name": "Mikel K. Ngueajio"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Yi-Ling Chung"
                    },
                    {
                        "name": "Danda B. Rawat"
                    },
                    {
                        "name": "Amanda Cercas Curry"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Cercas Curry"
                },
                "author": "Amanda Cercas Curry",
                "arxiv_comment": "Accepted at ACL WOAH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04041v1",
                "updated": "2025-06-04T15:06:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    6,
                    27,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:06:27Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    6,
                    27,
                    2,
                    155,
                    0
                ],
                "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexTime: A Benchmark for Temporal Ordering of Legal Events"
                },
                "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning."
                },
                "authors": [
                    {
                        "name": "Claire Barale"
                    },
                    {
                        "name": "Leslie Barrett"
                    },
                    {
                        "name": "Vikram Sunil Bajaj"
                    },
                    {
                        "name": "Michael Rovatsos"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rovatsos"
                },
                "author": "Michael Rovatsos",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04039v1",
                "updated": "2025-06-04T15:03:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    3,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:03:50Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    3,
                    50,
                    2,
                    155,
                    0
                ],
                "title": "Mitigating Hallucinations in Large Vision-Language Models via\n  Entity-Centric Multimodal Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucinations in Large Vision-Language Models via\n  Entity-Centric Multimodal Preference Optimization"
                },
                "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench."
                },
                "authors": [
                    {
                        "name": "Jiulong Wu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Jizhou Huang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Min Cao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04038v1",
                "updated": "2025-06-04T15:01:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    1,
                    59,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:01:59Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    1,
                    59,
                    2,
                    155,
                    0
                ],
                "title": "Generating Automotive Code: Large Language Models for Software\n  Development and Verification in Safety-Critical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Automotive Code: Large Language Models for Software\n  Development and Verification in Safety-Critical Systems"
                },
                "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements."
                },
                "authors": [
                    {
                        "name": "Sven Kirchner"
                    },
                    {
                        "name": "Alois C. Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois C. Knoll"
                },
                "author": "Alois C. Knoll",
                "arxiv_comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02867v2",
                "updated": "2025-06-04T15:00:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    0,
                    58,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T13:31:10Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    31,
                    10,
                    1,
                    154,
                    0
                ],
                "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens\n  are Information Peaks in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens\n  are Information Peaks in LLM Reasoning"
                },
                "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks."
                },
                "authors": [
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Haochen Wen"
                    },
                    {
                        "name": "Zhen Bai"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04036v1",
                "updated": "2025-06-04T14:58:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    58,
                    29,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:58:29Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    58,
                    29,
                    2,
                    155,
                    0
                ],
                "title": "Privacy and Security Threat for OpenAI GPTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy and Security Threat for OpenAI GPTs"
                },
                "summary": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications."
                },
                "authors": [
                    {
                        "name": "Wei Wenying"
                    },
                    {
                        "name": "Zhao Kaifa"
                    },
                    {
                        "name": "Xue Lei"
                    },
                    {
                        "name": "Fan Ming"
                    }
                ],
                "author_detail": {
                    "name": "Fan Ming"
                },
                "author": "Fan Ming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02524v3",
                "updated": "2025-06-04T14:57:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    57,
                    0,
                    2,
                    155,
                    0
                ],
                "published": "2024-06-04T17:42:21Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    42,
                    21,
                    1,
                    156,
                    0
                ],
                "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks"
                },
                "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Tanja Srindran"
                    },
                    {
                        "name": "Tomasz Lehmann"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04032v1",
                "updated": "2025-06-04T14:56:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    56,
                    8,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:56:08Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    56,
                    8,
                    2,
                    155,
                    0
                ],
                "title": "AI Agents for Conversational Patient Triage: Preliminary\n  Simulation-Based Evaluation with Real-World EHR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents for Conversational Patient Triage: Preliminary\n  Simulation-Based Evaluation with Real-World EHR Data"
                },
                "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale."
                },
                "authors": [
                    {
                        "name": "Sina Rashidian"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Jonathan Amar"
                    },
                    {
                        "name": "Jong Ha Lee"
                    },
                    {
                        "name": "Sam Pugh"
                    },
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Geoff Masterson"
                    },
                    {
                        "name": "Myoung Cha"
                    },
                    {
                        "name": "Yugang Jia"
                    },
                    {
                        "name": "Akhil Vaid"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Vaid"
                },
                "author": "Akhil Vaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04024v1",
                "updated": "2025-06-04T14:53:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    53,
                    11,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:53:11Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    53,
                    11,
                    2,
                    155,
                    0
                ],
                "title": "MudiNet: Task-guided Disentangled Representation Learning for 5G Indoor\n  Multipath-assisted Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MudiNet: Task-guided Disentangled Representation Learning for 5G Indoor\n  Multipath-assisted Positioning"
                },
                "summary": "In the fifth-generation communication system (5G), multipath-assisted\npositioning (MAP) has emerged as a promising approach. With the enhancement of\nsignal resolution, multipath component (MPC) are no longer regarded as noise\nbut rather as valuable information that can contribute to positioning. However,\nexisting research often treats reflective surfaces as ideal reflectors, while\nbeing powerless in the face of indistinguishable multipath caused by diffuse\nreflectors. This study approaches diffuse reflectors from the perspective of\nuncertainty, investigating the statistical distribution characteristics of\nindoor diffuse and specular reflectors. Based on these insights, a task-guided\ndisentangled representation learning method leveraging multi-time channel\nimpulse response (CIR) observations is designed to directly map CIRs to\npositions, while mitigating the adverse effects of components that contribute\nminimally to localization accuracy (e.g., diffuse multipath).In this\nsemi-supervised learning framework, a global feature extraction architecture\nbased on self-attention is proposed to capture location-independent wireless\nenvironmental information, while an MLP is employed to extract the time-varying\nfeatures related to user equipment (UE) positions. Variational inference based\non a latent variable model (LVM) is applied to separate independent features\nwithin the CIR, with position labels guiding the LVM to express components more\nbeneficial for localization. Additionally, we provide a feasibility proof for\nthe separability of diffuse and specular environmental features in CIRs.\nSimulation results demonstrate that the proposed method achieves higher\nlocalization accuracy compared to conventional search-based localization\nmethods, with enhanced robustness against indistinguishable multipath from\ndiffuse reflectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fifth-generation communication system (5G), multipath-assisted\npositioning (MAP) has emerged as a promising approach. With the enhancement of\nsignal resolution, multipath component (MPC) are no longer regarded as noise\nbut rather as valuable information that can contribute to positioning. However,\nexisting research often treats reflective surfaces as ideal reflectors, while\nbeing powerless in the face of indistinguishable multipath caused by diffuse\nreflectors. This study approaches diffuse reflectors from the perspective of\nuncertainty, investigating the statistical distribution characteristics of\nindoor diffuse and specular reflectors. Based on these insights, a task-guided\ndisentangled representation learning method leveraging multi-time channel\nimpulse response (CIR) observations is designed to directly map CIRs to\npositions, while mitigating the adverse effects of components that contribute\nminimally to localization accuracy (e.g., diffuse multipath).In this\nsemi-supervised learning framework, a global feature extraction architecture\nbased on self-attention is proposed to capture location-independent wireless\nenvironmental information, while an MLP is employed to extract the time-varying\nfeatures related to user equipment (UE) positions. Variational inference based\non a latent variable model (LVM) is applied to separate independent features\nwithin the CIR, with position labels guiding the LVM to express components more\nbeneficial for localization. Additionally, we provide a feasibility proof for\nthe separability of diffuse and specular environmental features in CIRs.\nSimulation results demonstrate that the proposed method achieves higher\nlocalization accuracy compared to conventional search-based localization\nmethods, with enhanced robustness against indistinguishable multipath from\ndiffuse reflectors."
                },
                "authors": [
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Xueting Xu"
                    },
                    {
                        "name": "Ao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ao Peng"
                },
                "author": "Ao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04019v1",
                "updated": "2025-06-04T14:47:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    47,
                    14,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:47:14Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    47,
                    14,
                    2,
                    155,
                    0
                ],
                "title": "CETBench: A Novel Dataset constructed via Transformations over Programs\n  for Benchmarking LLMs for Code-Equivalence Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CETBench: A Novel Dataset constructed via Transformations over Programs\n  for Benchmarking LLMs for Code-Equivalence Checking"
                },
                "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code."
                },
                "authors": [
                    {
                        "name": "Neeva Oza"
                    },
                    {
                        "name": "Ishaan Govil"
                    },
                    {
                        "name": "Parul Gupta"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Dinesh Garg"
                    },
                    {
                        "name": "Parag Singla"
                    }
                ],
                "author_detail": {
                    "name": "Parag Singla"
                },
                "author": "Parag Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04018v1",
                "updated": "2025-06-04T14:46:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:46:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents"
                },
                "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent."
                },
                "authors": [
                    {
                        "name": "Akshat Naik"
                    },
                    {
                        "name": "Patrick Quinn"
                    },
                    {
                        "name": "Guillermo Bosch"
                    },
                    {
                        "name": "Emma Goun"
                    },
                    {
                        "name": "Francisco Javier Campos Zabala"
                    },
                    {
                        "name": "Jason Ross Brown"
                    },
                    {
                        "name": "Edward James Young"
                    }
                ],
                "author_detail": {
                    "name": "Edward James Young"
                },
                "author": "Edward James Young",
                "arxiv_comment": "Prepint, under review for NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; K.4.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04015v1",
                "updated": "2025-06-04T14:46:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    18,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:46:18Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    18,
                    2,
                    155,
                    0
                ],
                "title": "GORACS: Group-level Optimal Transport-guided Coreset Selection for\n  LLM-based Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GORACS: Group-level Optimal Transport-guided Coreset Selection for\n  LLM-based Recommender Systems"
                },
                "summary": "Although large language models (LLMs) have shown great potential in\nrecommender systems, the prohibitive computational costs for fine-tuning LLMs\non entire datasets hinder their successful deployment in real-world scenarios.\nTo develop affordable and effective LLM-based recommender systems, we focus on\nthe task of coreset selection which identifies a small subset of fine-tuning\ndata to optimize the test loss, thereby facilitating efficient LLMs'\nfine-tuning. Although there exist some intuitive solutions of subset selection,\nincluding distribution-based and importance-based approaches, they often lead\nto suboptimal performance due to the misalignment with downstream fine-tuning\nobjectives or weak generalization ability caused by individual-level sample\nselection. To overcome these challenges, we propose GORACS, which is a novel\nGroup-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based\nrecommender systems. GORACS is designed based on two key principles for coreset\nselection: 1) selecting the subsets that minimize the test loss to align with\nfine-tuning objectives, and 2) enhancing model generalization through\ngroup-level data selection. Corresponding to these two principles, GORACS has\ntwo key components: 1) a Proxy Optimization Objective (POO) leveraging optimal\ntransport and gradient information to bound the intractable test loss, thus\nreducing computational costs by avoiding repeated LLM retraining, and 2) a\ntwo-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient\ngroup-level selection. Our extensive experiments across diverse recommendation\ndatasets and tasks validate that GORACS significantly reduces fine-tuning costs\nof LLMs while achieving superior performance over the state-of-the-art\nbaselines and full data training. The source code of GORACS are available at\nhttps://github.com/Mithas-114/GORACS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have shown great potential in\nrecommender systems, the prohibitive computational costs for fine-tuning LLMs\non entire datasets hinder their successful deployment in real-world scenarios.\nTo develop affordable and effective LLM-based recommender systems, we focus on\nthe task of coreset selection which identifies a small subset of fine-tuning\ndata to optimize the test loss, thereby facilitating efficient LLMs'\nfine-tuning. Although there exist some intuitive solutions of subset selection,\nincluding distribution-based and importance-based approaches, they often lead\nto suboptimal performance due to the misalignment with downstream fine-tuning\nobjectives or weak generalization ability caused by individual-level sample\nselection. To overcome these challenges, we propose GORACS, which is a novel\nGroup-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based\nrecommender systems. GORACS is designed based on two key principles for coreset\nselection: 1) selecting the subsets that minimize the test loss to align with\nfine-tuning objectives, and 2) enhancing model generalization through\ngroup-level data selection. Corresponding to these two principles, GORACS has\ntwo key components: 1) a Proxy Optimization Objective (POO) leveraging optimal\ntransport and gradient information to bound the intractable test loss, thus\nreducing computational costs by avoiding repeated LLM retraining, and 2) a\ntwo-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient\ngroup-level selection. Our extensive experiments across diverse recommendation\ndatasets and tasks validate that GORACS significantly reduces fine-tuning costs\nof LLMs while achieving superior performance over the state-of-the-art\nbaselines and full data training. The source code of GORACS are available at\nhttps://github.com/Mithas-114/GORACS."
                },
                "authors": [
                    {
                        "name": "Tiehua Mei"
                    },
                    {
                        "name": "Hengrui Chen"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_doi": "10.1145/3711896.3736985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3736985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.04015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08669v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08669v8",
                "updated": "2025-06-04T14:45:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    45,
                    22,
                    2,
                    155,
                    0
                ],
                "published": "2023-06-14T17:59:03Z",
                "published_parsed": [
                    2023,
                    6,
                    14,
                    17,
                    59,
                    3,
                    2,
                    165,
                    0
                ],
                "title": "Plan B: New ${Z^\\prime}$ models for $b\\rightarrow sl^+l^-$ anomalies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan B: New ${Z^\\prime}$ models for $b\\rightarrow sl^+l^-$ anomalies"
                },
                "summary": "Measurements of $b \\rightarrow s \\mu^+ \\mu^-$ transitions indicate that there\nmay be a new physics field coupling to di-muon pairs associated with the $b$ to\n$s$ flavour transition. Including the 2022 LHCb reanalysis of $R_K$ and\n$R_{K^\\ast}$, one infers that there may also be associated new physics in\n$b\\rightarrow e^+ e^-$ transitions. Here, we examine the extent of the\nstatistical preference for $Z^\\prime$ models coupling to di-electron pairs\ntaking into account the relevant constraints, in particular from experiments at\nLEP-2. We identify an anomaly-free set of models which interpolates between the\n$Z^\\prime$ not coupling to electrons at all, to one in which there is an equal\n$Z^\\prime$ coupling to muons and electrons (but where in all models in the set,\nthe $Z^\\prime$ boson can mediate $b\\rightarrow \\mu^+ \\mu^-$ transitions). A\n$3B_3-L_e-2L_\\mu$ model provides a close-to-optimal fit to the pertinent\nmeasurements along the line of interpolation. We have (re-)calculated\npredictions for the relevant LEP-2 observables in terms of dimension-6 SMEFT\noperators and put them into the ${\\tt flavio}$ computer program, so that they\nare available for global fits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements of $b \\rightarrow s \\mu^+ \\mu^-$ transitions indicate that there\nmay be a new physics field coupling to di-muon pairs associated with the $b$ to\n$s$ flavour transition. Including the 2022 LHCb reanalysis of $R_K$ and\n$R_{K^\\ast}$, one infers that there may also be associated new physics in\n$b\\rightarrow e^+ e^-$ transitions. Here, we examine the extent of the\nstatistical preference for $Z^\\prime$ models coupling to di-electron pairs\ntaking into account the relevant constraints, in particular from experiments at\nLEP-2. We identify an anomaly-free set of models which interpolates between the\n$Z^\\prime$ not coupling to electrons at all, to one in which there is an equal\n$Z^\\prime$ coupling to muons and electrons (but where in all models in the set,\nthe $Z^\\prime$ boson can mediate $b\\rightarrow \\mu^+ \\mu^-$ transitions). A\n$3B_3-L_e-2L_\\mu$ model provides a close-to-optimal fit to the pertinent\nmeasurements along the line of interpolation. We have (re-)calculated\npredictions for the relevant LEP-2 observables in terms of dimension-6 SMEFT\noperators and put them into the ${\\tt flavio}$ computer program, so that they\nare available for global fits."
                },
                "authors": [
                    {
                        "name": "Ben Allanach"
                    },
                    {
                        "name": "Anna Mullin"
                    }
                ],
                "author_detail": {
                    "name": "Anna Mullin"
                },
                "author": "Anna Mullin",
                "arxiv_comment": "26 pages, 7 figures. Factor of 2 error in chi^2 values presented\n  corrected. Bug-fixed LEP2 constraint and p-values and updated results to\n  smelli2.4.2 with consequent numerical changes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08669v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08669v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02943v2",
                "updated": "2025-06-04T14:43:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    43,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T14:43:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "A Multi-agent LLM-based JUnit Test Generation with Strong Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent LLM-based JUnit Test Generation with Strong Oracles"
                },
                "summary": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy."
                },
                "authors": [
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17341v2",
                "updated": "2025-06-04T14:38:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    38,
                    7,
                    2,
                    155,
                    0
                ],
                "published": "2024-10-22T18:18:03Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    18,
                    18,
                    3,
                    1,
                    296,
                    0
                ],
                "title": "A 3D Model of the Local Bubble's Magnetic Field: Insights from Dust and\n  Starlight Polarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 3D Model of the Local Bubble's Magnetic Field: Insights from Dust and\n  Starlight Polarization"
                },
                "summary": "Clustered stellar feedback creates expanding voids in the magnetized\ninterstellar medium known as superbubbles. Although theory suggests that\nsuperbubble expansion is influenced by interstellar magnetic fields, direct\nobservational data on 3D superbubble magnetic field geometry is limited. The\nSun's location inside the Local Bubble provides a unique opportunity to infer a\nsuperbubble's 3D magnetic field orientation, under the assumptions that:\n$\\mathrm{I}$) the Local Bubble's surface is the primary contributor to\nplane-of-the-sky polarization observations across much of the sky, and\n$\\mathrm{II}$) the Local Bubble's magnetic field is tangent to its dust-traced\nshell. In this work, we validate these assumptions and construct a model of the\nLocal Bubble's 3D B-field orientation from $\\textit{Planck}$ 353 GHz\npolarization observations and a 3D-dust-derived model of the Local Bubble's\nshell. We test Assumption $\\mathrm{I}$ by examining correlations between the\nLocal Bubble's 3D geometry, dust polarization, and starlight polarization. We\nfind that the Local Bubble likely dominates the polarized signal in the\nmajority of lines of sight. We jointly test Assumptions $\\mathrm{I}$ and\n$\\mathrm{II}$ by applying our reconstruction method to a simulated superbubble,\nwhere we successfully reconstruct the 3D magnetic field orientation over the\nbulk of its surface. Finally, we use our 3D B-field model to infer the initial\nmagnetic field orientation in the solar neighborhood prior to the Local\nBubble's formation, and derive an orientation parallel to the present-day Local\nArm of the galaxy. These findings provide new insights into the co-evolution of\nsuperbubbles and the magnetized interstellar medium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustered stellar feedback creates expanding voids in the magnetized\ninterstellar medium known as superbubbles. Although theory suggests that\nsuperbubble expansion is influenced by interstellar magnetic fields, direct\nobservational data on 3D superbubble magnetic field geometry is limited. The\nSun's location inside the Local Bubble provides a unique opportunity to infer a\nsuperbubble's 3D magnetic field orientation, under the assumptions that:\n$\\mathrm{I}$) the Local Bubble's surface is the primary contributor to\nplane-of-the-sky polarization observations across much of the sky, and\n$\\mathrm{II}$) the Local Bubble's magnetic field is tangent to its dust-traced\nshell. In this work, we validate these assumptions and construct a model of the\nLocal Bubble's 3D B-field orientation from $\\textit{Planck}$ 353 GHz\npolarization observations and a 3D-dust-derived model of the Local Bubble's\nshell. We test Assumption $\\mathrm{I}$ by examining correlations between the\nLocal Bubble's 3D geometry, dust polarization, and starlight polarization. We\nfind that the Local Bubble likely dominates the polarized signal in the\nmajority of lines of sight. We jointly test Assumptions $\\mathrm{I}$ and\n$\\mathrm{II}$ by applying our reconstruction method to a simulated superbubble,\nwhere we successfully reconstruct the 3D magnetic field orientation over the\nbulk of its surface. Finally, we use our 3D B-field model to infer the initial\nmagnetic field orientation in the solar neighborhood prior to the Local\nBubble's formation, and derive an orientation parallel to the present-day Local\nArm of the galaxy. These findings provide new insights into the co-evolution of\nsuperbubbles and the magnetized interstellar medium."
                },
                "authors": [
                    {
                        "name": "Theo J. O'Neill"
                    },
                    {
                        "name": "Alyssa A. Goodman"
                    },
                    {
                        "name": "Juan D. Soler"
                    },
                    {
                        "name": "Catherine Zucker"
                    },
                    {
                        "name": "Jiwon Jesse Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiwon Jesse Han"
                },
                "author": "Jiwon Jesse Han",
                "arxiv_comment": "30 pages, 17 figures. Accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06706v2",
                "updated": "2025-06-04T14:31:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    31,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-09T17:43:30Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    30,
                    6,
                    68,
                    0
                ],
                "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts"
                },
                "summary": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Tingyi Yang"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Qinhao Chen"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02426v2",
                "updated": "2025-06-04T14:21:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    21,
                    2,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T04:19:47Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    4,
                    19,
                    47,
                    1,
                    154,
                    0
                ],
                "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of AI Agent Architectures for Entity Relationship\n  Classification"
                },
                "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at https://github.com/maryambrj/ALIEN.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at https://github.com/maryambrj/ALIEN.git."
                },
                "authors": [
                    {
                        "name": "Maryam Berijanian"
                    },
                    {
                        "name": "Kuldeep Singh"
                    },
                    {
                        "name": "Amin Sehati"
                    }
                ],
                "author_detail": {
                    "name": "Amin Sehati"
                },
                "author": "Amin Sehati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03990v1",
                "updated": "2025-06-04T14:17:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    17,
                    42,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:17:42Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    17,
                    42,
                    2,
                    155,
                    0
                ],
                "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective\n  Video Understanding"
                },
                "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques."
                },
                "authors": [
                    {
                        "name": "Hongzhi Zhang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fuzheng Zhang"
                },
                "author": "Fuzheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03984v1",
                "updated": "2025-06-04T14:14:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    14,
                    28,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:14:28Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    14,
                    28,
                    2,
                    155,
                    0
                ],
                "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place"
                },
                "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks."
                },
                "authors": [
                    {
                        "name": "Carolin Holtermann"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Anne Lauscher"
                    }
                ],
                "author_detail": {
                    "name": "Anne Lauscher"
                },
                "author": "Anne Lauscher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03978v1",
                "updated": "2025-06-04T14:08:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    8,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:08:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    8,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Pruning for Diverse Best-of-N Reasoning Optimization"
                },
                "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets."
                },
                "authors": [
                    {
                        "name": "Hieu Trung Nguyen"
                    },
                    {
                        "name": "Bao Nguyen"
                    },
                    {
                        "name": "Viet Anh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Viet Anh Nguyen"
                },
                "author": "Viet Anh Nguyen",
                "arxiv_comment": "Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09444v2",
                "updated": "2025-06-04T14:03:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    3,
                    14,
                    2,
                    155,
                    0
                ],
                "published": "2024-09-14T14:11:45Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    14,
                    11,
                    45,
                    5,
                    258,
                    0
                ],
                "title": "KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAN-HyperpointNet for Point Cloud Sequence-Based 3D Human Action\n  Recognition"
                },
                "summary": "Point cloud sequence-based 3D action recognition has achieved impressive\nperformance and efficiency. However, existing point cloud sequence modeling\nmethods cannot adequately balance the precision of limb micro-movements with\nthe integrity of posture macro-structure, leading to the loss of crucial\ninformation cues in action inference. To overcome this limitation, we introduce\nD-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding\nmodule. D-Hyperpoint encapsulates both regional-momentary motion and\nglobal-static posture, effectively summarizing the unit human action at each\nmoment. In addition, we present a D-Hyperpoint KANsMixer module, which is\nrecursively applied to nested groupings of D-Hyperpoints to learn the action\ndiscrimination information and creatively integrates Kolmogorov-Arnold Networks\n(KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we\npropose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for\n3D action recognition. Extensive experiments on two public datasets: MSR\nAction3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud sequence-based 3D action recognition has achieved impressive\nperformance and efficiency. However, existing point cloud sequence modeling\nmethods cannot adequately balance the precision of limb micro-movements with\nthe integrity of posture macro-structure, leading to the loss of crucial\ninformation cues in action inference. To overcome this limitation, we introduce\nD-Hyperpoint, a novel data type generated through a D-Hyperpoint Embedding\nmodule. D-Hyperpoint encapsulates both regional-momentary motion and\nglobal-static posture, effectively summarizing the unit human action at each\nmoment. In addition, we present a D-Hyperpoint KANsMixer module, which is\nrecursively applied to nested groupings of D-Hyperpoints to learn the action\ndiscrimination information and creatively integrates Kolmogorov-Arnold Networks\n(KAN) to enhance spatio-temporal interaction within D-Hyperpoints. Finally, we\npropose KAN-HyperpointNet, a spatio-temporal decoupled network architecture for\n3D action recognition. Extensive experiments on two public datasets: MSR\nAction3D and NTU-RGB+D 60, demonstrate the state-of-the-art performance of our\nmethod."
                },
                "authors": [
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Qian Huang"
                    },
                    {
                        "name": "Qiang Geng"
                    },
                    {
                        "name": "Tianjin Yang"
                    },
                    {
                        "name": "Shihao Han"
                    }
                ],
                "author_detail": {
                    "name": "Shihao Han"
                },
                "author": "Shihao Han",
                "arxiv_doi": "10.1109/ICASSP49660.2025.10889324",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICASSP49660.2025.10889324",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.09444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ICASSP, IEEE International Conference on Acoustics, Speech and\n  Signal Processing - Proceedings, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03972v1",
                "updated": "2025-06-04T14:02:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    2,
                    24,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:02:24Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    2,
                    24,
                    2,
                    155,
                    0
                ],
                "title": "MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell\n  Detection"
                },
                "summary": "Complete blood cell detection holds significant value in clinical\ndiagnostics. Conventional manual microscopy methods suffer from time\ninefficiency and diagnostic inaccuracies. Existing automated detection\napproaches remain constrained by high deployment costs and suboptimal accuracy.\nWhile deep learning has introduced powerful paradigms to this field, persistent\nchallenges in detecting overlapping cells and multi-scale objects hinder\npractical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a\nblood cell detection model based on the YOLOv11 framework, incorporating three\nkey architectural innovations to enhance detection performance. Specifically,\nthe multi-scale dilated residual module (MS-DRM) replaces the original C3K2\nmodules to improve multi-scale discriminability; the dynamic cross-path feature\nenhancement module (DCFEM) enables the fusion of hierarchical features from the\nbackbone with aggregated features from the neck to enhance feature\nrepresentations; and the light adaptive-weight downsampling module (LADS)\nimproves feature downsampling through adaptive spatial weighting while reducing\ncomputational complexity. Experimental results on the CBC benchmark demonstrate\nthat MS-YOLO achieves precise detection of overlapping cells and multi-scale\nobjects, particularly small targets such as platelets, achieving an mAP@50 of\n97.4% that outperforms existing models. Further validation on the supplementary\nWBCDD dataset confirms its robust generalization capability. Additionally, with\na lightweight architecture and real-time inference efficiency, MS-YOLO meets\nclinical deployment requirements, providing reliable technical support for\nstandardized blood pathology assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complete blood cell detection holds significant value in clinical\ndiagnostics. Conventional manual microscopy methods suffer from time\ninefficiency and diagnostic inaccuracies. Existing automated detection\napproaches remain constrained by high deployment costs and suboptimal accuracy.\nWhile deep learning has introduced powerful paradigms to this field, persistent\nchallenges in detecting overlapping cells and multi-scale objects hinder\npractical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a\nblood cell detection model based on the YOLOv11 framework, incorporating three\nkey architectural innovations to enhance detection performance. Specifically,\nthe multi-scale dilated residual module (MS-DRM) replaces the original C3K2\nmodules to improve multi-scale discriminability; the dynamic cross-path feature\nenhancement module (DCFEM) enables the fusion of hierarchical features from the\nbackbone with aggregated features from the neck to enhance feature\nrepresentations; and the light adaptive-weight downsampling module (LADS)\nimproves feature downsampling through adaptive spatial weighting while reducing\ncomputational complexity. Experimental results on the CBC benchmark demonstrate\nthat MS-YOLO achieves precise detection of overlapping cells and multi-scale\nobjects, particularly small targets such as platelets, achieving an mAP@50 of\n97.4% that outperforms existing models. Further validation on the supplementary\nWBCDD dataset confirms its robust generalization capability. Additionally, with\na lightweight architecture and real-time inference efficiency, MS-YOLO meets\nclinical deployment requirements, providing reliable technical support for\nstandardized blood pathology assessment."
                },
                "authors": [
                    {
                        "name": "Guohua Wu"
                    },
                    {
                        "name": "Shengqi Chen"
                    },
                    {
                        "name": "Pengchao Deng"
                    },
                    {
                        "name": "Wenting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Wenting Yu"
                },
                "author": "Wenting Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03969v1",
                "updated": "2025-06-04T14:01:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    1,
                    32,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:01:32Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    1,
                    32,
                    2,
                    155,
                    0
                ],
                "title": "Differentiable Fuzzy Cosmic-Web for Field Level Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Fuzzy Cosmic-Web for Field Level Inference"
                },
                "summary": "A comprehensive analysis of the cosmological large-scale structure derived\nfrom galaxy surveys involves field-level inference, which requires a forward\nmodelling framework that simultaneously accounts for structure formation and\ntracer bias. While structure formation models are well-understood, the\ndevelopment of an effective field-level bias model remains imprecise,\nparticularly in the context of tracer perturbation theory within Bayesian\nreconstruction methods, which we address in this work. To bridge this gap, we\nhave developed a differentiable model that integrates augmented Lagrangian\nperturbation theory, nonlinear, nonlocal, and stochastic biasing. At the core\nof our approach is the Hierarchical Cosmic-Web Biasing Nonlocal (HICOBIAN)\nmodel, which provides a positive definite description of tracer bias while\nincorporating a long- and short-range nonlocal framework via cosmic-web regions\nand deviations from Poissonity in the likelihood. A key insight of our model is\nthat transitions between cosmic-web regions are inherently smooth, which we\nimplement using sigmoid-based gradient operations. This enables a fuzzy, and,\nhence, differentiable hierarchical cosmic-web description, making the model\nwell-suited for machine learning frameworks. Our approach accurately reproduces\nthe primordial density field within associated error bars, as validated by two-\nand three-point statistics in Fourier space. Furthermore, we demonstrate that\nthe methodology approaches the maximum encoded information consistent with\nPoisson noise. We introduce a Bayesian field-level inference algorithm that\nleverages the same forward modelling framework used in galaxy, quasar, and\nLyman alpha forest mock catalog generation -- including nonlinear, nonlocal and\nstochastic bias with redshift space distortions -- providing a unified and\nconsistent approach to the analysis of large-scale cosmic structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive analysis of the cosmological large-scale structure derived\nfrom galaxy surveys involves field-level inference, which requires a forward\nmodelling framework that simultaneously accounts for structure formation and\ntracer bias. While structure formation models are well-understood, the\ndevelopment of an effective field-level bias model remains imprecise,\nparticularly in the context of tracer perturbation theory within Bayesian\nreconstruction methods, which we address in this work. To bridge this gap, we\nhave developed a differentiable model that integrates augmented Lagrangian\nperturbation theory, nonlinear, nonlocal, and stochastic biasing. At the core\nof our approach is the Hierarchical Cosmic-Web Biasing Nonlocal (HICOBIAN)\nmodel, which provides a positive definite description of tracer bias while\nincorporating a long- and short-range nonlocal framework via cosmic-web regions\nand deviations from Poissonity in the likelihood. A key insight of our model is\nthat transitions between cosmic-web regions are inherently smooth, which we\nimplement using sigmoid-based gradient operations. This enables a fuzzy, and,\nhence, differentiable hierarchical cosmic-web description, making the model\nwell-suited for machine learning frameworks. Our approach accurately reproduces\nthe primordial density field within associated error bars, as validated by two-\nand three-point statistics in Fourier space. Furthermore, we demonstrate that\nthe methodology approaches the maximum encoded information consistent with\nPoisson noise. We introduce a Bayesian field-level inference algorithm that\nleverages the same forward modelling framework used in galaxy, quasar, and\nLyman alpha forest mock catalog generation -- including nonlinear, nonlocal and\nstochastic bias with redshift space distortions -- providing a unified and\nconsistent approach to the analysis of large-scale cosmic structure."
                },
                "authors": [
                    {
                        "name": "P. Rossell"
                    },
                    {
                        "name": "F. -S. Kitaura"
                    },
                    {
                        "name": "D. Forero-Snchez"
                    },
                    {
                        "name": "F. Sinigaglia"
                    },
                    {
                        "name": "G. Favole"
                    }
                ],
                "author_detail": {
                    "name": "G. Favole"
                },
                "author": "G. Favole",
                "arxiv_comment": "17 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03968v1",
                "updated": "2025-06-04T14:00:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    0,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:00:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    0,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding"
                },
                "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions."
                },
                "authors": [
                    {
                        "name": "Chiwei Zhu"
                    },
                    {
                        "name": "Benfeng Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "To be published at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00816v3",
                "updated": "2025-06-04T13:46:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    46,
                    0,
                    2,
                    155,
                    0
                ],
                "published": "2025-04-01T14:05:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    5,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images"
                },
                "summary": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging."
                },
                "authors": [
                    {
                        "name": "Yeqi Fang"
                    },
                    {
                        "name": "Rong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Rong Zhou"
                },
                "author": "Rong Zhou",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v2",
                "updated": "2025-06-04T13:45:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    45,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01203v2",
                "updated": "2025-06-04T13:43:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    43,
                    34,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-03T09:50:30Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    50,
                    30,
                    0,
                    34,
                    0
                ],
                "title": "Theoretical Analysis of KL-regularized RLHF with Multiple Reference\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Analysis of KL-regularized RLHF with Multiple Reference\n  Models"
                },
                "summary": "Recent methods for aligning large language models (LLMs) with human feedback\npredominantly rely on a single reference model, which limits diversity, model\noverfitting, and underutilizes the wide range of available pre-trained models.\nIncorporating multiple reference models has the potential to address these\nlimitations by broadening perspectives, reducing bias, and leveraging the\nstrengths of diverse open-source LLMs. However, integrating multiple reference\nmodels into reinforcement learning with human feedback (RLHF) frameworks poses\nsignificant theoretical challenges, where achieving exact solutions has\nremained an open problem. This paper presents the first \\emph{exact solution}\nto the multiple reference model problem in reverse KL-regularized RLHF. We\nintroduce a comprehensive theoretical framework that includes rigorous\nstatistical analysis and provides sample complexity guarantees. Additionally,\nwe extend our analysis to forward KL-regularized RLHF, offering new insights\ninto sample complexity requirements in multiple reference scenarios. Our\ncontributions lay the foundation for more advanced and adaptable LLM alignment\ntechniques, enabling the effective use of multiple reference models. This work\npaves the way for developing alignment frameworks that are both theoretically\nsound and better suited to the challenges of modern AI ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methods for aligning large language models (LLMs) with human feedback\npredominantly rely on a single reference model, which limits diversity, model\noverfitting, and underutilizes the wide range of available pre-trained models.\nIncorporating multiple reference models has the potential to address these\nlimitations by broadening perspectives, reducing bias, and leveraging the\nstrengths of diverse open-source LLMs. However, integrating multiple reference\nmodels into reinforcement learning with human feedback (RLHF) frameworks poses\nsignificant theoretical challenges, where achieving exact solutions has\nremained an open problem. This paper presents the first \\emph{exact solution}\nto the multiple reference model problem in reverse KL-regularized RLHF. We\nintroduce a comprehensive theoretical framework that includes rigorous\nstatistical analysis and provides sample complexity guarantees. Additionally,\nwe extend our analysis to forward KL-regularized RLHF, offering new insights\ninto sample complexity requirements in multiple reference scenarios. Our\ncontributions lay the foundation for more advanced and adaptable LLM alignment\ntechniques, enabling the effective use of multiple reference models. This work\npaves the way for developing alignment frameworks that are both theoretically\nsound and better suited to the challenges of modern AI ecosystems."
                },
                "authors": [
                    {
                        "name": "Gholamali Aminian"
                    },
                    {
                        "name": "Amir R. Asadi"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Youssef Mroueh"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Mroueh"
                },
                "author": "Youssef Mroueh",
                "arxiv_comment": "Experiments are added in new version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00539v2",
                "updated": "2025-06-04T13:39:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    54,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-31T12:54:49Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    12,
                    54,
                    49,
                    5,
                    151,
                    0
                ],
                "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation"
                },
                "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03949v1",
                "updated": "2025-06-04T13:39:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    1,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:39:01Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    1,
                    2,
                    155,
                    0
                ],
                "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering"
                },
                "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Nan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Xu"
                },
                "author": "Nan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14309v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14309v4",
                "updated": "2025-06-04T13:35:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    35,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2024-10-18T09:15:35Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    15,
                    35,
                    4,
                    292,
                    0
                ],
                "title": "LoGU: Long-form Generation with Uncertainty Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoGU: Long-form Generation with Uncertainty Expressions"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14309v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14309v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03946v1",
                "updated": "2025-06-04T13:33:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    33,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:33:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    33,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "Automatic Multi-level Feature Tree Construction for Domain-Specific\n  Reusable Artifacts Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Multi-level Feature Tree Construction for Domain-Specific\n  Reusable Artifacts Management"
                },
                "summary": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises."
                },
                "authors": [
                    {
                        "name": "Dongming Jin"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Nianyu Li"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Linyu Li"
                    },
                    {
                        "name": "Suijing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Suijing Guan"
                },
                "author": "Suijing Guan",
                "arxiv_comment": "9pages, 2figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03939v1",
                "updated": "2025-06-04T13:31:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    31,
                    21,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:31:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    31,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to\n  Enhance LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to\n  Enhance LLM Reasoning"
                },
                "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git."
                },
                "authors": [
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Xiang Zou"
                    },
                    {
                        "name": "YIng Ai"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Yichen Niu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Jianxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxing Liu"
                },
                "author": "Jianxing Liu",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05470v3",
                "updated": "2025-06-04T13:31:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    31,
                    15,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-08T17:58:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    58,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-GRPO: Training Flow Matching Models via Online RL"
                },
                "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Gongye Liu"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "arxiv_comment": "Code: https://github.com/yifan123/flow_grpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.04213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04213v2",
                "updated": "2025-06-05T03:35:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    35,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    57,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers"
                },
                "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."
                },
                "authors": [
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Quande Liu"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04209v1",
                "updated": "2025-06-04T17:51:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    51,
                    56,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:51:56Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    51,
                    56,
                    2,
                    155,
                    0
                ],
                "title": "Language-Image Alignment with Fixed Text Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Image Alignment with Fixed Text Encoders"
                },
                "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations."
                },
                "authors": [
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yi Ma"
                },
                "author": "Yi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04205v1",
                "updated": "2025-06-04T17:49:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    49,
                    10,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:49:10Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    49,
                    10,
                    2,
                    155,
                    0
                ],
                "title": "EPiC: Towards Lossless Speedup for Reasoning Training through\n  Edge-Preserving CoT Condensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPiC: Towards Lossless Speedup for Reasoning Training through\n  Edge-Preserving CoT Condensation"
                },
                "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation."
                },
                "authors": [
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Hadi Reisizadeh"
                    },
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Nathalie Baracaldo"
                    },
                    {
                        "name": "Mingyi Hong"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04203v1",
                "updated": "2025-06-04T17:48:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:48:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "Cascadia: A Cascade Serving System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascadia: A Cascade Serving System for Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality answers. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this\nlatency-quality trade-off using model cascades, which route simpler queries to\nsmaller models and more complex ones to larger models. However, enabling\nefficient cascade serving remains challenging. Current frameworks lack\neffective mechanisms for handling (i) the huge and varying resource demands of\ndifferent LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the\nco-optimization of system deployment and routing strategy. Motivated by these\nobservations, we introduce Cascadia, a novel cascade serving framework designed\nexplicitly to schedule request routing and deploy model cascades for fast,\nquality-preserving LLM serving. Cascadia employs a bi-level optimization\nmethod: at the inner level, it uses a mixed-integer linear program to select\nresource allocations and parallelism strategies based on LLM information and\nworkload characteristics; at the outer level, it applies a weighted Tchebycheff\nalgorithm to iteratively co-optimize the routing strategy and the system\ndeployment produced by the inner level. Our extensive evaluation on diverse\nworkload traces and different model cascades (DeepSeek and the Llama series)\ndemonstrates that Cascadia significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to\n4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher\nthroughput while maintaining target answer quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality answers. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this\nlatency-quality trade-off using model cascades, which route simpler queries to\nsmaller models and more complex ones to larger models. However, enabling\nefficient cascade serving remains challenging. Current frameworks lack\neffective mechanisms for handling (i) the huge and varying resource demands of\ndifferent LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the\nco-optimization of system deployment and routing strategy. Motivated by these\nobservations, we introduce Cascadia, a novel cascade serving framework designed\nexplicitly to schedule request routing and deploy model cascades for fast,\nquality-preserving LLM serving. Cascadia employs a bi-level optimization\nmethod: at the inner level, it uses a mixed-integer linear program to select\nresource allocations and parallelism strategies based on LLM information and\nworkload characteristics; at the outer level, it applies a weighted Tchebycheff\nalgorithm to iteratively co-optimize the routing strategy and the system\ndeployment produced by the inner level. Our extensive evaluation on diverse\nworkload traces and different model cascades (DeepSeek and the Llama series)\ndemonstrates that Cascadia significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to\n4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher\nthroughput while maintaining target answer quality."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Stephan Rabanser"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04202v1",
                "updated": "2025-06-04T17:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    16,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    48,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracLLM: A Generic Framework for Attributing Long Context LLMs"
                },
                "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM."
                },
                "authors": [
                    {
                        "name": "Yanting Wang"
                    },
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Jinyuan Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Jia"
                },
                "author": "Jinyuan Jia",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04185v1",
                "updated": "2025-06-04T17:29:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    29,
                    22,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:29:22Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    29,
                    22,
                    2,
                    155,
                    0
                ],
                "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Dingling Xu"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Limin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Limin Liu"
                },
                "author": "Limin Liu",
                "arxiv_comment": "16 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13865v2",
                "updated": "2025-06-04T17:29:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    29,
                    13,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-27T17:58:31Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    58,
                    31,
                    3,
                    86,
                    0
                ],
                "title": "A Survey on (M)LLM-Based GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on (M)LLM-Based GUI Agents"
                },
                "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation."
                },
                "authors": [
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Haolei Xu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Siqi Chen"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Zeqi Tan"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04180v1",
                "updated": "2025-06-04T17:27:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    27,
                    42,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:27:42Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    27,
                    42,
                    2,
                    155,
                    0
                ],
                "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models"
                },
                "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16733v2",
                "updated": "2025-06-04T17:26:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    26,
                    55,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-23T22:14:42Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    22,
                    14,
                    42,
                    6,
                    54,
                    0
                ],
                "title": "Coreset Selection via LLM-based Concept Bottlenecks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreset Selection via LLM-based Concept Bottlenecks"
                },
                "summary": "Coreset Selection (CS) aims to identify a subset of the training dataset that\nachieves model performance comparable to using the entire dataset. Many\nstate-of-the-art CS methods select coresets using scores whose computation\nrequires training the downstream model on the entire dataset first and\nrecording changes in the model's behavior on samples as it trains (training\ndynamics). These scores are inefficient to compute and hard to interpret, as\nthey do not indicate whether a sample is difficult to learn in general or only\nfor a specific downstream model. Our work addresses these challenges by\nproposing a score that computes a sample's difficulty using\nhuman-understandable textual attributes (concepts) independent of any\ndownstream model. Specifically, we measure the alignment between a sample's\nvisual features and concept bottlenecks, derived via large language models, by\ntraining a linear concept bottleneck layer and computing the sample's\ndifficulty score using it.We then use stratified sampling based on this score\nto generate a coreset of the dataset.Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10/100,\nand ImageNet-1K, we show that our coresets outperform random subsets, even at\nhigh pruning rates, and achieve model performance comparable to or better than\ncoresets found by training dynamics-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreset Selection (CS) aims to identify a subset of the training dataset that\nachieves model performance comparable to using the entire dataset. Many\nstate-of-the-art CS methods select coresets using scores whose computation\nrequires training the downstream model on the entire dataset first and\nrecording changes in the model's behavior on samples as it trains (training\ndynamics). These scores are inefficient to compute and hard to interpret, as\nthey do not indicate whether a sample is difficult to learn in general or only\nfor a specific downstream model. Our work addresses these challenges by\nproposing a score that computes a sample's difficulty using\nhuman-understandable textual attributes (concepts) independent of any\ndownstream model. Specifically, we measure the alignment between a sample's\nvisual features and concept bottlenecks, derived via large language models, by\ntraining a linear concept bottleneck layer and computing the sample's\ndifficulty score using it.We then use stratified sampling based on this score\nto generate a coreset of the dataset.Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10/100,\nand ImageNet-1K, we show that our coresets outperform random subsets, even at\nhigh pruning rates, and achieve model performance comparable to or better than\ncoresets found by training dynamics-based methods."
                },
                "authors": [
                    {
                        "name": "Akshay Mehra"
                    },
                    {
                        "name": "Trisha Mittal"
                    },
                    {
                        "name": "Subhadra Gopalakrishnan"
                    },
                    {
                        "name": "Joshua Kimball"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Kimball"
                },
                "author": "Joshua Kimball",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04179v1",
                "updated": "2025-06-04T17:26:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    26,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:26:31Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    26,
                    31,
                    2,
                    155,
                    0
                ],
                "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\n  Module Decoupling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\n  Module Decoupling"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT."
                },
                "authors": [
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Zhiwei Fei"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14522v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14522v3",
                "updated": "2025-06-05T06:39:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    6,
                    39,
                    10,
                    3,
                    156,
                    0
                ],
                "published": "2025-04-20T07:39:00Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    39,
                    0,
                    6,
                    110,
                    0
                ],
                "title": "Biased by Design: Leveraging Inherent AI Biases to Enhance Critical\n  Thinking of News Readers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased by Design: Leveraging Inherent AI Biases to Enhance Critical\n  Thinking of News Readers"
                },
                "summary": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection."
                },
                "authors": [
                    {
                        "name": "Liudmila Zavolokina"
                    },
                    {
                        "name": "Kilian Sprenkamp"
                    },
                    {
                        "name": "Zoya Katashinskaya"
                    },
                    {
                        "name": "Daniel Gordon Jones"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gordon Jones"
                },
                "author": "Daniel Gordon Jones",
                "arxiv_comment": "European Conference on Information Systems (ECIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14522v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14522v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04172v1",
                "updated": "2025-06-04T17:15:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    15,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:15:19Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    15,
                    19,
                    2,
                    155,
                    0
                ],
                "title": "Does Prompt Design Impact Quality of Data Imputation by LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Prompt Design Impact Quality of Data Imputation by LLMs?"
                },
                "summary": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation."
                },
                "authors": [
                    {
                        "name": "Shreenidhi Srinivasan"
                    },
                    {
                        "name": "Lydia Manikonda"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Manikonda"
                },
                "author": "Lydia Manikonda",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16748v2",
                "updated": "2025-06-04T17:05:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    5,
                    12,
                    2,
                    155,
                    0
                ],
                "published": "2025-01-28T06:58:25Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    58,
                    25,
                    1,
                    28,
                    0
                ],
                "title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04167v1",
                "updated": "2025-06-04T17:04:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    48,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:04:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity\n  on Human-LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity\n  on Human-LLM Collaboration"
                },
                "summary": "AI-based interactive assistants are advancing human-augmenting technology,\nyet their effects on users' mental and physiological states remain\nunder-explored. We address this gap by analyzing how Copilot for Microsoft\nWord, a LLM-based assistant, impacts users. Using tasks ranging from objective\n(SAT reading comprehension) to subjective (personal reflection), and with\nmeasurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we\nmeasure Copilot's effects on users. We also evaluate users' performance with\nand without Copilot across tasks. In objective tasks, participants reported a\nreduction of workload and an increase in enjoyment, which was paired with\nobjective performance increases. Participants reported reduced workload and\nincreased enjoyment with no change in performance in a creative poetry writing\ntask. However, no benefits due to Copilot use were reported in a highly\nsubjective self-reflection task. Although no physiological changes were\nrecorded due to Copilot use, task-dependent differences in prefrontal cortex\nactivation offer complementary insights into the cognitive processes associated\nwith successful and unsuccessful human-AI collaboration. These findings suggest\nthat AI assistants' effectiveness varies with task type-particularly showing\ndecreased usefulness in tasks that engage episodic memory-and presents a\nbrain-network based hypothesis of human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based interactive assistants are advancing human-augmenting technology,\nyet their effects on users' mental and physiological states remain\nunder-explored. We address this gap by analyzing how Copilot for Microsoft\nWord, a LLM-based assistant, impacts users. Using tasks ranging from objective\n(SAT reading comprehension) to subjective (personal reflection), and with\nmeasurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we\nmeasure Copilot's effects on users. We also evaluate users' performance with\nand without Copilot across tasks. In objective tasks, participants reported a\nreduction of workload and an increase in enjoyment, which was paired with\nobjective performance increases. Participants reported reduced workload and\nincreased enjoyment with no change in performance in a creative poetry writing\ntask. However, no benefits due to Copilot use were reported in a highly\nsubjective self-reflection task. Although no physiological changes were\nrecorded due to Copilot use, task-dependent differences in prefrontal cortex\nactivation offer complementary insights into the cognitive processes associated\nwith successful and unsuccessful human-AI collaboration. These findings suggest\nthat AI assistants' effectiveness varies with task type-particularly showing\ndecreased usefulness in tasks that engage episodic memory-and presents a\nbrain-network based hypothesis of human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Matthew Russell"
                    },
                    {
                        "name": "Aman Shah"
                    },
                    {
                        "name": "Giles Blaney"
                    },
                    {
                        "name": "Judith Amores"
                    },
                    {
                        "name": "Mary Czerwinski"
                    },
                    {
                        "name": "Robert J. K. Jacob"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. K. Jacob"
                },
                "author": "Robert J. K. Jacob",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04166v1",
                "updated": "2025-06-04T17:04:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    34,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:04:34Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    4,
                    34,
                    2,
                    155,
                    0
                ],
                "title": "N$^2$: A Unified Python Package and Test Bench for Nearest\n  Neighbor-Based Matrix Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N$^2$: A Unified Python Package and Test Bench for Nearest\n  Neighbor-Based Matrix Completion"
                },
                "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings."
                },
                "authors": [
                    {
                        "name": "Caleb Chin"
                    },
                    {
                        "name": "Aashish Khubchandani"
                    },
                    {
                        "name": "Harshvardhan Maskara"
                    },
                    {
                        "name": "Kyuseong Choi"
                    },
                    {
                        "name": "Jacob Feitelberg"
                    },
                    {
                        "name": "Albert Gong"
                    },
                    {
                        "name": "Manit Paul"
                    },
                    {
                        "name": "Tathagata Sadhukhan"
                    },
                    {
                        "name": "Anish Agarwal"
                    },
                    {
                        "name": "Raaz Dwivedi"
                    }
                ],
                "author_detail": {
                    "name": "Raaz Dwivedi"
                },
                "author": "Raaz Dwivedi",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04161v1",
                "updated": "2025-06-04T17:00:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    0,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:00:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    0,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing"
                },
                "summary": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%."
                },
                "authors": [
                    {
                        "name": "Parsa Alian"
                    },
                    {
                        "name": "Martin Tang"
                    },
                    {
                        "name": "Ali Mesbah"
                    }
                ],
                "author_detail": {
                    "name": "Ali Mesbah"
                },
                "author": "Ali Mesbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04156v1",
                "updated": "2025-06-04T16:55:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    55,
                    8,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:55:08Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    55,
                    8,
                    2,
                    155,
                    0
                ],
                "title": "A Dataset for Addressing Patient's Information Needs related to Clinical\n  Course of Hospitalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dataset for Addressing Patient's Information Needs related to Clinical\n  Course of Hospitalization"
                },
                "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts."
                },
                "authors": [
                    {
                        "name": "Sarvesh Soni"
                    },
                    {
                        "name": "Dina Demner-Fushman"
                    }
                ],
                "author_detail": {
                    "name": "Dina Demner-Fushman"
                },
                "author": "Dina Demner-Fushman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14748v2",
                "updated": "2025-06-04T16:49:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    49,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-20T17:19:41Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    17,
                    19,
                    41,
                    3,
                    51,
                    0
                ],
                "title": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of Topic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of Topic Models"
                },
                "summary": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints."
                },
                "authors": [
                    {
                        "name": "Zongxia Li"
                    },
                    {
                        "name": "Lorena Calvo-Bartolom"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Alden Dima"
                    },
                    {
                        "name": "Juan Francisco Fung"
                    },
                    {
                        "name": "Jordan Boyd-Graber"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Boyd-Graber"
                },
                "author": "Jordan Boyd-Graber",
                "arxiv_comment": "22 Pages. LLM for Data Exploration and content analysis, Topic\n  Models. 63rd Annual Meeting of the Association for Computational Linguistics\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13656v2",
                "updated": "2025-06-04T16:39:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    39,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-19T12:07:53Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    7,
                    53,
                    2,
                    50,
                    0
                ],
                "title": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models"
                },
                "summary": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis."
                },
                "authors": [
                    {
                        "name": "Liyang He"
                    },
                    {
                        "name": "Chenglong Liu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Shulan Ruan"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04142v1",
                "updated": "2025-06-04T16:33:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    33,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:33:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    33,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis"
                },
                "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation"
                },
                "authors": [
                    {
                        "name": "Kejian Zhu"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted to ACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04139v1",
                "updated": "2025-06-04T16:31:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    31,
                    37,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:31:37Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    31,
                    37,
                    2,
                    155,
                    0
                ],
                "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in\n  Low-Resource Flemish?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in\n  Low-Resource Flemish?"
                },
                "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research."
                },
                "authors": [
                    {
                        "name": "Ratna Kandala"
                    },
                    {
                        "name": "Katie Hoemann"
                    }
                ],
                "author_detail": {
                    "name": "Katie Hoemann"
                },
                "author": "Katie Hoemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10981v2",
                "updated": "2025-06-04T16:27:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    27,
                    57,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-16T08:28:57Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    28,
                    57,
                    4,
                    136,
                    0
                ],
                "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A\n  Perspective of Probability Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A\n  Perspective of Probability Theory"
                },
                "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a probabilistic method to efficiently predict scaling\nperformance and identify the best prompting strategy under large sampling\ntimes, eliminating the need for resource-intensive inference processes in\npractical applications. Furthermore, we introduce two ways derived from our\ntheoretical analysis to significantly improve the scaling performance. We hope\nthat our research can promote to re-examine the role of complicated prompting,\nunleash the potential of simple prompting strategies, and provide new insights\nfor enhancing test-time scaling performance. Code is available at\nhttps://github.com/MraDonkey/rethinking_prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a probabilistic method to efficiently predict scaling\nperformance and identify the best prompting strategy under large sampling\ntimes, eliminating the need for resource-intensive inference processes in\npractical applications. Furthermore, we introduce two ways derived from our\ntheoretical analysis to significantly improve the scaling performance. We hope\nthat our research can promote to re-examine the role of complicated prompting,\nunleash the potential of simple prompting strategies, and provide new insights\nfor enhancing test-time scaling performance. Code is available at\nhttps://github.com/MraDonkey/rethinking_prompting."
                },
                "authors": [
                    {
                        "name": "Yexiang Liu"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Zhi Fang"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "ACL 2025 Main, 33 pages, 51 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01747v2",
                "updated": "2025-06-04T16:26:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    58,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-04T02:08:59Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    59,
                    0,
                    309,
                    0
                ],
                "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaSaur: Large Language Agents Beyond Predefined Actions"
                },
                "summary": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM agent systems typically select actions from a fixed and\npredefined set at every step. While this approach is effective in closed,\nnarrowly scoped environments, it presents two major challenges for real-world,\nopen-ended scenarios: (1) it significantly restricts the planning and acting\ncapabilities of LLM agents, and (2) it requires substantial human effort to\nenumerate and implement all possible actions, which is impractical in complex\nenvironments with a vast number of potential actions. To address these\nlimitations, we propose an LLM agent framework that can dynamically create and\ncompose actions as needed. In this framework, the agent interacts with its\nenvironment by generating and executing programs written in a general-purpose\nprogramming language. Moreover, generated actions are accumulated over time for\nfuture reuse. Our extensive experiments across multiple benchmarks show that\nthis framework significantly improves flexibility and outperforms prior methods\nthat rely on a fixed action set. Notably, it enables LLM agents to adapt and\nrecover in scenarios where predefined actions are insufficient or fail due to\nunforeseen edge cases. Our code can be found in\nhttps://github.com/adobe-research/dynasaur."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Nedim Lipka"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04133v1",
                "updated": "2025-06-04T16:26:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    11,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:26:11Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    11,
                    2,
                    155,
                    0
                ],
                "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems"
                },
                "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Emmanouilidis"
                },
                "author": "Christos Emmanouilidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23899v2",
                "updated": "2025-06-04T16:23:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    23,
                    54,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-31T09:48:59Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    48,
                    59,
                    0,
                    90,
                    0
                ],
                "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\n  CUBE dataset"
                },
                "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code are available at\nhttps://github.com/RubriksCube/rubriks_cube.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code are available at\nhttps://github.com/RubriksCube/rubriks_cube."
                },
                "authors": [
                    {
                        "name": "Diana Galvan-Sosa"
                    },
                    {
                        "name": "Gabrielle Gaudeau"
                    },
                    {
                        "name": "Pride Kavumba"
                    },
                    {
                        "name": "Yunmeng Li"
                    },
                    {
                        "name": "Hongyi gu"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Paula Buttery"
                    }
                ],
                "author_detail": {
                    "name": "Paula Buttery"
                },
                "author": "Paula Buttery",
                "arxiv_comment": "10 main pages (24 appendix pages), 9 figures, accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19176v2",
                "updated": "2025-06-04T16:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    16,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-25T14:48:49Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    14,
                    48,
                    49,
                    6,
                    145,
                    0
                ],
                "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."
                },
                "authors": [
                    {
                        "name": "Zhuo Liu"
                    },
                    {
                        "name": "Moxin Li"
                    },
                    {
                        "name": "Xun Deng"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04118v1",
                "updated": "2025-06-04T16:12:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    12,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:12:26Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    12,
                    26,
                    2,
                    155,
                    0
                ],
                "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs"
                },
                "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI ."
                },
                "authors": [
                    {
                        "name": "Jonathan Geuter"
                    },
                    {
                        "name": "Youssef Mroueh"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    }
                ],
                "author_detail": {
                    "name": "David Alvarez-Melis"
                },
                "author": "David Alvarez-Melis",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v4",
                "updated": "2025-06-04T16:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    2,
                    28,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate significant persuasive capabilities\nin one-on-one interactions, but their influence within social networks, where\ninterconnected users and complex opinion dynamics pose unique challenges,\nremains underexplored. This paper addresses the research question: \\emph{Can\nLLMs generate meaningful content that maximizes user engagement on social\nnetworks?}\n  To answer this, we propose a pipeline using reinforcement learning with\nsimulated feedback, where the network's response to LLM-generated content\n(i.e., the reward) is simulated through a formal engagement model. This\napproach bypasses the temporal cost and complexity of live experiments,\nenabling an efficient feedback loop between the LLM and the network under\nstudy. It also allows to control over endogenous factors such as the LLM's\nposition within the social network and the distribution of opinions on a given\ntopic. Our approach is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. Such flexibility makes it suitable for\nmore complex engagement tasks and interventions in computational social\nscience.\n  Using our framework, we analyze the performance of LLMs in generating social\nengagement under different conditions, showcasing their full potential in this\ntask. The experimental code is publicly available at\nhttps://github.com/mminici/Engagement-Driven-Content-Generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate significant persuasive capabilities\nin one-on-one interactions, but their influence within social networks, where\ninterconnected users and complex opinion dynamics pose unique challenges,\nremains underexplored. This paper addresses the research question: \\emph{Can\nLLMs generate meaningful content that maximizes user engagement on social\nnetworks?}\n  To answer this, we propose a pipeline using reinforcement learning with\nsimulated feedback, where the network's response to LLM-generated content\n(i.e., the reward) is simulated through a formal engagement model. This\napproach bypasses the temporal cost and complexity of live experiments,\nenabling an efficient feedback loop between the LLM and the network under\nstudy. It also allows to control over endogenous factors such as the LLM's\nposition within the social network and the distribution of opinions on a given\ntopic. Our approach is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. Such flexibility makes it suitable for\nmore complex engagement tasks and interventions in computational social\nscience.\n  Using our framework, we analyze the performance of LLMs in generating social\nengagement under different conditions, showcasing their full potential in this\ntask. The experimental code is publicly available at\nhttps://github.com/mminici/Engagement-Driven-Content-Generation."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05239v2",
                "updated": "2025-06-04T15:59:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    59,
                    6,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-07T23:28:23Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    23,
                    28,
                    23,
                    3,
                    312,
                    0
                ],
                "title": "ZipNN: Lossless Compression for AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipNN: Lossless Compression for AI Models"
                },
                "summary": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growth of model sizes and the scale of their deployment, their sheer\nsize burdens the infrastructure requiring more network and more storage to\naccommodate these. While there is a vast model compression literature deleting\nparts of the model weights for faster inference, we investigate a more\ntraditional type of compression - one that represents the model in a compact\nform and is coupled with a decompression algorithm that returns it to its\noriginal form and size - namely lossless compression.\n  We present ZipNN a lossless compression tailored to neural networks. Somewhat\nsurprisingly, we show that specific lossless compression can gain significant\nnetwork and storage reduction on popular models, often saving 33% and at times\nreducing over 50% of the model size. We investigate the source of model\ncompressibility and introduce specialized compression variants tailored for\nmodels that further increase the effectiveness of compression. On popular\nmodels (e.g. Llama 3) ZipNN shows space savings that are over 17% better than\nvanilla compression while also improving compression and decompression speeds\nby 62%. We estimate that these methods could save over an ExaByte per month of\nnetwork traffic downloaded from a large model hub like Hugging Face."
                },
                "authors": [
                    {
                        "name": "Moshik Hershcovitch"
                    },
                    {
                        "name": "Andrew Wood"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Guy Girmonsky"
                    },
                    {
                        "name": "Roy Leibovitz"
                    },
                    {
                        "name": "Ilias Ennmouri"
                    },
                    {
                        "name": "Michal Malka"
                    },
                    {
                        "name": "Peter Chin"
                    },
                    {
                        "name": "Swaminathan Sundararaman"
                    },
                    {
                        "name": "Danny Harnik"
                    }
                ],
                "author_detail": {
                    "name": "Danny Harnik"
                },
                "author": "Danny Harnik",
                "arxiv_comment": "IEEE Cloud. arXiv admin note: text overlap with arXiv:2404.15198",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04089v1",
                "updated": "2025-06-04T15:47:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    47,
                    7,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:47:07Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    47,
                    7,
                    2,
                    155,
                    0
                ],
                "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment"
                },
                "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ivanova"
                    },
                    {
                        "name": "Eva Bakaeva"
                    },
                    {
                        "name": "Zoya Volovikova"
                    },
                    {
                        "name": "Alexey K. Kovalev"
                    },
                    {
                        "name": "Aleksandr I. Panov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr I. Panov"
                },
                "author": "Aleksandr I. Panov",
                "arxiv_comment": "ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04088v1",
                "updated": "2025-06-04T15:46:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    46,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:46:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    46,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "Multimodal Tabular Reasoning with Privileged Structured Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Tabular Reasoning with Privileged Structured Information"
                },
                "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets."
                },
                "authors": [
                    {
                        "name": "Jun-Peng Jiang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04079v1",
                "updated": "2025-06-04T15:43:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    31,
                    2,
                    155,
                    0
                ],
                "title": "EuroLLM-9B: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuroLLM-9B: Technical Report"
                },
                "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset."
                },
                "authors": [
                    {
                        "name": "Pedro Henrique Martins"
                    },
                    {
                        "name": "Joo Alves"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "Mateusz Klimaszewski"
                    },
                    {
                        "name": "Duarte M. Alves"
                    },
                    {
                        "name": "Jos Pombal"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Franois Yvon"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Jos G. C. de Souza"
                    },
                    {
                        "name": "Alexandra Birch"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "56 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04078v1",
                "updated": "2025-06-04T15:43:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    14,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:43:14Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    14,
                    2,
                    155,
                    0
                ],
                "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation"
                },
                "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Binze Hu"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Chenhao Huang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04077v1",
                "updated": "2025-06-04T15:42:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    42,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:42:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    42,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on\n  Opinion Expressions"
                },
                "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information."
                },
                "authors": [
                    {
                        "name": "Chung-Chun Wang"
                    },
                    {
                        "name": "Jhen-Ke Lin"
                    },
                    {
                        "name": "Hao-Chien Lu"
                    },
                    {
                        "name": "Hong-Yun Lin"
                    },
                    {
                        "name": "Berlin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Berlin Chen"
                },
                "author": "Berlin Chen",
                "arxiv_comment": "submitted to the ISCA SLaTE-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04072v1",
                "updated": "2025-06-04T15:38:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    38,
                    21,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:38:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    38,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Controlling Difficulty of Generated Text for AI-Assisted Language\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Difficulty of Generated Text for AI-Assisted Language\n  Learning"
                },
                "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset."
                },
                "authors": [
                    {
                        "name": "Meiqing Jin"
                    },
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "Submitted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14194v3",
                "updated": "2025-06-04T15:35:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    35,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-04-19T06:12:33Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    12,
                    33,
                    5,
                    109,
                    0
                ],
                "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training\n  Language Models"
                },
                "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose four\ndimensions to evaluate data quality: professionalism, readability, reasoning,\nand cleanliness. We further introduce Meta-rater,a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with advantages that scale to models as large as 7.2B\nparameters. Our work establishes that holistic, multi-dimensional quality\nintegration significantly outperforms conventional single-dimension approaches,\noffering a scalable paradigm for enhancing pre-training efficiency and model\ncapability. To advance future research, we release scripts, data, and models at\nhttps://github.com/opendatalab/Meta-rater."
                },
                "authors": [
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Yinfan Wang"
                    },
                    {
                        "name": "Tianyi Bai"
                    },
                    {
                        "name": "Xingjian Wei"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Ying Qian"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04070v1",
                "updated": "2025-06-04T15:34:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    34,
                    33,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:34:33Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    34,
                    33,
                    2,
                    155,
                    0
                ],
                "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\n  Impaired via GRPO with LLM-as-Follower Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually\n  Impaired via GRPO with LLM-as-Follower Reward"
                },
                "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17169v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17169v4",
                "updated": "2025-06-04T15:32:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    32,
                    37,
                    2,
                    155,
                    0
                ],
                "published": "2024-09-17T22:40:54Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    22,
                    40,
                    54,
                    1,
                    261,
                    0
                ],
                "title": "REAL: Response Embedding-based Alignment for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REAL: Response Embedding-based Alignment for LLMs"
                },
                "summary": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\n(DPO) rely on pairs of AI-generated responses ranked according to human\nannotation. The response pair annotation process might bring human bias.\nBuilding a correct preference dataset is the costly part of the alignment\npipeline. To improve annotation efficiency and quality in the LLMs alignment,\nwe propose REAL: Response Embedding-based Alignment for LLMs, a strategy for\nconstructing a high-quality training dataset that focuses on acquiring the less\nambiguous preference pairs for labeling out of a set of response candidates.\nOur selection process is based on the similarity of embedding responses\nindependently of prompts, which guarantees the selection process in an\noff-policy setting, avoiding adaptively measuring the similarity during the\ntraining. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF\nbenchmarks indicate that choosing dissimilar response pairs enhances the direct\nalignment of LLMs while reducing inherited labeling errors. The model aligned\nwith dissimilar response pairs obtained a better margin and win rate on the\ndialogue task. Our findings suggest that focusing on distinct pairs can reduce\nthe label error and improve LLM alignment efficiency, saving up to $65\\%$ of\nannotators' work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) to human preferences is a crucial step\nin building helpful and safe AI tools, which usually involve training on\nsupervised datasets. Popular algorithms such as Direct Preference Optimization\n(DPO) rely on pairs of AI-generated responses ranked according to human\nannotation. The response pair annotation process might bring human bias.\nBuilding a correct preference dataset is the costly part of the alignment\npipeline. To improve annotation efficiency and quality in the LLMs alignment,\nwe propose REAL: Response Embedding-based Alignment for LLMs, a strategy for\nconstructing a high-quality training dataset that focuses on acquiring the less\nambiguous preference pairs for labeling out of a set of response candidates.\nOur selection process is based on the similarity of embedding responses\nindependently of prompts, which guarantees the selection process in an\noff-policy setting, avoiding adaptively measuring the similarity during the\ntraining. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF\nbenchmarks indicate that choosing dissimilar response pairs enhances the direct\nalignment of LLMs while reducing inherited labeling errors. The model aligned\nwith dissimilar response pairs obtained a better margin and win rate on the\ndialogue task. Our findings suggest that focusing on distinct pairs can reduce\nthe label error and improve LLM alignment efficiency, saving up to $65\\%$ of\nannotators' work."
                },
                "authors": [
                    {
                        "name": "Honggen Zhang"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Igor Molybog"
                    },
                    {
                        "name": "June Zhang"
                    }
                ],
                "author_detail": {
                    "name": "June Zhang"
                },
                "author": "June Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17169v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17169v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04065v1",
                "updated": "2025-06-04T15:31:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    31,
                    46,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:31:46Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    31,
                    46,
                    2,
                    155,
                    0
                ],
                "title": "Progressive Mastery: Customized Curriculum Learning with Guided\n  Prompting for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Mastery: Customized Curriculum Learning with Guided\n  Prompting for Mathematical Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance."
                },
                "authors": [
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Qi Qian"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Zisu Huang"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "LI Miao"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Zhenghua Wang"
                    },
                    {
                        "name": "Zhibo Xu"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Tianlong Li"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04063v1",
                "updated": "2025-06-04T15:26:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    26,
                    38,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:26:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    26,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "Crowd-SFT: Crowdsourcing for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowd-SFT: Crowdsourcing for LLM Alignment"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation."
                },
                "authors": [
                    {
                        "name": "Alex Sotiropoulos"
                    },
                    {
                        "name": "Sulyab Thottungal Valapu"
                    },
                    {
                        "name": "Linus Lei"
                    },
                    {
                        "name": "Jared Coleman"
                    },
                    {
                        "name": "Bhaskar Krishnamachari"
                    }
                ],
                "author_detail": {
                    "name": "Bhaskar Krishnamachari"
                },
                "author": "Bhaskar Krishnamachari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04055v1",
                "updated": "2025-06-04T15:19:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    19,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:19:26Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    19,
                    26,
                    2,
                    155,
                    0
                ],
                "title": "chemtrain-deploy: A parallel and scalable framework for machine learning\n  potentials in million-atom MD simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "chemtrain-deploy: A parallel and scalable framework for machine learning\n  potentials in million-atom MD simulations"
                },
                "summary": "Machine learning potentials (MLPs) have advanced rapidly and show great\npromise to transform molecular dynamics (MD) simulations. However, most\nexisting software tools are tied to specific MLP architectures, lack\nintegration with standard MD packages, or are not parallelizable across GPUs.\nTo address these challenges, we present chemtrain-deploy, a framework that\nenables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports\nany JAX-defined semi-local potential, allowing users to exploit the\nfunctionality of LAMMPS and perform large-scale MLP-based MD simulations on\nmultiple GPUs. It achieves state-of-the-art efficiency and scales to systems\ncontaining millions of atoms. We validate its performance and scalability using\ngraph neural network architectures, including MACE, Allegro, and PaiNN, applied\nto a variety of systems, such as liquid-vapor interfaces, crystalline\nmaterials, and solvated peptides. Our results highlight the practical utility\nof chemtrain-deploy for real-world, high-performance simulations and provide\nguidance for MLP architecture selection and future design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning potentials (MLPs) have advanced rapidly and show great\npromise to transform molecular dynamics (MD) simulations. However, most\nexisting software tools are tied to specific MLP architectures, lack\nintegration with standard MD packages, or are not parallelizable across GPUs.\nTo address these challenges, we present chemtrain-deploy, a framework that\nenables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports\nany JAX-defined semi-local potential, allowing users to exploit the\nfunctionality of LAMMPS and perform large-scale MLP-based MD simulations on\nmultiple GPUs. It achieves state-of-the-art efficiency and scales to systems\ncontaining millions of atoms. We validate its performance and scalability using\ngraph neural network architectures, including MACE, Allegro, and PaiNN, applied\nto a variety of systems, such as liquid-vapor interfaces, crystalline\nmaterials, and solvated peptides. Our results highlight the practical utility\nof chemtrain-deploy for real-world, high-performance simulations and provide\nguidance for MLP architecture selection and future design."
                },
                "authors": [
                    {
                        "name": "Paul Fuchs"
                    },
                    {
                        "name": "Weilong Chen"
                    },
                    {
                        "name": "Stephan Thaler"
                    },
                    {
                        "name": "Julija Zavadlav"
                    }
                ],
                "author_detail": {
                    "name": "Julija Zavadlav"
                },
                "author": "Julija Zavadlav",
                "arxiv_comment": "Source code available at: https://github.com/tummfm/chemtrain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04051v1",
                "updated": "2025-06-04T15:16:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    16,
                    21,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:16:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    16,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through\n  Capability-Aligned Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Accuracy, Less Talk (HALT): Reliable LLMs through\n  Capability-Aligned Finetuning"
                },
                "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning."
                },
                "authors": [
                    {
                        "name": "Tim Franzmeyer"
                    },
                    {
                        "name": "Archie Sravankumar"
                    },
                    {
                        "name": "Lijuan Liu"
                    },
                    {
                        "name": "Yuning Mao"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Sinong Wang"
                    },
                    {
                        "name": "Jakob N. Foerster"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Madian Khabsa"
                    }
                ],
                "author_detail": {
                    "name": "Madian Khabsa"
                },
                "author": "Madian Khabsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23276v2",
                "updated": "2025-06-04T15:16:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    16,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-29T09:24:00Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    24,
                    0,
                    3,
                    149,
                    0
                ],
                "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large\n  Language Models Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large\n  Language Models Text"
                },
                "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts."
                },
                "authors": [
                    {
                        "name": "Maged S. Al-Shaibani"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04050v1",
                "updated": "2025-06-04T15:15:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    15,
                    42,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:15:42Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    15,
                    42,
                    2,
                    155,
                    0
                ],
                "title": "Explainability-Based Token Replacement on LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability-Based Token Replacement on LLM-Generated Text"
                },
                "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    },
                    {
                        "name": "Daniel L. Oberski"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    }
                ],
                "author_detail": {
                    "name": "Ayoub Bagheri"
                },
                "author": "Ayoub Bagheri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v7",
                "updated": "2025-06-04T15:11:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    11,
                    0,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper, we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two\nnovel approaches to repair unwanted false mappings caused by Phase 2 text\npreprocessing. One is an ad hoc logic-based repair approach that employs an\nontology-specific check to find common words that cause false mappings. These\nwords are stored in a reserved word set and applied before the text\npreprocessing. By leveraging the power of large language models (LLMs), we also\npropose a post hoc LLM-based repair approach. This approach utilises the strong\nbackground knowledge provided by LLMs to repair non-existent and\ncounter-intuitive false mappings after the text preprocessing. It also\novercomes the tendency towards unstable true mappings by injecting the classic\ntext preprocessing pipeline via function calling. The experimental results show\nthat these two approaches can improve the matching correctness and the overall\nmatching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper, we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two\nnovel approaches to repair unwanted false mappings caused by Phase 2 text\npreprocessing. One is an ad hoc logic-based repair approach that employs an\nontology-specific check to find common words that cause false mappings. These\nwords are stored in a reserved word set and applied before the text\npreprocessing. By leveraging the power of large language models (LLMs), we also\npropose a post hoc LLM-based repair approach. This approach utilises the strong\nbackground knowledge provided by LLMs to repair non-existent and\ncounter-intuitive false mappings after the text preprocessing. It also\novercomes the tendency towards unstable true mappings by injecting the classic\ntext preprocessing pipeline via function calling. The experimental results show\nthat these two approaches can improve the matching correctness and the overall\nmatching performance."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "13 pages, 14 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04044v1",
                "updated": "2025-06-04T15:10:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    10,
                    9,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:10:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    10,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based\n  Unlearning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based\n  Unlearning for LLMs"
                },
                "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task."
                },
                "authors": [
                    {
                        "name": "Aleksey Kudelya"
                    },
                    {
                        "name": "Alexander Shirnin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Shirnin"
                },
                "author": "Alexander Shirnin",
                "arxiv_comment": "Accepted to SemEval-2025, an ACL 2025 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04043v1",
                "updated": "2025-06-04T15:09:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    9,
                    20,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:09:20Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    9,
                    20,
                    2,
                    155,
                    0
                ],
                "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of\n  Persona-Guided LLMs for Countering Hate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of\n  Persona-Guided LLMs for Countering Hate"
                },
                "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness."
                },
                "authors": [
                    {
                        "name": "Mikel K. Ngueajio"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Yi-Ling Chung"
                    },
                    {
                        "name": "Danda B. Rawat"
                    },
                    {
                        "name": "Amanda Cercas Curry"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Cercas Curry"
                },
                "author": "Amanda Cercas Curry",
                "arxiv_comment": "Accepted at ACL WOAH 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04041v1",
                "updated": "2025-06-04T15:06:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    6,
                    27,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:06:27Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    6,
                    27,
                    2,
                    155,
                    0
                ],
                "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexTime: A Benchmark for Temporal Ordering of Legal Events"
                },
                "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning."
                },
                "authors": [
                    {
                        "name": "Claire Barale"
                    },
                    {
                        "name": "Leslie Barrett"
                    },
                    {
                        "name": "Vikram Sunil Bajaj"
                    },
                    {
                        "name": "Michael Rovatsos"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rovatsos"
                },
                "author": "Michael Rovatsos",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04039v1",
                "updated": "2025-06-04T15:03:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    3,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:03:50Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    3,
                    50,
                    2,
                    155,
                    0
                ],
                "title": "Mitigating Hallucinations in Large Vision-Language Models via\n  Entity-Centric Multimodal Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucinations in Large Vision-Language Models via\n  Entity-Centric Multimodal Preference Optimization"
                },
                "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench."
                },
                "authors": [
                    {
                        "name": "Jiulong Wu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Jizhou Huang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Min Cao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04038v1",
                "updated": "2025-06-04T15:01:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    1,
                    59,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T15:01:59Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    1,
                    59,
                    2,
                    155,
                    0
                ],
                "title": "Generating Automotive Code: Large Language Models for Software\n  Development and Verification in Safety-Critical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Automotive Code: Large Language Models for Software\n  Development and Verification in Safety-Critical Systems"
                },
                "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements."
                },
                "authors": [
                    {
                        "name": "Sven Kirchner"
                    },
                    {
                        "name": "Alois C. Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois C. Knoll"
                },
                "author": "Alois C. Knoll",
                "arxiv_comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02867v2",
                "updated": "2025-06-04T15:00:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    0,
                    58,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T13:31:10Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    31,
                    10,
                    1,
                    154,
                    0
                ],
                "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens\n  are Information Peaks in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens\n  are Information Peaks in LLM Reasoning"
                },
                "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks."
                },
                "authors": [
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Haochen Wen"
                    },
                    {
                        "name": "Zhen Bai"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04036v1",
                "updated": "2025-06-04T14:58:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    58,
                    29,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:58:29Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    58,
                    29,
                    2,
                    155,
                    0
                ],
                "title": "Privacy and Security Threat for OpenAI GPTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy and Security Threat for OpenAI GPTs"
                },
                "summary": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications."
                },
                "authors": [
                    {
                        "name": "Wei Wenying"
                    },
                    {
                        "name": "Zhao Kaifa"
                    },
                    {
                        "name": "Xue Lei"
                    },
                    {
                        "name": "Fan Ming"
                    }
                ],
                "author_detail": {
                    "name": "Fan Ming"
                },
                "author": "Fan Ming",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02524v3",
                "updated": "2025-06-04T14:57:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    57,
                    0,
                    2,
                    155,
                    0
                ],
                "published": "2024-06-04T17:42:21Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    42,
                    21,
                    1,
                    156,
                    0
                ],
                "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks"
                },
                "summary": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Tanja Srindran"
                    },
                    {
                        "name": "Tomasz Lehmann"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04032v1",
                "updated": "2025-06-04T14:56:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    56,
                    8,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:56:08Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    56,
                    8,
                    2,
                    155,
                    0
                ],
                "title": "AI Agents for Conversational Patient Triage: Preliminary\n  Simulation-Based Evaluation with Real-World EHR Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents for Conversational Patient Triage: Preliminary\n  Simulation-Based Evaluation with Real-World EHR Data"
                },
                "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale."
                },
                "authors": [
                    {
                        "name": "Sina Rashidian"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Jonathan Amar"
                    },
                    {
                        "name": "Jong Ha Lee"
                    },
                    {
                        "name": "Sam Pugh"
                    },
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Geoff Masterson"
                    },
                    {
                        "name": "Myoung Cha"
                    },
                    {
                        "name": "Yugang Jia"
                    },
                    {
                        "name": "Akhil Vaid"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Vaid"
                },
                "author": "Akhil Vaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04019v1",
                "updated": "2025-06-04T14:47:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    47,
                    14,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:47:14Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    47,
                    14,
                    2,
                    155,
                    0
                ],
                "title": "CETBench: A Novel Dataset constructed via Transformations over Programs\n  for Benchmarking LLMs for Code-Equivalence Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CETBench: A Novel Dataset constructed via Transformations over Programs\n  for Benchmarking LLMs for Code-Equivalence Checking"
                },
                "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code."
                },
                "authors": [
                    {
                        "name": "Neeva Oza"
                    },
                    {
                        "name": "Ishaan Govil"
                    },
                    {
                        "name": "Parul Gupta"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Dinesh Garg"
                    },
                    {
                        "name": "Parag Singla"
                    }
                ],
                "author_detail": {
                    "name": "Parag Singla"
                },
                "author": "Parag Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04018v1",
                "updated": "2025-06-04T14:46:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:46:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in\n  LLM-Based Agents"
                },
                "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent."
                },
                "authors": [
                    {
                        "name": "Akshat Naik"
                    },
                    {
                        "name": "Patrick Quinn"
                    },
                    {
                        "name": "Guillermo Bosch"
                    },
                    {
                        "name": "Emma Goun"
                    },
                    {
                        "name": "Francisco Javier Campos Zabala"
                    },
                    {
                        "name": "Jason Ross Brown"
                    },
                    {
                        "name": "Edward James Young"
                    }
                ],
                "author_detail": {
                    "name": "Edward James Young"
                },
                "author": "Edward James Young",
                "arxiv_comment": "Prepint, under review for NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; K.4.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04015v1",
                "updated": "2025-06-04T14:46:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    18,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:46:18Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    46,
                    18,
                    2,
                    155,
                    0
                ],
                "title": "GORACS: Group-level Optimal Transport-guided Coreset Selection for\n  LLM-based Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GORACS: Group-level Optimal Transport-guided Coreset Selection for\n  LLM-based Recommender Systems"
                },
                "summary": "Although large language models (LLMs) have shown great potential in\nrecommender systems, the prohibitive computational costs for fine-tuning LLMs\non entire datasets hinder their successful deployment in real-world scenarios.\nTo develop affordable and effective LLM-based recommender systems, we focus on\nthe task of coreset selection which identifies a small subset of fine-tuning\ndata to optimize the test loss, thereby facilitating efficient LLMs'\nfine-tuning. Although there exist some intuitive solutions of subset selection,\nincluding distribution-based and importance-based approaches, they often lead\nto suboptimal performance due to the misalignment with downstream fine-tuning\nobjectives or weak generalization ability caused by individual-level sample\nselection. To overcome these challenges, we propose GORACS, which is a novel\nGroup-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based\nrecommender systems. GORACS is designed based on two key principles for coreset\nselection: 1) selecting the subsets that minimize the test loss to align with\nfine-tuning objectives, and 2) enhancing model generalization through\ngroup-level data selection. Corresponding to these two principles, GORACS has\ntwo key components: 1) a Proxy Optimization Objective (POO) leveraging optimal\ntransport and gradient information to bound the intractable test loss, thus\nreducing computational costs by avoiding repeated LLM retraining, and 2) a\ntwo-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient\ngroup-level selection. Our extensive experiments across diverse recommendation\ndatasets and tasks validate that GORACS significantly reduces fine-tuning costs\nof LLMs while achieving superior performance over the state-of-the-art\nbaselines and full data training. The source code of GORACS are available at\nhttps://github.com/Mithas-114/GORACS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have shown great potential in\nrecommender systems, the prohibitive computational costs for fine-tuning LLMs\non entire datasets hinder their successful deployment in real-world scenarios.\nTo develop affordable and effective LLM-based recommender systems, we focus on\nthe task of coreset selection which identifies a small subset of fine-tuning\ndata to optimize the test loss, thereby facilitating efficient LLMs'\nfine-tuning. Although there exist some intuitive solutions of subset selection,\nincluding distribution-based and importance-based approaches, they often lead\nto suboptimal performance due to the misalignment with downstream fine-tuning\nobjectives or weak generalization ability caused by individual-level sample\nselection. To overcome these challenges, we propose GORACS, which is a novel\nGroup-level Optimal tRAnsport-guided Coreset Selection framework for LLM-based\nrecommender systems. GORACS is designed based on two key principles for coreset\nselection: 1) selecting the subsets that minimize the test loss to align with\nfine-tuning objectives, and 2) enhancing model generalization through\ngroup-level data selection. Corresponding to these two principles, GORACS has\ntwo key components: 1) a Proxy Optimization Objective (POO) leveraging optimal\ntransport and gradient information to bound the intractable test loss, thus\nreducing computational costs by avoiding repeated LLM retraining, and 2) a\ntwo-stage Initialization-Then-Refinement Algorithm (ITRA) for efficient\ngroup-level selection. Our extensive experiments across diverse recommendation\ndatasets and tasks validate that GORACS significantly reduces fine-tuning costs\nof LLMs while achieving superior performance over the state-of-the-art\nbaselines and full data training. The source code of GORACS are available at\nhttps://github.com/Mithas-114/GORACS."
                },
                "authors": [
                    {
                        "name": "Tiehua Mei"
                    },
                    {
                        "name": "Hengrui Chen"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_doi": "10.1145/3711896.3736985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3736985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.04015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02943v2",
                "updated": "2025-06-04T14:43:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    43,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T14:43:05Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    43,
                    5,
                    1,
                    154,
                    0
                ],
                "title": "A Multi-agent LLM-based JUnit Test Generation with Strong Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent LLM-based JUnit Test Generation with Strong Oracles"
                },
                "summary": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy."
                },
                "authors": [
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Guancheng Wang"
                    },
                    {
                        "name": "Lionel Briand"
                    },
                    {
                        "name": "Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Liu"
                },
                "author": "Kui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06706v2",
                "updated": "2025-06-04T14:31:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    31,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-09T17:43:30Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    30,
                    6,
                    68,
                    0
                ],
                "title": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts"
                },
                "summary": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Tingyi Yang"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Qinhao Chen"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03998v1",
                "updated": "2025-06-04T14:27:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    27,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:27:50Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    27,
                    50,
                    2,
                    155,
                    0
                ],
                "title": "The QTF-Backbone: Proposal for a Nationwide Optical Fibre Backbone in\n  Germany for Quantum Technology and Time and Frequency Metrology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The QTF-Backbone: Proposal for a Nationwide Optical Fibre Backbone in\n  Germany for Quantum Technology and Time and Frequency Metrology"
                },
                "summary": "The recent breakthroughs in the distribution of quantum information and\nhigh-precision time and frequency (T&F) signals over long-haul optical fibre\nnetworks have transformative potential for physically secure communications,\nresilience of Global Navigation Satellite Systems (GNSS) and fundamental\nphysics. However, so far these capabilities remain confined to isolated\ntestbeds, with quantum and T&F signals accessible, for example in Germany, to\nonly a few institutions.\n  We propose the QTF-Backbone: a dedicated national fibre-optic infrastructure\nin Germany for the networked distribution of quantum and T&F signals using dark\nfibres and specialized hardware. The QTF-Backbone is planned as a four-phase\ndeployment over ten years to ensure scalable, sustainable access for research\ninstitutions and industry. The concept builds on successful demonstrations of\nhigh-TRL time and frequency distribution across Europe, including PTB-MPQ links\nin Germany, REFIMEVE in France, and the Italian LIFT network. The QTF-Backbone\nwill enable transformative R&D, support a nationwide QTF ecosystem, and ensure\nthe transition from innovation to deployment. As a national and European hub,\nit will position Germany and Europe at the forefront of quantum networking, as\nwell as time and frequency transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthroughs in the distribution of quantum information and\nhigh-precision time and frequency (T&F) signals over long-haul optical fibre\nnetworks have transformative potential for physically secure communications,\nresilience of Global Navigation Satellite Systems (GNSS) and fundamental\nphysics. However, so far these capabilities remain confined to isolated\ntestbeds, with quantum and T&F signals accessible, for example in Germany, to\nonly a few institutions.\n  We propose the QTF-Backbone: a dedicated national fibre-optic infrastructure\nin Germany for the networked distribution of quantum and T&F signals using dark\nfibres and specialized hardware. The QTF-Backbone is planned as a four-phase\ndeployment over ten years to ensure scalable, sustainable access for research\ninstitutions and industry. The concept builds on successful demonstrations of\nhigh-TRL time and frequency distribution across Europe, including PTB-MPQ links\nin Germany, REFIMEVE in France, and the Italian LIFT network. The QTF-Backbone\nwill enable transformative R&D, support a nationwide QTF ecosystem, and ensure\nthe transition from innovation to deployment. As a national and European hub,\nit will position Germany and Europe at the forefront of quantum networking, as\nwell as time and frequency transfer."
                },
                "authors": [
                    {
                        "name": "Klaus Blaum"
                    },
                    {
                        "name": "Peter Kaufmann"
                    },
                    {
                        "name": "Jochen Kronjger"
                    },
                    {
                        "name": "Stefan Kck"
                    },
                    {
                        "name": "Tara Cubel Liebisch"
                    },
                    {
                        "name": "Dieter Meschede"
                    },
                    {
                        "name": "Susanne Naegele-Jackson"
                    },
                    {
                        "name": "Stephan Schiller"
                    },
                    {
                        "name": "Harald Schnatz"
                    }
                ],
                "author_detail": {
                    "name": "Harald Schnatz"
                },
                "arxiv_affiliation": "Physikalisch-Technische Bundesanstalt",
                "author": "Harald Schnatz",
                "arxiv_comment": "51 pages, 7 figures, 9 tables, 79 contributors in addition to the 9\n  authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02426v2",
                "updated": "2025-06-04T14:21:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    21,
                    2,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T04:19:47Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    4,
                    19,
                    47,
                    1,
                    154,
                    0
                ],
                "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of AI Agent Architectures for Entity Relationship\n  Classification"
                },
                "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at https://github.com/maryambrj/ALIEN.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at https://github.com/maryambrj/ALIEN.git."
                },
                "authors": [
                    {
                        "name": "Maryam Berijanian"
                    },
                    {
                        "name": "Kuldeep Singh"
                    },
                    {
                        "name": "Amin Sehati"
                    }
                ],
                "author_detail": {
                    "name": "Amin Sehati"
                },
                "author": "Amin Sehati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03990v1",
                "updated": "2025-06-04T14:17:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    17,
                    42,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:17:42Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    17,
                    42,
                    2,
                    155,
                    0
                ],
                "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective\n  Video Understanding"
                },
                "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques."
                },
                "authors": [
                    {
                        "name": "Hongzhi Zhang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fuzheng Zhang"
                },
                "author": "Fuzheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03984v1",
                "updated": "2025-06-04T14:14:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    14,
                    28,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:14:28Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    14,
                    28,
                    2,
                    155,
                    0
                ],
                "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place"
                },
                "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks."
                },
                "authors": [
                    {
                        "name": "Carolin Holtermann"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Anne Lauscher"
                    }
                ],
                "author_detail": {
                    "name": "Anne Lauscher"
                },
                "author": "Anne Lauscher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12593v2",
                "updated": "2025-06-04T14:08:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    8,
                    57,
                    2,
                    155,
                    0
                ],
                "published": "2024-10-16T14:12:11Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    12,
                    11,
                    2,
                    290,
                    0
                ],
                "title": "Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expand and Compress: Exploring Tuning Principles for Continual\n  Spatio-Temporal Graph Forecasting"
                },
                "summary": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of sensing devices leads to a surge in data for\nspatio-temporal forecasting applications such as traffic flow, air quality, and\nwind energy. Although spatio-temporal graph neural networks have achieved\nsuccess in modeling various static spatio-temporal forecasting scenarios,\nreal-world spatio-temporal data are typically received in a streaming manner,\nand the network continuously expands with the installation of new sensors.\nThus, spatio-temporal forecasting in streaming scenarios faces dual challenges:\nthe inefficiency of retraining models over newly arrived data and the\ndetrimental effects of catastrophic forgetting over long-term history. To\naddress these challenges, we propose a novel prompt tuning-based continuous\nforecasting method, following two fundamental tuning principles guided by\nempirical and theoretical analysis: expand and compress, which effectively\nresolve the aforementioned problems with lightweight tuning parameters.\nSpecifically, we integrate the base spatio-temporal graph neural network with a\ncontinuous prompt pool, utilizing stored prompts (i.e., few learnable\nparameters) in memory, and jointly optimize them with the base spatio-temporal\ngraph neural network. This method ensures that the model sequentially learns\nfrom the spatio-temporal data stream to accomplish tasks for corresponding\nperiods. Extensive experimental results on multiple real-world datasets\ndemonstrate the multi-faceted superiority of our method over the\nstate-of-the-art baselines, including effectiveness, efficiency, universality,\netc."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03972v1",
                "updated": "2025-06-04T14:02:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    2,
                    24,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:02:24Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    2,
                    24,
                    2,
                    155,
                    0
                ],
                "title": "MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell\n  Detection"
                },
                "summary": "Complete blood cell detection holds significant value in clinical\ndiagnostics. Conventional manual microscopy methods suffer from time\ninefficiency and diagnostic inaccuracies. Existing automated detection\napproaches remain constrained by high deployment costs and suboptimal accuracy.\nWhile deep learning has introduced powerful paradigms to this field, persistent\nchallenges in detecting overlapping cells and multi-scale objects hinder\npractical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a\nblood cell detection model based on the YOLOv11 framework, incorporating three\nkey architectural innovations to enhance detection performance. Specifically,\nthe multi-scale dilated residual module (MS-DRM) replaces the original C3K2\nmodules to improve multi-scale discriminability; the dynamic cross-path feature\nenhancement module (DCFEM) enables the fusion of hierarchical features from the\nbackbone with aggregated features from the neck to enhance feature\nrepresentations; and the light adaptive-weight downsampling module (LADS)\nimproves feature downsampling through adaptive spatial weighting while reducing\ncomputational complexity. Experimental results on the CBC benchmark demonstrate\nthat MS-YOLO achieves precise detection of overlapping cells and multi-scale\nobjects, particularly small targets such as platelets, achieving an mAP@50 of\n97.4% that outperforms existing models. Further validation on the supplementary\nWBCDD dataset confirms its robust generalization capability. Additionally, with\na lightweight architecture and real-time inference efficiency, MS-YOLO meets\nclinical deployment requirements, providing reliable technical support for\nstandardized blood pathology assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complete blood cell detection holds significant value in clinical\ndiagnostics. Conventional manual microscopy methods suffer from time\ninefficiency and diagnostic inaccuracies. Existing automated detection\napproaches remain constrained by high deployment costs and suboptimal accuracy.\nWhile deep learning has introduced powerful paradigms to this field, persistent\nchallenges in detecting overlapping cells and multi-scale objects hinder\npractical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a\nblood cell detection model based on the YOLOv11 framework, incorporating three\nkey architectural innovations to enhance detection performance. Specifically,\nthe multi-scale dilated residual module (MS-DRM) replaces the original C3K2\nmodules to improve multi-scale discriminability; the dynamic cross-path feature\nenhancement module (DCFEM) enables the fusion of hierarchical features from the\nbackbone with aggregated features from the neck to enhance feature\nrepresentations; and the light adaptive-weight downsampling module (LADS)\nimproves feature downsampling through adaptive spatial weighting while reducing\ncomputational complexity. Experimental results on the CBC benchmark demonstrate\nthat MS-YOLO achieves precise detection of overlapping cells and multi-scale\nobjects, particularly small targets such as platelets, achieving an mAP@50 of\n97.4% that outperforms existing models. Further validation on the supplementary\nWBCDD dataset confirms its robust generalization capability. Additionally, with\na lightweight architecture and real-time inference efficiency, MS-YOLO meets\nclinical deployment requirements, providing reliable technical support for\nstandardized blood pathology assessment."
                },
                "authors": [
                    {
                        "name": "Guohua Wu"
                    },
                    {
                        "name": "Shengqi Chen"
                    },
                    {
                        "name": "Pengchao Deng"
                    },
                    {
                        "name": "Wenting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Wenting Yu"
                },
                "author": "Wenting Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03968v1",
                "updated": "2025-06-04T14:00:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    0,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T14:00:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    14,
                    0,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding"
                },
                "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions."
                },
                "authors": [
                    {
                        "name": "Chiwei Zhu"
                    },
                    {
                        "name": "Benfeng Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "To be published at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v2",
                "updated": "2025-06-04T13:45:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    45,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01203v2",
                "updated": "2025-06-04T13:43:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    43,
                    34,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-03T09:50:30Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    50,
                    30,
                    0,
                    34,
                    0
                ],
                "title": "Theoretical Analysis of KL-regularized RLHF with Multiple Reference\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Analysis of KL-regularized RLHF with Multiple Reference\n  Models"
                },
                "summary": "Recent methods for aligning large language models (LLMs) with human feedback\npredominantly rely on a single reference model, which limits diversity, model\noverfitting, and underutilizes the wide range of available pre-trained models.\nIncorporating multiple reference models has the potential to address these\nlimitations by broadening perspectives, reducing bias, and leveraging the\nstrengths of diverse open-source LLMs. However, integrating multiple reference\nmodels into reinforcement learning with human feedback (RLHF) frameworks poses\nsignificant theoretical challenges, where achieving exact solutions has\nremained an open problem. This paper presents the first \\emph{exact solution}\nto the multiple reference model problem in reverse KL-regularized RLHF. We\nintroduce a comprehensive theoretical framework that includes rigorous\nstatistical analysis and provides sample complexity guarantees. Additionally,\nwe extend our analysis to forward KL-regularized RLHF, offering new insights\ninto sample complexity requirements in multiple reference scenarios. Our\ncontributions lay the foundation for more advanced and adaptable LLM alignment\ntechniques, enabling the effective use of multiple reference models. This work\npaves the way for developing alignment frameworks that are both theoretically\nsound and better suited to the challenges of modern AI ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methods for aligning large language models (LLMs) with human feedback\npredominantly rely on a single reference model, which limits diversity, model\noverfitting, and underutilizes the wide range of available pre-trained models.\nIncorporating multiple reference models has the potential to address these\nlimitations by broadening perspectives, reducing bias, and leveraging the\nstrengths of diverse open-source LLMs. However, integrating multiple reference\nmodels into reinforcement learning with human feedback (RLHF) frameworks poses\nsignificant theoretical challenges, where achieving exact solutions has\nremained an open problem. This paper presents the first \\emph{exact solution}\nto the multiple reference model problem in reverse KL-regularized RLHF. We\nintroduce a comprehensive theoretical framework that includes rigorous\nstatistical analysis and provides sample complexity guarantees. Additionally,\nwe extend our analysis to forward KL-regularized RLHF, offering new insights\ninto sample complexity requirements in multiple reference scenarios. Our\ncontributions lay the foundation for more advanced and adaptable LLM alignment\ntechniques, enabling the effective use of multiple reference models. This work\npaves the way for developing alignment frameworks that are both theoretically\nsound and better suited to the challenges of modern AI ecosystems."
                },
                "authors": [
                    {
                        "name": "Gholamali Aminian"
                    },
                    {
                        "name": "Amir R. Asadi"
                    },
                    {
                        "name": "Idan Shenfeld"
                    },
                    {
                        "name": "Youssef Mroueh"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Mroueh"
                },
                "author": "Youssef Mroueh",
                "arxiv_comment": "Experiments are added in new version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00539v2",
                "updated": "2025-06-04T13:39:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    54,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-31T12:54:49Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    12,
                    54,
                    49,
                    5,
                    151,
                    0
                ],
                "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation"
                },
                "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03949v1",
                "updated": "2025-06-04T13:39:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    1,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:39:01Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    39,
                    1,
                    2,
                    155,
                    0
                ],
                "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableEval: A Real-World Benchmark for Complex, Multilingual, and\n  Multi-Structured Table Question Answering"
                },
                "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval."
                },
                "authors": [
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Junbo Li"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Nan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Xu"
                },
                "author": "Nan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14309v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14309v4",
                "updated": "2025-06-04T13:35:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    35,
                    31,
                    2,
                    155,
                    0
                ],
                "published": "2024-10-18T09:15:35Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    15,
                    35,
                    4,
                    292,
                    0
                ],
                "title": "LoGU: Long-form Generation with Uncertainty Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoGU: Long-form Generation with Uncertainty Expressions"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14309v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14309v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03946v1",
                "updated": "2025-06-04T13:33:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    33,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:33:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    33,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "Automatic Multi-level Feature Tree Construction for Domain-Specific\n  Reusable Artifacts Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Multi-level Feature Tree Construction for Domain-Specific\n  Reusable Artifacts Management"
                },
                "summary": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises."
                },
                "authors": [
                    {
                        "name": "Dongming Jin"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Nianyu Li"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Linyu Li"
                    },
                    {
                        "name": "Suijing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Suijing Guan"
                },
                "author": "Suijing Guan",
                "arxiv_comment": "9pages, 2figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03939v1",
                "updated": "2025-06-04T13:31:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    31,
                    21,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:31:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    31,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to\n  Enhance LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to\n  Enhance LLM Reasoning"
                },
                "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git."
                },
                "authors": [
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Xiang Zou"
                    },
                    {
                        "name": "YIng Ai"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Yichen Niu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Jianxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxing Liu"
                },
                "author": "Jianxing Liu",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03938v1",
                "updated": "2025-06-04T13:30:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    30,
                    47,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:30:47Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    30,
                    47,
                    2,
                    155,
                    0
                ],
                "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A\n  Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPGA-Enabled Machine Learning Applications in Earth Observation: A\n  Systematic Review"
                },
                "summary": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA."
                },
                "authors": [
                    {
                        "name": "Cdric Lonard"
                    },
                    {
                        "name": "Dirk Stober"
                    },
                    {
                        "name": "Martin Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schulz"
                },
                "arxiv_affiliation": "Technical University of Munich, Munich, Germany",
                "author": "Martin Schulz",
                "arxiv_comment": "35 pages, 3 figures, 2 tables. Submitted to ACM Computing Surveys\n  (ACM CSUR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03930v1",
                "updated": "2025-06-04T13:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    24,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:24:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    24,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation"
                },
                "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation."
                },
                "authors": [
                    {
                        "name": "Yuansheng Ni"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00264v2",
                "updated": "2025-06-04T13:23:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    23,
                    25,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-30T21:55:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    21,
                    55,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "MultiHoax: A Dataset of Multi-hop False-Premise Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiHoax: A Dataset of Multi-hop False-Premise Questions"
                },
                "summary": "As Large Language Models are increasingly deployed in high-stakes domains,\ntheir ability to detect false assumptions and reason critically is crucial for\nensuring reliable outputs. False-premise questions (FPQs) serve as an important\nevaluation method by exposing cases where flawed assumptions lead to incorrect\nresponses. While existing benchmarks focus on single-hop FPQs, real-world\nreasoning often requires multi-hop inference, where models must verify\nconsistency across multiple reasoning steps rather than relying on\nsurface-level cues. To address this gap, we introduce MultiHoax, a benchmark\nfor evaluating LLMs' ability to handle false premises in complex, multi-step\nreasoning tasks. Our dataset spans seven countries and ten diverse knowledge\ncategories, using Wikipedia as the primary knowledge source to enable factual\nreasoning across regions. Experiments reveal that state-of-the-art LLMs\nstruggle to detect false premises across different countries, knowledge\ncategories, and multi-hop reasoning types, highlighting the need for improved\nfalse premise detection and more robust multi-hop reasoning capabilities in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models are increasingly deployed in high-stakes domains,\ntheir ability to detect false assumptions and reason critically is crucial for\nensuring reliable outputs. False-premise questions (FPQs) serve as an important\nevaluation method by exposing cases where flawed assumptions lead to incorrect\nresponses. While existing benchmarks focus on single-hop FPQs, real-world\nreasoning often requires multi-hop inference, where models must verify\nconsistency across multiple reasoning steps rather than relying on\nsurface-level cues. To address this gap, we introduce MultiHoax, a benchmark\nfor evaluating LLMs' ability to handle false premises in complex, multi-step\nreasoning tasks. Our dataset spans seven countries and ten diverse knowledge\ncategories, using Wikipedia as the primary knowledge source to enable factual\nreasoning across regions. Experiments reveal that state-of-the-art LLMs\nstruggle to detect false premises across different countries, knowledge\ncategories, and multi-hop reasoning types, highlighting the need for improved\nfalse premise detection and more robust multi-hop reasoning capabilities in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mohammadamin Shafiei"
                    },
                    {
                        "name": "Hamidreza Saffari"
                    },
                    {
                        "name": "Nafise Sadat Moosavi"
                    }
                ],
                "author_detail": {
                    "name": "Nafise Sadat Moosavi"
                },
                "author": "Nafise Sadat Moosavi",
                "arxiv_comment": "accepted at ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03928v1",
                "updated": "2025-06-04T13:22:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    22,
                    35,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:22:35Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    22,
                    35,
                    2,
                    155,
                    0
                ],
                "title": "Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with\n  Vision Feature Resample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with\n  Vision Feature Resample"
                },
                "summary": "In this work, we study the Efficient Multimodal Large Language Model.\nRedundant vision tokens consume a significant amount of computational memory\nand resources. Therefore, many previous works compress them in the Vision\nProjector to reduce the number of vision tokens. However, simply compressing in\nthe Vision Projector can lead to the loss of visual information, especially for\ntasks that rely on fine-grained spatial relationships, such as OCR and Chart \\&\nTable Understanding. To address this problem, we propose Vision Remember, which\nis inserted between the LLM decoder layers to allow vision tokens to\nre-memorize vision features. Specifically, we retain multi-level vision\nfeatures and resample them with the vision tokens that have interacted with the\ntext token. During the resampling process, each vision token only attends to a\nlocal region in vision features, which is referred to as saliency-enhancing\nlocal attention. Saliency-enhancing local attention not only improves\ncomputational efficiency but also captures more fine-grained contextual\ninformation and spatial relationships within the region. Comprehensive\nexperiments on multiple visual understanding benchmarks validate the\neffectiveness of our method when combined with various Efficient Vision\nProjectors, showing performance gains without sacrificing efficiency. Based on\nVision Remember, LLaVA-VR with only 2B parameters is also superior to previous\nrepresentative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we study the Efficient Multimodal Large Language Model.\nRedundant vision tokens consume a significant amount of computational memory\nand resources. Therefore, many previous works compress them in the Vision\nProjector to reduce the number of vision tokens. However, simply compressing in\nthe Vision Projector can lead to the loss of visual information, especially for\ntasks that rely on fine-grained spatial relationships, such as OCR and Chart \\&\nTable Understanding. To address this problem, we propose Vision Remember, which\nis inserted between the LLM decoder layers to allow vision tokens to\nre-memorize vision features. Specifically, we retain multi-level vision\nfeatures and resample them with the vision tokens that have interacted with the\ntext token. During the resampling process, each vision token only attends to a\nlocal region in vision features, which is referred to as saliency-enhancing\nlocal attention. Saliency-enhancing local attention not only improves\ncomputational efficiency but also captures more fine-grained contextual\ninformation and spatial relationships within the region. Comprehensive\nexperiments on multiple visual understanding benchmarks validate the\neffectiveness of our method when combined with various Efficient Vision\nProjectors, showing performance gains without sacrificing efficiency. Based on\nVision Remember, LLaVA-VR with only 2B parameters is also superior to previous\nrepresentative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B."
                },
                "authors": [
                    {
                        "name": "Ze Feng"
                    },
                    {
                        "name": "Jiang-Jiang Liu"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Lingyu Xiao"
                    },
                    {
                        "name": "Xiaofan Li"
                    },
                    {
                        "name": "Wankou Yang"
                    },
                    {
                        "name": "Jingdong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingdong Wang"
                },
                "author": "Jingdong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23703v2",
                "updated": "2025-06-04T13:18:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    18,
                    59,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-29T17:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability"
                },
                "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials."
                },
                "authors": [
                    {
                        "name": "Ruida Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16540v2",
                "updated": "2025-06-04T13:17:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    17,
                    52,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-23T11:19:44Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    19,
                    44,
                    6,
                    54,
                    0
                ],
                "title": "D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model\n  Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model\n  Generation Using Large Language Models"
                },
                "summary": "In electronic design, engineers often manually search through extensive\ndocuments to retrieve component parameters required for constructing SPICE\nmodels, a process that is both labor-intensive and time-consuming. To address\nthis challenge, we present an automated framework called D2S-FLOW that\nleverages large language models (LLMs) to extract electrical parameters from\ndatasheets and generate SPICE models with high precision and efficiency,\nsignificantly reducing the need for manual intervention. Unlike traditional RAG\nsystems, D2S-FLOW employs a workflow to enhance precision in handling\nunstructured documents and inconsistent naming conventions through three\ninnovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical\nDocument-Enhanced Retrieval (HDER), and Heterogeneous Named Entity\nNormalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER\nutilizes document structure for precise parameter localization, and HNEN\nstandardizes terminology via semantic inference. Experimental results\ndemonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1\nscore of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the\nstrongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it\nreduces API token consumption by 38% and minimizes the irrelevant information\nratio to 4%, showcasing substantial improvements in resource efficiency. This\nresearch provides an effective automated solution for circuit design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In electronic design, engineers often manually search through extensive\ndocuments to retrieve component parameters required for constructing SPICE\nmodels, a process that is both labor-intensive and time-consuming. To address\nthis challenge, we present an automated framework called D2S-FLOW that\nleverages large language models (LLMs) to extract electrical parameters from\ndatasheets and generate SPICE models with high precision and efficiency,\nsignificantly reducing the need for manual intervention. Unlike traditional RAG\nsystems, D2S-FLOW employs a workflow to enhance precision in handling\nunstructured documents and inconsistent naming conventions through three\ninnovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical\nDocument-Enhanced Retrieval (HDER), and Heterogeneous Named Entity\nNormalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER\nutilizes document structure for precise parameter localization, and HNEN\nstandardizes terminology via semantic inference. Experimental results\ndemonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1\nscore of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the\nstrongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it\nreduces API token consumption by 38% and minimizes the irrelevant information\nratio to 4%, showcasing substantial improvements in resource efficiency. This\nresearch provides an effective automated solution for circuit design."
                },
                "authors": [
                    {
                        "name": "Hong Cai Chen"
                    },
                    {
                        "name": "Yi Pin Xu"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "14 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03923v1",
                "updated": "2025-06-04T13:15:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    15,
                    1,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:15:01Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    15,
                    1,
                    2,
                    155,
                    0
                ],
                "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative\n  Reasoning"
                },
                "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mohammadamin Shafiei"
                    },
                    {
                        "name": "Hamidreza Saffari"
                    },
                    {
                        "name": "Nafise Sadat Moosavi"
                    }
                ],
                "author_detail": {
                    "name": "Nafise Sadat Moosavi"
                },
                "author": "Nafise Sadat Moosavi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03921v1",
                "updated": "2025-06-04T13:13:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    13,
                    58,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T13:13:58Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    13,
                    58,
                    2,
                    155,
                    0
                ],
                "title": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and\n  LLM-Guided Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and\n  LLM-Guided Reinforcement Learning"
                },
                "summary": "Several closed-source LLMs have consistently outperformed open-source\nalternatives in program repair tasks, primarily due to their superior reasoning\ncapabilities and extensive pre-training. This paper introduces Repairity, a\nnovel three-stage methodology that significantly narrows this performance gap\nthrough reasoning extraction and reinforcement learning. Our approach: (1)\nsystematically filters high-quality reasoning traces from closed-source models\nusing correctness verification, (2) transfers this reasoning knowledge to\nopen-source models via supervised fine-tuning, and (3) develops reinforcement\nlearning with LLM-based feedback to further optimize performance. Empirical\nevaluation across multiple program repair benchmarks demonstrates that\nRepairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open\nsource LLM, by 8.68\\% on average, reducing the capability gap with\nClaude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%.\nAblation studies confirm that both reasoning extraction and LLM-guided\nreinforcement learning contribute significantly to these improvements. Our\nmethodology generalizes effectively to additional code-related tasks, enabling\norganizations to leverage high-quality program repair capabilities while\nmaintaining the customizability, transparency, and deployment flexibility\ninherent to open-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several closed-source LLMs have consistently outperformed open-source\nalternatives in program repair tasks, primarily due to their superior reasoning\ncapabilities and extensive pre-training. This paper introduces Repairity, a\nnovel three-stage methodology that significantly narrows this performance gap\nthrough reasoning extraction and reinforcement learning. Our approach: (1)\nsystematically filters high-quality reasoning traces from closed-source models\nusing correctness verification, (2) transfers this reasoning knowledge to\nopen-source models via supervised fine-tuning, and (3) develops reinforcement\nlearning with LLM-based feedback to further optimize performance. Empirical\nevaluation across multiple program repair benchmarks demonstrates that\nRepairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open\nsource LLM, by 8.68\\% on average, reducing the capability gap with\nClaude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%.\nAblation studies confirm that both reasoning extraction and LLM-guided\nreinforcement learning contribute significantly to these improvements. Our\nmethodology generalizes effectively to additional code-related tasks, enabling\norganizations to leverage high-quality program repair capabilities while\nmaintaining the customizability, transparency, and deployment flexibility\ninherent to open-source models."
                },
                "authors": [
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    }
                ],
                "author_detail": {
                    "name": "Tegawend F. Bissyand"
                },
                "author": "Tegawend F. Bissyand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04670v2",
                "updated": "2025-06-04T12:57:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    57,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-07T08:26:54Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    26,
                    54,
                    2,
                    127,
                    0
                ],
                "title": "LLM Code Customization with Visual Results: A Benchmark on TikZ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Code Customization with Visual Results: A Benchmark on TikZ"
                },
                "summary": "With the rise of AI-based code generation, customizing existing code out of\nnatural language instructions to modify visual results -such as figures or\nimages -has become possible, promising to reduce the need for deep programming\nexpertise. However, even experienced developers can struggle with this task, as\nit requires identifying relevant code regions (feature location), generating\nvalid code variants, and ensuring the modifications reliably align with user\nintent. In this paper, we introduce vTikZ, the first benchmark designed to\nevaluate the ability of Large Language Models (LLMs) to customize code while\npreserving coherent visual outcomes. Our benchmark consists of carefully\ncurated vTikZ editing scenarios, parameterized ground truths, and a reviewing\ntool that leverages visual feedback to assess correctness. Empirical evaluation\nwith stateof-the-art LLMs shows that existing solutions struggle to reliably\nmodify code in alignment with visual intent, highlighting a gap in current\nAI-assisted code editing approaches. We argue that vTikZ opens new research\ndirections for integrating LLMs with visual feedback mechanisms to improve code\ncustomization tasks in various domains beyond TikZ, including image processing,\nart creation, Web design, and 3D modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of AI-based code generation, customizing existing code out of\nnatural language instructions to modify visual results -such as figures or\nimages -has become possible, promising to reduce the need for deep programming\nexpertise. However, even experienced developers can struggle with this task, as\nit requires identifying relevant code regions (feature location), generating\nvalid code variants, and ensuring the modifications reliably align with user\nintent. In this paper, we introduce vTikZ, the first benchmark designed to\nevaluate the ability of Large Language Models (LLMs) to customize code while\npreserving coherent visual outcomes. Our benchmark consists of carefully\ncurated vTikZ editing scenarios, parameterized ground truths, and a reviewing\ntool that leverages visual feedback to assess correctness. Empirical evaluation\nwith stateof-the-art LLMs shows that existing solutions struggle to reliably\nmodify code in alignment with visual intent, highlighting a gap in current\nAI-assisted code editing approaches. We argue that vTikZ opens new research\ndirections for integrating LLMs with visual feedback mechanisms to improve code\ncustomization tasks in various domains beyond TikZ, including image processing,\nart creation, Web design, and 3D modeling."
                },
                "authors": [
                    {
                        "name": "Charly Reux"
                    },
                    {
                        "name": "Mathieu Acher"
                    },
                    {
                        "name": "Djamel Eddine Khelladi"
                    },
                    {
                        "name": "Olivier Barais"
                    },
                    {
                        "name": "Clment Quinton"
                    }
                ],
                "author_detail": {
                    "name": "Clment Quinton"
                },
                "arxiv_affiliation": "SPIRALS",
                "author": "Clment Quinton",
                "arxiv_journal_ref": "EASE 2025 - Evaluation and Assessment in Software Engineering, Jun\n  2025, Istanbul, Turkey. pp.1-10",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03901v2",
                "updated": "2025-06-05T06:44:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    6,
                    44,
                    23,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    55,
                    59,
                    2,
                    155,
                    0
                ],
                "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of\n  Retrieval Noise Erosion in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of\n  Retrieval Noise Erosion in RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge. However, RAG systems are highly\nsensitive to retrieval noise prevalent in real-world scenarios. Existing\nbenchmarks fail to emulate the complex and heterogeneous noise distributions\nencountered in real-world retrieval environments, undermining reliable\nrobustness assessment. In this paper, we define four categories of retrieval\nnoise based on linguistic properties and noise characteristics, aiming to\nreflect the heterogeneity of noise in real-world scenarios. Building on this,\nwe introduce Magic Mushroom, a benchmark for replicating \"magic mushroom\"\nnoise: contexts that appear relevant on the surface but covertly mislead RAG\nsystems. Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop\nquestion-answer pairs. More importantly, Magic Mushroom enables researchers to\nflexibly configure combinations of retrieval noise according to specific\nresearch objectives or application scenarios, allowing for highly controlled\nevaluation setups. We evaluate LLM generators of varying parameter scales and\nclassic RAG denoising strategies under diverse noise distributions to\ninvestigate their performance dynamics during progressive noise encroachment.\nOur analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions. Magic Mushroom emerges as a promising tool for evaluating and\nadvancing noise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications. The Magic Mushroom benchmark is available at\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge. However, RAG systems are highly\nsensitive to retrieval noise prevalent in real-world scenarios. Existing\nbenchmarks fail to emulate the complex and heterogeneous noise distributions\nencountered in real-world retrieval environments, undermining reliable\nrobustness assessment. In this paper, we define four categories of retrieval\nnoise based on linguistic properties and noise characteristics, aiming to\nreflect the heterogeneity of noise in real-world scenarios. Building on this,\nwe introduce Magic Mushroom, a benchmark for replicating \"magic mushroom\"\nnoise: contexts that appear relevant on the surface but covertly mislead RAG\nsystems. Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop\nquestion-answer pairs. More importantly, Magic Mushroom enables researchers to\nflexibly configure combinations of retrieval noise according to specific\nresearch objectives or application scenarios, allowing for highly controlled\nevaluation setups. We evaluate LLM generators of varying parameter scales and\nclassic RAG denoising strategies under diverse noise distributions to\ninvestigate their performance dynamics during progressive noise encroachment.\nOur analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions. Magic Mushroom emerges as a promising tool for evaluating and\nadvancing noise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications. The Magic Mushroom benchmark is available at\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Shenyu Zhang"
                    },
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Sheng Bi"
                    },
                    {
                        "name": "Guilin Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Qi"
                },
                "author": "Guilin Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03887v1",
                "updated": "2025-06-04T12:30:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    30,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T12:30:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    30,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured\n  LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured\n  LLM Generation"
                },
                "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Shihao Bai"
                    },
                    {
                        "name": "Zaijun Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Published as a conference paper at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03880v1",
                "updated": "2025-06-04T12:16:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    16,
                    41,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T12:16:41Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    16,
                    41,
                    2,
                    155,
                    0
                ],
                "title": "RadialRouter: Structured Representation for Efficient and Robust Large\n  Language Models Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadialRouter: Structured Representation for Efficient and Robust Large\n  Language Models Routing"
                },
                "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential."
                },
                "authors": [
                    {
                        "name": "Ruihan Jin"
                    },
                    {
                        "name": "Pengpeng Shao"
                    },
                    {
                        "name": "Zhengqi Wen"
                    },
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22604v2",
                "updated": "2025-06-04T12:08:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    8,
                    7,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-28T16:47:42Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    47,
                    42,
                    4,
                    87,
                    0
                ],
                "title": "Enhanced Variational Quantum Kolmogorov-Arnold Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Variational Quantum Kolmogorov-Arnold Network"
                },
                "summary": "The Kolmogorov-Arnold Network (KAN) is a novel multi-layer network model\nrecognized for its efficiency in neuromorphic computing, where synapses between\nneurons are trained linearly. Computations in KAN are performed by generating a\npolynomial vector from the state vector and layer-wise trained synapses,\nenabling efficient processing. While KAN can be implemented on quantum\ncomputers using block encoding and Quantum Signal Processing, these methods\nrequire fault-tolerant quantum devices, making them impractical for current\nNoisy Intermediate-Scale Quantum (NISQ) hardware. We propose the Enhanced\nVariational Quantum Kolmogorov-Arnold Network (EVQKAN) to overcome this\nlimitation, which emulates KAN through variational quantum algorithms. The\nEVQKAN ansatz employs a tiling technique to emulate layer matrices, leading to\nsignificantly higher accuracy compared to conventional Variational Quantum\nKolmogorov-Arnold Network (VQKAN) and Quantum Neural Networks (QNN), even with\na smaller number of layers. EVQKAN achieves superior performance with a\nsingle-layer architecture, whereas QNN and VQKAN typically struggle.\nAdditionally, EVQKAN eliminates the need for Quantum Signal Processing,\nenhancing its robustness to noise and making it well-suited for practical\ndeployment on NISQ-era quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Kolmogorov-Arnold Network (KAN) is a novel multi-layer network model\nrecognized for its efficiency in neuromorphic computing, where synapses between\nneurons are trained linearly. Computations in KAN are performed by generating a\npolynomial vector from the state vector and layer-wise trained synapses,\nenabling efficient processing. While KAN can be implemented on quantum\ncomputers using block encoding and Quantum Signal Processing, these methods\nrequire fault-tolerant quantum devices, making them impractical for current\nNoisy Intermediate-Scale Quantum (NISQ) hardware. We propose the Enhanced\nVariational Quantum Kolmogorov-Arnold Network (EVQKAN) to overcome this\nlimitation, which emulates KAN through variational quantum algorithms. The\nEVQKAN ansatz employs a tiling technique to emulate layer matrices, leading to\nsignificantly higher accuracy compared to conventional Variational Quantum\nKolmogorov-Arnold Network (VQKAN) and Quantum Neural Networks (QNN), even with\na smaller number of layers. EVQKAN achieves superior performance with a\nsingle-layer architecture, whereas QNN and VQKAN typically struggle.\nAdditionally, EVQKAN eliminates the need for Quantum Signal Processing,\nenhancing its robustness to noise and making it well-suited for practical\ndeployment on NISQ-era quantum devices."
                },
                "authors": [
                    {
                        "name": "Hikaru Wakaura"
                    },
                    {
                        "name": "Rahmat Mulyawan"
                    },
                    {
                        "name": "Andriyan B. Suksmono"
                    }
                ],
                "author_detail": {
                    "name": "Andriyan B. Suksmono"
                },
                "author": "Andriyan B. Suksmono",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2503.21336",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01723v2",
                "updated": "2025-06-04T12:05:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    5,
                    1,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-02T14:29:46Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    14,
                    29,
                    46,
                    0,
                    153,
                    0
                ],
                "title": "Tug-of-war between idiom's figurative and literal meanings in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tug-of-war between idiom's figurative and literal meanings in LLMs"
                },
                "summary": "Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer."
                },
                "authors": [
                    {
                        "name": "Soyoung Oh"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Mathis Pink"
                    },
                    {
                        "name": "Michael Hahn"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03870v1",
                "updated": "2025-06-04T12:01:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    1,
                    17,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T12:01:17Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    1,
                    17,
                    2,
                    155,
                    0
                ],
                "title": "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large\n  Language Model-Based Inference Attacks: Insights from Early Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large\n  Language Model-Based Inference Attacks: Insights from Early Datasets"
                },
                "summary": "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems."
                },
                "authors": [
                    {
                        "name": "Mohd. Farhan Israk Soumik"
                    },
                    {
                        "name": "Syed Mhamudul Hasan"
                    },
                    {
                        "name": "Abdur R. Shahid"
                    }
                ],
                "author_detail": {
                    "name": "Abdur R. Shahid"
                },
                "author": "Abdur R. Shahid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03867v1",
                "updated": "2025-06-04T11:58:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    58,
                    18,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:58:18Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    58,
                    18,
                    2,
                    155,
                    0
                ],
                "title": "EuroGEST: Investigating gender stereotypes in multilingual language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuroGEST: Investigating gender stereotypes in multilingual language\n  models"
                },
                "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages."
                },
                "authors": [
                    {
                        "name": "Jacqueline Rowe"
                    },
                    {
                        "name": "Mateusz Klimaszewski"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Shannon Vallor"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "arxiv_comment": "8 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06910v2",
                "updated": "2025-06-04T11:50:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    50,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2025-04-09T14:14:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Identifying Aspects in Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Aspects in Peer Reviews"
                },
                "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review."
                },
                "authors": [
                    {
                        "name": "Sheng Lu"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03861v1",
                "updated": "2025-06-04T11:48:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    48,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:48:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    48,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in\n  High-Frequency Cryptocurrency Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in\n  High-Frequency Cryptocurrency Trading"
                },
                "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia."
                },
                "authors": [
                    {
                        "name": "Qiuhan Han"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Atsushi Yoshikawa"
                    },
                    {
                        "name": "Masayuki Yamamura"
                    }
                ],
                "author_detail": {
                    "name": "Masayuki Yamamura"
                },
                "author": "Masayuki Yamamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16270v3",
                "updated": "2025-06-04T11:45:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    45,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2024-10-21T17:59:50Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    59,
                    50,
                    0,
                    295,
                    0
                ],
                "title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models"
                },
                "summary": "With large language models (LLMs) increasingly deployed as cognitive engines\nfor AI agents, the reliability and effectiveness critically hinge on their\nintrinsic epistemic agency, which remains understudied. Epistemic agency, the\nability to flexibly construct, adapt, and monitor beliefs about dynamic\nenvironments, represents a base-model-level capacity independent of specific\ntools, modules, or applications. We characterize the holistic process\nunderlying epistemic agency, which unfolds in seven interrelated dimensions:\nprediction, decision-making, perception, memory, counterfactual thinking,\nbelief updating, and meta-reflection. Correspondingly, we propose\nReflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven\ntasks with long-term relevance and minimization of data leakage. Through a\ncomprehensive evaluation of 16 models using three prompting strategies, we\nidentify a clear three-tier performance hierarchy and significant limitations\nof current LLMs, particularly in meta-reflection capabilities. While\nstate-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our\nfindings suggest several promising research directions, including enhancing\ncore cognitive functions, improving cross-functional coordination, and\ndeveloping adaptive processing mechanisms. Our code and data are available at\nhttps://github.com/AI45Lab/ReflectionBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) increasingly deployed as cognitive engines\nfor AI agents, the reliability and effectiveness critically hinge on their\nintrinsic epistemic agency, which remains understudied. Epistemic agency, the\nability to flexibly construct, adapt, and monitor beliefs about dynamic\nenvironments, represents a base-model-level capacity independent of specific\ntools, modules, or applications. We characterize the holistic process\nunderlying epistemic agency, which unfolds in seven interrelated dimensions:\nprediction, decision-making, perception, memory, counterfactual thinking,\nbelief updating, and meta-reflection. Correspondingly, we propose\nReflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven\ntasks with long-term relevance and minimization of data leakage. Through a\ncomprehensive evaluation of 16 models using three prompting strategies, we\nidentify a clear three-tier performance hierarchy and significant limitations\nof current LLMs, particularly in meta-reflection capabilities. While\nstate-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our\nfindings suggest several promising research directions, including enhancing\ncore cognitive functions, improving cross-functional coordination, and\ndeveloping adaptive processing mechanisms. Our code and data are available at\nhttps://github.com/AI45Lab/ReflectionBench."
                },
                "authors": [
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Haiquan Zhao"
                    },
                    {
                        "name": "Shuqi Kong"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Chunbo Li"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "arxiv_comment": "29 pages, 19 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03857v1",
                "updated": "2025-06-04T11:42:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    42,
                    37,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:42:37Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    42,
                    37,
                    2,
                    155,
                    0
                ],
                "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation"
                },
                "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist."
                },
                "authors": [
                    {
                        "name": "Mingxuan Xia"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Zewei Yu"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Runze Wu"
                    }
                ],
                "author_detail": {
                    "name": "Runze Wu"
                },
                "author": "Runze Wu",
                "arxiv_comment": "Accepted to ACL 2025 (Main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24298v2",
                "updated": "2025-06-04T11:42:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    42,
                    19,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-30T07:18:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    7,
                    18,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning"
                },
                "summary": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/."
                },
                "authors": [
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xujie Shen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Guo Wei"
                    },
                    {
                        "name": "Jun Mei"
                    },
                    {
                        "name": "Jiashu Wang"
                    },
                    {
                        "name": "Tongkai Yang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03850v1",
                "updated": "2025-06-04T11:33:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    33,
                    36,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:33:36Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    33,
                    36,
                    2,
                    155,
                    0
                ],
                "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful\n  Fine-Tuning"
                },
                "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11753v2",
                "updated": "2025-06-04T11:13:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    13,
                    28,
                    2,
                    155,
                    0
                ],
                "published": "2025-02-17T12:49:55Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    49,
                    55,
                    0,
                    48,
                    0
                ],
                "title": "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real\n  and Synthetic Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real\n  and Synthetic Claims"
                },
                "summary": "Misinformation can be countered with fact-checking, but the process is costly\nand slow. Identifying checkworthy claims is the first step, where automation\ncan help scale fact-checkers' efforts. However, detection methods struggle with\ncontent that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We\nintroduce HintsOfTruth, a public dataset for multimodal checkworthiness\ndetection with 27K real-world and synthetic image/claim pairs. The mix of real\nand synthetic data makes this dataset unique and ideal for benchmarking\ndetection methods. We compare fine-tuned and prompted Large Language Models\n(LLMs). We find that well-configured lightweight text-based encoders perform\ncomparably to multimodal models but the former only focus on identifying\nnon-claim-like content. Multimodal LLMs can be more accurate but come at a\nsignificant computational cost, making them impractical for large-scale\napplications. When faced with synthetic data, multimodal models perform more\nrobustly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation can be countered with fact-checking, but the process is costly\nand slow. Identifying checkworthy claims is the first step, where automation\ncan help scale fact-checkers' efforts. However, detection methods struggle with\ncontent that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We\nintroduce HintsOfTruth, a public dataset for multimodal checkworthiness\ndetection with 27K real-world and synthetic image/claim pairs. The mix of real\nand synthetic data makes this dataset unique and ideal for benchmarking\ndetection methods. We compare fine-tuned and prompted Large Language Models\n(LLMs). We find that well-configured lightweight text-based encoders perform\ncomparably to multimodal models but the former only focus on identifying\nnon-claim-like content. Multimodal LLMs can be more accurate but come at a\nsignificant computational cost, making them impractical for large-scale\napplications. When faced with synthetic data, multimodal models perform more\nrobustly."
                },
                "authors": [
                    {
                        "name": "Michiel van der Meer"
                    },
                    {
                        "name": "Pavel Korshunov"
                    },
                    {
                        "name": "Sbastien Marcel"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    }
                ],
                "author_detail": {
                    "name": "Lonneke van der Plas"
                },
                "author": "Lonneke van der Plas",
                "arxiv_comment": "Accepted at ACL2025 (main track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21082v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21082v3",
                "updated": "2025-06-04T11:03:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    3,
                    1,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-27T12:06:16Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    6,
                    16,
                    1,
                    147,
                    0
                ],
                "title": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for\n  Black-Box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for\n  Black-Box Large Language Models"
                },
                "summary": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved impressive performance\nacross a wide range of natural language tasks and are now widely used in\nreal-world applications. Among them, black-box LLMs--served via APIs without\naccess to model internals--are especially dominant due to their scalability and\nease of deployment. Despite their strong capabilities, these models typically\nproduce generalized responses that overlook personal preferences and reasoning\nstyles. This has led to growing interest in black-box LLM personalization,\nwhich aims to tailor model outputs to user-specific context without modifying\nmodel parameters. However, existing approaches primarily focus on\nresponse-level personalization, attempting to match final outputs without\nmodeling personal thought process. To address this limitation, we propose RPM,\na framework for reasoning-level personalization that aligns the model's\nreasoning process with a user's personalized logic. RPM first constructs\nstatistical user-specific factors by extracting and grouping\nresponse-influential features from user history. It then builds personalized\nreasoning paths that reflect how these factors are used in context. In the\ninference stage, RPM retrieves reasoning-aligned examples for new queries via\nfeature-level similarity and performs inference conditioned on the structured\nfactors and retrieved reasoning paths, enabling the model to follow\nuser-specific reasoning trajectories. This reasoning-level personalization\nenhances both predictive accuracy and interpretability by grounding model\noutputs in user-specific logic through structured information. Extensive\nexperiments across diverse tasks show that RPM consistently outperforms\nresponse-level personalization methods, demonstrating the effectiveness of\nreasoning-level personalization in black-box LLMs."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21082v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21082v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15358v3",
                "updated": "2025-06-04T10:58:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    10,
                    58,
                    56,
                    2,
                    155,
                    0
                ],
                "published": "2025-03-19T15:58:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation"
                },
                "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."
                },
                "authors": [
                    {
                        "name": "Thomas Pickard"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Maggie Mi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Dylan Phelps"
                    },
                    {
                        "name": "Marco Idiart"
                    }
                ],
                "author_detail": {
                    "name": "Marco Idiart"
                },
                "author": "Marco Idiart",
                "arxiv_comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025. This version corrects a typo in the results table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v4",
                "updated": "2025-06-04T10:58:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    10,
                    58,
                    3,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "Enabling LLM Knowledge Analysis via Extensive Materialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling LLM Knowledge Analysis via Extensive Materialization"
                },
                "summary": "Large language models (LLMs) have majorly advanced NLP and AI, and next to\ntheir ability to perform a wide range of procedural tasks, a major success\nfactor is their internalized factual knowledge. Since Petroni et al. (2019),\nanalyzing this knowledge has gained attention. However, most approaches\ninvestigate one question at a time via modest-sized pre-defined samples,\nintroducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents\nthe analysis of knowledge (or beliefs) of LLMs beyond the experimenter's\npredisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterialize an LLM's factual knowledge through recursive querying and result\nconsolidation. Our approach is a milestone for LLM research, for the first time\nproviding constructive insights into the scope and structure of LLM knowledge\n(or beliefs).\n  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million\nrelational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB\nto exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale,\naccuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible\nat https://gptkb.org",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have majorly advanced NLP and AI, and next to\ntheir ability to perform a wide range of procedural tasks, a major success\nfactor is their internalized factual knowledge. Since Petroni et al. (2019),\nanalyzing this knowledge has gained attention. However, most approaches\ninvestigate one question at a time via modest-sized pre-defined samples,\nintroducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents\nthe analysis of knowledge (or beliefs) of LLMs beyond the experimenter's\npredisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterialize an LLM's factual knowledge through recursive querying and result\nconsolidation. Our approach is a milestone for LLM research, for the first time\nproviding constructive insights into the scope and structure of LLM knowledge\n(or beliefs).\n  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million\nrelational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB\nto exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale,\naccuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible\nat https://gptkb.org"
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "14 pages, 4 tables, 12 figures",
                "arxiv_journal_ref": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]