[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v2",
                "updated": "2025-05-03T06:55:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    6,
                    55,
                    56,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21035v2",
                "updated": "2025-05-02T17:57:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    57,
                    4,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-28T01:16:27Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    1,
                    16,
                    27,
                    0,
                    118,
                    0
                ],
                "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond\n  Surface-level Privacy Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond\n  Surface-level Privacy Leakage"
                },
                "summary": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage."
                },
                "authors": [
                    {
                        "name": "Rui Xin"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Michael Duan"
                    },
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Pang Wei Koh"
                    }
                ],
                "author_detail": {
                    "name": "Pang Wei Koh"
                },
                "author": "Pang Wei Koh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17662v2",
                "updated": "2025-05-02T17:36:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    36,
                    47,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-26T18:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    26,
                    17,
                    1,
                    331,
                    0
                ],
                "title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training"
                },
                "summary": "Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time."
                },
                "authors": [
                    {
                        "name": "Raktim Gautam Goswami"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01406v1",
                "updated": "2025-05-02T17:35:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    35,
                    3,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T17:35:03Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    35,
                    3,
                    4,
                    122,
                    0
                ],
                "title": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in\n  Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in\n  Video Diffusion Models"
                },
                "summary": "The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}"
                },
                "authors": [
                    {
                        "name": "Mohammadreza Teymoorianfard"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04520v3",
                "updated": "2025-05-02T17:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    0,
                    10,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-08T14:13:12Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    13,
                    12,
                    2,
                    8,
                    0
                ],
                "title": "Inferring resource competition in microbial communities from time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring resource competition in microbial communities from time series"
                },
                "summary": "The competition for resources is a defining feature of microbial communities.\nIn many contexts, from soils to host-associated communities, highly diverse\nmicrobes are organized into metabolic groups or guilds with similar resource\npreferences. The resource preferences of individual taxa that give rise to\nthese guilds are critical for understanding fluxes of resources through the\ncommunity and the structure of diversity in the system. However, inferring the\nmetabolic capabilities of individual taxa, and their competition with other\ntaxa, within a community is challenging and unresolved. Here we address this\ngap in knowledge by leveraging dynamic measurements of abundances in\ncommunities. We show that simple correlations are often misleading in\npredicting resource competition. We show that spectral methods such as the\ncross-power spectral density (CPSD) and coherence that account for time-delayed\neffects are superior metrics for inferring the structure of resource\ncompetition in communities. We first demonstrate this fact on synthetic data\ngenerated from consumer-resource models with time-dependent resource\navailability, where taxa are organized into groups or guilds with similar\nresource preferences. By applying spectral methods to oceanic plankton\ntime-series data, we demonstrate that these methods detect interaction\nstructures among species with similar genomic sequences. Our results indicate\nthat analyzing temporal data across multiple timescales can reveal the\nunderlying structure of resource competition within communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The competition for resources is a defining feature of microbial communities.\nIn many contexts, from soils to host-associated communities, highly diverse\nmicrobes are organized into metabolic groups or guilds with similar resource\npreferences. The resource preferences of individual taxa that give rise to\nthese guilds are critical for understanding fluxes of resources through the\ncommunity and the structure of diversity in the system. However, inferring the\nmetabolic capabilities of individual taxa, and their competition with other\ntaxa, within a community is challenging and unresolved. Here we address this\ngap in knowledge by leveraging dynamic measurements of abundances in\ncommunities. We show that simple correlations are often misleading in\npredicting resource competition. We show that spectral methods such as the\ncross-power spectral density (CPSD) and coherence that account for time-delayed\neffects are superior metrics for inferring the structure of resource\ncompetition in communities. We first demonstrate this fact on synthetic data\ngenerated from consumer-resource models with time-dependent resource\navailability, where taxa are organized into groups or guilds with similar\nresource preferences. By applying spectral methods to oceanic plankton\ntime-series data, we demonstrate that these methods detect interaction\nstructures among species with similar genomic sequences. Our results indicate\nthat analyzing temporal data across multiple timescales can reveal the\nunderlying structure of resource competition within communities."
                },
                "authors": [
                    {
                        "name": "Xiaowen Chen"
                    },
                    {
                        "name": "Kyle Crocker"
                    },
                    {
                        "name": "Seppe Kuehn"
                    },
                    {
                        "name": "Aleksandra M. Walczak"
                    },
                    {
                        "name": "Thierry Mora"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Mora"
                },
                "author": "Thierry Mora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06360v3",
                "updated": "2025-05-02T16:58:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    58,
                    10,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-10T04:56:14Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    4,
                    56,
                    14,
                    6,
                    315,
                    0
                ],
                "title": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks"
                },
                "summary": "Despite their tremendous success and versatility, Deep Neural Networks (DNNs)\nsuch as Large Language Models (LLMs) suffer from inference inefficiency and\nrely on advanced computational infrastructure. To address these challenges and\nmake these models more accessible and cost-effective, in this paper, we propose\nalgorithms to improve the inference time and memory efficiency of DNNs with\nbinary and ternary weight matrices. Particularly focusing on matrix\nmultiplication as the bottleneck operation of inference, we observe that, once\ntrained, the weight matrices of a model no longer change. This allows us to\npreprocess these matrices and create indices that help reduce the storage\nrequirements by a logarithmic factor while enabling our efficient inference\nalgorithms. Specifically, for a $n\\times n$ weight matrix, our efficient\nalgorithm guarantees a time complexity of $O(\\frac{n^2}{\\log n})$, a\nlogarithmic factor improvement over the standard vector-matrix multiplication.\nBesides theoretical analysis, we conduct extensive experiments to evaluate the\npractical efficiency of our algorithms. Our results confirm the superiority of\nour approach both with respect to time and memory, as we observed a reduction\nin the multiplication time up to 29x and memory usage up to 6x. When applied to\nLLMs, our experiments show up to a 5.24x speedup in the inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their tremendous success and versatility, Deep Neural Networks (DNNs)\nsuch as Large Language Models (LLMs) suffer from inference inefficiency and\nrely on advanced computational infrastructure. To address these challenges and\nmake these models more accessible and cost-effective, in this paper, we propose\nalgorithms to improve the inference time and memory efficiency of DNNs with\nbinary and ternary weight matrices. Particularly focusing on matrix\nmultiplication as the bottleneck operation of inference, we observe that, once\ntrained, the weight matrices of a model no longer change. This allows us to\npreprocess these matrices and create indices that help reduce the storage\nrequirements by a logarithmic factor while enabling our efficient inference\nalgorithms. Specifically, for a $n\\times n$ weight matrix, our efficient\nalgorithm guarantees a time complexity of $O(\\frac{n^2}{\\log n})$, a\nlogarithmic factor improvement over the standard vector-matrix multiplication.\nBesides theoretical analysis, we conduct extensive experiments to evaluate the\npractical efficiency of our algorithms. Our results confirm the superiority of\nour approach both with respect to time and memory, as we observed a reduction\nin the multiplication time up to 29x and memory usage up to 6x. When applied to\nLLMs, our experiments show up to a 5.24x speedup in the inference time."
                },
                "authors": [
                    {
                        "name": "Mohsen Dehghankar"
                    },
                    {
                        "name": "Mahdi Erfanian"
                    },
                    {
                        "name": "Abolfazl Asudeh"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Asudeh"
                },
                "author": "Abolfazl Asudeh",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01386v1",
                "updated": "2025-05-02T16:49:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    49,
                    10,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T16:49:10Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    49,
                    10,
                    4,
                    122,
                    0
                ],
                "title": "Carbon Aware Transformers Through Joint Model-Hardware Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Aware Transformers Through Joint Model-Hardware Optimization"
                },
                "summary": "The rapid growth of machine learning (ML) systems necessitates a more\ncomprehensive evaluation of their environmental impact, particularly their\ncarbon footprint, which comprises operational carbon from training and\ninference execution and embodied carbon from hardware manufacturing and its\nentire life-cycle. Despite the increasing importance of embodied emissions,\nthere is a lack of tools and frameworks to holistically quantify and optimize\nthe total carbon footprint of ML systems. To address this, we propose\nCATransformers, a carbon-aware architecture search framework that enables\nsustainability-driven co-optimization of ML models and hardware architectures.\nBy incorporating both operational and embodied carbon metrics into early design\nspace exploration of domain-specific hardware accelerators, CATransformers\ndemonstrates that optimizing for carbon yields design choices distinct from\nthose optimized solely for latency or energy efficiency. We apply our framework\nto multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models\nachieving up to 17% reduction in total carbon emissions while maintaining\naccuracy and latency compared to state-of-the-art edge small CLIP baselines.\nThis work underscores the need for holistic optimization methods to design\nhigh-performance, environmentally sustainable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of machine learning (ML) systems necessitates a more\ncomprehensive evaluation of their environmental impact, particularly their\ncarbon footprint, which comprises operational carbon from training and\ninference execution and embodied carbon from hardware manufacturing and its\nentire life-cycle. Despite the increasing importance of embodied emissions,\nthere is a lack of tools and frameworks to holistically quantify and optimize\nthe total carbon footprint of ML systems. To address this, we propose\nCATransformers, a carbon-aware architecture search framework that enables\nsustainability-driven co-optimization of ML models and hardware architectures.\nBy incorporating both operational and embodied carbon metrics into early design\nspace exploration of domain-specific hardware accelerators, CATransformers\ndemonstrates that optimizing for carbon yields design choices distinct from\nthose optimized solely for latency or energy efficiency. We apply our framework\nto multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models\nachieving up to 17% reduction in total carbon emissions while maintaining\naccuracy and latency compared to state-of-the-art edge small CLIP baselines.\nThis work underscores the need for holistic optimization methods to design\nhigh-performance, environmentally sustainable AI systems."
                },
                "authors": [
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Newsha Ardalani"
                    },
                    {
                        "name": "Mostafa Elhoushi"
                    },
                    {
                        "name": "Daniel Jiang"
                    },
                    {
                        "name": "Samuel Hsia"
                    },
                    {
                        "name": "Ekin Sumbul"
                    },
                    {
                        "name": "Divya Mahajan"
                    },
                    {
                        "name": "Carole-Jean Wu"
                    },
                    {
                        "name": "Bilge Acun"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Acun"
                },
                "author": "Bilge Acun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01383v1",
                "updated": "2025-05-02T16:47:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    47,
                    5,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T16:47:05Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    47,
                    5,
                    4,
                    122,
                    0
                ],
                "title": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft\n  Research"
                },
                "summary": "We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach."
                },
                "authors": [
                    {
                        "name": "Yan Miao"
                    },
                    {
                        "name": "Will Shen"
                    },
                    {
                        "name": "Hang Cui"
                    },
                    {
                        "name": "Sayan Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Mitra"
                },
                "author": "Sayan Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00234v2",
                "updated": "2025-05-02T16:44:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    44,
                    2,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-01T00:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    48,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks"
                },
                "summary": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering."
                },
                "authors": [
                    {
                        "name": "Vishnu Sarukkai"
                    },
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    }
                ],
                "author_detail": {
                    "name": "Kayvon Fatahalian"
                },
                "author": "Kayvon Fatahalian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01381v1",
                "updated": "2025-05-02T16:43:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    43,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T16:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    43,
                    37,
                    4,
                    122,
                    0
                ],
                "title": "Probing the Cores of Subdwarf B Stars: How do They Compare to Cores in\n  Helium Core-Burning Red Giants?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Cores of Subdwarf B Stars: How do They Compare to Cores in\n  Helium Core-Burning Red Giants?"
                },
                "summary": "The mixing of material from stellar convective cores into their adjacent\nradiative layers has been a matter of long-standing debate. Pulsating subdwarf\nB stars offer excellent conditions to advance our understanding of this\nproblem. In this work we use a model-independent approach to infer information\nabout the cores of three subdwarf B stars and compare it with similar\ninferences from earlier analysis of red giants in the helium core-burning\nphase. This is achieved by fitting an analytical description of the\ngravity-mode pulsation periods to pulsation data collected by the Kepler\nsatellite. From the fits we infer the reduced asymptotic period spacings and\nthe amplitude and position of sharp structural variations associated with\nchemical discontinuities in the stellar interiors. Our results indicate the\npresence of sharp structural variations with similar properties in all three\nstars, located near the edge of the gravity-mode propagation cavity and likely\nassociated with the C-O/He transition. We find that these structural variations\ndiffer systematically from those of helium core-burning red giant stars, having\nlarger amplitudes and being located at a larger buoyancy radius. This suggests\nthat chemical mixing beyond the adiabatically stratified core into the\nradiatively stratified layers may be more extensive in subdwarf B stars than in\nhelium core-burning red giants. Alternatively, the stratification of the mixing\nregion beyond the adiabatically stratified core may differ significantly\nbetween the two types of stars. The model-independent constraints set on the\nstructural variations inside these three stars are the first of a kind and will\nbe key to enhance the modelling of layers adjacent to stellar convective cores\nand to test non-canonical stellar evolution channels leading to the formation\nof hot subdwarf stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mixing of material from stellar convective cores into their adjacent\nradiative layers has been a matter of long-standing debate. Pulsating subdwarf\nB stars offer excellent conditions to advance our understanding of this\nproblem. In this work we use a model-independent approach to infer information\nabout the cores of three subdwarf B stars and compare it with similar\ninferences from earlier analysis of red giants in the helium core-burning\nphase. This is achieved by fitting an analytical description of the\ngravity-mode pulsation periods to pulsation data collected by the Kepler\nsatellite. From the fits we infer the reduced asymptotic period spacings and\nthe amplitude and position of sharp structural variations associated with\nchemical discontinuities in the stellar interiors. Our results indicate the\npresence of sharp structural variations with similar properties in all three\nstars, located near the edge of the gravity-mode propagation cavity and likely\nassociated with the C-O/He transition. We find that these structural variations\ndiffer systematically from those of helium core-burning red giant stars, having\nlarger amplitudes and being located at a larger buoyancy radius. This suggests\nthat chemical mixing beyond the adiabatically stratified core into the\nradiatively stratified layers may be more extensive in subdwarf B stars than in\nhelium core-burning red giants. Alternatively, the stratification of the mixing\nregion beyond the adiabatically stratified core may differ significantly\nbetween the two types of stars. The model-independent constraints set on the\nstructural variations inside these three stars are the first of a kind and will\nbe key to enhance the modelling of layers adjacent to stellar convective cores\nand to test non-canonical stellar evolution channels leading to the formation\nof hot subdwarf stars."
                },
                "authors": [
                    {
                        "name": "Margarida S. Cunha"
                    },
                    {
                        "name": "Juliana Amaral"
                    },
                    {
                        "name": "Sofia Avelino"
                    },
                    {
                        "name": "Anselmo Falorca"
                    },
                    {
                        "name": "Yuri Damasceno"
                    },
                    {
                        "name": "Pedro Avelino"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Avelino"
                },
                "author": "Pedro Avelino",
                "arxiv_comment": "Accepted for publication in Astronomy and Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20211v2",
                "updated": "2025-05-02T16:29:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    29,
                    14,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-28T19:24:57Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    19,
                    24,
                    57,
                    0,
                    118,
                    0
                ],
                "title": "A rigorous data-driven approach to the nucleation of defects in metals\n  exploiting the link between kinetic properties and (dis)order parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A rigorous data-driven approach to the nucleation of defects in metals\n  exploiting the link between kinetic properties and (dis)order parameters"
                },
                "summary": "Nucleation processes, through which a new structure progressively forms\nwithin a pre-existing homogeneous phase, are fundamental in materials science,\nbut are also typically non-trivial to elucidate. Cases in which to nucleate are\ndefects (or disorder) in an initially ordered structure make no exception. A\nprominent example is the nucleation of dislocations in metals, which critically\ngovern their mechanical, electronic, thermal, and chemical properties. While\natomic-level insights can be attained using, \\textit{e.g.}, molecular dynamics\nsimulations, systematically characterizing nucleation mechanisms and accurately\nquantifying kinetic rates remain challenging tasks. In this work, we\ndemonstrate how the choice of the order parameter used to track the transition\nhas a very strong effect on the accuracy of the kinetic rate predicted from the\ncorresponding free-energy barrier and diffusion coefficient, a fact that has\nbeen often overlooked in the past. By exploiting this systematic error to our\nadvantage, we demonstrate that it is possible to rigorously characterize the\nnucleation process using a data-driven scheme based on a variational principle,\nleading to optimal order parameters and a faithful mechanistic description. We\napply this method to characterize, as a representative case study, the\nnucleation of dislocations in crystalline $fcc$ copper by analyzing replica\nmolecular dynamics simulations at the elastic-plastic limit. By means of\ncommittor analysis and Langevin modeling, our approach allows to systematically\nrank candidate (dis)order parameters, identify the critical nuclei (transition\nstates), and infer the free-energy landscapes. Given its general foundations,\nthis method can be extended to nucleation phenomena in a broad class of\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nucleation processes, through which a new structure progressively forms\nwithin a pre-existing homogeneous phase, are fundamental in materials science,\nbut are also typically non-trivial to elucidate. Cases in which to nucleate are\ndefects (or disorder) in an initially ordered structure make no exception. A\nprominent example is the nucleation of dislocations in metals, which critically\ngovern their mechanical, electronic, thermal, and chemical properties. While\natomic-level insights can be attained using, \\textit{e.g.}, molecular dynamics\nsimulations, systematically characterizing nucleation mechanisms and accurately\nquantifying kinetic rates remain challenging tasks. In this work, we\ndemonstrate how the choice of the order parameter used to track the transition\nhas a very strong effect on the accuracy of the kinetic rate predicted from the\ncorresponding free-energy barrier and diffusion coefficient, a fact that has\nbeen often overlooked in the past. By exploiting this systematic error to our\nadvantage, we demonstrate that it is possible to rigorously characterize the\nnucleation process using a data-driven scheme based on a variational principle,\nleading to optimal order parameters and a faithful mechanistic description. We\napply this method to characterize, as a representative case study, the\nnucleation of dislocations in crystalline $fcc$ copper by analyzing replica\nmolecular dynamics simulations at the elastic-plastic limit. By means of\ncommittor analysis and Langevin modeling, our approach allows to systematically\nrank candidate (dis)order parameters, identify the critical nuclei (transition\nstates), and infer the free-energy landscapes. Given its general foundations,\nthis method can be extended to nucleation phenomena in a broad class of\nmaterials."
                },
                "authors": [
                    {
                        "name": "Mattia Perrone"
                    },
                    {
                        "name": "David D. Girardier"
                    },
                    {
                        "name": "Giovanni M. Pavan"
                    },
                    {
                        "name": "Fabio Pietrucci"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Pietrucci"
                },
                "author": "Fabio Pietrucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14432v2",
                "updated": "2025-05-02T16:03:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    3,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-21T18:59:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks."
                },
                "authors": [
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18789v2",
                "updated": "2025-05-02T15:56:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    56,
                    59,
                    4,
                    122,
                    0
                ],
                "published": "2024-02-29T01:33:08Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    1,
                    33,
                    8,
                    3,
                    60,
                    0
                ],
                "title": "FlexLLM: A System for Co-Serving Large Language Model Inference and\n  Parameter-Efficient Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexLLM: A System for Co-Serving Large Language Model Inference and\n  Parameter-Efficient Finetuning"
                },
                "summary": "Finetuning large language models (LLMs) is essential for task adaptation, yet\nserving stacks today isolate inference and finetuning on separate GPU clusters\n-- wasting resources and under-utilizing hardware. We introduce FlexLLM, the\nfirst system to co-serve LLM inference and PEFT-based finetuning on shared GPUs\nby fusing computation at the token level. The static compilation optimizations\nin FlexLLM -- dependent parallelization and graph pruning significantly shrink\nactivation memory, leading to end-to-end GPU memory savings by up to 80%. At\nruntime, a novel token-level finetuning mechanism paired with a hybrid token\nscheduler dynamically interleaves inference and training tokens within each\nco-serving iteration, meeting strict latency SLOs while maximizing utilization.\nIn end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B,\nFlexLLM sustains the inference SLO requirements up to 20 req/s, and improves\nfinetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x\nunder light loads, preserving over 76% of peak finetuning progress even at peak\ndemand. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models (LLMs) is essential for task adaptation, yet\nserving stacks today isolate inference and finetuning on separate GPU clusters\n-- wasting resources and under-utilizing hardware. We introduce FlexLLM, the\nfirst system to co-serve LLM inference and PEFT-based finetuning on shared GPUs\nby fusing computation at the token level. The static compilation optimizations\nin FlexLLM -- dependent parallelization and graph pruning significantly shrink\nactivation memory, leading to end-to-end GPU memory savings by up to 80%. At\nruntime, a novel token-level finetuning mechanism paired with a hybrid token\nscheduler dynamically interleaves inference and training tokens within each\nco-serving iteration, meeting strict latency SLOs while maximizing utilization.\nIn end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B,\nFlexLLM sustains the inference SLO requirements up to 20 req/s, and improves\nfinetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x\nunder light loads, preserving over 76% of peak finetuning progress even at peak\ndemand. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow/."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xinhao Cheng"
                    },
                    {
                        "name": "Vineeth Kada"
                    },
                    {
                        "name": "Ruohan Gao"
                    },
                    {
                        "name": "Yingyi Huang"
                    },
                    {
                        "name": "Remi Delacourt"
                    },
                    {
                        "name": "April Yang"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Mengdi Wu"
                    },
                    {
                        "name": "Colin Unger"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18851v3",
                "updated": "2025-05-02T15:52:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    52,
                    3,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-31T02:24:13Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    2,
                    24,
                    13,
                    4,
                    31,
                    0
                ],
                "title": "Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph\n  Convolution Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph\n  Convolution Networks"
                },
                "summary": "Most existing RGB-D semantic segmentation methods focus on the feature level\nfusion, including complex cross-modality and cross-scale fusion modules.\nHowever, these methods may cause misalignment problem in the feature fusion\nprocess and counter-intuitive patches in the segmentation results. Inspired by\nthe popular pixel-node-pixel pipeline, we propose to 1) fuse features from two\nmodalities in a late fusion style, during which the geometric feature injection\nis guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on\nthe fused feature to alleviate the emergence of irregular patches by inferring\npatch relationship. At the 3D feature extraction stage, we argue that\ntraditional CNNs are not efficient enough for depth maps. So, we encode depth\nmap into normal map, after which CNNs can easily extract object surface\ntendencies.At projection matrix generation stage, we find the existence of\nBiased-Assignment and Ambiguous-Locality issues in the original pipeline.\nTherefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no\nmissing important pixel features, which can be viewed as hard pixel mining\nprocess; 2) connect regions that are close to each other in the Euclidean space\nas well as in the semantic space with larger edge weights so that location\ninformations can been considered. Extensive experiments on two public datasets,\nNYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost\nthe performance of RGB-D semantic segmentation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing RGB-D semantic segmentation methods focus on the feature level\nfusion, including complex cross-modality and cross-scale fusion modules.\nHowever, these methods may cause misalignment problem in the feature fusion\nprocess and counter-intuitive patches in the segmentation results. Inspired by\nthe popular pixel-node-pixel pipeline, we propose to 1) fuse features from two\nmodalities in a late fusion style, during which the geometric feature injection\nis guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on\nthe fused feature to alleviate the emergence of irregular patches by inferring\npatch relationship. At the 3D feature extraction stage, we argue that\ntraditional CNNs are not efficient enough for depth maps. So, we encode depth\nmap into normal map, after which CNNs can easily extract object surface\ntendencies.At projection matrix generation stage, we find the existence of\nBiased-Assignment and Ambiguous-Locality issues in the original pipeline.\nTherefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no\nmissing important pixel features, which can be viewed as hard pixel mining\nprocess; 2) connect regions that are close to each other in the Euclidean space\nas well as in the semantic space with larger edge weights so that location\ninformations can been considered. Extensive experiments on two public datasets,\nNYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost\nthe performance of RGB-D semantic segmentation task."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Jiang"
                    },
                    {
                        "name": "Bohan Wang"
                    },
                    {
                        "name": "Xinlong Wan"
                    },
                    {
                        "name": "Shanshan Chen"
                    },
                    {
                        "name": "Hamido Fujita"
                    },
                    {
                        "name": "Hanan Abd. Al Juaid"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Abd. Al Juaid"
                },
                "author": "Hanan Abd. Al Juaid",
                "arxiv_comment": "I have decided to withdraw this paper because I have recently\n  obtained some new data and insights during my ongoing research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01351v1",
                "updated": "2025-05-02T15:38:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    38,
                    55,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T15:38:55Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    38,
                    55,
                    4,
                    122,
                    0
                ],
                "title": "Closing the Loop: A Systematic Review of Experience-Driven Game\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Loop: A Systematic Review of Experience-Driven Game\n  Adaptation"
                },
                "summary": "Adaptive game systems aim to enrich player experiences by dynamically\nadjusting game content in response to user data. While extensive research has\naddressed content personalization and player experience modeling, the\nintegration of these components into fully operational adaptive gameplay\nsystems remains limited. This systematic review, conducted in accordance with\nPRISMA guidelines, analyzes 17 empirical studies published between January 2015\nand May 2024, identifying and analyzing approaches that implement the complete\nexperience-driven loop -- including player sensing, modeling, and content\nadaptation. Game telemetry remains the most prevalent sensing modality,\nalthough other non-invasive methods suitable for affective modeling -- such as\nfacial expression analysis (FEA) and peripheral interaction data -- remain\nunderutilized despite their potential for real-time emotional inference.\nKnowledge-based methods, such as rule-based systems and heuristics, dominate\nmodeling and adaptation due to their interpretability and low resource demands,\nwhereas machine learning approaches face challenges related to data\navailability and transparency. Despite their relevance to immersive and\ntherapeutic experiences, affective states such as stress and anxiety remain\nlargely ignored, as systems continue to favor performance over\nemotion-sensitive adaptation. These findings highlight a crucial research\ndirection: advancing emotionally responsive game systems that move beyond\nperformance optimization by incorporating underutilized sensing modalities --\nsuch as FEA and peripheral interaction -- to enable real-time affect-driven\npersonalization. Advancing in this direction holds strong potential to increase\nimmersion, personalize gameplay, and support affect regulation across\nentertainment and therapeutic contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive game systems aim to enrich player experiences by dynamically\nadjusting game content in response to user data. While extensive research has\naddressed content personalization and player experience modeling, the\nintegration of these components into fully operational adaptive gameplay\nsystems remains limited. This systematic review, conducted in accordance with\nPRISMA guidelines, analyzes 17 empirical studies published between January 2015\nand May 2024, identifying and analyzing approaches that implement the complete\nexperience-driven loop -- including player sensing, modeling, and content\nadaptation. Game telemetry remains the most prevalent sensing modality,\nalthough other non-invasive methods suitable for affective modeling -- such as\nfacial expression analysis (FEA) and peripheral interaction data -- remain\nunderutilized despite their potential for real-time emotional inference.\nKnowledge-based methods, such as rule-based systems and heuristics, dominate\nmodeling and adaptation due to their interpretability and low resource demands,\nwhereas machine learning approaches face challenges related to data\navailability and transparency. Despite their relevance to immersive and\ntherapeutic experiences, affective states such as stress and anxiety remain\nlargely ignored, as systems continue to favor performance over\nemotion-sensitive adaptation. These findings highlight a crucial research\ndirection: advancing emotionally responsive game systems that move beyond\nperformance optimization by incorporating underutilized sensing modalities --\nsuch as FEA and peripheral interaction -- to enable real-time affect-driven\npersonalization. Advancing in this direction holds strong potential to increase\nimmersion, personalize gameplay, and support affect regulation across\nentertainment and therapeutic contexts."
                },
                "authors": [
                    {
                        "name": "Phil Lopes"
                    },
                    {
                        "name": "Nuno Fachada"
                    },
                    {
                        "name": "Maria Fonseca"
                    }
                ],
                "author_detail": {
                    "name": "Maria Fonseca"
                },
                "author": "Maria Fonseca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09632v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09632v5",
                "updated": "2025-05-02T15:34:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    34,
                    42,
                    4,
                    122,
                    0
                ],
                "published": "2024-08-19T01:30:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    30,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDeGPT: Modular Decomposition for Large Language Model Compression"
                },
                "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."
                },
                "authors": [
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Abhishek Patel"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "ICLR 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09632v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09632v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A23 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09309v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09309v4",
                "updated": "2025-05-02T15:08:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    8,
                    15,
                    4,
                    122,
                    0
                ],
                "published": "2024-04-14T17:37:30Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    17,
                    37,
                    30,
                    6,
                    105,
                    0
                ],
                "title": "Julia as a universal platform for statistical software development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Julia as a universal platform for statistical software development"
                },
                "summary": "The julia package integrates the Julia programming language into Stata. Users\ncan transfer data between Stata and Julia, issue Julia commands to analyze and\nplot, and pass results back to Stata. Julia's econometric ecosystem is not as\nmature as Stata's or R's or Python's. But Julia is an excellent environment for\ndeveloping high-performance numerical applications, which can then be called\nfrom many platforms. For example, the boottest program for wild bootstrap-based\ninference (Roodman et al. 2019) and fwildclusterboot for R (Fischer and Roodman\n2021) can use the same Julia back end. And the program reghdfejl mimics reghdfe\n(Correia 2016) in fitting linear models with high-dimensional fixed effects\nwhile calling a Julia package for tenfold acceleration on hard problems.\nreghdfejl also supports nonlinear fixed-effect models that cannot otherwise be\nfit in Stata--though preliminarily, as the Julia package for that purpose is\nimmature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The julia package integrates the Julia programming language into Stata. Users\ncan transfer data between Stata and Julia, issue Julia commands to analyze and\nplot, and pass results back to Stata. Julia's econometric ecosystem is not as\nmature as Stata's or R's or Python's. But Julia is an excellent environment for\ndeveloping high-performance numerical applications, which can then be called\nfrom many platforms. For example, the boottest program for wild bootstrap-based\ninference (Roodman et al. 2019) and fwildclusterboot for R (Fischer and Roodman\n2021) can use the same Julia back end. And the program reghdfejl mimics reghdfe\n(Correia 2016) in fitting linear models with high-dimensional fixed effects\nwhile calling a Julia package for tenfold acceleration on hard problems.\nreghdfejl also supports nonlinear fixed-effect models that cannot otherwise be\nfit in Stata--though preliminarily, as the Julia package for that purpose is\nimmature."
                },
                "authors": [
                    {
                        "name": "David Roodman"
                    }
                ],
                "author_detail": {
                    "name": "David Roodman"
                },
                "author": "David Roodman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09309v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09309v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01325v1",
                "updated": "2025-05-02T14:56:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    56,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:56:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    56,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague,\n  Implicit and Explicit References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague,\n  Implicit and Explicit References"
                },
                "summary": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER"
                },
                "authors": [
                    {
                        "name": "Svenja Kenneweg"
                    },
                    {
                        "name": "Jörg Deigmöller"
                    },
                    {
                        "name": "Philipp Cimiano"
                    },
                    {
                        "name": "Julian Eggert"
                    }
                ],
                "author_detail": {
                    "name": "Julian Eggert"
                },
                "author": "Julian Eggert",
                "arxiv_comment": "24 pages, 6 figures, submitted to Springer Nature Computer Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01324v1",
                "updated": "2025-05-02T14:55:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    55,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:55:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    55,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation"
                },
                "summary": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments."
                },
                "authors": [
                    {
                        "name": "Yukai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yukai Yang"
                },
                "author": "Yukai Yang",
                "arxiv_comment": "37 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for\n  journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G20, 62K99, 62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01322v1",
                "updated": "2025-05-02T14:53:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    53,
                    56,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:53:56Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    53,
                    56,
                    4,
                    122,
                    0
                ],
                "title": "FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian\n  Scene without Spatial Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian\n  Scene without Spatial Priors"
                },
                "summary": "Text-driven object insertion in 3D scenes is an emerging task that enables\nintuitive scene editing through natural language. However, existing 2D\nediting-based methods often rely on spatial priors such as 2D masks or 3D\nbounding boxes, and they struggle to ensure consistency of the inserted object.\nThese limitations hinder flexibility and scalability in real-world\napplications. In this paper, we propose FreeInsert, a novel framework that\nleverages foundation models including MLLMs, LGMs, and diffusion models to\ndisentangle object generation from spatial placement. This enables unsupervised\nand flexible object insertion in 3D scenes without spatial priors. FreeInsert\nstarts with an MLLM-based parser that extracts structured semantics, including\nobject types, spatial relationships, and attachment regions, from user\ninstructions. These semantics guide both the reconstruction of the inserted\nobject for 3D consistency and the learning of its degrees of freedom. We\nleverage the spatial reasoning capabilities of MLLMs to initialize object pose\nand scale. A hierarchical, spatially aware refinement stage further integrates\nspatial semantics and MLLM-inferred priors to enhance placement. Finally, the\nappearance of the object is improved using the inserted-object image to enhance\nvisual fidelity. Experimental results demonstrate that FreeInsert achieves\nsemantically coherent, spatially precise, and visually realistic 3D insertions\nwithout relying on spatial priors, offering a user-friendly and flexible\nediting experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-driven object insertion in 3D scenes is an emerging task that enables\nintuitive scene editing through natural language. However, existing 2D\nediting-based methods often rely on spatial priors such as 2D masks or 3D\nbounding boxes, and they struggle to ensure consistency of the inserted object.\nThese limitations hinder flexibility and scalability in real-world\napplications. In this paper, we propose FreeInsert, a novel framework that\nleverages foundation models including MLLMs, LGMs, and diffusion models to\ndisentangle object generation from spatial placement. This enables unsupervised\nand flexible object insertion in 3D scenes without spatial priors. FreeInsert\nstarts with an MLLM-based parser that extracts structured semantics, including\nobject types, spatial relationships, and attachment regions, from user\ninstructions. These semantics guide both the reconstruction of the inserted\nobject for 3D consistency and the learning of its degrees of freedom. We\nleverage the spatial reasoning capabilities of MLLMs to initialize object pose\nand scale. A hierarchical, spatially aware refinement stage further integrates\nspatial semantics and MLLM-inferred priors to enhance placement. Finally, the\nappearance of the object is improved using the inserted-object image to enhance\nvisual fidelity. Experimental results demonstrate that FreeInsert achieves\nsemantically coherent, spatially precise, and visually realistic 3D insertions\nwithout relying on spatial priors, offering a user-friendly and flexible\nediting experience."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Nicu Sebe"
                    },
                    {
                        "name": "Weizhi Nie"
                    }
                ],
                "author_detail": {
                    "name": "Weizhi Nie"
                },
                "author": "Weizhi Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01315v1",
                "updated": "2025-05-02T14:42:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    42,
                    26,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:42:26Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    42,
                    26,
                    4,
                    122,
                    0
                ],
                "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping Big Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System"
                },
                "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."
                },
                "authors": [
                    {
                        "name": "Sheikh Samit Muhaimin"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12025v2",
                "updated": "2025-05-02T14:37:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    37,
                    9,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-18T20:03:38Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    3,
                    38,
                    0,
                    323,
                    0
                ],
                "title": "Characterization of DESI fiber assignment incompleteness effect on\n  2-point clustering and mitigation methods for DR1 analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of DESI fiber assignment incompleteness effect on\n  2-point clustering and mitigation methods for DR1 analysis"
                },
                "summary": "We present an in-depth analysis of the fiber assignment incompleteness in the\nDark Energy Spectroscopic Instrument (DESI) Data Release 1 (DR1). This\nincompleteness is caused by the restricted mobility of the robotic fiber\npositioner in the DESI focal plane, which limits the number of galaxies that\ncan be observed at the same time, especially at small angular separations. As a\nresult, the observed clustering amplitude is suppressed in a scale-dependent\nmanner, which, if not addressed, can severely impact the inference of\ncosmological parameters. We discuss the methods adopted for simulating fiber\nassignment on mocks and data. In particular, we introduce the fast fiber\nassignment (FFA) emulator, which was employed to obtain the power spectrum\ncovariance adopted for the DR1 full-shape analysis. We present the mitigation\ntechniques, organised in two classes: measurement stage and model stage. We\nthen use high fidelity mocks as a reference to quantify both the accuracy of\nthe FFA emulator and the effectiveness of the different measurement-stage\nmitigation techniques. This complements the studies conducted in a parallel\npaper for the model-stage techniques, namely the $\\theta$-cut approach. We find\nthat pairwise inverse probability (PIP) weights with angular upweighting\nrecover the \"true\" clustering in all the cases considered, in both Fourier and\nconfiguration space. Notably, we present the first ever power spectrum\nmeasurement with PIP weights from real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an in-depth analysis of the fiber assignment incompleteness in the\nDark Energy Spectroscopic Instrument (DESI) Data Release 1 (DR1). This\nincompleteness is caused by the restricted mobility of the robotic fiber\npositioner in the DESI focal plane, which limits the number of galaxies that\ncan be observed at the same time, especially at small angular separations. As a\nresult, the observed clustering amplitude is suppressed in a scale-dependent\nmanner, which, if not addressed, can severely impact the inference of\ncosmological parameters. We discuss the methods adopted for simulating fiber\nassignment on mocks and data. In particular, we introduce the fast fiber\nassignment (FFA) emulator, which was employed to obtain the power spectrum\ncovariance adopted for the DR1 full-shape analysis. We present the mitigation\ntechniques, organised in two classes: measurement stage and model stage. We\nthen use high fidelity mocks as a reference to quantify both the accuracy of\nthe FFA emulator and the effectiveness of the different measurement-stage\nmitigation techniques. This complements the studies conducted in a parallel\npaper for the model-stage techniques, namely the $\\theta$-cut approach. We find\nthat pairwise inverse probability (PIP) weights with angular upweighting\nrecover the \"true\" clustering in all the cases considered, in both Fourier and\nconfiguration space. Notably, we present the first ever power spectrum\nmeasurement with PIP weights from real data."
                },
                "authors": [
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "M. M. S Hanif"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "J. Lasker"
                    },
                    {
                        "name": "A. J. Ross"
                    },
                    {
                        "name": "M. Pinon"
                    },
                    {
                        "name": "A. de Mattia"
                    },
                    {
                        "name": "M. White"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "S. Bailey"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "E. Burtin"
                    },
                    {
                        "name": "E. Chaussidon"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "S. Cole"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "S. Ferraro"
                    },
                    {
                        "name": "A. Font-Ribera"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "E. Gaztañaga"
                    },
                    {
                        "name": "S. Gontcho A Gontcho"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "J. Guy"
                    },
                    {
                        "name": "C. Hahn"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "C. Howlett"
                    },
                    {
                        "name": "S. Juneau"
                    },
                    {
                        "name": "D. Kirkby"
                    },
                    {
                        "name": "T. Kisner"
                    },
                    {
                        "name": "A. Kremin"
                    },
                    {
                        "name": "M. Landriau"
                    },
                    {
                        "name": "L. Le Guillou"
                    },
                    {
                        "name": "M. E. Levi"
                    },
                    {
                        "name": "P. McDonald"
                    },
                    {
                        "name": "A. Meisner"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Moustakas"
                    },
                    {
                        "name": "N. Palanque-Delabrouille"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "F. Prada"
                    },
                    {
                        "name": "I. Pérez-Ràfols"
                    },
                    {
                        "name": "A. Raichoor"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "R. Sharples"
                    },
                    {
                        "name": "J. Silber"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "G. Tarlé"
                    },
                    {
                        "name": "M. Vargas-Magaña"
                    },
                    {
                        "name": "B. A. Weaver"
                    },
                    {
                        "name": "P. Zarrouk"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "H. Zou"
                    }
                ],
                "author_detail": {
                    "name": "H. Zou"
                },
                "arxiv_affiliation": "DESI collaboration",
                "author": "H. Zou",
                "arxiv_comment": "46 pages, 20 figures, published in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01307v1",
                "updated": "2025-05-02T14:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    34,
                    33,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:34:33Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    34,
                    33,
                    4,
                    122,
                    0
                ],
                "title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical\n  software assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical\n  software assessments"
                },
                "summary": "Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains."
                },
                "authors": [
                    {
                        "name": "Regan Bolton"
                    },
                    {
                        "name": "Mohammadreza Sheikhfathollahi"
                    },
                    {
                        "name": "Simon Parkinson"
                    },
                    {
                        "name": "Vanessa Vulovic"
                    },
                    {
                        "name": "Gary Bamford"
                    },
                    {
                        "name": "Dan Basher"
                    },
                    {
                        "name": "Howard Parkinson"
                    }
                ],
                "author_detail": {
                    "name": "Howard Parkinson"
                },
                "author": "Howard Parkinson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17412v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17412v4",
                "updated": "2025-05-02T14:24:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    24,
                    42,
                    4,
                    122,
                    0
                ],
                "published": "2024-05-27T17:57:12Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    57,
                    12,
                    0,
                    148,
                    0
                ],
                "title": "Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE"
                },
                "summary": "This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in Ravuri et al. (2023), that describes the graph Laplacian\n(an estimate of the data precision matrix) using a Wishart distribution, with a\nmean given by a non-linear covariance function evaluated on the latents. This\ninterpretation offers deeper theoretical and semantic insights into such\nalgorithms, and forging a connection to Gaussian process latent variable models\nby showing that well-known kernels can be used to describe covariances implied\nby graph Laplacians. We also introduce tools with which similar dimensionality\nreduction methods can be studied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in Ravuri et al. (2023), that describes the graph Laplacian\n(an estimate of the data precision matrix) using a Wishart distribution, with a\nmean given by a non-linear covariance function evaluated on the latents. This\ninterpretation offers deeper theoretical and semantic insights into such\nalgorithms, and forging a connection to Gaussian process latent variable models\nby showing that well-known kernels can be used to describe covariances implied\nby graph Laplacians. We also introduce tools with which similar dimensionality\nreduction methods can be studied."
                },
                "authors": [
                    {
                        "name": "Aditya Ravuri"
                    },
                    {
                        "name": "Neil D. Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil D. Lawrence"
                },
                "author": "Neil D. Lawrence",
                "arxiv_comment": "AABI version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17412v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17412v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00165v3",
                "updated": "2025-05-02T14:14:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    14,
                    59,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-31T19:16:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    19,
                    16,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Accurate and Efficient Cardiac Digital Twin from surface ECGs: Insights\n  into Identifiability of Ventricular Conduction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Efficient Cardiac Digital Twin from surface ECGs: Insights\n  into Identifiability of Ventricular Conduction System"
                },
                "summary": "Digital twins for cardiac electrophysiology are an enabling technology for\nprecision cardiology. Current forward models are advanced enough to simulate\nthe cardiac electric activity under different pathophysiological conditions and\naccurately replicate clinical signals like torso electrocardiograms (ECGs). In\nthis work, we address the challenge of matching subject-specific QRS complexes\nusing anatomically accurate, physiologically grounded cardiac digital twins. By\nfitting the initial conditions of a cardiac propagation model, our non-invasive\nmethod predicts activation patterns during sinus rhythm. For the first time, we\ndemonstrate that distinct activation maps can generate identical surface ECGs.\nTo address this non-uniqueness, we introduce a physiological prior based on the\ndistribution of Purkinje-muscle junctions. Additionally, we develop a digital\ntwin ensemble for probabilistic inference of cardiac activation. Our approach\nmarks a significant advancement in the calibration of cardiac digital twins and\nenhances their credibility for clinical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins for cardiac electrophysiology are an enabling technology for\nprecision cardiology. Current forward models are advanced enough to simulate\nthe cardiac electric activity under different pathophysiological conditions and\naccurately replicate clinical signals like torso electrocardiograms (ECGs). In\nthis work, we address the challenge of matching subject-specific QRS complexes\nusing anatomically accurate, physiologically grounded cardiac digital twins. By\nfitting the initial conditions of a cardiac propagation model, our non-invasive\nmethod predicts activation patterns during sinus rhythm. For the first time, we\ndemonstrate that distinct activation maps can generate identical surface ECGs.\nTo address this non-uniqueness, we introduce a physiological prior based on the\ndistribution of Purkinje-muscle junctions. Additionally, we develop a digital\ntwin ensemble for probabilistic inference of cardiac activation. Our approach\nmarks a significant advancement in the calibration of cardiac digital twins and\nenhances their credibility for clinical application."
                },
                "authors": [
                    {
                        "name": "Thomas Grandits"
                    },
                    {
                        "name": "Karli Gillette"
                    },
                    {
                        "name": "Gernot Plank"
                    },
                    {
                        "name": "Simone Pezzuto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Pezzuto"
                },
                "author": "Simone Pezzuto",
                "arxiv_comment": "27 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07404v3",
                "updated": "2025-05-02T14:03:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    3,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2024-03-12T08:33:26Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    8,
                    33,
                    26,
                    1,
                    72,
                    0
                ],
                "title": "Improving Continual Learning Performance and Efficiency with Auxiliary\n  Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Continual Learning Performance and Efficiency with Auxiliary\n  Classifiers"
                },
                "summary": "Continual learning is crucial for applying machine learning in challenging,\ndynamic, and often resource-constrained environments. However, catastrophic\nforgetting - overwriting previously learned knowledge when new information is\nacquired - remains a major challenge. In this work, we examine the intermediate\nrepresentations in neural network layers during continual learning and find\nthat such representations are less prone to forgetting, highlighting their\npotential to accelerate computation. Motivated by these findings, we propose to\nuse auxiliary classifiers(ACs) to enhance performance and demonstrate that\nintegrating ACs into various continual learning methods consistently improves\naccuracy across diverse evaluation settings, yielding an average 10% relative\ngain. We also leverage the ACs to reduce the average cost of the inference by\n10-60% without compromising accuracy, enabling the model to return the\npredictions before computing all the layers. Our approach provides a scalable\nand efficient solution for continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning is crucial for applying machine learning in challenging,\ndynamic, and often resource-constrained environments. However, catastrophic\nforgetting - overwriting previously learned knowledge when new information is\nacquired - remains a major challenge. In this work, we examine the intermediate\nrepresentations in neural network layers during continual learning and find\nthat such representations are less prone to forgetting, highlighting their\npotential to accelerate computation. Motivated by these findings, we propose to\nuse auxiliary classifiers(ACs) to enhance performance and demonstrate that\nintegrating ACs into various continual learning methods consistently improves\naccuracy across diverse evaluation settings, yielding an average 10% relative\ngain. We also leverage the ACs to reduce the average cost of the inference by\n10-60% without compromising accuracy, enabling the model to return the\npredictions before computing all the layers. Our approach provides a scalable\nand efficient solution for continual learning."
                },
                "authors": [
                    {
                        "name": "Filip Szatkowski"
                    },
                    {
                        "name": "Yaoyue Zheng"
                    },
                    {
                        "name": "Fei Yang"
                    },
                    {
                        "name": "Bartłomiej Twardowski"
                    },
                    {
                        "name": "Tomasz Trzciński"
                    },
                    {
                        "name": "Joost van de Weijer"
                    }
                ],
                "author_detail": {
                    "name": "Joost van de Weijer"
                },
                "author": "Joost van de Weijer",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01279v1",
                "updated": "2025-05-02T13:55:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    22,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    22,
                    4,
                    122,
                    0
                ],
                "title": "MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for\n  Multi-Granular Spatiotemporal Traffic Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for\n  Multi-Granular Spatiotemporal Traffic Forecasting"
                },
                "summary": "Accurate traffic forecasting and swift inference provision are essential for\nintelligent transportation systems. However, the present Graph Convolutional\nNetwork (GCN)-based approaches cannot extract and fuse multi-granular\nspatiotemporal features across various spatial and temporal scales\nsufficiently, proven to yield less accurate forecasts. Besides, additional\nfeature extraction branches introduced in prior studies critically increased\nmodel complexity and extended inference time, making it challenging to provide\nfast inference for traffic forecasting. In this paper, we propose\nMultiGran-STGCNFog, an efficient fog distributed inference system with a novel\ntraffic forecasting model that employs multi-granular spatiotemporal feature\nfusion on generated dynamic traffic graphs to fully capture interdependent\ntraffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer\nexecution order and layer-device scheduling scheme simultaneously, contributes\nto considerable inference throughput improvement by leveraging heterogeneous\nfog devices in a pipelined manner. Extensive experiments on real-world datasets\ndemonstrate the superiority of the proposed method over selected baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate traffic forecasting and swift inference provision are essential for\nintelligent transportation systems. However, the present Graph Convolutional\nNetwork (GCN)-based approaches cannot extract and fuse multi-granular\nspatiotemporal features across various spatial and temporal scales\nsufficiently, proven to yield less accurate forecasts. Besides, additional\nfeature extraction branches introduced in prior studies critically increased\nmodel complexity and extended inference time, making it challenging to provide\nfast inference for traffic forecasting. In this paper, we propose\nMultiGran-STGCNFog, an efficient fog distributed inference system with a novel\ntraffic forecasting model that employs multi-granular spatiotemporal feature\nfusion on generated dynamic traffic graphs to fully capture interdependent\ntraffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer\nexecution order and layer-device scheduling scheme simultaneously, contributes\nto considerable inference throughput improvement by leveraging heterogeneous\nfog devices in a pipelined manner. Extensive experiments on real-world datasets\ndemonstrate the superiority of the proposed method over selected baselines."
                },
                "authors": [
                    {
                        "name": "Zhaoyan Wang"
                    },
                    {
                        "name": "Xiangchi Song"
                    },
                    {
                        "name": "In-Young Ko"
                    }
                ],
                "author_detail": {
                    "name": "In-Young Ko"
                },
                "author": "In-Young Ko",
                "arxiv_comment": "nine pages and five figures included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01263v1",
                "updated": "2025-05-02T13:30:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    30,
                    19,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:30:19Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    30,
                    19,
                    4,
                    122,
                    0
                ],
                "title": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and\n  Flow Matching based Voice Enhancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and\n  Flow Matching based Voice Enhancing"
                },
                "summary": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}."
                },
                "authors": [
                    {
                        "name": "Gaoxiang Cong"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Jiadong Pan"
                    },
                    {
                        "name": "Zhedong Zhang"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Anton van den Hengel"
                    },
                    {
                        "name": "Yuankai Qi"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06024v2",
                "updated": "2025-05-02T13:27:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    27,
                    2,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-10T15:01:50Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    1,
                    50,
                    4,
                    10,
                    0
                ],
                "title": "Doubly-Robust Functional Average Treatment Effect Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly-Robust Functional Average Treatment Effect Estimation"
                },
                "summary": "Understanding causal relationships in the presence of complex, structured\ndata remains a central challenge in modern statistics and science in general.\nWhile traditional causal inference methods are well-suited for scalar outcomes,\nmany scientific applications demand tools capable of handling functional data\n-- outcomes observed as functions over continuous domains such as time or\nspace. Motivated by this need, we propose DR-FoS, a novel method for estimating\nthe Functional Average Treatment Effect (FATE) in observational studies with\nfunctional outcomes. DR-FoS exhibits double robustness properties, ensuring\nconsistent estimation of FATE even if either the outcome or the treatment\nassignment model is misspecified. By leveraging recent advances in functional\ndata analysis and causal inference, we establish the asymptotic properties of\nthe estimator, proving its convergence to a Gaussian process. This guarantees\nvalid inference with simultaneous confidence bands across the entire functional\ndomain. Through extensive simulations, we show that DR-FoS achieves robust\nperformance under a wide range of model specifications. Finally, we illustrate\nthe utility of DR-FoS in a real-world application, analyzing functional\noutcomes to uncover meaningful causal insights in the SHARE (Survey of Health,\nAging and Retirement in Europe) dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relationships in the presence of complex, structured\ndata remains a central challenge in modern statistics and science in general.\nWhile traditional causal inference methods are well-suited for scalar outcomes,\nmany scientific applications demand tools capable of handling functional data\n-- outcomes observed as functions over continuous domains such as time or\nspace. Motivated by this need, we propose DR-FoS, a novel method for estimating\nthe Functional Average Treatment Effect (FATE) in observational studies with\nfunctional outcomes. DR-FoS exhibits double robustness properties, ensuring\nconsistent estimation of FATE even if either the outcome or the treatment\nassignment model is misspecified. By leveraging recent advances in functional\ndata analysis and causal inference, we establish the asymptotic properties of\nthe estimator, proving its convergence to a Gaussian process. This guarantees\nvalid inference with simultaneous confidence bands across the entire functional\ndomain. Through extensive simulations, we show that DR-FoS achieves robust\nperformance under a wide range of model specifications. Finally, we illustrate\nthe utility of DR-FoS in a real-world application, analyzing functional\noutcomes to uncover meaningful causal insights in the SHARE (Survey of Health,\nAging and Retirement in Europe) dataset."
                },
                "authors": [
                    {
                        "name": "Lorenzo Testa"
                    },
                    {
                        "name": "Tobia Boschi"
                    },
                    {
                        "name": "Francesca Chiaromonte"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    },
                    {
                        "name": "Matthew Reimherr"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Reimherr"
                },
                "author": "Matthew Reimherr",
                "arxiv_comment": "22 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01259v1",
                "updated": "2025-05-02T13:26:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    26,
                    47,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:26:47Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    26,
                    47,
                    4,
                    122,
                    0
                ],
                "title": "Digital Pathway Curation (DPC): a comparative pipeline to assess the\n  reproducibility, consensus and accuracy across Gemini, PubMed, and scientific\n  reviewers in biomedical research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Pathway Curation (DPC): a comparative pipeline to assess the\n  reproducibility, consensus and accuracy across Gemini, PubMed, and scientific\n  reviewers in biomedical research"
                },
                "summary": "A scientific study begins with a central question, and search engines like\nPubMed are the first tools for retrieving knowledge and understanding the\ncurrent state of the art. Large Language Models (LLMs) have been used in\nresearch, promising acceleration and deeper results. However, besides caution,\nthey demand rigorous validation. Assessing complex biological relationships\nremains challenging for SQL-based tools and LLM models. Here, we introduce the\nDigital Pathway Curation (DPC) pipeline to evaluate the reproducibility and\naccuracy of the Gemini models against PubMed search and human expert curation.\nUsing two omics experiments, we created a large dataset (Ensemble) based on\ndetermining pathway-disease associations. With the Ensemble dataset, we\ndemonstrate that Gemini achieves high run-to-run reproducibility of\napproximately 99% and inter-model reproducibility of around 75%. Next, we\ncalculate the crowdsourced consensus using a smaller dataset. The CSC allows us\nto calculate accuracies, and the Gemini multi-model consensus reached a\nsignificant accuracy of about 87%. Our findings demonstrate that LLMs are\nreproducible, reliable, and valuable tools for navigating complex biomedical\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scientific study begins with a central question, and search engines like\nPubMed are the first tools for retrieving knowledge and understanding the\ncurrent state of the art. Large Language Models (LLMs) have been used in\nresearch, promising acceleration and deeper results. However, besides caution,\nthey demand rigorous validation. Assessing complex biological relationships\nremains challenging for SQL-based tools and LLM models. Here, we introduce the\nDigital Pathway Curation (DPC) pipeline to evaluate the reproducibility and\naccuracy of the Gemini models against PubMed search and human expert curation.\nUsing two omics experiments, we created a large dataset (Ensemble) based on\ndetermining pathway-disease associations. With the Ensemble dataset, we\ndemonstrate that Gemini achieves high run-to-run reproducibility of\napproximately 99% and inter-model reproducibility of around 75%. Next, we\ncalculate the crowdsourced consensus using a smaller dataset. The CSC allows us\nto calculate accuracies, and the Gemini multi-model consensus reached a\nsignificant accuracy of about 87%. Our findings demonstrate that LLMs are\nreproducible, reliable, and valuable tools for navigating complex biomedical\nknowledge."
                },
                "authors": [
                    {
                        "name": "Flavio Lichtenstein"
                    },
                    {
                        "name": "Daniel Alexandre de Souza"
                    },
                    {
                        "name": "Carlos Eduardo Madureira Trufen"
                    },
                    {
                        "name": "Victor Wendel da Silva Gonçalves"
                    },
                    {
                        "name": "Juliana de Paula Bernardes"
                    },
                    {
                        "name": "Vinicius Miranda Baroni"
                    },
                    {
                        "name": "Carlos DeOcesano-Pereira"
                    },
                    {
                        "name": "Leonardo Fontoura Ormundo"
                    },
                    {
                        "name": "Fabio Augusto Labre de Souza"
                    },
                    {
                        "name": "Olga Celia Martinez Ibañez"
                    },
                    {
                        "name": "Nancy Starobinas"
                    },
                    {
                        "name": "Luciano Rodrigo Lopes"
                    },
                    {
                        "name": "Aparecida Maria Fontes"
                    },
                    {
                        "name": "Sonia Aparecida de Andrade"
                    },
                    {
                        "name": "Ana Marisa Chudzinski-Tavassi"
                    }
                ],
                "author_detail": {
                    "name": "Ana Marisa Chudzinski-Tavassi"
                },
                "author": "Ana Marisa Chudzinski-Tavassi",
                "arxiv_comment": "Main article: 7 figures and 10 tables. Supplementary information: 22\n  figures and 59 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01249v1",
                "updated": "2025-05-02T13:17:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    17,
                    8,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:17:08Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    17,
                    8,
                    4,
                    122,
                    0
                ],
                "title": "Fusing Foveal Fixations Using Linear Retinal Transformations and\n  Bayesian Experimental Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Foveal Fixations Using Linear Retinal Transformations and\n  Bayesian Experimental Design"
                },
                "summary": "Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models."
                },
                "authors": [
                    {
                        "name": "Christopher K. I. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Christopher K. I. Williams"
                },
                "author": "Christopher K. I. Williams",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01244v1",
                "updated": "2025-05-02T13:12:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    12,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:12:37Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    12,
                    37,
                    4,
                    122,
                    0
                ],
                "title": "A CFL-type Condition and Theoretical Insights for Discrete-Time Sparse\n  Full-Order Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A CFL-type Condition and Theoretical Insights for Discrete-Time Sparse\n  Full-Order Model Inference"
                },
                "summary": "In this work, we investigate the data-driven inference of a discrete-time\ndynamical system via a sparse Full-Order Model (sFOM). We first formulate the\ninvolved Least Squares (LS) problem and discuss the need for regularization,\nindicating a connection between the typically employed $l_2$ regularization and\nthe stability of the inferred discrete-time sFOM. We then provide theoretical\ninsights considering the consistency and stability properties of the inferred\nnumerical schemes that form the sFOM and exemplify them via illustrative, 1D\ntest cases of linear diffusion and linear advection. For linear advection, we\nanalytically derive a \"sampling CFL\" condition, which dictates a bound for the\nratio of spatial and temporal discretization steps in the training data that\nensures stability of the inferred sFOM. Finally, we investigate the sFOM\ninference for two nonlinear problems, namely a 2D Burgers' test case and the\nincompressible flow in an oscillating lid driven cavity, and draw connections\nbetween the theoretical findings and the properties of the inferred, nonlinear\nsFOMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate the data-driven inference of a discrete-time\ndynamical system via a sparse Full-Order Model (sFOM). We first formulate the\ninvolved Least Squares (LS) problem and discuss the need for regularization,\nindicating a connection between the typically employed $l_2$ regularization and\nthe stability of the inferred discrete-time sFOM. We then provide theoretical\ninsights considering the consistency and stability properties of the inferred\nnumerical schemes that form the sFOM and exemplify them via illustrative, 1D\ntest cases of linear diffusion and linear advection. For linear advection, we\nanalytically derive a \"sampling CFL\" condition, which dictates a bound for the\nratio of spatial and temporal discretization steps in the training data that\nensures stability of the inferred sFOM. Finally, we investigate the sFOM\ninference for two nonlinear problems, namely a 2D Burgers' test case and the\nincompressible flow in an oscillating lid driven cavity, and draw connections\nbetween the theoretical findings and the properties of the inferred, nonlinear\nsFOMs."
                },
                "authors": [
                    {
                        "name": "Leonidas Gkimisis"
                    },
                    {
                        "name": "Süleyman Yıldız"
                    },
                    {
                        "name": "Peter Benner"
                    },
                    {
                        "name": "Thomas Richter"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Richter"
                },
                "author": "Thomas Richter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01238v1",
                "updated": "2025-05-02T13:00:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    0,
                    5,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:00:05Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    0,
                    5,
                    4,
                    122,
                    0
                ],
                "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods\n  on NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods\n  on NLP Models"
                },
                "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP."
                },
                "authors": [
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Kafaite Zahra Hussain"
                    },
                    {
                        "name": "Efstratios Zaradoukas"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to the xAI World Conference (2025) - System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06217v2",
                "updated": "2025-05-02T12:44:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    12,
                    44,
                    24,
                    4,
                    122,
                    0
                ],
                "published": "2024-08-12T15:16:17Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    16,
                    17,
                    0,
                    225,
                    0
                ],
                "title": "Euclid: The Early Release Observations Lens Search Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid: The Early Release Observations Lens Search Experiment"
                },
                "summary": "We investigated the ability of the Euclid telescope to detect galaxy-scale\ngravitational lenses. To do so, we performed a systematic visual inspection of\nthe $0.7\\,\\rm{deg}^2$ Euclid Early Release Observations data towards the\nPerseus cluster using both the high-resolution $I_{\\scriptscriptstyle\\rm E}$\nband and the lower-resolution $Y_{\\scriptscriptstyle\\rm E}$,\n$J_{\\scriptscriptstyle\\rm E}$, $H_{\\scriptscriptstyle\\rm E}$ bands. Each\nextended source brighter than magnitude 23 in $I_{\\scriptscriptstyle\\rm E}$ was\ninspected by 41 expert human classifiers. This amounts to $12\\,086$ stamps of\n$10^{\\prime\\prime}\\,\\times\\,10^{\\prime\\prime}$. We found $3$ grade A and $13$\ngrade B candidates. We assessed the validity of these $16$ candidates by\nmodelling them and checking that they are consistent with a single source\nlensed by a plausible mass distribution. Five of the candidates pass this\ncheck, five others are rejected by the modelling, and six are inconclusive.\nExtrapolating from the five successfully modelled candidates, we infer that the\nfull $14\\,000\\,{\\rm deg}^2$ of the Euclid Wide Survey should contain\n$100\\,000^{+70\\,000}_{-30\\,000}$ galaxy-galaxy lenses that are both\ndiscoverable through visual inspection and have valid lens models. This is\nconsistent with theoretical forecasts of $170\\,000$ discoverable galaxy-galaxy\nlenses in Euclid. Our five modelled lenses have Einstein radii in the range\n$0.\\!\\!^{\\prime\\prime}68\\,<\\,\\theta_\\mathrm{E}\\,<1.\\!\\!^{\\prime\\prime}24$, but\ntheir Einstein radius distribution is on the higher side when compared to\ntheoretical forecasts. This suggests that our methodology is likely missing\nsmall-Einstein-radius systems. Whilst it is implausible to visually inspect the\nfull Euclid dataset, our results corroborate the promise that Euclid will\nultimately deliver a sample of around $10^5$ galaxy-scale lenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigated the ability of the Euclid telescope to detect galaxy-scale\ngravitational lenses. To do so, we performed a systematic visual inspection of\nthe $0.7\\,\\rm{deg}^2$ Euclid Early Release Observations data towards the\nPerseus cluster using both the high-resolution $I_{\\scriptscriptstyle\\rm E}$\nband and the lower-resolution $Y_{\\scriptscriptstyle\\rm E}$,\n$J_{\\scriptscriptstyle\\rm E}$, $H_{\\scriptscriptstyle\\rm E}$ bands. Each\nextended source brighter than magnitude 23 in $I_{\\scriptscriptstyle\\rm E}$ was\ninspected by 41 expert human classifiers. This amounts to $12\\,086$ stamps of\n$10^{\\prime\\prime}\\,\\times\\,10^{\\prime\\prime}$. We found $3$ grade A and $13$\ngrade B candidates. We assessed the validity of these $16$ candidates by\nmodelling them and checking that they are consistent with a single source\nlensed by a plausible mass distribution. Five of the candidates pass this\ncheck, five others are rejected by the modelling, and six are inconclusive.\nExtrapolating from the five successfully modelled candidates, we infer that the\nfull $14\\,000\\,{\\rm deg}^2$ of the Euclid Wide Survey should contain\n$100\\,000^{+70\\,000}_{-30\\,000}$ galaxy-galaxy lenses that are both\ndiscoverable through visual inspection and have valid lens models. This is\nconsistent with theoretical forecasts of $170\\,000$ discoverable galaxy-galaxy\nlenses in Euclid. Our five modelled lenses have Einstein radii in the range\n$0.\\!\\!^{\\prime\\prime}68\\,<\\,\\theta_\\mathrm{E}\\,<1.\\!\\!^{\\prime\\prime}24$, but\ntheir Einstein radius distribution is on the higher side when compared to\ntheoretical forecasts. This suggests that our methodology is likely missing\nsmall-Einstein-radius systems. Whilst it is implausible to visually inspect the\nfull Euclid dataset, our results corroborate the promise that Euclid will\nultimately deliver a sample of around $10^5$ galaxy-scale lenses."
                },
                "authors": [
                    {
                        "name": "J. A. Acevedo Barroso"
                    },
                    {
                        "name": "C. M. O'Riordan"
                    },
                    {
                        "name": "B. Clément"
                    },
                    {
                        "name": "C. Tortora"
                    },
                    {
                        "name": "T. E. Collett"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "R. Gavazzi"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "V. Busillo"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "J. Crook-Mansour"
                    },
                    {
                        "name": "L. Delchambre"
                    },
                    {
                        "name": "G. Despali"
                    },
                    {
                        "name": "L. R. Ecker"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "P. Holloway"
                    },
                    {
                        "name": "N. Jackson"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "G. Mahler"
                    },
                    {
                        "name": "L. Marchetti"
                    },
                    {
                        "name": "P. Matavulj"
                    },
                    {
                        "name": "A. Melo"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "L. A. Moustakas"
                    },
                    {
                        "name": "O. Müller"
                    },
                    {
                        "name": "A. A. Nucita"
                    },
                    {
                        "name": "A. Paulino-Afonso"
                    },
                    {
                        "name": "J. Pearson"
                    },
                    {
                        "name": "K. Rojas"
                    },
                    {
                        "name": "C. Scarlata"
                    },
                    {
                        "name": "S. Schuldt"
                    },
                    {
                        "name": "S. Serjeant"
                    },
                    {
                        "name": "D. Sluse"
                    },
                    {
                        "name": "S. H. Suyu"
                    },
                    {
                        "name": "M. Vaccari"
                    },
                    {
                        "name": "A. Verma"
                    },
                    {
                        "name": "G. Vernardos"
                    },
                    {
                        "name": "M. Walmsley"
                    },
                    {
                        "name": "H. Bouy"
                    },
                    {
                        "name": "G. L. Walth"
                    },
                    {
                        "name": "D. M. Powell"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "J. -C. Cuillandre"
                    },
                    {
                        "name": "M. Kluge"
                    },
                    {
                        "name": "T. Saifollahi"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "C. Stone"
                    },
                    {
                        "name": "A. Acebron"
                    },
                    {
                        "name": "L. Bazzanini"
                    },
                    {
                        "name": "A. Díaz-Sánchez"
                    },
                    {
                        "name": "N. B. Hogg"
                    },
                    {
                        "name": "L. V. E. Koopmans"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "A. Manjón-García"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "B. C. Nagam"
                    },
                    {
                        "name": "R. Pearce-Casey"
                    },
                    {
                        "name": "L. Scharré"
                    },
                    {
                        "name": "J. Wilde"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "A. Balestra"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "A. Basset"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "R. Bender"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "A. Caillat"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. P. Candini"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "L. Corcione"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "J. Dinis"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Garilli"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "W. Gillard"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "P. Gómez-Alvarez"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. Hook"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihänen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "D. Le Mignant"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "J. W. Nightingale"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "E. Rossetti"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. Sánchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "J. Skottfelt"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Crespí"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "E. A. Valentijn"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "D. Scott"
                    },
                    {
                        "name": "S. Vegetti"
                    }
                ],
                "author_detail": {
                    "name": "S. Vegetti"
                },
                "arxiv_affiliation": "Max-Planck-Institut für Astrophysik, Karl-Schwarzschild-Str.~1, 85748 Garching, Germany",
                "author": "S. Vegetti",
                "arxiv_doi": "10.1051/0004-6361/202451868",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202451868",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Replacement after peer review. 16 pages, 17 figures, Zenodo appendix\n  at https://zenodo.org/records/14946028",
                "arxiv_journal_ref": "A&A 697, A14 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15181v2",
                "updated": "2025-05-02T12:35:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    12,
                    35,
                    19,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-25T11:44:24Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    11,
                    44,
                    24,
                    5,
                    25,
                    0
                ],
                "title": "From Bugs to Benefits: Improving User Stories by Leveraging Crowd\n  Knowledge with CrUISE-AC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Bugs to Benefits: Improving User Stories by Leveraging Crowd\n  Knowledge with CrUISE-AC"
                },
                "summary": "Costs for resolving software defects increase exponentially in late stages.\nIncomplete or ambiguous requirements are one of the biggest sources for\ndefects, since stakeholders might not be able to communicate their needs or\nfail to share their domain specific knowledge. Combined with insufficient\ndeveloper experience, teams are prone to constructing incorrect or incomplete\nfeatures. To prevent this, requirements engineering has to explore knowledge\nsources beyond stakeholder interviews. Publicly accessible issue trackers for\nsystems within the same application domain hold essential information on\nidentified weaknesses, edge cases, and potential error sources, all documented\nby actual users. Our research aims at (1) identifying, and (2) leveraging such\nissues to improve an agile requirements artifact known as a \"user story\". We\npresent CrUISE-AC (Crowd and User Informed Suggestion Engine for Acceptance\nCriteria) as a fully automated method that investigates issues and generates\nnon-trivial additional acceptance criteria for a given user story by employing\nNLP techniques and an ensemble of LLMs. CrUISE- AC was evaluated by five\nindependent experts in two distinct business domains. Our findings suggest that\nissue trackers hold valuable information pertinent to requirements engineering.\nOur evaluation shows that 80-82% of the generated acceptance criteria add\nrelevant requirements to the user stories. Limitations are the dependence on\naccessible input issues and the fact that we do not check generated criteria\nfor being conflict-free or non-overlapping with criteria from other user\nstories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Costs for resolving software defects increase exponentially in late stages.\nIncomplete or ambiguous requirements are one of the biggest sources for\ndefects, since stakeholders might not be able to communicate their needs or\nfail to share their domain specific knowledge. Combined with insufficient\ndeveloper experience, teams are prone to constructing incorrect or incomplete\nfeatures. To prevent this, requirements engineering has to explore knowledge\nsources beyond stakeholder interviews. Publicly accessible issue trackers for\nsystems within the same application domain hold essential information on\nidentified weaknesses, edge cases, and potential error sources, all documented\nby actual users. Our research aims at (1) identifying, and (2) leveraging such\nissues to improve an agile requirements artifact known as a \"user story\". We\npresent CrUISE-AC (Crowd and User Informed Suggestion Engine for Acceptance\nCriteria) as a fully automated method that investigates issues and generates\nnon-trivial additional acceptance criteria for a given user story by employing\nNLP techniques and an ensemble of LLMs. CrUISE- AC was evaluated by five\nindependent experts in two distinct business domains. Our findings suggest that\nissue trackers hold valuable information pertinent to requirements engineering.\nOur evaluation shows that 80-82% of the generated acceptance criteria add\nrelevant requirements to the user stories. Limitations are the dependence on\naccessible input issues and the fact that we do not check generated criteria\nfor being conflict-free or non-overlapping with criteria from other user\nstories."
                },
                "authors": [
                    {
                        "name": "Stefan Schwedt"
                    },
                    {
                        "name": "Thomas Ströder"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Ströder"
                },
                "author": "Thomas Ströder",
                "arxiv_doi": "10.1109/ICSE55347.2025.00217",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICSE55347.2025.00217",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 2025 IEEE/ACM International Conference on Software\n  Engineering (ICSE). Final version at\n  https://doi.org/10.1109/ICSE55347.2025.00217",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01197v1",
                "updated": "2025-05-02T11:40:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    40,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T11:40:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    40,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "Gaussian Differential Private Bootstrap by Subsampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Differential Private Bootstrap by Subsampling"
                },
                "summary": "Bootstrap is a common tool for quantifying uncertainty in data analysis.\nHowever, besides additional computational costs in the application of the\nbootstrap on massive data, a challenging problem in bootstrap based inference\nunder Differential Privacy consists in the fact that it requires repeated\naccess to the data. As a consequence, bootstrap based differentially private\ninference requires a significant increase of the privacy budget, which on the\nother hand comes with a substantial loss in statistical accuracy.\n  A potential solution to reconcile the conflicting goals of statistical\naccuracy and privacy is to analyze the data under parametric model assumptions\nand in the last decade, several parametric bootstrap methods for inference\nunder privacy have been investigated. However, uncertainty quantification by\nparametric bootstrap is only valid if the the quantities of interest can be\nidentified as the parameters of a statistical model and the imposed model\nassumptions are (at least approximately) satisfied. An alternative to\nparametric methods is the empirical bootstrap that is a widely used tool for\nnon-parametric inference and well studied in the non-private regime. However,\nunder privacy, less insight is available. In this paper, we propose a private\nempirical $m$ out of $n$ bootstrap and validate its consistency and privacy\nguarantees under Gaussian Differential Privacy. Compared to the the private $n$\nout of $n$ bootstrap, our approach has several advantages. First, it comes with\nless computational costs, in particular for massive data. Second, the proposed\nprocedure needs less additional noise in the bootstrap iterations, which leads\nto an improved statistical accuracy while asymptotically guaranteeing the same\nlevel of privacy. Third, we demonstrate much better finite sample properties\ncompared to the currently available procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap is a common tool for quantifying uncertainty in data analysis.\nHowever, besides additional computational costs in the application of the\nbootstrap on massive data, a challenging problem in bootstrap based inference\nunder Differential Privacy consists in the fact that it requires repeated\naccess to the data. As a consequence, bootstrap based differentially private\ninference requires a significant increase of the privacy budget, which on the\nother hand comes with a substantial loss in statistical accuracy.\n  A potential solution to reconcile the conflicting goals of statistical\naccuracy and privacy is to analyze the data under parametric model assumptions\nand in the last decade, several parametric bootstrap methods for inference\nunder privacy have been investigated. However, uncertainty quantification by\nparametric bootstrap is only valid if the the quantities of interest can be\nidentified as the parameters of a statistical model and the imposed model\nassumptions are (at least approximately) satisfied. An alternative to\nparametric methods is the empirical bootstrap that is a widely used tool for\nnon-parametric inference and well studied in the non-private regime. However,\nunder privacy, less insight is available. In this paper, we propose a private\nempirical $m$ out of $n$ bootstrap and validate its consistency and privacy\nguarantees under Gaussian Differential Privacy. Compared to the the private $n$\nout of $n$ bootstrap, our approach has several advantages. First, it comes with\nless computational costs, in particular for massive data. Second, the proposed\nprocedure needs less additional noise in the bootstrap iterations, which leads\nto an improved statistical accuracy while asymptotically guaranteeing the same\nlevel of privacy. Third, we demonstrate much better finite sample properties\ncompared to the currently available procedures."
                },
                "authors": [
                    {
                        "name": "Holger Dette"
                    },
                    {
                        "name": "Carina Graw"
                    }
                ],
                "author_detail": {
                    "name": "Carina Graw"
                },
                "author": "Carina Graw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00016v2",
                "updated": "2025-05-02T11:34:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    34,
                    0,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-23T19:02:04Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    19,
                    2,
                    4,
                    2,
                    113,
                    0
                ],
                "title": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning"
                },
                "summary": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a relative 33.9\\% increase in accuracy when trained on\nText-to-SQL tasks, while Qwen achieved a relative 14.5\\% increase. These\nresults suggest that SQL can serve not only as a target formalism but also as\nan effective scaffold for learning robust, transferable reasoning over\nstructured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a relative 33.9\\% increase in accuracy when trained on\nText-to-SQL tasks, while Qwen achieved a relative 14.5\\% increase. These\nresults suggest that SQL can serve not only as a target formalism but also as\nan effective scaffold for learning robust, transferable reasoning over\nstructured data."
                },
                "authors": [
                    {
                        "name": "Josefa Lia Stoisser"
                    },
                    {
                        "name": "Marc Boubnovski Martell"
                    },
                    {
                        "name": "Julien Fauqueur"
                    }
                ],
                "author_detail": {
                    "name": "Julien Fauqueur"
                },
                "author": "Julien Fauqueur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07446v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07446v3",
                "updated": "2025-05-02T11:32:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    32,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2024-12-10T12:05:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    5,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "A Causal World Model Underlying Next Token Prediction: Exploring GPT in\n  a Controlled Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal World Model Underlying Next Token Prediction: Exploring GPT in\n  a Controlled Environment"
                },
                "summary": "Do generative pre-trained transformer (GPT) models, trained only to predict\nthe next token, implicitly learn a world model from which a sequence is\ngenerated one token at a time? We address this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that GPT\nmodels, at inference time, can be utilized for zero-shot causal structure\nlearning for input sequences and present a confidence score. Empirical\nevaluation is conducted in a controlled environment using the setup and rules\nof the Othello and Chess strategy games. A GPT, pre-trained on real-world games\nplayed with the intention of winning, is tested on out-of-distribution\nsynthetic data consisting of sequences of random legal moves. We find that the\nGPT model is likely to generate legal next moves for out-of-distribution\nsequences for which a causal structure is encoded in the attention mechanism\nwith high confidence. In cases for which the GPT model generates illegal moves\nit also fails to capture any causal structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do generative pre-trained transformer (GPT) models, trained only to predict\nthe next token, implicitly learn a world model from which a sequence is\ngenerated one token at a time? We address this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that GPT\nmodels, at inference time, can be utilized for zero-shot causal structure\nlearning for input sequences and present a confidence score. Empirical\nevaluation is conducted in a controlled environment using the setup and rules\nof the Othello and Chess strategy games. A GPT, pre-trained on real-world games\nplayed with the intention of winning, is tested on out-of-distribution\nsynthetic data consisting of sequences of random legal moves. We find that the\nGPT model is likely to generate legal next moves for out-of-distribution\nsequences for which a causal structure is encoded in the attention mechanism\nwith high confidence. In cases for which the GPT model generates illegal moves\nit also fails to capture any causal structure."
                },
                "authors": [
                    {
                        "name": "Raanan Y. Rohekar"
                    },
                    {
                        "name": "Yaniv Gurwicz"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Estelle Aflalo"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "International Conference on Machine Learning (ICML), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07446v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07446v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01179v1",
                "updated": "2025-05-02T10:47:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    47,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:47:21Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    47,
                    21,
                    4,
                    122,
                    0
                ],
                "title": "Fast Flow-based Visuomotor Policies via Conditional Optimal Transport\n  Couplings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Flow-based Visuomotor Policies via Conditional Optimal Transport\n  Couplings"
                },
                "summary": "Diffusion and flow matching policies have recently demonstrated remarkable\nperformance in robotic applications by accurately capturing multimodal robot\ntrajectory distributions. However, their computationally expensive inference,\ndue to the numerical integration of an ODE or SDE, limits their applicability\nas real-time controllers for robots. We introduce a methodology that utilizes\nconditional Optimal Transport couplings between noise and samples to enforce\nstraight solutions in the flow ODE for robot action generation tasks. We show\nthat naively coupling noise and samples fails in conditional tasks and propose\nincorporating condition variables into the coupling process to improve few-step\nperformance. The proposed few-step policy achieves a 4% higher success rate\nwith a 10x speed-up compared to Diffusion Policy on a diverse set of simulation\ntasks. Moreover, it produces high-quality and diverse action trajectories\nwithin 1-2 steps on a set of real-world robot tasks. Our method also retains\nthe same training complexity as Diffusion Policy and vanilla Flow Matching, in\ncontrast to distillation-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion and flow matching policies have recently demonstrated remarkable\nperformance in robotic applications by accurately capturing multimodal robot\ntrajectory distributions. However, their computationally expensive inference,\ndue to the numerical integration of an ODE or SDE, limits their applicability\nas real-time controllers for robots. We introduce a methodology that utilizes\nconditional Optimal Transport couplings between noise and samples to enforce\nstraight solutions in the flow ODE for robot action generation tasks. We show\nthat naively coupling noise and samples fails in conditional tasks and propose\nincorporating condition variables into the coupling process to improve few-step\nperformance. The proposed few-step policy achieves a 4% higher success rate\nwith a 10x speed-up compared to Diffusion Policy on a diverse set of simulation\ntasks. Moreover, it produces high-quality and diverse action trajectories\nwithin 1-2 steps on a set of real-world robot tasks. Our method also retains\nthe same training complexity as Diffusion Policy and vanilla Flow Matching, in\ncontrast to distillation-based approaches."
                },
                "authors": [
                    {
                        "name": "Andreas Sochopoulos"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Nikolaos Tsagkas"
                    },
                    {
                        "name": "João Moura"
                    },
                    {
                        "name": "Michael Gienger"
                    },
                    {
                        "name": "Sethu Vijayakumar"
                    }
                ],
                "author_detail": {
                    "name": "Sethu Vijayakumar"
                },
                "author": "Sethu Vijayakumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09830v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09830v4",
                "updated": "2025-05-02T10:41:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    41,
                    18,
                    4,
                    122,
                    0
                ],
                "published": "2023-11-16T11:55:27Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    55,
                    27,
                    3,
                    320,
                    0
                ],
                "title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning"
                },
                "summary": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut."
                },
                "authors": [
                    {
                        "name": "Katharina Stein"
                    },
                    {
                        "name": "Daniel Fišer"
                    },
                    {
                        "name": "Jörg Hoffmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "Extended version of the paper from the ICAPS'25 proceedings (same\n  main part + additional appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09830v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09830v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01178v1",
                "updated": "2025-05-02T10:38:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    38,
                    45,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:38:45Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    38,
                    45,
                    4,
                    122,
                    0
                ],
                "title": "A flexible Bayesian non-parametric mixture model reveals multiple\n  dependencies of swap errors in visual working memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A flexible Bayesian non-parametric mixture model reveals multiple\n  dependencies of swap errors in visual working memory"
                },
                "summary": "Human behavioural data in psychophysics has been used to elucidate the\nunderlying mechanisms of many cognitive processes, such as attention,\nsensorimotor integration, and perceptual decision making. Visual working memory\nhas particularly benefited from this approach: analyses of VWM errors have\nproven crucial for understanding VWM capacity and coding schemes, in turn\nconstraining neural models of both. One poorly understood class of VWM errors\nare swap errors, whereby participants recall an uncued item from memory. Swap\nerrors could arise from erroneous memory encoding, noisy storage, or errors at\nretrieval time - previous research has mostly implicated the latter two.\nHowever, these studies made strong a priori assumptions on the detailed\nmechanisms and/or parametric form of errors contributed by these sources. Here,\nwe pursue a data-driven approach instead, introducing a Bayesian non-parametric\nmixture model of swap errors (BNS) which provides a flexible descriptive model\nof swapping behaviour, such that swaps are allowed to depend on both the probed\nand reported features of every stimulus item. We fit BNS to the trial-by-trial\nbehaviour of human participants and show that it recapitulates the strong\ndependence of swaps on cue similarity in multiple datasets. Critically, BNS\nreveals that this dependence coexists with a non-monotonic modulation in the\nreport feature dimension for a random dot motion direction-cued,\nlocation-reported dataset. The form of the modulation inferred by BNS opens new\nquestions about the importance of memory encoding in causing swap errors in\nVWM, a distinct source to the previously suggested binding and cueing errors.\nOur analyses, combining qualitative comparisons of the highly interpretable BNS\nparameter structure with rigorous quantitative model comparison and recovery\nmethods, show that previous interpretations of swap errors may have been\nincomplete.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human behavioural data in psychophysics has been used to elucidate the\nunderlying mechanisms of many cognitive processes, such as attention,\nsensorimotor integration, and perceptual decision making. Visual working memory\nhas particularly benefited from this approach: analyses of VWM errors have\nproven crucial for understanding VWM capacity and coding schemes, in turn\nconstraining neural models of both. One poorly understood class of VWM errors\nare swap errors, whereby participants recall an uncued item from memory. Swap\nerrors could arise from erroneous memory encoding, noisy storage, or errors at\nretrieval time - previous research has mostly implicated the latter two.\nHowever, these studies made strong a priori assumptions on the detailed\nmechanisms and/or parametric form of errors contributed by these sources. Here,\nwe pursue a data-driven approach instead, introducing a Bayesian non-parametric\nmixture model of swap errors (BNS) which provides a flexible descriptive model\nof swapping behaviour, such that swaps are allowed to depend on both the probed\nand reported features of every stimulus item. We fit BNS to the trial-by-trial\nbehaviour of human participants and show that it recapitulates the strong\ndependence of swaps on cue similarity in multiple datasets. Critically, BNS\nreveals that this dependence coexists with a non-monotonic modulation in the\nreport feature dimension for a random dot motion direction-cued,\nlocation-reported dataset. The form of the modulation inferred by BNS opens new\nquestions about the importance of memory encoding in causing swap errors in\nVWM, a distinct source to the previously suggested binding and cueing errors.\nOur analyses, combining qualitative comparisons of the highly interpretable BNS\nparameter structure with rigorous quantitative model comparison and recovery\nmethods, show that previous interpretations of swap errors may have been\nincomplete."
                },
                "authors": [
                    {
                        "name": "Puria Radmard"
                    },
                    {
                        "name": "Paul M. Bays"
                    },
                    {
                        "name": "Máté Lengyel"
                    }
                ],
                "author_detail": {
                    "name": "Máté Lengyel"
                },
                "author": "Máté Lengyel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01177v1",
                "updated": "2025-05-02T10:35:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    35,
                    26,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:35:26Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    35,
                    26,
                    4,
                    122,
                    0
                ],
                "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures"
                },
                "summary": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges."
                },
                "authors": [
                    {
                        "name": "Francisco Aguilera-Martínez"
                    },
                    {
                        "name": "Fernando Berzal"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Berzal"
                },
                "author": "Fernando Berzal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01174v1",
                "updated": "2025-05-02T10:32:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    32,
                    39,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:32:39Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    32,
                    39,
                    4,
                    122,
                    0
                ],
                "title": "Self-moderation in the decentralized era: decoding blocking behavior on\n  Bluesky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-moderation in the decentralized era: decoding blocking behavior on\n  Bluesky"
                },
                "summary": "Moderation and blocking behavior, both closely related to the mitigation of\nabuse and misinformation on social platforms, are fundamental mechanisms for\nmaintaining healthy online communities. However, while centralized platforms\ntypically employ top-down moderation, decentralized networks rely on users to\nself-regulate through mechanisms like blocking actions to safeguard their\nonline experience. Given the novelty of the decentralized paradigm, addressing\nself-moderation is critical for understanding how community safety and user\nautonomy can be effectively balanced. This study examines user blocking on\nBluesky, a decentralized social networking platform, providing a comprehensive\nanalysis of over three months of user activity through the lens of blocking\nbehaviour. We define profiles based on 86 features that describe user activity,\ncontent characteristics, and network interactions, addressing two primary\nquestions: (1) Is the likelihood of a user being blocked inferable from their\nonline behavior? and (2) What behavioral features are associated with an\nincreased likelihood of being blocked? Our findings offer valuable insights and\ncontribute with a robust analytical framework to advance research in moderation\non decentralized social networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moderation and blocking behavior, both closely related to the mitigation of\nabuse and misinformation on social platforms, are fundamental mechanisms for\nmaintaining healthy online communities. However, while centralized platforms\ntypically employ top-down moderation, decentralized networks rely on users to\nself-regulate through mechanisms like blocking actions to safeguard their\nonline experience. Given the novelty of the decentralized paradigm, addressing\nself-moderation is critical for understanding how community safety and user\nautonomy can be effectively balanced. This study examines user blocking on\nBluesky, a decentralized social networking platform, providing a comprehensive\nanalysis of over three months of user activity through the lens of blocking\nbehaviour. We define profiles based on 86 features that describe user activity,\ncontent characteristics, and network interactions, addressing two primary\nquestions: (1) Is the likelihood of a user being blocked inferable from their\nonline behavior? and (2) What behavioral features are associated with an\nincreased likelihood of being blocked? Our findings offer valuable insights and\ncontribute with a robust analytical framework to advance research in moderation\non decentralized social networks."
                },
                "authors": [
                    {
                        "name": "Carlo Bono"
                    },
                    {
                        "name": "Nick Liu"
                    },
                    {
                        "name": "Giuseppe Russo"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01170v1",
                "updated": "2025-05-02T10:21:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    21,
                    44,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:21:44Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    21,
                    44,
                    4,
                    122,
                    0
                ],
                "title": "Realizing Fully-Connected Layers Over the Air via Reconfigurable\n  Intelligent Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realizing Fully-Connected Layers Over the Air via Reconfigurable\n  Intelligent Surfaces"
                },
                "summary": "By leveraging the waveform superposition property of the multiple access\nchannel, over-the-air computation (AirComp) enables the execution of digital\ncomputations through analog means in the wireless domain, leading to faster\nprocessing and reduced latency. In this paper, we propose a novel approach to\nimplement a neural network (NN) consisting of digital fully connected (FC)\nlayers using physically reconfigurable hardware. Specifically, we investigate\nreconfigurable intelligent surfaces (RISs)-assisted multiple-input\nmultiple-output (MIMO) systems to emulate the functionality of a NN for\nover-the-air inference. In this setup, both the RIS and the transceiver are\njointly configured to manipulate the ambient wireless propagation environment,\neffectively reproducing the adjustable weights of a digital FC layer. We refer\nto this new computational paradigm as \\textit{AirFC}. We formulate an imitation\nerror minimization problem between the effective channel created by RIS and a\ntarget FC layer by jointly optimizing over-the-air parameters. To solve this\nnon-convex optimization problem, an extremely low-complexity alternating\noptimization algorithm is proposed, where semi-closed-form/closed-form\nsolutions for all optimization variables are derived. Simulation results show\nthat the RIS-assisted MIMO-based AirFC can achieve competitive classification\naccuracy. Furthermore, it is also shown that a multi-RIS configuration\nsignificantly outperforms a single-RIS setup, particularly in line-of-sight\n(LoS)-dominated channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By leveraging the waveform superposition property of the multiple access\nchannel, over-the-air computation (AirComp) enables the execution of digital\ncomputations through analog means in the wireless domain, leading to faster\nprocessing and reduced latency. In this paper, we propose a novel approach to\nimplement a neural network (NN) consisting of digital fully connected (FC)\nlayers using physically reconfigurable hardware. Specifically, we investigate\nreconfigurable intelligent surfaces (RISs)-assisted multiple-input\nmultiple-output (MIMO) systems to emulate the functionality of a NN for\nover-the-air inference. In this setup, both the RIS and the transceiver are\njointly configured to manipulate the ambient wireless propagation environment,\neffectively reproducing the adjustable weights of a digital FC layer. We refer\nto this new computational paradigm as \\textit{AirFC}. We formulate an imitation\nerror minimization problem between the effective channel created by RIS and a\ntarget FC layer by jointly optimizing over-the-air parameters. To solve this\nnon-convex optimization problem, an extremely low-complexity alternating\noptimization algorithm is proposed, where semi-closed-form/closed-form\nsolutions for all optimization variables are derived. Simulation results show\nthat the RIS-assisted MIMO-based AirFC can achieve competitive classification\naccuracy. Furthermore, it is also shown that a multi-RIS configuration\nsignificantly outperforms a single-RIS setup, particularly in line-of-sight\n(LoS)-dominated channels."
                },
                "authors": [
                    {
                        "name": "Meng Hua"
                    },
                    {
                        "name": "Chenghong Bian"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Deniz Gündüz"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Gündüz"
                },
                "author": "Deniz Gündüz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01166v1",
                "updated": "2025-05-02T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    16,
                    16,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    16,
                    16,
                    4,
                    122,
                    0
                ],
                "title": "Low-rank bilinear autoregressive models for three-way criminal activity\n  tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank bilinear autoregressive models for three-way criminal activity\n  tensors"
                },
                "summary": "Criminal activity data are typically available via a three-way tensor\nencoding the reported frequencies of different crime categories across time and\nspace. The challenges that arise in the design of interpretable, yet realistic,\nmodel-based representations of the complex dependencies within and across these\nthree dimensions have led to an increasing adoption of black-box predictive\nstrategies. Although this perspective has proved successful in producing\naccurate forecasts guiding targeted interventions, the lack of interpretable\nmodel-based characterizations of the dependence structures underlying criminal\nactivity tensors prevents from inferring the cascading effects of these\ninterventions across the different dimensions. We address this gap through the\ndesign of a low-rank bilinear autoregressive model which achieves comparable\npredictive performance to black-box strategies, while allowing interpretable\ninference on the dependence structures of criminal activity reports across\ncrime categories, time, and space. This representation incorporates the time\ndimension via an autoregressive construction, accounting for spatial effects\nand dependencies among crime categories through a separable low-rank bilinear\nformulation. When applied to Chicago police reports, the proposed model\nshowcases remarkable predictive performance and also reveals interpretable\ndependence structures unveiling fundamental crime dynamics. These results\nfacilitate the design of more refined intervention policies informed by\ncascading effects of the policy itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criminal activity data are typically available via a three-way tensor\nencoding the reported frequencies of different crime categories across time and\nspace. The challenges that arise in the design of interpretable, yet realistic,\nmodel-based representations of the complex dependencies within and across these\nthree dimensions have led to an increasing adoption of black-box predictive\nstrategies. Although this perspective has proved successful in producing\naccurate forecasts guiding targeted interventions, the lack of interpretable\nmodel-based characterizations of the dependence structures underlying criminal\nactivity tensors prevents from inferring the cascading effects of these\ninterventions across the different dimensions. We address this gap through the\ndesign of a low-rank bilinear autoregressive model which achieves comparable\npredictive performance to black-box strategies, while allowing interpretable\ninference on the dependence structures of criminal activity reports across\ncrime categories, time, and space. This representation incorporates the time\ndimension via an autoregressive construction, accounting for spatial effects\nand dependencies among crime categories through a separable low-rank bilinear\nformulation. When applied to Chicago police reports, the proposed model\nshowcases remarkable predictive performance and also reveals interpretable\ndependence structures unveiling fundamental crime dynamics. These results\nfacilitate the design of more refined intervention policies informed by\ncascading effects of the policy itself."
                },
                "authors": [
                    {
                        "name": "Gregor Zens"
                    },
                    {
                        "name": "Carlos Díaz"
                    },
                    {
                        "name": "Daniele Durante"
                    },
                    {
                        "name": "Eleonora Patacchini"
                    }
                ],
                "author_detail": {
                    "name": "Eleonora Patacchini"
                },
                "author": "Eleonora Patacchini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01162v1",
                "updated": "2025-05-02T10:08:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    8,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:08:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    8,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "On the Limitations of Steering in Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limitations of Steering in Language Model Alignment"
                },
                "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models."
                },
                "authors": [
                    {
                        "name": "Chebrolu Niranjan"
                    },
                    {
                        "name": "Kokil Jaidka"
                    },
                    {
                        "name": "Gerard Christopher Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard Christopher Yeo"
                },
                "author": "Gerard Christopher Yeo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01161v1",
                "updated": "2025-05-02T10:06:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    6,
                    39,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:06:39Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    6,
                    39,
                    4,
                    122,
                    0
                ],
                "title": "Model Checks in a Kernel Ridge Regression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Checks in a Kernel Ridge Regression Framework"
                },
                "summary": "We propose new reproducing kernel-based tests for model checking in\nconditional moment restriction models. By regressing estimated residuals on\nkernel functions via kernel ridge regression (KRR), we obtain a coefficient\nfunction in a reproducing kernel Hilbert space (RKHS) that is zero if and only\nif the model is correctly specified. We introduce two classes of test\nstatistics: (i) projection-based tests, using RKHS inner products to capture\nglobal deviations, and (ii) random location tests, evaluating the KRR estimator\nat randomly chosen covariate points to detect local departures. The tests are\nconsistent against fixed alternatives and sensitive to local alternatives at\nthe $n^{-1/2}$ rate. When nuisance parameters are estimated, Neyman\northogonality projections ensure valid inference without repeated estimation in\nbootstrap samples. The random location tests are interpretable and can\nvisualize model misspecification. Simulations show strong power and size\ncontrol, especially in higher dimensions, outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose new reproducing kernel-based tests for model checking in\nconditional moment restriction models. By regressing estimated residuals on\nkernel functions via kernel ridge regression (KRR), we obtain a coefficient\nfunction in a reproducing kernel Hilbert space (RKHS) that is zero if and only\nif the model is correctly specified. We introduce two classes of test\nstatistics: (i) projection-based tests, using RKHS inner products to capture\nglobal deviations, and (ii) random location tests, evaluating the KRR estimator\nat randomly chosen covariate points to detect local departures. The tests are\nconsistent against fixed alternatives and sensitive to local alternatives at\nthe $n^{-1/2}$ rate. When nuisance parameters are estimated, Neyman\northogonality projections ensure valid inference without repeated estimation in\nbootstrap samples. The random location tests are interpretable and can\nvisualize model misspecification. Simulations show strong power and size\ncontrol, especially in higher dimensions, outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Yuhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Li"
                },
                "author": "Yuhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01160v1",
                "updated": "2025-05-02T10:05:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    5,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:05:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    5,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "TActiLE: Tiny Active LEarning for wearable devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TActiLE: Tiny Active LEarning for wearable devices"
                },
                "summary": "Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent\nyears, enabling wearable devices to be not only connected but also genuinely\nintelligent by running machine learning (ML) computations directly on-device.\nAmong such devices, smart glasses have particularly benefited from TinyML\nadvancements. TinyML facilitates the on-device execution of the inference phase\nof ML algorithms on embedded and wearable devices, and more recently, it has\nexpanded into On-device Learning (ODL), which allows both inference and\nlearning phases to occur directly on the device. The application of ODL\ntechniques to wearable devices is particularly compelling, as it enables the\ndevelopment of more personalized models that adapt based on the data of the\nuser. However, one of the major challenges of ODL algorithms is the scarcity of\nlabeled data collected on-device. In smart wearable contexts, requiring users\nto manually label large amounts of data is often impractical and could lead to\nuser disengagement with the technology. To address this issue, this paper\nexplores the application of Active Learning (AL) techniques, i.e., techniques\nthat aim at minimizing the labeling effort, by actively selecting from a large\nquantity of unlabeled data only a small subset to be labeled and added to the\ntraining set of the algorithm. In particular, we propose TActiLE, a novel AL\nalgorithm that selects from the stream of on-device sensor data the ones that\nwould help the ML algorithm improve the most once coupled with labels provided\nby the user. TActiLE is the first Active Learning technique specifically\ndesigned for the TinyML context. We evaluate its effectiveness and efficiency\nthrough experiments on multiple image classification datasets. The results\ndemonstrate its suitability for tiny and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent\nyears, enabling wearable devices to be not only connected but also genuinely\nintelligent by running machine learning (ML) computations directly on-device.\nAmong such devices, smart glasses have particularly benefited from TinyML\nadvancements. TinyML facilitates the on-device execution of the inference phase\nof ML algorithms on embedded and wearable devices, and more recently, it has\nexpanded into On-device Learning (ODL), which allows both inference and\nlearning phases to occur directly on the device. The application of ODL\ntechniques to wearable devices is particularly compelling, as it enables the\ndevelopment of more personalized models that adapt based on the data of the\nuser. However, one of the major challenges of ODL algorithms is the scarcity of\nlabeled data collected on-device. In smart wearable contexts, requiring users\nto manually label large amounts of data is often impractical and could lead to\nuser disengagement with the technology. To address this issue, this paper\nexplores the application of Active Learning (AL) techniques, i.e., techniques\nthat aim at minimizing the labeling effort, by actively selecting from a large\nquantity of unlabeled data only a small subset to be labeled and added to the\ntraining set of the algorithm. In particular, we propose TActiLE, a novel AL\nalgorithm that selects from the stream of on-device sensor data the ones that\nwould help the ML algorithm improve the most once coupled with labels provided\nby the user. TActiLE is the first Active Learning technique specifically\ndesigned for the TinyML context. We evaluate its effectiveness and efficiency\nthrough experiments on multiple image classification datasets. The results\ndemonstrate its suitability for tiny and wearable devices."
                },
                "authors": [
                    {
                        "name": "Massimo Pavan"
                    },
                    {
                        "name": "Claudio Galimberti"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_comment": "Accepted to the \"Eyes Of The Future: Integrating Artificial\n  Intelligence in Smart Eyewear (IAISE)\" Workshop, Held at the \"International\n  Joint Conference on Neural Networks (IJCNN) 2025\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04744v2",
                "updated": "2025-05-02T09:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    58,
                    28,
                    4,
                    122,
                    0
                ],
                "published": "2024-09-07T07:40:43Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    40,
                    43,
                    5,
                    251,
                    0
                ],
                "title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language\n  Models: The LMGT Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language\n  Models: The LMGT Framework"
                },
                "summary": "The inherent uncertainty in the environmental transition model of\nReinforcement Learning (RL) necessitates a delicate balance between exploration\nand exploitation. This balance is crucial for optimizing computational\nresources to accurately estimate expected rewards for the agent. In scenarios\nwith sparse rewards, such as robotic control systems, achieving this balance is\nparticularly challenging. However, given that many environments possess\nextensive prior knowledge, learning from the ground up in such contexts may be\nredundant. To address this issue, we propose Language Model Guided reward\nTuning (LMGT), a novel, sample-efficient framework. LMGT leverages the\ncomprehensive prior knowledge embedded in Large Language Models (LLMs) and\ntheir proficiency in processing non-standard data forms, such as wiki\ntutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances\nexploration and exploitation, thereby guiding the agent's exploratory behavior\nand enhancing sample efficiency. We have rigorously evaluated LMGT across\nvarious RL tasks and evaluated it in the embodied robotic environment\nHousekeep. Our results demonstrate that LMGT consistently outperforms baseline\nmethods. Furthermore, the findings suggest that our framework can substantially\nreduce the computational resources required during the RL training phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent uncertainty in the environmental transition model of\nReinforcement Learning (RL) necessitates a delicate balance between exploration\nand exploitation. This balance is crucial for optimizing computational\nresources to accurately estimate expected rewards for the agent. In scenarios\nwith sparse rewards, such as robotic control systems, achieving this balance is\nparticularly challenging. However, given that many environments possess\nextensive prior knowledge, learning from the ground up in such contexts may be\nredundant. To address this issue, we propose Language Model Guided reward\nTuning (LMGT), a novel, sample-efficient framework. LMGT leverages the\ncomprehensive prior knowledge embedded in Large Language Models (LLMs) and\ntheir proficiency in processing non-standard data forms, such as wiki\ntutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances\nexploration and exploitation, thereby guiding the agent's exploratory behavior\nand enhancing sample efficiency. We have rigorously evaluated LMGT across\nvarious RL tasks and evaluated it in the embodied robotic environment\nHousekeep. Our results demonstrate that LMGT consistently outperforms baseline\nmethods. Furthermore, the findings suggest that our framework can substantially\nreduce the computational resources required during the RL training phase."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Tan"
                },
                "author": "Xiaoyu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01150v1",
                "updated": "2025-05-02T09:50:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    50,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:50:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    50,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Methodological Foundations for AI-Driven Survey Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methodological Foundations for AI-Driven Survey Question Generation"
                },
                "summary": "This paper presents a methodological framework for using generative AI in\neducational survey research. We explore how Large Language Models (LLMs) can\ngenerate adaptive, context-aware survey questions and introduce the Synthetic\nQuestion-Response Analysis (SQRA) framework, which enables iterative testing\nand refinement of AI-generated prompts prior to deployment with human\nparticipants. Guided by Activity Theory, we analyze how AI tools mediate\nparticipant engagement and learning, and we examine ethical issues such as\nbias, privacy, and transparency. Through sentiment, lexical, and structural\nanalyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the\nalignment and effectiveness of these questions. Our findings highlight the\npromise and limitations of AI-driven survey instruments, emphasizing the need\nfor robust prompt engineering and validation to support trustworthy, scalable,\nand contextually relevant data collection in engineering education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a methodological framework for using generative AI in\neducational survey research. We explore how Large Language Models (LLMs) can\ngenerate adaptive, context-aware survey questions and introduce the Synthetic\nQuestion-Response Analysis (SQRA) framework, which enables iterative testing\nand refinement of AI-generated prompts prior to deployment with human\nparticipants. Guided by Activity Theory, we analyze how AI tools mediate\nparticipant engagement and learning, and we examine ethical issues such as\nbias, privacy, and transparency. Through sentiment, lexical, and structural\nanalyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the\nalignment and effectiveness of these questions. Our findings highlight the\npromise and limitations of AI-driven survey instruments, emphasizing the need\nfor robust prompt engineering and validation to support trustworthy, scalable,\nand contextually relevant data collection in engineering education."
                },
                "authors": [
                    {
                        "name": "Ted K. Mburu"
                    },
                    {
                        "name": "Kangxuan Rong"
                    },
                    {
                        "name": "Campbell J. McColley"
                    },
                    {
                        "name": "Alexandra Werth"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Werth"
                },
                "author": "Alexandra Werth",
                "arxiv_comment": "32 pages, 6 figures. Accepted for publication in the Journal of\n  Engineering Education (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01146v1",
                "updated": "2025-05-02T09:44:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    44,
                    51,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:44:51Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    44,
                    51,
                    4,
                    122,
                    0
                ],
                "title": "Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies,\n  Datasets, and Clinical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies,\n  Datasets, and Clinical Applications"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks. However, their application\nin the biomedical domain presents unique challenges, particularly regarding\nfactual accuracy and up-to-date knowledge integration. Retrieval Augmented\nGeneration (RAG) has emerged as a promising solution to address these\nchallenges by combining the generative capabilities of LLMs with external\nknowledge retrieval. This comprehensive survey examines the application of RAG\nin the biomedical domain, focusing on its technological components, available\ndatasets, and clinical applications. We present a systematic analysis of\nretrieval methods, ranking strategies, and generation models, while also\nexploring the challenges and future directions in this rapidly evolving field.\nOur work provides researchers and practitioners with a thorough understanding\nof the current state of biomedical RAG systems and identifies key areas for\nfuture research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks. However, their application\nin the biomedical domain presents unique challenges, particularly regarding\nfactual accuracy and up-to-date knowledge integration. Retrieval Augmented\nGeneration (RAG) has emerged as a promising solution to address these\nchallenges by combining the generative capabilities of LLMs with external\nknowledge retrieval. This comprehensive survey examines the application of RAG\nin the biomedical domain, focusing on its technological components, available\ndatasets, and clinical applications. We present a systematic analysis of\nretrieval methods, ranking strategies, and generation models, while also\nexploring the challenges and future directions in this rapidly evolving field.\nOur work provides researchers and practitioners with a thorough understanding\nof the current state of biomedical RAG systems and identifies key areas for\nfuture research and development."
                },
                "authors": [
                    {
                        "name": "Jiawei He"
                    },
                    {
                        "name": "Boya Zhang"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Xudong Chen"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Douglas Teodoro"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Teodoro"
                },
                "author": "Douglas Teodoro",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18208v2",
                "updated": "2025-05-02T09:15:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    15,
                    49,
                    4,
                    122,
                    0
                ],
                "published": "2024-09-26T18:38:10Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    38,
                    10,
                    3,
                    270,
                    0
                ],
                "title": "\"Little red dots\" cannot reside in the same dark matter halos as\n  comparably luminous unobscured quasars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Little red dots\" cannot reside in the same dark matter halos as\n  comparably luminous unobscured quasars"
                },
                "summary": "The James Webb Space Telescope (JWST) has uncovered a new population of\ncandidate broad-line AGN emerging in the early Universe, named ''little red\ndots'' (LRDs) because of their compactness and red colors at optical\nwavelengths. LRDs appear to be surprisingly abundant ($\\approx\n10^{-5}\\,\\mathrm{cMpc}^{-3}$) given that their inferred bolometric luminosities\nlargely overlap with the ones of the UV-luminous quasars identified at high $z$\nin wide-field spectroscopic surveys. In this work, we investigate how the\npopulation of LRDs and/or other UV-obscured AGN relates to the one of\nunobscured, UV-selected quasars. By comparing their number densities, we infer\nan extremely large and rapidly evolving obscured:unobscured ratio, ranging from\n$\\approx20:1$ at $z\\approx4$ to $\\approx2300:1$ at $z\\approx7$, and possibly\nextending out to very high ($\\approx10^{47}\\,\\mathrm{erg}\\,\\mathrm{s}^{-1}$)\nbolometric luminosities. This large obscured:unobscured ratio is incompatible\nwith the UV-luminous duty cycle measured for unobscured quasars at\n$z\\approx4-6$, suggesting that LRDs are too abundant to be hosted by the same\nhalos as unobscured quasars. This implies that either (a) the bolometric\nluminosities of LRDs are strongly overestimated (possibly because LRDs are\ndominated by stellar galaxy light) or (b) LRDs follow different scaling\nrelations than the ones of UV-selected quasars, representing a new population\nof accreting SMBHs emerging in the early Universe. A direct comparison between\nthe clustering of LRDs and the one of faint UV-selected quasars will ultimately\nconfirm these findings, and shed light on key properties of LRDs such as their\nhost mass distribution and duty cycle. We provide a mock analysis for the\nclustering of LRDs and show that it is feasible with current and upcoming JWST\nsurveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope (JWST) has uncovered a new population of\ncandidate broad-line AGN emerging in the early Universe, named ''little red\ndots'' (LRDs) because of their compactness and red colors at optical\nwavelengths. LRDs appear to be surprisingly abundant ($\\approx\n10^{-5}\\,\\mathrm{cMpc}^{-3}$) given that their inferred bolometric luminosities\nlargely overlap with the ones of the UV-luminous quasars identified at high $z$\nin wide-field spectroscopic surveys. In this work, we investigate how the\npopulation of LRDs and/or other UV-obscured AGN relates to the one of\nunobscured, UV-selected quasars. By comparing their number densities, we infer\nan extremely large and rapidly evolving obscured:unobscured ratio, ranging from\n$\\approx20:1$ at $z\\approx4$ to $\\approx2300:1$ at $z\\approx7$, and possibly\nextending out to very high ($\\approx10^{47}\\,\\mathrm{erg}\\,\\mathrm{s}^{-1}$)\nbolometric luminosities. This large obscured:unobscured ratio is incompatible\nwith the UV-luminous duty cycle measured for unobscured quasars at\n$z\\approx4-6$, suggesting that LRDs are too abundant to be hosted by the same\nhalos as unobscured quasars. This implies that either (a) the bolometric\nluminosities of LRDs are strongly overestimated (possibly because LRDs are\ndominated by stellar galaxy light) or (b) LRDs follow different scaling\nrelations than the ones of UV-selected quasars, representing a new population\nof accreting SMBHs emerging in the early Universe. A direct comparison between\nthe clustering of LRDs and the one of faint UV-selected quasars will ultimately\nconfirm these findings, and shed light on key properties of LRDs such as their\nhost mass distribution and duty cycle. We provide a mock analysis for the\nclustering of LRDs and show that it is feasible with current and upcoming JWST\nsurveys."
                },
                "authors": [
                    {
                        "name": "Elia Pizzati"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Anna-Christina Eilers"
                    },
                    {
                        "name": "Jiamu Huang"
                    },
                    {
                        "name": "Jan-Torge Schindler"
                    },
                    {
                        "name": "Feige Wang"
                    }
                ],
                "author_detail": {
                    "name": "Feige Wang"
                },
                "author": "Feige Wang",
                "arxiv_doi": "10.1093/mnras/staf660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.18208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages. Accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01129v1",
                "updated": "2025-05-02T09:15:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    15,
                    41,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:15:41Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    15,
                    41,
                    4,
                    122,
                    0
                ],
                "title": "Reheating ACTs on Starobinsky and Higgs inflation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reheating ACTs on Starobinsky and Higgs inflation"
                },
                "summary": "In the recent sixth data release (DR6) of the Atacama Cosmology Telescope\n(ACT) collaboration, the value of $n_{\\rm s}=0.9743 \\pm 0.0034$ for the scalar\nspectral index is reported, which excludes the Starobinsky and Higgs\ninflationary models at $2\\sigma$ level. In this paper, we perform a Bayesian\ninference of the parameters of the Starobinsky or Higgs inflationary model with\nnon-instantaneous reheating using the Markov chain Monte Carlo method. For the\nanalysis, we use observational data on the cosmic microwave background\ncollected by the Planck and ACT collaborations and on baryonic acoustic\noscillations from the DESI collaboration. The reheating stage is modelled by a\nsingle parameter $R_{\\rm reh}$. Using the modified Boltzmann code CLASS and the\ncobaya software with the GetDist package, we perform a direct inference of the\nmodel parameter space and obtain their posterior distributions. Using the\nKullback--Leibler divergence, we estimate the information gain from the data,\nyielding $2.52$ bits for the reheating parameter. Inclusion of the ACT DR6 data\nprovides $75\\%$ more information about the reheating stage compared to analysis\nwithout ACT data. We draw constraints on the reheating temperature and the\naverage equation of state. While the former can vary within $10$ orders of\nmagnitude, values in the $95\\,\\%$ credible interval indicate a sufficiently low\nreheating temperature; for the latter there is a clear preference for values\ngreater than $0.5$, which means that the conventional equations of state for\ndust $\\omega=0$ and relativistic matter $\\omega=1/3$ are excluded with more\nthan $2\\sigma$ level of significance. However, there still is a big part of\nparameter space where Starobinsky and Higgs inflationary models exhibit a high\ndegree of consistency with the latest observational data, particularly from ACT\nDR6. Therefore, it is premature to reject these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the recent sixth data release (DR6) of the Atacama Cosmology Telescope\n(ACT) collaboration, the value of $n_{\\rm s}=0.9743 \\pm 0.0034$ for the scalar\nspectral index is reported, which excludes the Starobinsky and Higgs\ninflationary models at $2\\sigma$ level. In this paper, we perform a Bayesian\ninference of the parameters of the Starobinsky or Higgs inflationary model with\nnon-instantaneous reheating using the Markov chain Monte Carlo method. For the\nanalysis, we use observational data on the cosmic microwave background\ncollected by the Planck and ACT collaborations and on baryonic acoustic\noscillations from the DESI collaboration. The reheating stage is modelled by a\nsingle parameter $R_{\\rm reh}$. Using the modified Boltzmann code CLASS and the\ncobaya software with the GetDist package, we perform a direct inference of the\nmodel parameter space and obtain their posterior distributions. Using the\nKullback--Leibler divergence, we estimate the information gain from the data,\nyielding $2.52$ bits for the reheating parameter. Inclusion of the ACT DR6 data\nprovides $75\\%$ more information about the reheating stage compared to analysis\nwithout ACT data. We draw constraints on the reheating temperature and the\naverage equation of state. While the former can vary within $10$ orders of\nmagnitude, values in the $95\\,\\%$ credible interval indicate a sufficiently low\nreheating temperature; for the latter there is a clear preference for values\ngreater than $0.5$, which means that the conventional equations of state for\ndust $\\omega=0$ and relativistic matter $\\omega=1/3$ are excluded with more\nthan $2\\sigma$ level of significance. However, there still is a big part of\nparameter space where Starobinsky and Higgs inflationary models exhibit a high\ndegree of consistency with the latest observational data, particularly from ACT\nDR6. Therefore, it is premature to reject these models."
                },
                "authors": [
                    {
                        "name": "D. S. Zharov"
                    },
                    {
                        "name": "O. O. Sobol"
                    },
                    {
                        "name": "S. I. Vilchinskii"
                    }
                ],
                "author_detail": {
                    "name": "S. I. Vilchinskii"
                },
                "author": "S. I. Vilchinskii",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01123v1",
                "updated": "2025-05-02T09:02:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    2,
                    36,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:02:36Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    2,
                    36,
                    4,
                    122,
                    0
                ],
                "title": "Poster: Machine Learning for Vulnerability Detection as Target Oracle in\n  Automated Fuzz Driver Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poster: Machine Learning for Vulnerability Detection as Target Oracle in\n  Automated Fuzz Driver Generation"
                },
                "summary": "In vulnerability detection, machine learning has been used as an effective\nstatic analysis technique, although it suffers from a significant rate of false\npositives. Contextually, in vulnerability discovery, fuzzing has been used as\nan effective dynamic analysis technique, although it requires manually writing\nfuzz drivers. Fuzz drivers usually target a limited subset of functions in a\nlibrary that must be chosen according to certain criteria, e.g., the depth of a\nfunction, the number of paths. These criteria are verified by components called\ntarget oracles. In this work, we propose an automated fuzz driver generation\nworkflow composed of: (1) identifying a likely vulnerable function by\nleveraging a machine learning for vulnerability detection model as a target\noracle, (2) automatically generating fuzz drivers, (3) fuzzing the target\nfunction to find bugs which could confirm the vulnerability inferred by the\ntarget oracle. We show our method on an existing vulnerability in libgd, with a\nplan for large-scale evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In vulnerability detection, machine learning has been used as an effective\nstatic analysis technique, although it suffers from a significant rate of false\npositives. Contextually, in vulnerability discovery, fuzzing has been used as\nan effective dynamic analysis technique, although it requires manually writing\nfuzz drivers. Fuzz drivers usually target a limited subset of functions in a\nlibrary that must be chosen according to certain criteria, e.g., the depth of a\nfunction, the number of paths. These criteria are verified by components called\ntarget oracles. In this work, we propose an automated fuzz driver generation\nworkflow composed of: (1) identifying a likely vulnerable function by\nleveraging a machine learning for vulnerability detection model as a target\noracle, (2) automatically generating fuzz drivers, (3) fuzzing the target\nfunction to find bugs which could confirm the vulnerability inferred by the\ntarget oracle. We show our method on an existing vulnerability in libgd, with a\nplan for large-scale evaluation."
                },
                "authors": [
                    {
                        "name": "Gianpietro Castiglione"
                    },
                    {
                        "name": "Marcello Maugeri"
                    },
                    {
                        "name": "Giampaolo Bella"
                    }
                ],
                "author_detail": {
                    "name": "Giampaolo Bella"
                },
                "author": "Giampaolo Bella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01110v1",
                "updated": "2025-05-02T08:45:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    45,
                    45,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T08:45:45Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    45,
                    45,
                    4,
                    122,
                    0
                ],
                "title": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context\n  Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL."
                },
                "authors": [
                    {
                        "name": "Murtadha Ahmed"
                    },
                    {
                        "name": "Wenbo"
                    },
                    {
                        "name": "Liu yunfeng"
                    }
                ],
                "author_detail": {
                    "name": "Liu yunfeng"
                },
                "author": "Liu yunfeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01104v1",
                "updated": "2025-05-02T08:31:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    31,
                    43,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T08:31:43Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    31,
                    43,
                    4,
                    122,
                    0
                ],
                "title": "VSC: Visual Search Compositional Text-to-Image Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSC: Visual Search Compositional Text-to-Image Diffusion Model"
                },
                "summary": "Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt."
                },
                "authors": [
                    {
                        "name": "Do Huu Dat"
                    },
                    {
                        "name": "Nam Hyeonu"
                    },
                    {
                        "name": "Po-Yuan Mao"
                    },
                    {
                        "name": "Tae-Hyun Oh"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Hyun Oh"
                },
                "author": "Tae-Hyun Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00650v2",
                "updated": "2025-05-02T08:21:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    21,
                    36,
                    4,
                    122,
                    0
                ],
                "published": "2024-06-02T07:30:55Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    7,
                    30,
                    55,
                    6,
                    154,
                    0
                ],
                "title": "Cluster-robust jackknife and bootstrap inference for logistic regression\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster-robust jackknife and bootstrap inference for logistic regression\n  models"
                },
                "summary": "We study cluster-robust inference for logistic regression (logit) models.\nInference based on the most commonly-used cluster-robust variance matrix\nestimator (CRVE) can be very unreliable. We study several alternatives.\nConceptually the simplest of these, but also the most computationally\ndemanding, involves jackknifing at the cluster level. We also propose a\nlinearized version of the cluster-jackknife variance matrix estimator as well\nas linearized versions of the wild cluster bootstrap. The linearizations are\nbased on empirical scores and are computationally efficient. Our results can\nreadily be generalized to other binary response models. We also discuss a new\nStata software package called logitjack which implements these procedures.\nSimulation results strongly favor the new methods, and two empirical examples\nsuggest that it can be important to use them in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study cluster-robust inference for logistic regression (logit) models.\nInference based on the most commonly-used cluster-robust variance matrix\nestimator (CRVE) can be very unreliable. We study several alternatives.\nConceptually the simplest of these, but also the most computationally\ndemanding, involves jackknifing at the cluster level. We also propose a\nlinearized version of the cluster-jackknife variance matrix estimator as well\nas linearized versions of the wild cluster bootstrap. The linearizations are\nbased on empirical scores and are computationally efficient. Our results can\nreadily be generalized to other binary response models. We also discuss a new\nStata software package called logitjack which implements these procedures.\nSimulation results strongly favor the new methods, and two empirical examples\nsuggest that it can be important to use them in practice."
                },
                "authors": [
                    {
                        "name": "James G. MacKinnon"
                    },
                    {
                        "name": "Morten Ørregaard Nielsen"
                    },
                    {
                        "name": "Matthew D. Webb"
                    }
                ],
                "author_detail": {
                    "name": "Matthew D. Webb"
                },
                "author": "Matthew D. Webb",
                "arxiv_comment": "An earlier version of this paper was circulated under the title\n  \"Cluster-robust jackknife and bootstrap inference for binary response\n  models'' and the same ArXiv number",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01081v1",
                "updated": "2025-05-02T07:39:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    39,
                    8,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:39:08Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    39,
                    8,
                    4,
                    122,
                    0
                ],
                "title": "MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC\n  Benchmark"
                },
                "summary": "Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations."
                },
                "authors": [
                    {
                        "name": "Sébastien Ferré"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Ferré"
                },
                "author": "Sébastien Ferré",
                "arxiv_comment": "54 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01077v1",
                "updated": "2025-05-02T07:33:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    33,
                    20,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:33:20Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    33,
                    20,
                    4,
                    122,
                    0
                ],
                "title": "Zero-Shot Document-Level Biomedical Relation Extraction via\n  Scenario-based Prompt Design in Two-Stage with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Document-Level Biomedical Relation Extraction via\n  Scenario-based Prompt Design in Two-Stage with LLM"
                },
                "summary": "With the advent of artificial intelligence (AI), many researchers are\nattempting to extract structured information from document-level biomedical\nliterature by fine-tuning large language models (LLMs). However, they face\nsignificant challenges such as the need for expensive hardware, like\nhigh-performance GPUs and the high labor costs associated with annotating\ntraining datasets, especially in biomedical realm. Recent research on LLMs,\nsuch as GPT-4 and Llama3, has shown promising performance in zero-shot\nsettings, inspiring us to explore a novel approach to achieve the same results\nfrom unannotated full documents using general LLMs with lower hardware and\nlabor costs. Our approach combines two major stages: named entity recognition\n(NER) and relation extraction (RE). NER identifies chemical, disease and gene\nentities from the document with synonym and hypernym extraction using an LLM\nwith a crafted prompt. RE extracts relations between entities based on\npredefined relation schemas and prompts. To enhance the effectiveness of\nprompt, we propose a five-part template structure and a scenario-based prompt\ndesign principles, along with evaluation method to systematically assess the\nprompts. Finally, we evaluated our approach against fine-tuning and pre-trained\nmodels on two biomedical datasets: ChemDisGene and CDR. The experimental\nresults indicate that our proposed method can achieve comparable accuracy\nlevels to fine-tuning and pre-trained models but with reduced human and\nhardware expenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of artificial intelligence (AI), many researchers are\nattempting to extract structured information from document-level biomedical\nliterature by fine-tuning large language models (LLMs). However, they face\nsignificant challenges such as the need for expensive hardware, like\nhigh-performance GPUs and the high labor costs associated with annotating\ntraining datasets, especially in biomedical realm. Recent research on LLMs,\nsuch as GPT-4 and Llama3, has shown promising performance in zero-shot\nsettings, inspiring us to explore a novel approach to achieve the same results\nfrom unannotated full documents using general LLMs with lower hardware and\nlabor costs. Our approach combines two major stages: named entity recognition\n(NER) and relation extraction (RE). NER identifies chemical, disease and gene\nentities from the document with synonym and hypernym extraction using an LLM\nwith a crafted prompt. RE extracts relations between entities based on\npredefined relation schemas and prompts. To enhance the effectiveness of\nprompt, we propose a five-part template structure and a scenario-based prompt\ndesign principles, along with evaluation method to systematically assess the\nprompts. Finally, we evaluated our approach against fine-tuning and pre-trained\nmodels on two biomedical datasets: ChemDisGene and CDR. The experimental\nresults indicate that our proposed method can achieve comparable accuracy\nlevels to fine-tuning and pre-trained models but with reduced human and\nhardware expenses."
                },
                "authors": [
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Ling Kang"
                    },
                    {
                        "name": "Quan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Quan Guo"
                },
                "author": "Quan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01074v1",
                "updated": "2025-05-02T07:29:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    29,
                    19,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:29:19Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    29,
                    19,
                    4,
                    122,
                    0
                ],
                "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks"
                },
                "summary": "The rapid evolution of wireless networks presents unprecedented challenges in\nmanaging complex and dynamic systems. Existing methods are increasingly facing\nfundamental limitations in addressing these challenges. In this paper, we\nintroduce WirelessAgent, a novel framework that harnesses large language models\n(LLMs) to create autonomous AI agents for diverse wireless network tasks. This\nframework integrates four core modules that mirror human cognitive processes:\nperception, memory, planning, and action. To implement it, we provide a basic\nusage based on agentic workflows and the LangGraph architecture. We demonstrate\nthe effectiveness of WirelessAgent through a comprehensive case study on\nnetwork slicing. The numerical results show that WirelessAgent achieves\n$44.4\\%$ higher bandwidth utilization than the \\emph{Prompt-based} method,\nwhile performing only $4.3\\%$ below the \\emph{Rule-based optimality}. Notably,\nWirelessAgent delivers near-optimal network throughput across diverse network\nscenarios. These underscore the framework's potential for intelligent and\nautonomous resource management in future wireless networks. The code is\navailable at \\url{https://github.com/jwentong/WirelessAgent_R1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of wireless networks presents unprecedented challenges in\nmanaging complex and dynamic systems. Existing methods are increasingly facing\nfundamental limitations in addressing these challenges. In this paper, we\nintroduce WirelessAgent, a novel framework that harnesses large language models\n(LLMs) to create autonomous AI agents for diverse wireless network tasks. This\nframework integrates four core modules that mirror human cognitive processes:\nperception, memory, planning, and action. To implement it, we provide a basic\nusage based on agentic workflows and the LangGraph architecture. We demonstrate\nthe effectiveness of WirelessAgent through a comprehensive case study on\nnetwork slicing. The numerical results show that WirelessAgent achieves\n$44.4\\%$ higher bandwidth utilization than the \\emph{Prompt-based} method,\nwhile performing only $4.3\\%$ below the \\emph{Rule-based optimality}. Notably,\nWirelessAgent delivers near-optimal network throughput across diverse network\nscenarios. These underscore the framework's potential for intelligent and\nautonomous resource management in future wireless networks. The code is\navailable at \\url{https://github.com/jwentong/WirelessAgent_R1}."
                },
                "authors": [
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "This manuscript is an extended version of a previous magazine version\n  and is now submitted to a journal for possible publication. arXiv admin note:\n  text overlap with arXiv:2409.07964",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01073v1",
                "updated": "2025-05-02T07:25:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    25,
                    1,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:25:01Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    25,
                    1,
                    4,
                    122,
                    0
                ],
                "title": "Retrieval Augmented Learning: A Retrial-based Large Language Model\n  Self-Supervised Learning and Autonomous Knowledge Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Learning: A Retrial-based Large Language Model\n  Self-Supervised Learning and Autonomous Knowledge Generation"
                },
                "summary": "The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation."
                },
                "authors": [
                    {
                        "name": "Zongyuan Li"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Runnan Qi"
                    },
                    {
                        "name": "Yanan Ni"
                    },
                    {
                        "name": "Lumin Jiang"
                    },
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xuebo Zhang"
                    },
                    {
                        "name": "Kuihua Huang"
                    },
                    {
                        "name": "Xian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Xian Guo"
                },
                "author": "Xian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05348v2",
                "updated": "2025-05-02T07:20:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    20,
                    36,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-08T06:04:22Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    4,
                    22,
                    4,
                    313,
                    0
                ],
                "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PySC2: Starcraft II learning environment for Large Language Models"
                },
                "summary": "The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions."
                },
                "authors": [
                    {
                        "name": "Zongyuan Li"
                    },
                    {
                        "name": "Yanan Ni"
                    },
                    {
                        "name": "Runnan Qi"
                    },
                    {
                        "name": "Lumin Jiang"
                    },
                    {
                        "name": "Chang Lu"
                    },
                    {
                        "name": "Xiaojie Xu"
                    },
                    {
                        "name": "Xiangbei Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yunzheng Guo"
                    },
                    {
                        "name": "Zhe Ma"
                    },
                    {
                        "name": "Huanyu Li"
                    },
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xian Guo"
                    },
                    {
                        "name": "Kuihua Huang"
                    },
                    {
                        "name": "Xuebo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuebo Zhang"
                },
                "author": "Xuebo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01067v1",
                "updated": "2025-05-02T07:16:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    16,
                    20,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:16:20Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    16,
                    20,
                    4,
                    122,
                    0
                ],
                "title": "A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in\n  Model Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in\n  Model Repositories"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms."
                },
                "authors": [
                    {
                        "name": "Ziqi Ding"
                    },
                    {
                        "name": "Qian Fu"
                    },
                    {
                        "name": "Junchen Ding"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01065v1",
                "updated": "2025-05-02T07:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    15,
                    22,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:15:22Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    15,
                    22,
                    4,
                    122,
                    0
                ],
                "title": "Good News for Script Kiddies? Evaluating Large Language Models for\n  Automated Exploit Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good News for Script Kiddies? Evaluating Large Language Models for\n  Automated Exploit Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements."
                },
                "authors": [
                    {
                        "name": "David Jin"
                    },
                    {
                        "name": "Qian Fu"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01064v1",
                "updated": "2025-05-02T07:14:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    14,
                    58,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:14:58Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    14,
                    58,
                    4,
                    122,
                    0
                ],
                "title": "Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of\n  Multimodal LLMs"
                },
                "summary": "Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR."
                },
                "authors": [
                    {
                        "name": "Hari Chandana Kuchibhotla"
                    },
                    {
                        "name": "Sai Srinivas Kancheti"
                    },
                    {
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "name": "Vineeth N Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Vineeth N Balasubramanian"
                },
                "author": "Vineeth N Balasubramanian",
                "arxiv_comment": "preprint; earlier version accepted at NeurIPS 2024 Workshop on\n  Adaptive Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01052v1",
                "updated": "2025-05-02T06:52:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    52,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:52:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    52,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "On dimension reduction in conditional dependence models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On dimension reduction in conditional dependence models"
                },
                "summary": "Inference of the conditional dependence structure is challenging when many\ncovariates are present. In numerous applications, only a low-dimensional\nprojection of the covariates influences the conditional distribution. The\nsmallest subspace that captures this effect is called the central subspace in\nthe literature. We show that inference of the central subspace of a vector\nrandom variable $\\mathbf Y$ conditioned on a vector of covariates $\\mathbf X$\ncan be separated into inference of the marginal central subspaces of the\ncomponents of $\\mathbf Y$ conditioned on $\\mathbf X$ and on the copula central\nsubspace, that we define in this paper. Further discussion addresses sufficient\ndimension reduction subspaces for conditional association measures. An adaptive\nnonparametric method is introduced for estimating the central dependence\nsubspaces, achieving parametric convergence rates under mild conditions.\nSimulation studies illustrate the practical performance of the proposed\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of the conditional dependence structure is challenging when many\ncovariates are present. In numerous applications, only a low-dimensional\nprojection of the covariates influences the conditional distribution. The\nsmallest subspace that captures this effect is called the central subspace in\nthe literature. We show that inference of the central subspace of a vector\nrandom variable $\\mathbf Y$ conditioned on a vector of covariates $\\mathbf X$\ncan be separated into inference of the marginal central subspaces of the\ncomponents of $\\mathbf Y$ conditioned on $\\mathbf X$ and on the copula central\nsubspace, that we define in this paper. Further discussion addresses sufficient\ndimension reduction subspaces for conditional association measures. An adaptive\nnonparametric method is introduced for estimating the central dependence\nsubspaces, achieving parametric convergence rates under mild conditions.\nSimulation studies illustrate the practical performance of the proposed\napproach."
                },
                "authors": [
                    {
                        "name": "Thomas Nagler"
                    },
                    {
                        "name": "Gerda Claeskens"
                    },
                    {
                        "name": "Irène Gijbels"
                    }
                ],
                "author_detail": {
                    "name": "Irène Gijbels"
                },
                "author": "Irène Gijbels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09022v3",
                "updated": "2025-05-02T06:50:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    50,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2025-03-12T03:20:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    20,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tingsong Xiao"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "To appear at IEEE Symposium on Security and Privacy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01047v1",
                "updated": "2025-05-02T06:43:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    43,
                    9,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:43:09Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    43,
                    9,
                    4,
                    122,
                    0
                ],
                "title": "Transforming physics-informed machine learning to convex optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming physics-informed machine learning to convex optimization"
                },
                "summary": "Physics-Informed Machine Learning (PIML) offers a powerful paradigm of\nintegrating data with physical laws to address important scientific problems,\nsuch as parameter estimation, inferring hidden physics, equation discovery, and\nstate prediction, etc. However, PIML still faces many serious optimization\nchallenges that significantly restrict its applications. In this study, we\npropose a comprehensive framework that transforms PIML to convex optimization\nto overcome all these limitations, referred to as Convex-PIML. The linear\ncombination of B-splines is utilized to approximate the data, promoting the\nconvexity of the loss function. By replacing the non-convex components of the\nloss function with convex approximations, the problem is further converted into\na sequence of successively refined approximated convex optimization problems.\nThis conversion allows the use of well-established convex optimization\nalgorithms, obtaining solutions effectively and efficiently. Furthermore, an\nadaptive knot optimization method based on error estimate is introduced to\nmitigate the spectral bias issue of PIML, further improving the performance.\nThe proposed theoretically guaranteed framework is tested in scenarios with\ndistinct types of physical prior. The results indicate that optimization\nproblems are effectively solved in these scenarios, highlighting the potential\nof the framework for broad applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Machine Learning (PIML) offers a powerful paradigm of\nintegrating data with physical laws to address important scientific problems,\nsuch as parameter estimation, inferring hidden physics, equation discovery, and\nstate prediction, etc. However, PIML still faces many serious optimization\nchallenges that significantly restrict its applications. In this study, we\npropose a comprehensive framework that transforms PIML to convex optimization\nto overcome all these limitations, referred to as Convex-PIML. The linear\ncombination of B-splines is utilized to approximate the data, promoting the\nconvexity of the loss function. By replacing the non-convex components of the\nloss function with convex approximations, the problem is further converted into\na sequence of successively refined approximated convex optimization problems.\nThis conversion allows the use of well-established convex optimization\nalgorithms, obtaining solutions effectively and efficiently. Furthermore, an\nadaptive knot optimization method based on error estimate is introduced to\nmitigate the spectral bias issue of PIML, further improving the performance.\nThe proposed theoretically guaranteed framework is tested in scenarios with\ndistinct types of physical prior. The results indicate that optimization\nproblems are effectively solved in these scenarios, highlighting the potential\nof the framework for broad applications."
                },
                "authors": [
                    {
                        "name": "Letian Yi"
                    },
                    {
                        "name": "Siyuan Yang"
                    },
                    {
                        "name": "Ying Cui"
                    },
                    {
                        "name": "Zhilu Lai"
                    }
                ],
                "author_detail": {
                    "name": "Zhilu Lai"
                },
                "author": "Zhilu Lai",
                "arxiv_comment": "41 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01043v1",
                "updated": "2025-05-02T06:33:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    33,
                    25,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:33:25Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    33,
                    25,
                    4,
                    122,
                    0
                ],
                "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training."
                },
                "authors": [
                    {
                        "name": "Zhiwei Hao"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Yonggang Wen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01035v1",
                "updated": "2025-05-02T06:17:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    17,
                    51,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:17:51Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    17,
                    51,
                    4,
                    122,
                    0
                ],
                "title": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large\n  Language Models?"
                },
                "summary": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs."
                },
                "authors": [
                    {
                        "name": "Lui Yoshida"
                    }
                ],
                "author_detail": {
                    "name": "Lui Yoshida"
                },
                "author": "Lui Yoshida",
                "arxiv_comment": "Accepted in AIED 2025. This preprint has not undergone any\n  post-submission improvements or corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00323v2",
                "updated": "2025-05-02T06:14:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    14,
                    29,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-01T05:47:17Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    47,
                    17,
                    3,
                    121,
                    0
                ],
                "title": "Recursive Algorithms for Sparse Parameter Identification of Multivariate\n  Stochastic Systems with Non-stationary Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Algorithms for Sparse Parameter Identification of Multivariate\n  Stochastic Systems with Non-stationary Observations"
                },
                "summary": "The classical sparse parameter identification methods are usually based on\nthe iterative basis selection such as greedy algorithms, or the numerical\noptimization of regularized cost functions such as LASSO and Bayesian posterior\nprobability distribution, etc., which, however, are not suitable for online\nsparsity inference when data arrive sequentially. This paper presents recursive\nalgorithms for sparse parameter identification of multivariate stochastic\nsystems with non-stationary observations. First, a new bivariate criterion\nfunction is presented by introducing an auxiliary variable matrix into a\nweighted $L_1$ regularization criterion. The new criterion function is\nsubsequently decomposed into two solvable subproblems via alternating\noptimization of the two variable matrices, for which the optimizers can be\nexplicitly formulated into recursive equations. Second, under the\nnon-stationary and non-persistent excitation conditions on the systems,\ntheoretical properties of the recursive algorithms are established. That is,\nthe estimates are proved to be with (i) set convergence, i.e., the accurate\nestimation of the sparse index set of the unknown parameter matrix, and (ii)\nparameter convergence, i.e., the consistent estimation for values of the\nnon-zero elements of the unknown parameter matrix. Finally, numerical examples\nare given to support the theoretical analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The classical sparse parameter identification methods are usually based on\nthe iterative basis selection such as greedy algorithms, or the numerical\noptimization of regularized cost functions such as LASSO and Bayesian posterior\nprobability distribution, etc., which, however, are not suitable for online\nsparsity inference when data arrive sequentially. This paper presents recursive\nalgorithms for sparse parameter identification of multivariate stochastic\nsystems with non-stationary observations. First, a new bivariate criterion\nfunction is presented by introducing an auxiliary variable matrix into a\nweighted $L_1$ regularization criterion. The new criterion function is\nsubsequently decomposed into two solvable subproblems via alternating\noptimization of the two variable matrices, for which the optimizers can be\nexplicitly formulated into recursive equations. Second, under the\nnon-stationary and non-persistent excitation conditions on the systems,\ntheoretical properties of the recursive algorithms are established. That is,\nthe estimates are proved to be with (i) set convergence, i.e., the accurate\nestimation of the sparse index set of the unknown parameter matrix, and (ii)\nparameter convergence, i.e., the consistent estimation for values of the\nnon-zero elements of the unknown parameter matrix. Finally, numerical examples\nare given to support the theoretical analysis."
                },
                "authors": [
                    {
                        "name": "Yanxin Fu"
                    },
                    {
                        "name": "Wenxiao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wenxiao Zhao"
                },
                "author": "Wenxiao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01023v1",
                "updated": "2025-05-02T05:40:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    40,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T05:40:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    40,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Quantum Simulations Based on Parameterized Circuit of an Antisymmetric\n  Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Simulations Based on Parameterized Circuit of an Antisymmetric\n  Matrix"
                },
                "summary": "Given an antisymmetric matrix $A$ or the unitary matrix $U_A = e^A$-or an\noracle whose answers can be used to infer information about $A$-in this paper\nwe present a parameterized circuit framework that can be used to approximate a\nquantum circuit for $e^A$. We design the circuit based on a uniform\nantisymmetric matrix with $\\{\\pm 1\\}$ elements, which has an eigenbasis that is\na phase-shifted version of the quantum Fourier transform, and its eigenspectrum\ncan be constructed by using rotation $Z$ gates. Therefore, we show that it can\nbe used to directly estimate $e^A$ and its quantum circuit representation.\nSince the circuit is based on $O(n^2)$ quantum gates, which form the\neigendecomposition of $e^A$ with separate building blocks, it can also be used\nto approximate the eigenvalues of $A$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an antisymmetric matrix $A$ or the unitary matrix $U_A = e^A$-or an\noracle whose answers can be used to infer information about $A$-in this paper\nwe present a parameterized circuit framework that can be used to approximate a\nquantum circuit for $e^A$. We design the circuit based on a uniform\nantisymmetric matrix with $\\{\\pm 1\\}$ elements, which has an eigenbasis that is\na phase-shifted version of the quantum Fourier transform, and its eigenspectrum\ncan be constructed by using rotation $Z$ gates. Therefore, we show that it can\nbe used to directly estimate $e^A$ and its quantum circuit representation.\nSince the circuit is based on $O(n^2)$ quantum gates, which form the\neigendecomposition of $e^A$ with separate building blocks, it can also be used\nto approximate the eigenvalues of $A$."
                },
                "authors": [
                    {
                        "name": "Ammar Daskin"
                    }
                ],
                "author_detail": {
                    "name": "Ammar Daskin"
                },
                "author": "Ammar Daskin",
                "arxiv_comment": "simulations can be accessed on the link:\n  https://github.com/adaskin/pqc-antisymmetric-matrix.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00070v2",
                "updated": "2025-05-02T05:27:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    27,
                    38,
                    4,
                    122,
                    0
                ],
                "published": "2024-12-29T18:58:09Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    58,
                    9,
                    6,
                    364,
                    0
                ],
                "title": "ICLR: In-Context Learning of Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICLR: In-Context Learning of Representations"
                },
                "summary": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities."
                },
                "authors": [
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Andrew Lee"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Yongyi Yang"
                    },
                    {
                        "name": "Maya Okawa"
                    },
                    {
                        "name": "Kento Nishi"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Hidenori Tanaka"
                },
                "author": "Hidenori Tanaka",
                "arxiv_comment": "ICLR 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01015v1",
                "updated": "2025-05-02T05:26:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    26,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T05:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    26,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "Value Portrait: Understanding Values of LLMs with Human-aligned\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Portrait: Understanding Values of LLMs with Human-aligned\n  Benchmark"
                },
                "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data."
                },
                "authors": [
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Dongmin Choi"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "32 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01003v4",
                "updated": "2025-05-02T05:25:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    25,
                    53,
                    4,
                    122,
                    0
                ],
                "published": "2024-12-01T23:35:53Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    23,
                    35,
                    53,
                    6,
                    336,
                    0
                ],
                "title": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competition Dynamics Shape Algorithmic Phases of In-Context Learning"
                },
                "summary": "In-Context Learning (ICL) has significantly expanded the general-purpose\nnature of large language models, allowing them to adapt to novel tasks using\nmerely the inputted context. This has motivated a series of papers that analyze\ntractable synthetic domains and postulate precise mechanisms that may underlie\nICL. However, the use of relatively distinct setups that often lack a sequence\nmodeling nature to them makes it unclear how general the reported insights from\nsuch studies are. Motivated by this, we propose a synthetic sequence modeling\ntask that involves learning to simulate a finite mixture of Markov chains. As\nwe show, models trained on this task reproduce most well-known results on ICL,\nhence offering a unified setting for studying the concept. Building on this\nsetup, we demonstrate we can explain a model's behavior by decomposing it into\nfour broad algorithms that combine a fuzzy retrieval vs. inference approach\nwith either unigram or bigram statistics of the context. These algorithms\nengage in a competition dynamics to dominate model behavior, with the precise\nexperimental conditions dictating which algorithm ends up superseding others:\ne.g., we find merely varying context size or amount of training yields (at\ntimes sharp) transitions between which algorithm dictates the model behavior,\nrevealing a mechanism that explains the transient nature of ICL. In this sense,\nwe argue ICL is best thought of as a mixture of different algorithms, each with\nits own peculiarities, instead of a monolithic capability. This also implies\nthat making general claims about ICL that hold universally across all settings\nmay be infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) has significantly expanded the general-purpose\nnature of large language models, allowing them to adapt to novel tasks using\nmerely the inputted context. This has motivated a series of papers that analyze\ntractable synthetic domains and postulate precise mechanisms that may underlie\nICL. However, the use of relatively distinct setups that often lack a sequence\nmodeling nature to them makes it unclear how general the reported insights from\nsuch studies are. Motivated by this, we propose a synthetic sequence modeling\ntask that involves learning to simulate a finite mixture of Markov chains. As\nwe show, models trained on this task reproduce most well-known results on ICL,\nhence offering a unified setting for studying the concept. Building on this\nsetup, we demonstrate we can explain a model's behavior by decomposing it into\nfour broad algorithms that combine a fuzzy retrieval vs. inference approach\nwith either unigram or bigram statistics of the context. These algorithms\nengage in a competition dynamics to dominate model behavior, with the precise\nexperimental conditions dictating which algorithm ends up superseding others:\ne.g., we find merely varying context size or amount of training yields (at\ntimes sharp) transitions between which algorithm dictates the model behavior,\nrevealing a mechanism that explains the transient nature of ICL. In this sense,\nwe argue ICL is best thought of as a mixture of different algorithms, each with\nits own peculiarities, instead of a monolithic capability. This also implies\nthat making general claims about ICL that hold universally across all settings\nmay be infeasible."
                },
                "authors": [
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Hidenori Tanaka"
                },
                "author": "Hidenori Tanaka",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "arxiv_journal_ref": "International Conference on Learning Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01009v1",
                "updated": "2025-05-02T05:16:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    16,
                    17,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T05:16:17Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    16,
                    17,
                    4,
                    122,
                    0
                ],
                "title": "Improving Large Language Model Planning with Action Sequence Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Model Planning with Action Sequence Similarity"
                },
                "summary": "Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Hanie Sedghi"
                    },
                    {
                        "name": "Bernd Bohnet"
                    },
                    {
                        "name": "Dale Schuurmans"
                    },
                    {
                        "name": "Azade Nova"
                    }
                ],
                "author_detail": {
                    "name": "Azade Nova"
                },
                "author": "Azade Nova",
                "arxiv_comment": "25 pages, 11 figures",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14359v2",
                "updated": "2025-05-02T05:08:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    8,
                    48,
                    4,
                    122,
                    0
                ],
                "published": "2024-02-22T07:58:29Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    7,
                    58,
                    29,
                    3,
                    53,
                    0
                ],
                "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable\n  Metrics on Facet-aware Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Scientific Summarization Evaluation: Grounding Explainable\n  Metrics on Facet-aware Benchmark"
                },
                "summary": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs."
                },
                "authors": [
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Tairan Wang"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Taicheng Guo"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00989v1",
                "updated": "2025-05-02T04:27:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    27,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:27:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    27,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel\n  Traffic Services through Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel\n  Traffic Services through Natural Language"
                },
                "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management."
                },
                "authors": [
                    {
                        "name": "Sijin Sun"
                    },
                    {
                        "name": "Liangbin Zhao"
                    },
                    {
                        "name": "Ming Deng"
                    },
                    {
                        "name": "Xiuju Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuju Fu"
                },
                "author": "Xiuju Fu",
                "arxiv_comment": "8 pages, 5 figures, 7 tablels, submitted to ITSC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20334v2",
                "updated": "2025-05-02T04:16:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    16,
                    24,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-29T00:54:15Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    54,
                    15,
                    1,
                    119,
                    0
                ],
                "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Flow-Matching-based TTS without Classifier-Free Guidance"
                },
                "summary": "Flow matching has demonstrated strong generative capabilities and has become\na core component in modern Text-to-Speech (TTS) systems. To ensure high-quality\nspeech synthesis, Classifier-Free Guidance (CFG) is widely used during the\ninference of flow-matching-based TTS models. However, CFG incurs substantial\ncomputational cost as it requires two forward passes, which hinders its\napplicability in real-time scenarios. In this paper, we explore removing CFG\nfrom flow-matching-based TTS models to improve inference efficiency, while\nmaintaining performance. Specifically, we reformulated the flow matching\ntraining target to directly approximate the CFG optimization trajectory. This\ntraining method eliminates the need for unconditional model evaluation and\nguided tuning during inference, effectively cutting the computational overhead\nin half. Furthermore, It can be seamlessly integrated with existing optimized\nsampling strategies. We validate our approach using the F5-TTS model on the\nLibriTTS dataset. Experimental results show that our method achieves a\n9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving\ncomparable speech quality. We will release the code and models to support\nreproducibility and foster further research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching has demonstrated strong generative capabilities and has become\na core component in modern Text-to-Speech (TTS) systems. To ensure high-quality\nspeech synthesis, Classifier-Free Guidance (CFG) is widely used during the\ninference of flow-matching-based TTS models. However, CFG incurs substantial\ncomputational cost as it requires two forward passes, which hinders its\napplicability in real-time scenarios. In this paper, we explore removing CFG\nfrom flow-matching-based TTS models to improve inference efficiency, while\nmaintaining performance. Specifically, we reformulated the flow matching\ntraining target to directly approximate the CFG optimization trajectory. This\ntraining method eliminates the need for unconditional model evaluation and\nguided tuning during inference, effectively cutting the computational overhead\nin half. Furthermore, It can be seamlessly integrated with existing optimized\nsampling strategies. We validate our approach using the F5-TTS model on the\nLibriTTS dataset. Experimental results show that our method achieves a\n9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving\ncomparable speech quality. We will release the code and models to support\nreproducibility and foster further research in this area."
                },
                "authors": [
                    {
                        "name": "Yuzhe Liang"
                    },
                    {
                        "name": "Wenzhe Liu"
                    },
                    {
                        "name": "Chunyu Qiang"
                    },
                    {
                        "name": "Zhikang Niu"
                    },
                    {
                        "name": "Yushen Chen"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Wenxi Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00985v1",
                "updated": "2025-05-02T04:13:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    13,
                    27,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:13:27Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    13,
                    27,
                    4,
                    122,
                    0
                ],
                "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling"
                },
                "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Yash Goel"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00981v1",
                "updated": "2025-05-02T04:01:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    1,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:01:31Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    1,
                    31,
                    4,
                    122,
                    0
                ],
                "title": "Multi-agents based User Values Mining for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agents based User Values Mining for Recommendation"
                },
                "summary": "Recommender systems have rapidly evolved and become integral to many online\nservices. However, existing systems sometimes produce unstable and\nunsatisfactory recommendations that fail to align with users' fundamental and\nlong-term preferences. This is because they primarily focus on extracting\nshallow and short-term interests from user behavior data, which is inherently\ndynamic and challenging to model. Unlike these transient interests, user values\nare more stable and play a crucial role in shaping user behaviors, such as\npurchasing items and consuming content. Incorporating user values into\nrecommender systems can help stabilize recommendation performance and ensure\nresults better reflect users' latent preferences. However, acquiring user\nvalues is typically difficult and costly. To address this challenge, we\nleverage the strong language understanding, zero-shot inference, and\ngeneralization capabilities of Large Language Models (LLMs) to extract user\nvalues from users' historical interactions. Unfortunately, direct extraction\nusing LLMs presents several challenges such as length constraints and\nhallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM\ncollaborative framework for effective and accurate user value extraction. In\nZOOM, we apply text summarization techniques to condense item content while\npreserving essential meaning. To mitigate hallucinations, ZOOM introduces two\nspecialized agent roles: evaluators and supervisors, to collaboratively\ngenerate accurate user values. Extensive experiments on two widely used\nrecommendation datasets with two state-of-the-art recommendation models\ndemonstrate the effectiveness and generalization of our framework in automatic\nuser value mining and recommendation performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have rapidly evolved and become integral to many online\nservices. However, existing systems sometimes produce unstable and\nunsatisfactory recommendations that fail to align with users' fundamental and\nlong-term preferences. This is because they primarily focus on extracting\nshallow and short-term interests from user behavior data, which is inherently\ndynamic and challenging to model. Unlike these transient interests, user values\nare more stable and play a crucial role in shaping user behaviors, such as\npurchasing items and consuming content. Incorporating user values into\nrecommender systems can help stabilize recommendation performance and ensure\nresults better reflect users' latent preferences. However, acquiring user\nvalues is typically difficult and costly. To address this challenge, we\nleverage the strong language understanding, zero-shot inference, and\ngeneralization capabilities of Large Language Models (LLMs) to extract user\nvalues from users' historical interactions. Unfortunately, direct extraction\nusing LLMs presents several challenges such as length constraints and\nhallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM\ncollaborative framework for effective and accurate user value extraction. In\nZOOM, we apply text summarization techniques to condense item content while\npreserving essential meaning. To mitigate hallucinations, ZOOM introduces two\nspecialized agent roles: evaluators and supervisors, to collaboratively\ngenerate accurate user values. Extensive experiments on two widely used\nrecommendation datasets with two state-of-the-art recommendation models\ndemonstrate the effectiveness and generalization of our framework in automatic\nuser value mining and recommendation performance improvement."
                },
                "authors": [
                    {
                        "name": "Lijian Chen"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Nguyen Quoc Viet Hung"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00979v1",
                "updated": "2025-05-02T03:40:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    40,
                    39,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:40:39Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    40,
                    39,
                    4,
                    122,
                    0
                ],
                "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for\n  Continue Pre-training of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for\n  Continue Pre-training of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability."
                },
                "authors": [
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Liyu Zhang"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00976v1",
                "updated": "2025-05-02T03:37:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    52,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:37:52Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    52,
                    4,
                    122,
                    0
                ],
                "title": "Attack and defense techniques in large language models: A survey and new\n  perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attack and defense techniques in large language models: A survey and new\n  perspectives"
                },
                "summary": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhiyu Liao"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Yuanguo Lin"
                    },
                    {
                        "name": "Kangkang Li"
                    },
                    {
                        "name": "Yunxuan Liu"
                    },
                    {
                        "name": "Hefeng Chen"
                    },
                    {
                        "name": "Xingwang Huang"
                    },
                    {
                        "name": "Yuanhui Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanhui Yu"
                },
                "author": "Yuanhui Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00975v1",
                "updated": "2025-05-02T03:37:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    9,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:37:09Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    9,
                    4,
                    122,
                    0
                ],
                "title": "Generating Animated Layouts as Structured Text Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Animated Layouts as Structured Text Representations"
                },
                "summary": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker"
                },
                "authors": [
                    {
                        "name": "Yeonsang Shin"
                    },
                    {
                        "name": "Jihwan Kim"
                    },
                    {
                        "name": "Yumin Song"
                    },
                    {
                        "name": "Kyungseung Lee"
                    },
                    {
                        "name": "Hyunhee Chung"
                    },
                    {
                        "name": "Taeyoung Na"
                    }
                ],
                "author_detail": {
                    "name": "Taeyoung Na"
                },
                "author": "Taeyoung Na",
                "arxiv_comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00972v1",
                "updated": "2025-05-02T03:22:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    22,
                    0,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:22:00Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    22,
                    0,
                    4,
                    122,
                    0
                ],
                "title": "Seeking to Collide: Online Safety-Critical Scenario Generation for\n  Autonomous Driving with Retrieval Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeking to Collide: Online Safety-Critical Scenario Generation for\n  Autonomous Driving with Retrieval Augmented Large Language Models"
                },
                "summary": "Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines."
                },
                "authors": [
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08324v2",
                "updated": "2025-05-02T03:07:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    7,
                    14,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-14T18:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease\n  Detection and Microbiome-Clinical Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease\n  Detection and Microbiome-Clinical Data Integration"
                },
                "summary": "Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large\nlanguage model (LLM) framework designed to integrate and analyze multimodal\ndata, including microbiome profiles, clinical datasets, and external knowledge\nbases, to enhance the understanding and classification of Alzheimer's disease\n(AD). By leveraging the agentic system with LLM, ADAM produces insights from\ndiverse data sources and contextualizes the findings with literature-driven\nevidence. A comparative evaluation with XGBoost revealed a significantly\nimproved mean F1 score and significantly reduced variance for ADAM,\nhighlighting its robustness and consistency, particularly when utilizing human\nbiological data. Although currently tailored for binary classification tasks\nwith two data modalities, future iterations will aim to incorporate additional\ndata types, such as neuroimaging and peripheral biomarkers, and expand them to\npredict disease progression, thereby broadening ADAM's scalability and\napplicability in AD research and diagnostic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large\nlanguage model (LLM) framework designed to integrate and analyze multimodal\ndata, including microbiome profiles, clinical datasets, and external knowledge\nbases, to enhance the understanding and classification of Alzheimer's disease\n(AD). By leveraging the agentic system with LLM, ADAM produces insights from\ndiverse data sources and contextualizes the findings with literature-driven\nevidence. A comparative evaluation with XGBoost revealed a significantly\nimproved mean F1 score and significantly reduced variance for ADAM,\nhighlighting its robustness and consistency, particularly when utilizing human\nbiological data. Although currently tailored for binary classification tasks\nwith two data modalities, future iterations will aim to incorporate additional\ndata types, such as neuroimaging and peripheral biomarkers, and expand them to\npredict disease progression, thereby broadening ADAM's scalability and\napplicability in AD research and diagnostic applications."
                },
                "authors": [
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Vishaldeep Kaur Sekhon"
                    },
                    {
                        "name": "Roozbeh Sadeghian"
                    },
                    {
                        "name": "Maria L. Vaida"
                    },
                    {
                        "name": "Cynthia Jo"
                    },
                    {
                        "name": "Doyle Ward"
                    },
                    {
                        "name": "Vanni Bucci"
                    },
                    {
                        "name": "John P. Haran"
                    }
                ],
                "author_detail": {
                    "name": "John P. Haran"
                },
                "author": "John P. Haran",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18837v2",
                "updated": "2025-05-02T02:39:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    39,
                    9,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-26T07:48:35Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    7,
                    48,
                    35,
                    5,
                    116,
                    0
                ],
                "title": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events"
                },
                "summary": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era."
                },
                "authors": [
                    {
                        "name": "Pouya Shaeri"
                    },
                    {
                        "name": "Yasaman Mohammadpour"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Ariane Middel"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "13 Pages, 1 figure, submitted to ACM Hypertext 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06382v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06382v3",
                "updated": "2025-05-02T02:25:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    25,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-10T23:18:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    23,
                    18,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with\n  Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with\n  Self-Attention"
                },
                "summary": "Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought."
                },
                "authors": [
                    {
                        "name": "Mumin Jia"
                    },
                    {
                        "name": "Jairo Diaz-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Jairo Diaz-Rodriguez"
                },
                "author": "Jairo Diaz-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06382v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06382v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00958v1",
                "updated": "2025-05-02T02:21:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    21,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:21:31Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    21,
                    31,
                    4,
                    122,
                    0
                ],
                "title": "Extended Persistent Homology Distinguishes Simple and Complex Contagions\n  with High Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Persistent Homology Distinguishes Simple and Complex Contagions\n  with High Accuracy"
                },
                "summary": "The social contagion literature makes a distinction between simple\n(independent cascade or bond percolation processes that pass infections through\nedges) and complex contagions (bootstrap percolation or threshold processes\nthat require local reinforcement to spread). However, distinguishing simple and\ncomplex contagions using observational data poses a significant challenge in\npractice. Estimating population-level activation functions from observed\ncontagion dynamics is hindered by confounding factors that influence adoptions\n(other than neighborhood interactions), as well as heterogeneity in individual\nbehaviors and modeling variations that make it difficult to design appropriate\nnull models for inferring contagion types. Here, we show that a new tool from\ntopological data analysis (TDA), called extended persistent homology (EPH),\nwhen applied to contagion processes over networks, can effectively detect\nsimple and complex contagion processes, as well as predict their parameters. We\ntrain classification and regression models using EPH-based topological\nsummaries computed on simulated simple and complex contagion dynamics on three\nreal-world network datasets and obtain high predictive performance over a wide\nrange of contagion parameters and under a variety of informational constraints,\nincluding uncertainty in model parameters, noise, and partial observability of\ncontagion dynamics. EPH captures the role of cycles of varying lengths in the\nobserved contagion dynamics and offers a useful metric to classify contagion\nmodels and predict their parameters. Analyzing geometrical features of network\ncontagion using TDA tools such as EPH can find applications in other network\nproblems such as seeding, vaccination, and quarantine optimization, as well as\nnetwork inference and reconstruction problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The social contagion literature makes a distinction between simple\n(independent cascade or bond percolation processes that pass infections through\nedges) and complex contagions (bootstrap percolation or threshold processes\nthat require local reinforcement to spread). However, distinguishing simple and\ncomplex contagions using observational data poses a significant challenge in\npractice. Estimating population-level activation functions from observed\ncontagion dynamics is hindered by confounding factors that influence adoptions\n(other than neighborhood interactions), as well as heterogeneity in individual\nbehaviors and modeling variations that make it difficult to design appropriate\nnull models for inferring contagion types. Here, we show that a new tool from\ntopological data analysis (TDA), called extended persistent homology (EPH),\nwhen applied to contagion processes over networks, can effectively detect\nsimple and complex contagion processes, as well as predict their parameters. We\ntrain classification and regression models using EPH-based topological\nsummaries computed on simulated simple and complex contagion dynamics on three\nreal-world network datasets and obtain high predictive performance over a wide\nrange of contagion parameters and under a variety of informational constraints,\nincluding uncertainty in model parameters, noise, and partial observability of\ncontagion dynamics. EPH captures the role of cycles of varying lengths in the\nobserved contagion dynamics and offers a useful metric to classify contagion\nmodels and predict their parameters. Analyzing geometrical features of network\ncontagion using TDA tools such as EPH can find applications in other network\nproblems such as seeding, vaccination, and quarantine optimization, as well as\nnetwork inference and reconstruction problems."
                },
                "authors": [
                    {
                        "name": "Vahid Shamsaddini"
                    },
                    {
                        "name": "M. Amin Rahimian"
                    }
                ],
                "author_detail": {
                    "name": "M. Amin Rahimian"
                },
                "author": "M. Amin Rahimian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00951v1",
                "updated": "2025-05-02T01:54:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    54,
                    8,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T01:54:08Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    54,
                    8,
                    4,
                    122,
                    0
                ],
                "title": "Preserving Privacy and Utility in LLM-Based Product Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Privacy and Utility in LLM-Based Product Recommendations"
                },
                "summary": "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use."
                },
                "authors": [
                    {
                        "name": "Tina Khezresmaeilzadeh"
                    },
                    {
                        "name": "Jiang Zhang"
                    },
                    {
                        "name": "Dimitrios Andreadis"
                    },
                    {
                        "name": "Konstantinos Psounis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Psounis"
                },
                "author": "Konstantinos Psounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00949v1",
                "updated": "2025-05-02T01:35:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    35,
                    35,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T01:35:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    35,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "Llama-Nemotron: Efficient Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Nemotron: Efficient Reasoning Models"
                },
                "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Alexander Bukharin"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Bilal Kartal"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Adi Renduchintala"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "George Armstrong"
                    },
                    {
                        "name": "Branislav Kisacanin"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Rabeeh Karimi"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Bor-Yiing Su"
                    },
                    {
                        "name": "Guyue Huang"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Robert McQueen"
                    },
                    {
                        "name": "Izzy Putterman"
                    },
                    {
                        "name": "George Lam"
                    },
                    {
                        "name": "Arun Venkatesan"
                    },
                    {
                        "name": "Sherry Wu"
                    },
                    {
                        "name": "Vinh Nguyen"
                    },
                    {
                        "name": "Manoj Kilaru"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Anna Warno"
                    },
                    {
                        "name": "Abhilash Somasamudramath"
                    },
                    {
                        "name": "Sandip Bhaskar"
                    },
                    {
                        "name": "Maka Dong"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Scot Junkin"
                    },
                    {
                        "name": "Oleksandr Romanenko"
                    },
                    {
                        "name": "Pedro Larroy"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Marco Rovinelli"
                    },
                    {
                        "name": "Viji Balas"
                    },
                    {
                        "name": "Nicholas Edelman"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Muthu Subramaniam"
                    },
                    {
                        "name": "Smita Ithape"
                    },
                    {
                        "name": "Karthik Ramamoorthy"
                    },
                    {
                        "name": "Yuting Wu"
                    },
                    {
                        "name": "Suguna Varshini Velury"
                    },
                    {
                        "name": "Omri Almog"
                    },
                    {
                        "name": "Joyjit Daw"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Nikki Pope"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Seth Schneider"
                    },
                    {
                        "name": "Guillermo Siman"
                    },
                    {
                        "name": "Tomasz Grzegorzek"
                    },
                    {
                        "name": "Pablo Ribalta"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Ann Guan"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jonah Alben"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Eric Chung"
                    }
                ],
                "author_detail": {
                    "name": "Eric Chung"
                },
                "author": "Eric Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00945v1",
                "updated": "2025-05-02T01:17:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    17,
                    3,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T01:17:03Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    17,
                    3,
                    4,
                    122,
                    0
                ],
                "title": "SSRLBot: Designing and Developing an LLM-based Agent using Socially\n  Shared Regulated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSRLBot: Designing and Developing an LLM-based Agent using Socially\n  Shared Regulated Learning"
                },
                "summary": "Large language model (LLM)-based agents are increasingly used to support\nhuman experts by streamlining complex tasks and offering actionable insights.\nHowever, their application in multi-professional decision-making, particularly\nin teamwork contexts, remains underexplored. This design-based study addresses\nthat gap by developing LLM functions to enhance collaboration, grounded in the\nSocially Shared Regulation of Learning (SSRL) framework and applied to medical\ndiagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational,\nand emotional processes in shared learning, focusing on how teams manage these\nprocesses to improve decision-making. This paper introduces SSRLBot, a\nprototype chatbot designed to help team members reflect on both their\ndiagnostic performance and key SSRL skills. Its core functions include\nsummarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic\noutcomes, annotating SSRL markers in conversation, assessing their impact on\nperformance, and identifying interpersonal regulatory dynamics. We compare\nSSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a\ncase study. SSRLBot demonstrates stronger alignment with SSRL theory, offering\ndetailed evaluations that link behaviors to regulatory dimensions and\nsuggesting improvements for collaboration. By integrating SSRL theory with LLM\ncapabilities, SSRLBot contributes a novel tool for enhancing team-based\ndecision-making and collaborative learning in high-stakes environments, such as\nmedical education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents are increasingly used to support\nhuman experts by streamlining complex tasks and offering actionable insights.\nHowever, their application in multi-professional decision-making, particularly\nin teamwork contexts, remains underexplored. This design-based study addresses\nthat gap by developing LLM functions to enhance collaboration, grounded in the\nSocially Shared Regulation of Learning (SSRL) framework and applied to medical\ndiagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational,\nand emotional processes in shared learning, focusing on how teams manage these\nprocesses to improve decision-making. This paper introduces SSRLBot, a\nprototype chatbot designed to help team members reflect on both their\ndiagnostic performance and key SSRL skills. Its core functions include\nsummarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic\noutcomes, annotating SSRL markers in conversation, assessing their impact on\nperformance, and identifying interpersonal regulatory dynamics. We compare\nSSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a\ncase study. SSRLBot demonstrates stronger alignment with SSRL theory, offering\ndetailed evaluations that link behaviors to regulatory dimensions and\nsuggesting improvements for collaboration. By integrating SSRL theory with LLM\ncapabilities, SSRLBot contributes a novel tool for enhancing team-based\ndecision-making and collaborative learning in high-stakes environments, such as\nmedical education."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Huang"
                    },
                    {
                        "name": "Jie Gao"
                    },
                    {
                        "name": "Haolun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Haolun Wu"
                },
                "author": "Haolun Wu",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08067v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08067v5",
                "updated": "2025-05-02T01:10:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    10,
                    28,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-10T16:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
                },
                "summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses, despite having access\nto preference data that includes reward scores from judge models during AI\nfeedback. Striving to maximize the implicit reward gap between the chosen and\nthe slightly inferior rejected responses can cause overfitting and unnecessary\nunlearning of the high-quality rejected responses. The unawareness of the\nreward scores also drives the LLM to indiscriminately favor the low-quality\nchosen responses and fail to generalize to optimal responses that are sparse in\ndata. To overcome these shortcomings, our study introduces reward-conditioned\nLLM policies that discern and learn from the entire spectrum of response\nquality within the dataset, helping extrapolate to more optimal regions. We\npropose an effective yet simple data relabeling method that conditions the\npreference pairs on quality scores to construct a reward-augmented dataset. The\nexperiments across various benchmarks and diverse models demonstrate that our\napproach consistently boosts DPO by a considerable margin. Through\ncomprehensive ablation studies, we demonstrate that our method not only\nmaximizes the utility of preference data but also mitigates the issue of\nunlearning, demonstrating its broad effectiveness beyond mere data expansion.\nOur code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses, despite having access\nto preference data that includes reward scores from judge models during AI\nfeedback. Striving to maximize the implicit reward gap between the chosen and\nthe slightly inferior rejected responses can cause overfitting and unnecessary\nunlearning of the high-quality rejected responses. The unawareness of the\nreward scores also drives the LLM to indiscriminately favor the low-quality\nchosen responses and fail to generalize to optimal responses that are sparse in\ndata. To overcome these shortcomings, our study introduces reward-conditioned\nLLM policies that discern and learn from the entire spectrum of response\nquality within the dataset, helping extrapolate to more optimal regions. We\npropose an effective yet simple data relabeling method that conditions the\npreference pairs on quality scores to construct a reward-augmented dataset. The\nexperiments across various benchmarks and diverse models demonstrate that our\napproach consistently boosts DPO by a considerable margin. Through\ncomprehensive ablation studies, we demonstrate that our method not only\nmaximizes the utility of preference data but also mitigates the issue of\nunlearning, demonstrating its broad effectiveness beyond mere data expansion.\nOur code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08067v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08067v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20027v2",
                "updated": "2025-05-02T00:50:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    50,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-26T00:51:39Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    0,
                    51,
                    39,
                    5,
                    300,
                    0
                ],
                "title": "Agentic Feedback Loop Modeling Improves Recommendation and User\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Feedback Loop Modeling Improves Recommendation and User\n  Simulation"
                },
                "summary": "Large language model-based agents are increasingly applied in the\nrecommendation field due to their extensive knowledge and strong planning\ncapabilities. While prior research has primarily focused on enhancing either\nthe recommendation agent or the user agent individually, the collaborative\ninteraction between the two has often been overlooked. Towards this research\ngap, we propose a novel framework that emphasizes the feedback loop process to\nfacilitate the collaboration between the recommendation agent and the user\nagent. Specifically, the recommendation agent refines its understanding of user\npreferences by analyzing the feedback from the user agent on the item\nrecommendation. Conversely, the user agent further identifies potential user\ninterests based on the items and recommendation reasons provided by the\nrecommendation agent. This iterative process enhances the ability of both\nagents to infer user behaviors, enabling more effective item recommendations\nand more accurate user simulations. Extensive experiments on three datasets\ndemonstrate the effectiveness of the agentic feedback loop: the agentic\nfeedback loop yields an average improvement of 11.52% over the single\nrecommendation agent and 21.12% over the single user agent. Furthermore, the\nresults show that the agentic feedback loop does not exacerbate popularity or\nposition bias, which are typically amplified by the real-world feedback loop,\nhighlighting its robustness. The source code is available at\nhttps://github.com/Lanyu0303/AFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents are increasingly applied in the\nrecommendation field due to their extensive knowledge and strong planning\ncapabilities. While prior research has primarily focused on enhancing either\nthe recommendation agent or the user agent individually, the collaborative\ninteraction between the two has often been overlooked. Towards this research\ngap, we propose a novel framework that emphasizes the feedback loop process to\nfacilitate the collaboration between the recommendation agent and the user\nagent. Specifically, the recommendation agent refines its understanding of user\npreferences by analyzing the feedback from the user agent on the item\nrecommendation. Conversely, the user agent further identifies potential user\ninterests based on the items and recommendation reasons provided by the\nrecommendation agent. This iterative process enhances the ability of both\nagents to infer user behaviors, enabling more effective item recommendations\nand more accurate user simulations. Extensive experiments on three datasets\ndemonstrate the effectiveness of the agentic feedback loop: the agentic\nfeedback loop yields an average improvement of 11.52% over the single\nrecommendation agent and 21.12% over the single user agent. Furthermore, the\nresults show that the agentic feedback loop does not exacerbate popularity or\nposition bias, which are typically amplified by the real-world feedback loop,\nhighlighting its robustness. The source code is available at\nhttps://github.com/Lanyu0303/AFL."
                },
                "authors": [
                    {
                        "name": "Shihao Cai"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00931v1",
                "updated": "2025-05-02T00:19:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    19,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:19:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    19,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy\n  in English Language Learner Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy\n  in English Language Learner Writing"
                },
                "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings."
                },
                "authors": [
                    {
                        "name": "Timur Jaganov"
                    },
                    {
                        "name": "John Blake"
                    },
                    {
                        "name": "Julián Villegas"
                    },
                    {
                        "name": "Nicholas Carr"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carr"
                },
                "author": "Nicholas Carr",
                "arxiv_comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00930v1",
                "updated": "2025-05-02T00:19:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    19,
                    43,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:19:43Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    19,
                    43,
                    4,
                    122,
                    0
                ],
                "title": "Robust Root Cause Diagnosis using In-Distribution Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Root Cause Diagnosis using In-Distribution Interventions"
                },
                "summary": "Diagnosing the root cause of an anomaly in a complex interconnected system is\na pressing problem in today's cloud services and industrial operations. We\npropose In-Distribution Interventions (IDI), a novel algorithm that predicts\nroot cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes\nshould take on anomalous values; 2) **Fix:** had the root cause nodes assumed\nusual values, the target node would not have been anomalous. Prior methods of\nassessing the fix condition rely on counterfactuals inferred from a Structural\nCausal Model (SCM) trained on historical data. But since anomalies are rare and\nfall outside the training distribution, the fitted SCMs yield unreliable\ncounterfactual estimates. IDI overcomes this by relying on interventional\nestimates obtained by solely probing the fitted SCM at in-distribution inputs.\nWe present a theoretical analysis comparing and bounding the errors in\nassessing the fix condition using interventional and counterfactual estimates.\nWe then conduct experiments by systematically varying the SCM's complexity to\ndemonstrate the cases where IDI's interventional approach outperforms the\ncounterfactual approach and vice versa. Experiments on both synthetic and\nPetShop RCD benchmark datasets demonstrate that \\our\\ consistently identifies\ntrue root causes more accurately and robustly than nine existing\nstate-of-the-art RCD baselines. Code is released at\nhttps://github.com/nlokeshiisc/IDI_release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing the root cause of an anomaly in a complex interconnected system is\na pressing problem in today's cloud services and industrial operations. We\npropose In-Distribution Interventions (IDI), a novel algorithm that predicts\nroot cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes\nshould take on anomalous values; 2) **Fix:** had the root cause nodes assumed\nusual values, the target node would not have been anomalous. Prior methods of\nassessing the fix condition rely on counterfactuals inferred from a Structural\nCausal Model (SCM) trained on historical data. But since anomalies are rare and\nfall outside the training distribution, the fitted SCMs yield unreliable\ncounterfactual estimates. IDI overcomes this by relying on interventional\nestimates obtained by solely probing the fitted SCM at in-distribution inputs.\nWe present a theoretical analysis comparing and bounding the errors in\nassessing the fix condition using interventional and counterfactual estimates.\nWe then conduct experiments by systematically varying the SCM's complexity to\ndemonstrate the cases where IDI's interventional approach outperforms the\ncounterfactual approach and vice versa. Experiments on both synthetic and\nPetShop RCD benchmark datasets demonstrate that \\our\\ consistently identifies\ntrue root causes more accurately and robustly than nine existing\nstate-of-the-art RCD baselines. Code is released at\nhttps://github.com/nlokeshiisc/IDI_release."
                },
                "authors": [
                    {
                        "name": "Lokesh Nagalapatti"
                    },
                    {
                        "name": "Ashutosh Srivastava"
                    },
                    {
                        "name": "Sunita Sarawagi"
                    },
                    {
                        "name": "Amit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sharma"
                },
                "author": "Amit Sharma",
                "arxiv_comment": "Accepted at ICLR-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v3",
                "updated": "2025-05-02T00:11:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    11,
                    48,
                    4,
                    122,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation..."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00926v1",
                "updated": "2025-05-02T00:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    7,
                    35,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    7,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study\n  on Training Dynamics and Implicit Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Learn Regular Language Recognition: A Theoretical Study\n  on Training Dynamics and Implicit Bias"
                },
                "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results."
                },
                "authors": [
                    {
                        "name": "Ruiquan Huang"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Jing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Yang"
                },
                "author": "Jing Yang",
                "arxiv_comment": "accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08954v2",
                "updated": "2025-05-01T23:50:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    23,
                    50,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-11T20:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    20,
                    16,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "Should you use LLMs to simulate opinions? Quality checks for early-stage\n  deliberation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should you use LLMs to simulate opinions? Quality checks for early-stage\n  deliberation"
                },
                "summary": "The emergent capabilities of large language models (LLMs) have sparked\ninterest in assessing their ability to simulate human opinions in a variety of\ncontexts, potentially serving as surrogates for human subjects in opinion\nsurveys. However, previous evaluations of this capability have depended heavily\non costly, domain-specific human survey data, and mixed empirical results about\nLLM effectiveness create uncertainty for managers about whether investing in\nthis technology is justified in early-stage research. To address these\nchallenges, we introduce a series of quality checks to support early-stage\ndeliberation about the viability of using LLMs for simulating human opinions.\nThese checks emphasize logical constraints, model stability, and alignment with\nstakeholder expectations of model outputs, thereby reducing dependence on\nhuman-generated data in the initial stages of evaluation. We demonstrate the\nusefulness of the proposed quality control tests in the context of AI-assisted\ncontent moderation, an application that both advocates and critics of LLMs'\ncapabilities to simulate human opinion see as a desirable potential use case.\nNone of the tested models passed all quality control checks, revealing several\nfailure modes. We conclude by discussing implications of these failure modes\nand recommend how organizations can utilize our proposed tests for prompt\nengineering and in their risk management practices when considering the use of\nLLMs for opinion simulation. We make our crowdsourced dataset of claims with\nhuman and LLM annotations publicly available for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergent capabilities of large language models (LLMs) have sparked\ninterest in assessing their ability to simulate human opinions in a variety of\ncontexts, potentially serving as surrogates for human subjects in opinion\nsurveys. However, previous evaluations of this capability have depended heavily\non costly, domain-specific human survey data, and mixed empirical results about\nLLM effectiveness create uncertainty for managers about whether investing in\nthis technology is justified in early-stage research. To address these\nchallenges, we introduce a series of quality checks to support early-stage\ndeliberation about the viability of using LLMs for simulating human opinions.\nThese checks emphasize logical constraints, model stability, and alignment with\nstakeholder expectations of model outputs, thereby reducing dependence on\nhuman-generated data in the initial stages of evaluation. We demonstrate the\nusefulness of the proposed quality control tests in the context of AI-assisted\ncontent moderation, an application that both advocates and critics of LLMs'\ncapabilities to simulate human opinion see as a desirable potential use case.\nNone of the tested models passed all quality control checks, revealing several\nfailure modes. We conclude by discussing implications of these failure modes\nand recommend how organizations can utilize our proposed tests for prompt\nengineering and in their risk management practices when considering the use of\nLLMs for opinion simulation. We make our crowdsourced dataset of claims with\nhuman and LLM annotations publicly available for future research."
                },
                "authors": [
                    {
                        "name": "Terrence Neumann"
                    },
                    {
                        "name": "Maria De-Arteaga"
                    },
                    {
                        "name": "Sina Fazelpour"
                    }
                ],
                "author_detail": {
                    "name": "Sina Fazelpour"
                },
                "author": "Sina Fazelpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13574v2",
                "updated": "2025-05-01T23:43:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    23,
                    43,
                    35,
                    3,
                    121,
                    0
                ],
                "published": "2025-02-19T09:29:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    29,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion\n  Models with Jointly Learned Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion\n  Models with Jointly Learned Prior"
                },
                "summary": "Denoising diffusion probabilistic models (DDPMs) can be utilized for\nrecovering a clean signal from its degraded observation(s) by conditioning the\nmodel on the degraded signal. The degraded signals are themselves contaminated\nversions of the clean signals; due to this correlation, they may encompass\ncertain useful information about the target clean data distribution. However,\nexisting adoption of the standard Gaussian as the prior distribution in turn\ndiscards such information, resulting in sub-optimal performance. In this paper,\nwe propose to improve conditional DDPMs for signal restoration by leveraging a\nmore informative prior that is jointly learned with the diffusion model. The\nproposed framework, called RestoreGrad, seamlessly integrates DDPMs into the\nvariational autoencoder framework and exploits the correlation between the\ndegraded and clean signals to encode a better diffusion prior. On speech and\nimage restoration tasks, we show that RestoreGrad demonstrates faster\nconvergence (5-10 times fewer training steps) to achieve better quality of\nrestored signals over existing DDPM baselines, and improved robustness to using\nfewer sampling steps in inference time (2-2.5 times fewer), advocating the\nadvantages of leveraging jointly learned prior for efficiency improvements in\nthe diffusion process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion probabilistic models (DDPMs) can be utilized for\nrecovering a clean signal from its degraded observation(s) by conditioning the\nmodel on the degraded signal. The degraded signals are themselves contaminated\nversions of the clean signals; due to this correlation, they may encompass\ncertain useful information about the target clean data distribution. However,\nexisting adoption of the standard Gaussian as the prior distribution in turn\ndiscards such information, resulting in sub-optimal performance. In this paper,\nwe propose to improve conditional DDPMs for signal restoration by leveraging a\nmore informative prior that is jointly learned with the diffusion model. The\nproposed framework, called RestoreGrad, seamlessly integrates DDPMs into the\nvariational autoencoder framework and exploits the correlation between the\ndegraded and clean signals to encode a better diffusion prior. On speech and\nimage restoration tasks, we show that RestoreGrad demonstrates faster\nconvergence (5-10 times fewer training steps) to achieve better quality of\nrestored signals over existing DDPM baselines, and improved robustness to using\nfewer sampling steps in inference time (2-2.5 times fewer), advocating the\nadvantages of leveraging jointly learned prior for efficiency improvements in\nthe diffusion process."
                },
                "authors": [
                    {
                        "name": "Ching-Hua Lee"
                    },
                    {
                        "name": "Chouchang Yang"
                    },
                    {
                        "name": "Jaejin Cho"
                    },
                    {
                        "name": "Yashas Malur Saidutta"
                    },
                    {
                        "name": "Rakshith Sharma Srinivasa"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Jin"
                },
                "author": "Hongxia Jin",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.01420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01420v1",
                "updated": "2025-05-02T17:57:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    57,
                    14,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T17:57:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    57,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Frontier Models for Stealth and Situational Awareness"
                },
                "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth."
                },
                "authors": [
                    {
                        "name": "Mary Phuong"
                    },
                    {
                        "name": "Roland S. Zimmermann"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "Victoria Krakovna"
                    },
                    {
                        "name": "Sarah Cogan"
                    },
                    {
                        "name": "Allan Dafoe"
                    },
                    {
                        "name": "Lewis Ho"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01828v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01828v3",
                "updated": "2025-05-02T17:53:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    53,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-02-03T21:11:02Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    11,
                    2,
                    0,
                    34,
                    0
                ],
                "title": "From Foresight to Forethought: VLM-In-the-Loop Policy Steering via\n  Latent Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Foresight to Forethought: VLM-In-the-Loop Policy Steering via\n  Latent Alignment"
                },
                "summary": "While generative robot policies have demonstrated significant potential in\nlearning complex, multimodal behaviors from demonstrations, they still exhibit\ndiverse failures at deployment-time. Policy steering offers an elegant solution\nto reducing the chance of failure by using an external verifier to select from\nlow-level actions proposed by an imperfect generative policy. Here, one might\nhope to use a Vision Language Model (VLM) as a verifier, leveraging its\nopen-world reasoning capabilities. However, off-the-shelf VLMs struggle to\nunderstand the consequences of low-level robot actions as they are represented\nfundamentally differently than the text and images the VLM was trained on. In\nresponse, we propose FOREWARN, a novel framework to unlock the potential of\nVLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is\nto decouple the VLM's burden of predicting action outcomes (foresight) from\nevaluation (forethought). For foresight, we leverage a latent world model to\nimagine future latent states given diverse low-level action plans. For\nforethought, we align the VLM with these predicted latent states to reason\nabout the consequences of actions in its native representation--natural\nlanguage--and effectively filter proposed plans. We validate our framework\nacross diverse robotic manipulation tasks, demonstrating its ability to bridge\nrepresentational gaps and provide robust, generalizable policy steering. Videos\ncan be found on the project website: https://yilin-wu98.github.io/forewarn/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While generative robot policies have demonstrated significant potential in\nlearning complex, multimodal behaviors from demonstrations, they still exhibit\ndiverse failures at deployment-time. Policy steering offers an elegant solution\nto reducing the chance of failure by using an external verifier to select from\nlow-level actions proposed by an imperfect generative policy. Here, one might\nhope to use a Vision Language Model (VLM) as a verifier, leveraging its\nopen-world reasoning capabilities. However, off-the-shelf VLMs struggle to\nunderstand the consequences of low-level robot actions as they are represented\nfundamentally differently than the text and images the VLM was trained on. In\nresponse, we propose FOREWARN, a novel framework to unlock the potential of\nVLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is\nto decouple the VLM's burden of predicting action outcomes (foresight) from\nevaluation (forethought). For foresight, we leverage a latent world model to\nimagine future latent states given diverse low-level action plans. For\nforethought, we align the VLM with these predicted latent states to reason\nabout the consequences of actions in its native representation--natural\nlanguage--and effectively filter proposed plans. We validate our framework\nacross diverse robotic manipulation tasks, demonstrating its ability to bridge\nrepresentational gaps and provide robust, generalizable policy steering. Videos\ncan be found on the project website: https://yilin-wu98.github.io/forewarn/."
                },
                "authors": [
                    {
                        "name": "Yilin Wu"
                    },
                    {
                        "name": "Ran Tian"
                    },
                    {
                        "name": "Gokul Swamy"
                    },
                    {
                        "name": "Andrea Bajcsy"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bajcsy"
                },
                "author": "Andrea Bajcsy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01828v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01828v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06360v3",
                "updated": "2025-05-02T16:58:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    58,
                    10,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-10T04:56:14Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    4,
                    56,
                    14,
                    6,
                    315,
                    0
                ],
                "title": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks"
                },
                "summary": "Despite their tremendous success and versatility, Deep Neural Networks (DNNs)\nsuch as Large Language Models (LLMs) suffer from inference inefficiency and\nrely on advanced computational infrastructure. To address these challenges and\nmake these models more accessible and cost-effective, in this paper, we propose\nalgorithms to improve the inference time and memory efficiency of DNNs with\nbinary and ternary weight matrices. Particularly focusing on matrix\nmultiplication as the bottleneck operation of inference, we observe that, once\ntrained, the weight matrices of a model no longer change. This allows us to\npreprocess these matrices and create indices that help reduce the storage\nrequirements by a logarithmic factor while enabling our efficient inference\nalgorithms. Specifically, for a $n\\times n$ weight matrix, our efficient\nalgorithm guarantees a time complexity of $O(\\frac{n^2}{\\log n})$, a\nlogarithmic factor improvement over the standard vector-matrix multiplication.\nBesides theoretical analysis, we conduct extensive experiments to evaluate the\npractical efficiency of our algorithms. Our results confirm the superiority of\nour approach both with respect to time and memory, as we observed a reduction\nin the multiplication time up to 29x and memory usage up to 6x. When applied to\nLLMs, our experiments show up to a 5.24x speedup in the inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their tremendous success and versatility, Deep Neural Networks (DNNs)\nsuch as Large Language Models (LLMs) suffer from inference inefficiency and\nrely on advanced computational infrastructure. To address these challenges and\nmake these models more accessible and cost-effective, in this paper, we propose\nalgorithms to improve the inference time and memory efficiency of DNNs with\nbinary and ternary weight matrices. Particularly focusing on matrix\nmultiplication as the bottleneck operation of inference, we observe that, once\ntrained, the weight matrices of a model no longer change. This allows us to\npreprocess these matrices and create indices that help reduce the storage\nrequirements by a logarithmic factor while enabling our efficient inference\nalgorithms. Specifically, for a $n\\times n$ weight matrix, our efficient\nalgorithm guarantees a time complexity of $O(\\frac{n^2}{\\log n})$, a\nlogarithmic factor improvement over the standard vector-matrix multiplication.\nBesides theoretical analysis, we conduct extensive experiments to evaluate the\npractical efficiency of our algorithms. Our results confirm the superiority of\nour approach both with respect to time and memory, as we observed a reduction\nin the multiplication time up to 29x and memory usage up to 6x. When applied to\nLLMs, our experiments show up to a 5.24x speedup in the inference time."
                },
                "authors": [
                    {
                        "name": "Mohsen Dehghankar"
                    },
                    {
                        "name": "Mahdi Erfanian"
                    },
                    {
                        "name": "Abolfazl Asudeh"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Asudeh"
                },
                "author": "Abolfazl Asudeh",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00234v2",
                "updated": "2025-05-02T16:44:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    44,
                    2,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-01T00:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    48,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Generated In-Context Examples Improve LLM Agents for Sequential\n  Decision-Making Tasks"
                },
                "summary": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering."
                },
                "authors": [
                    {
                        "name": "Vishnu Sarukkai"
                    },
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    }
                ],
                "author_detail": {
                    "name": "Kayvon Fatahalian"
                },
                "author": "Kayvon Fatahalian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14432v2",
                "updated": "2025-05-02T16:03:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    16,
                    3,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-21T18:59:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks."
                },
                "authors": [
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18789v2",
                "updated": "2025-05-02T15:56:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    56,
                    59,
                    4,
                    122,
                    0
                ],
                "published": "2024-02-29T01:33:08Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    1,
                    33,
                    8,
                    3,
                    60,
                    0
                ],
                "title": "FlexLLM: A System for Co-Serving Large Language Model Inference and\n  Parameter-Efficient Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexLLM: A System for Co-Serving Large Language Model Inference and\n  Parameter-Efficient Finetuning"
                },
                "summary": "Finetuning large language models (LLMs) is essential for task adaptation, yet\nserving stacks today isolate inference and finetuning on separate GPU clusters\n-- wasting resources and under-utilizing hardware. We introduce FlexLLM, the\nfirst system to co-serve LLM inference and PEFT-based finetuning on shared GPUs\nby fusing computation at the token level. The static compilation optimizations\nin FlexLLM -- dependent parallelization and graph pruning significantly shrink\nactivation memory, leading to end-to-end GPU memory savings by up to 80%. At\nruntime, a novel token-level finetuning mechanism paired with a hybrid token\nscheduler dynamically interleaves inference and training tokens within each\nco-serving iteration, meeting strict latency SLOs while maximizing utilization.\nIn end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B,\nFlexLLM sustains the inference SLO requirements up to 20 req/s, and improves\nfinetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x\nunder light loads, preserving over 76% of peak finetuning progress even at peak\ndemand. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models (LLMs) is essential for task adaptation, yet\nserving stacks today isolate inference and finetuning on separate GPU clusters\n-- wasting resources and under-utilizing hardware. We introduce FlexLLM, the\nfirst system to co-serve LLM inference and PEFT-based finetuning on shared GPUs\nby fusing computation at the token level. The static compilation optimizations\nin FlexLLM -- dependent parallelization and graph pruning significantly shrink\nactivation memory, leading to end-to-end GPU memory savings by up to 80%. At\nruntime, a novel token-level finetuning mechanism paired with a hybrid token\nscheduler dynamically interleaves inference and training tokens within each\nco-serving iteration, meeting strict latency SLOs while maximizing utilization.\nIn end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B,\nFlexLLM sustains the inference SLO requirements up to 20 req/s, and improves\nfinetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x\nunder light loads, preserving over 76% of peak finetuning progress even at peak\ndemand. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow/."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xinhao Cheng"
                    },
                    {
                        "name": "Vineeth Kada"
                    },
                    {
                        "name": "Ruohan Gao"
                    },
                    {
                        "name": "Yingyi Huang"
                    },
                    {
                        "name": "Remi Delacourt"
                    },
                    {
                        "name": "April Yang"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Mengdi Wu"
                    },
                    {
                        "name": "Colin Unger"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09632v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09632v5",
                "updated": "2025-05-02T15:34:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    34,
                    42,
                    4,
                    122,
                    0
                ],
                "published": "2024-08-19T01:30:14Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    1,
                    30,
                    14,
                    0,
                    232,
                    0
                ],
                "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDeGPT: Modular Decomposition for Large Language Model Compression"
                },
                "summary": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%."
                },
                "authors": [
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Abhishek Patel"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "ICLR 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09632v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09632v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "15A23 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01339v1",
                "updated": "2025-05-02T15:11:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    11,
                    22,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T15:11:22Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    11,
                    22,
                    4,
                    122,
                    0
                ],
                "title": "Toward Teach and Repeat Across Seasonal Deep Snow Accumulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Teach and Repeat Across Seasonal Deep Snow Accumulation"
                },
                "summary": "Teach and repeat is a rapid way to achieve autonomy in challenging terrain\nand off-road environments. A human operator pilots the vehicles to create a\nnetwork of paths that are mapped and associated with odometry. Immediately\nafter teaching, the system can drive autonomously within its tracks. This\nprecision lets operators remain confident that the robot will follow a\ntraversable route. However, this operational paradigm has rarely been explored\nin off-road environments that change significantly through seasonal variation.\nThis paper presents preliminary field trials using lidar and radar\nimplementations of teach and repeat. Using a subset of the data from the\nupcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,\nand 113 days old. Lidar teach and repeat demonstrated a stronger ability to\nlocalize when the ground points were removed. FMCW radar was often able to\nlocalize on older maps, but only with small deviations from the taught path.\nAdditionally, we highlight specific cases where radar localization failed with\nrecent maps due to the high pitch or roll of the vehicle. We highlight lessons\nlearned during the field deployment and highlight areas to improve to achieve\nreliable teach and repeat with seasonal changes in the environment. Please\nfollow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates\nand information on the data release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teach and repeat is a rapid way to achieve autonomy in challenging terrain\nand off-road environments. A human operator pilots the vehicles to create a\nnetwork of paths that are mapped and associated with odometry. Immediately\nafter teaching, the system can drive autonomously within its tracks. This\nprecision lets operators remain confident that the robot will follow a\ntraversable route. However, this operational paradigm has rarely been explored\nin off-road environments that change significantly through seasonal variation.\nThis paper presents preliminary field trials using lidar and radar\nimplementations of teach and repeat. Using a subset of the data from the\nupcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,\nand 113 days old. Lidar teach and repeat demonstrated a stronger ability to\nlocalize when the ground points were removed. FMCW radar was often able to\nlocalize on older maps, but only with small deviations from the taught path.\nAdditionally, we highlight specific cases where radar localization failed with\nrecent maps due to the high pitch or roll of the vehicle. We highlight lessons\nlearned during the field deployment and highlight areas to improve to achieve\nreliable teach and repeat with seasonal changes in the environment. Please\nfollow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates\nand information on the data release."
                },
                "authors": [
                    {
                        "name": "Matěj Boxan"
                    },
                    {
                        "name": "Alexander Krawciw"
                    },
                    {
                        "name": "Timothy D. Barfoot"
                    },
                    {
                        "name": "François Pomerleau"
                    }
                ],
                "author_detail": {
                    "name": "François Pomerleau"
                },
                "author": "François Pomerleau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01333v1",
                "updated": "2025-05-02T15:06:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    6,
                    47,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T15:06:47Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    6,
                    47,
                    4,
                    122,
                    0
                ],
                "title": "Cramér-Rao Bounds for Integrated Sensing and Communications in\n  Pinching-Antenna Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramér-Rao Bounds for Integrated Sensing and Communications in\n  Pinching-Antenna Systems"
                },
                "summary": "Pinching-antenna systems (PASs) have recently emerged as a flexible,\ncost-effective route to large-scale antenna deployments envisioned for\nintegrated sensing and communications (ISAC). This paper establishes the\nfundamental sensing limits of a bistatic PAS link by deriving closed-form\nCram\\'er-Rao lower bounds for the joint estimation of range and direction when\na target is illuminated by pinching antennas placed along a dielectric\nwaveguide and observed by a uniform linear array receiver. By rigorously\npreserving the amplitude and phase variations of each pinching antenna, as well\nas exploiting their non-uniform deployment, we gain valuable insights into the\nperformance gain of PASs over conventional antenna arrays. Numerical results\nvalidate that the PAS-based ISAC can achieve centimeter-level ranging and\nsub-degree angular resolution with significantly fewer hardware resources than\nconventional uniform linear arrays. The derived bounds provide practical design\nguidelines for next-generation PAS-enabled ISAC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinching-antenna systems (PASs) have recently emerged as a flexible,\ncost-effective route to large-scale antenna deployments envisioned for\nintegrated sensing and communications (ISAC). This paper establishes the\nfundamental sensing limits of a bistatic PAS link by deriving closed-form\nCram\\'er-Rao lower bounds for the joint estimation of range and direction when\na target is illuminated by pinching antennas placed along a dielectric\nwaveguide and observed by a uniform linear array receiver. By rigorously\npreserving the amplitude and phase variations of each pinching antenna, as well\nas exploiting their non-uniform deployment, we gain valuable insights into the\nperformance gain of PASs over conventional antenna arrays. Numerical results\nvalidate that the PAS-based ISAC can achieve centimeter-level ranging and\nsub-degree angular resolution with significantly fewer hardware resources than\nconventional uniform linear arrays. The derived bounds provide practical design\nguidelines for next-generation PAS-enabled ISAC systems."
                },
                "authors": [
                    {
                        "name": "Dimitrios Bozanis"
                    },
                    {
                        "name": "Vasilis K. Papanikolaou"
                    },
                    {
                        "name": "Sotiris A. Tegos"
                    },
                    {
                        "name": "George K. Karagiannidis"
                    }
                ],
                "author_detail": {
                    "name": "George K. Karagiannidis"
                },
                "author": "George K. Karagiannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01284v3",
                "updated": "2025-05-02T15:05:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    5,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2025-03-03T08:12:09Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    12,
                    9,
                    0,
                    62,
                    0
                ],
                "title": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating\n  MobileNetV2 and GraphSAGE with Cross-Modal Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating\n  MobileNetV2 and GraphSAGE with Cross-Modal Attention"
                },
                "summary": "Soybean leaf disease detection is critical for agricultural productivity but\nfaces challenges due to visually similar symptoms and limited interpretability\nin conventional methods. While Convolutional Neural Networks (CNNs) excel in\nspatial feature extraction, they often neglect inter-image relational\ndependencies, leading to misclassifications. This paper proposes an\ninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that\nsynergizes MobileNetV2 for localized feature extraction and GraphSAGE for\nrelational modeling. The framework constructs a graph where nodes represent\nleaf images, with edges defined by cosine similarity-based adjacency matrices\nand adaptive neighborhood sampling. This design captures fine-grained lesion\nfeatures and global symptom patterns, addressing inter-class similarity\nchallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM\nvisualizations, generating heatmaps to highlight disease-influential regions.\nEvaluated on a dataset of ten soybean leaf diseases, the model achieves\n$97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional\nmachine learning models ($\\le77.05\\%$). Ablation studies validate the\nsequential architecture's superiority over parallel or single-model\nconfigurations. With only 2.3 million parameters, the lightweight\nMobileNetV2-GraphSAGE combination ensures computational efficiency, enabling\nreal-time deployment in resource-constrained environments. The proposed\napproach bridges the gap between accurate classification and practical\napplicability, offering a robust, interpretable tool for agricultural\ndiagnostics while advancing CNN-GNN integration in plant pathology research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soybean leaf disease detection is critical for agricultural productivity but\nfaces challenges due to visually similar symptoms and limited interpretability\nin conventional methods. While Convolutional Neural Networks (CNNs) excel in\nspatial feature extraction, they often neglect inter-image relational\ndependencies, leading to misclassifications. This paper proposes an\ninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that\nsynergizes MobileNetV2 for localized feature extraction and GraphSAGE for\nrelational modeling. The framework constructs a graph where nodes represent\nleaf images, with edges defined by cosine similarity-based adjacency matrices\nand adaptive neighborhood sampling. This design captures fine-grained lesion\nfeatures and global symptom patterns, addressing inter-class similarity\nchallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM\nvisualizations, generating heatmaps to highlight disease-influential regions.\nEvaluated on a dataset of ten soybean leaf diseases, the model achieves\n$97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional\nmachine learning models ($\\le77.05\\%$). Ablation studies validate the\nsequential architecture's superiority over parallel or single-model\nconfigurations. With only 2.3 million parameters, the lightweight\nMobileNetV2-GraphSAGE combination ensures computational efficiency, enabling\nreal-time deployment in resource-constrained environments. The proposed\napproach bridges the gap between accurate classification and practical\napplicability, offering a robust, interpretable tool for agricultural\ndiagnostics while advancing CNN-GNN integration in plant pathology research."
                },
                "authors": [
                    {
                        "name": "Md Abrar Jahin"
                    },
                    {
                        "name": "Soudeep Shahriar"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Md. Jakir Hossen"
                    },
                    {
                        "name": "Nilanjan Dey"
                    }
                ],
                "author_detail": {
                    "name": "Nilanjan Dey"
                },
                "author": "Nilanjan Dey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01328v1",
                "updated": "2025-05-02T15:01:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    1,
                    42,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T15:01:42Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    1,
                    42,
                    4,
                    122,
                    0
                ],
                "title": "Constrained Network Adversarial Attacks: Validity, Robustness, and\n  Transferability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Network Adversarial Attacks: Validity, Robustness, and\n  Transferability"
                },
                "summary": "While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications."
                },
                "authors": [
                    {
                        "name": "Anass Grini"
                    },
                    {
                        "name": "Oumaima Taheri"
                    },
                    {
                        "name": "Btissam El Khamlichi"
                    },
                    {
                        "name": "Amal El Fallah-Seghrouchni"
                    }
                ],
                "author_detail": {
                    "name": "Amal El Fallah-Seghrouchni"
                },
                "author": "Amal El Fallah-Seghrouchni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01325v1",
                "updated": "2025-05-02T14:56:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    56,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:56:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    56,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague,\n  Implicit and Explicit References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague,\n  Implicit and Explicit References"
                },
                "summary": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER"
                },
                "authors": [
                    {
                        "name": "Svenja Kenneweg"
                    },
                    {
                        "name": "Jörg Deigmöller"
                    },
                    {
                        "name": "Philipp Cimiano"
                    },
                    {
                        "name": "Julian Eggert"
                    }
                ],
                "author_detail": {
                    "name": "Julian Eggert"
                },
                "author": "Julian Eggert",
                "arxiv_comment": "24 pages, 6 figures, submitted to Springer Nature Computer Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01315v1",
                "updated": "2025-05-02T14:42:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    42,
                    26,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:42:26Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    42,
                    26,
                    4,
                    122,
                    0
                ],
                "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping Big Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System"
                },
                "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."
                },
                "authors": [
                    {
                        "name": "Sheikh Samit Muhaimin"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01307v1",
                "updated": "2025-05-02T14:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    34,
                    33,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:34:33Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    34,
                    33,
                    4,
                    122,
                    0
                ],
                "title": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical\n  software assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical\n  software assessments"
                },
                "summary": "Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains."
                },
                "authors": [
                    {
                        "name": "Regan Bolton"
                    },
                    {
                        "name": "Mohammadreza Sheikhfathollahi"
                    },
                    {
                        "name": "Simon Parkinson"
                    },
                    {
                        "name": "Vanessa Vulovic"
                    },
                    {
                        "name": "Gary Bamford"
                    },
                    {
                        "name": "Dan Basher"
                    },
                    {
                        "name": "Howard Parkinson"
                    }
                ],
                "author_detail": {
                    "name": "Howard Parkinson"
                },
                "author": "Howard Parkinson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01295v1",
                "updated": "2025-05-02T14:13:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    13,
                    59,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T14:13:59Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    13,
                    59,
                    4,
                    122,
                    0
                ],
                "title": "Network-Level ISAC Design: State-of-the-Art, Challenges, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-Level ISAC Design: State-of-the-Art, Challenges, and\n  Opportunities"
                },
                "summary": "The ultimate goal of integrated sensing and communication (ISAC) deployment\nis to provide coordinated sensing and communication services at an\nunprecedented scale. This paper presents a comprehensive overview of\nnetwork-level ISAC systems, an emerging paradigm that significantly extends the\ncapabilities of link-level ISAC through distributed cooperation. We first\nexamine recent advancements in network-level ISAC architectures, emphasizing\nvarious cooperation schemes and distributed system designs. The sensing and\ncommunication (S\\&C) performance is analyzed with respect to interference\nmanagement and cooperative S\\&C, offering new insights into the design\nprinciples necessary for large-scale networked ISAC deployments. In addition,\ndistributed signaling strategies across different levels of cooperation are\nreviewed, focusing on key performance metrics such as sensing accuracy and\ncommunication quality-of-service (QoS). Next, we explore the key challenges for\npractical deployment where the critical role of synchronization is also\ndiscussed, highlighting advanced over-the-air synchronization techniques\nspecifically tailored for bi-static and distributed ISAC systems. Finally, open\nchallenges and future research directions in network-level ISAC design are\nidentified. The findings and discussions aim to serve as a foundational\nguideline for advancing scalable, high-performance, and resilient distributed\nISAC systems in next-generation wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ultimate goal of integrated sensing and communication (ISAC) deployment\nis to provide coordinated sensing and communication services at an\nunprecedented scale. This paper presents a comprehensive overview of\nnetwork-level ISAC systems, an emerging paradigm that significantly extends the\ncapabilities of link-level ISAC through distributed cooperation. We first\nexamine recent advancements in network-level ISAC architectures, emphasizing\nvarious cooperation schemes and distributed system designs. The sensing and\ncommunication (S\\&C) performance is analyzed with respect to interference\nmanagement and cooperative S\\&C, offering new insights into the design\nprinciples necessary for large-scale networked ISAC deployments. In addition,\ndistributed signaling strategies across different levels of cooperation are\nreviewed, focusing on key performance metrics such as sensing accuracy and\ncommunication quality-of-service (QoS). Next, we explore the key challenges for\npractical deployment where the critical role of synchronization is also\ndiscussed, highlighting advanced over-the-air synchronization techniques\nspecifically tailored for bi-static and distributed ISAC systems. Finally, open\nchallenges and future research directions in network-level ISAC design are\nidentified. The findings and discussions aim to serve as a foundational\nguideline for advancing scalable, high-performance, and resilient distributed\nISAC systems in next-generation wireless networks."
                },
                "authors": [
                    {
                        "name": "Kawon Han"
                    },
                    {
                        "name": "Kaitao Meng"
                    },
                    {
                        "name": "Xiao-Yang Wang"
                    },
                    {
                        "name": "Christos Masouros"
                    }
                ],
                "author_detail": {
                    "name": "Christos Masouros"
                },
                "author": "Christos Masouros",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01263v1",
                "updated": "2025-05-02T13:30:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    30,
                    19,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:30:19Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    30,
                    19,
                    4,
                    122,
                    0
                ],
                "title": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and\n  Flow Matching based Voice Enhancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and\n  Flow Matching based Voice Enhancing"
                },
                "summary": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}."
                },
                "authors": [
                    {
                        "name": "Gaoxiang Cong"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Jiadong Pan"
                    },
                    {
                        "name": "Zhedong Zhang"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Anton van den Hengel"
                    },
                    {
                        "name": "Yuankai Qi"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01259v1",
                "updated": "2025-05-02T13:26:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    26,
                    47,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:26:47Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    26,
                    47,
                    4,
                    122,
                    0
                ],
                "title": "Digital Pathway Curation (DPC): a comparative pipeline to assess the\n  reproducibility, consensus and accuracy across Gemini, PubMed, and scientific\n  reviewers in biomedical research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Pathway Curation (DPC): a comparative pipeline to assess the\n  reproducibility, consensus and accuracy across Gemini, PubMed, and scientific\n  reviewers in biomedical research"
                },
                "summary": "A scientific study begins with a central question, and search engines like\nPubMed are the first tools for retrieving knowledge and understanding the\ncurrent state of the art. Large Language Models (LLMs) have been used in\nresearch, promising acceleration and deeper results. However, besides caution,\nthey demand rigorous validation. Assessing complex biological relationships\nremains challenging for SQL-based tools and LLM models. Here, we introduce the\nDigital Pathway Curation (DPC) pipeline to evaluate the reproducibility and\naccuracy of the Gemini models against PubMed search and human expert curation.\nUsing two omics experiments, we created a large dataset (Ensemble) based on\ndetermining pathway-disease associations. With the Ensemble dataset, we\ndemonstrate that Gemini achieves high run-to-run reproducibility of\napproximately 99% and inter-model reproducibility of around 75%. Next, we\ncalculate the crowdsourced consensus using a smaller dataset. The CSC allows us\nto calculate accuracies, and the Gemini multi-model consensus reached a\nsignificant accuracy of about 87%. Our findings demonstrate that LLMs are\nreproducible, reliable, and valuable tools for navigating complex biomedical\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scientific study begins with a central question, and search engines like\nPubMed are the first tools for retrieving knowledge and understanding the\ncurrent state of the art. Large Language Models (LLMs) have been used in\nresearch, promising acceleration and deeper results. However, besides caution,\nthey demand rigorous validation. Assessing complex biological relationships\nremains challenging for SQL-based tools and LLM models. Here, we introduce the\nDigital Pathway Curation (DPC) pipeline to evaluate the reproducibility and\naccuracy of the Gemini models against PubMed search and human expert curation.\nUsing two omics experiments, we created a large dataset (Ensemble) based on\ndetermining pathway-disease associations. With the Ensemble dataset, we\ndemonstrate that Gemini achieves high run-to-run reproducibility of\napproximately 99% and inter-model reproducibility of around 75%. Next, we\ncalculate the crowdsourced consensus using a smaller dataset. The CSC allows us\nto calculate accuracies, and the Gemini multi-model consensus reached a\nsignificant accuracy of about 87%. Our findings demonstrate that LLMs are\nreproducible, reliable, and valuable tools for navigating complex biomedical\nknowledge."
                },
                "authors": [
                    {
                        "name": "Flavio Lichtenstein"
                    },
                    {
                        "name": "Daniel Alexandre de Souza"
                    },
                    {
                        "name": "Carlos Eduardo Madureira Trufen"
                    },
                    {
                        "name": "Victor Wendel da Silva Gonçalves"
                    },
                    {
                        "name": "Juliana de Paula Bernardes"
                    },
                    {
                        "name": "Vinicius Miranda Baroni"
                    },
                    {
                        "name": "Carlos DeOcesano-Pereira"
                    },
                    {
                        "name": "Leonardo Fontoura Ormundo"
                    },
                    {
                        "name": "Fabio Augusto Labre de Souza"
                    },
                    {
                        "name": "Olga Celia Martinez Ibañez"
                    },
                    {
                        "name": "Nancy Starobinas"
                    },
                    {
                        "name": "Luciano Rodrigo Lopes"
                    },
                    {
                        "name": "Aparecida Maria Fontes"
                    },
                    {
                        "name": "Sonia Aparecida de Andrade"
                    },
                    {
                        "name": "Ana Marisa Chudzinski-Tavassi"
                    }
                ],
                "author_detail": {
                    "name": "Ana Marisa Chudzinski-Tavassi"
                },
                "author": "Ana Marisa Chudzinski-Tavassi",
                "arxiv_comment": "Main article: 7 figures and 10 tables. Supplementary information: 22\n  figures and 59 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01238v1",
                "updated": "2025-05-02T13:00:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    0,
                    5,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T13:00:05Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    0,
                    5,
                    4,
                    122,
                    0
                ],
                "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods\n  on NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods\n  on NLP Models"
                },
                "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP."
                },
                "authors": [
                    {
                        "name": "Mahdi Dhaini"
                    },
                    {
                        "name": "Kafaite Zahra Hussain"
                    },
                    {
                        "name": "Efstratios Zaradoukas"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to the xAI World Conference (2025) - System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15181v2",
                "updated": "2025-05-02T12:35:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    12,
                    35,
                    19,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-25T11:44:24Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    11,
                    44,
                    24,
                    5,
                    25,
                    0
                ],
                "title": "From Bugs to Benefits: Improving User Stories by Leveraging Crowd\n  Knowledge with CrUISE-AC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Bugs to Benefits: Improving User Stories by Leveraging Crowd\n  Knowledge with CrUISE-AC"
                },
                "summary": "Costs for resolving software defects increase exponentially in late stages.\nIncomplete or ambiguous requirements are one of the biggest sources for\ndefects, since stakeholders might not be able to communicate their needs or\nfail to share their domain specific knowledge. Combined with insufficient\ndeveloper experience, teams are prone to constructing incorrect or incomplete\nfeatures. To prevent this, requirements engineering has to explore knowledge\nsources beyond stakeholder interviews. Publicly accessible issue trackers for\nsystems within the same application domain hold essential information on\nidentified weaknesses, edge cases, and potential error sources, all documented\nby actual users. Our research aims at (1) identifying, and (2) leveraging such\nissues to improve an agile requirements artifact known as a \"user story\". We\npresent CrUISE-AC (Crowd and User Informed Suggestion Engine for Acceptance\nCriteria) as a fully automated method that investigates issues and generates\nnon-trivial additional acceptance criteria for a given user story by employing\nNLP techniques and an ensemble of LLMs. CrUISE- AC was evaluated by five\nindependent experts in two distinct business domains. Our findings suggest that\nissue trackers hold valuable information pertinent to requirements engineering.\nOur evaluation shows that 80-82% of the generated acceptance criteria add\nrelevant requirements to the user stories. Limitations are the dependence on\naccessible input issues and the fact that we do not check generated criteria\nfor being conflict-free or non-overlapping with criteria from other user\nstories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Costs for resolving software defects increase exponentially in late stages.\nIncomplete or ambiguous requirements are one of the biggest sources for\ndefects, since stakeholders might not be able to communicate their needs or\nfail to share their domain specific knowledge. Combined with insufficient\ndeveloper experience, teams are prone to constructing incorrect or incomplete\nfeatures. To prevent this, requirements engineering has to explore knowledge\nsources beyond stakeholder interviews. Publicly accessible issue trackers for\nsystems within the same application domain hold essential information on\nidentified weaknesses, edge cases, and potential error sources, all documented\nby actual users. Our research aims at (1) identifying, and (2) leveraging such\nissues to improve an agile requirements artifact known as a \"user story\". We\npresent CrUISE-AC (Crowd and User Informed Suggestion Engine for Acceptance\nCriteria) as a fully automated method that investigates issues and generates\nnon-trivial additional acceptance criteria for a given user story by employing\nNLP techniques and an ensemble of LLMs. CrUISE- AC was evaluated by five\nindependent experts in two distinct business domains. Our findings suggest that\nissue trackers hold valuable information pertinent to requirements engineering.\nOur evaluation shows that 80-82% of the generated acceptance criteria add\nrelevant requirements to the user stories. Limitations are the dependence on\naccessible input issues and the fact that we do not check generated criteria\nfor being conflict-free or non-overlapping with criteria from other user\nstories."
                },
                "authors": [
                    {
                        "name": "Stefan Schwedt"
                    },
                    {
                        "name": "Thomas Ströder"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Ströder"
                },
                "author": "Thomas Ströder",
                "arxiv_doi": "10.1109/ICSE55347.2025.00217",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICSE55347.2025.00217",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 2025 IEEE/ACM International Conference on Software\n  Engineering (ICSE). Final version at\n  https://doi.org/10.1109/ICSE55347.2025.00217",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01222v1",
                "updated": "2025-05-02T12:18:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    12,
                    18,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T12:18:21Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    12,
                    18,
                    21,
                    4,
                    122,
                    0
                ],
                "title": "Performance of Cell-Free Massive MIMO in Realistic Urban Propagation\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Cell-Free Massive MIMO in Realistic Urban Propagation\n  Environments"
                },
                "summary": "While UE-centric cell-free massive MIMO (CF-mMIMO) provides high and uniform\nthroughput performance under the assumption of a uniform propagation\nenvironment modeled by the log-distance path loss channel model, the\nperformance under a realistic urban propagation environment is not yet fully\naddressed. In this paper we conduct the first comparative performance study of\nCF-mMIMO under both the widely assumed log-distance channel model and the\nrealistic urban propagation environment obtained via raytracing using real 3D\ncity layouts and practical AP locations. Our results show that with the\nraytracing channel model, CF-mMIMO cannot achieve as high and uniform\nthroughput performance as observed with the log-distance channel model, putting\ninto question the attractiveness in practice of CF-mMIMO for real urban\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While UE-centric cell-free massive MIMO (CF-mMIMO) provides high and uniform\nthroughput performance under the assumption of a uniform propagation\nenvironment modeled by the log-distance path loss channel model, the\nperformance under a realistic urban propagation environment is not yet fully\naddressed. In this paper we conduct the first comparative performance study of\nCF-mMIMO under both the widely assumed log-distance channel model and the\nrealistic urban propagation environment obtained via raytracing using real 3D\ncity layouts and practical AP locations. Our results show that with the\nraytracing channel model, CF-mMIMO cannot achieve as high and uniform\nthroughput performance as observed with the log-distance channel model, putting\ninto question the attractiveness in practice of CF-mMIMO for real urban\ndeployments."
                },
                "authors": [
                    {
                        "name": "Yunlu Xiao"
                    },
                    {
                        "name": "Ljiljana Simić"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Simić"
                },
                "author": "Ljiljana Simić",
                "arxiv_comment": "This paper is accepted to be published in IEEE WCNC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01203v1",
                "updated": "2025-05-02T11:48:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    48,
                    11,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T11:48:11Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    48,
                    11,
                    4,
                    122,
                    0
                ],
                "title": "Efficient Vision-based Vehicle Speed Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vision-based Vehicle Speed Estimation"
                },
                "summary": "This paper presents a computationally efficient method for vehicle speed\nestimation from traffic camera footage. Building upon previous work that\nutilizes 3D bounding boxes derived from 2D detections and vanishing point\ngeometry, we introduce several improvements to enhance real-time performance.\nWe evaluate our method in several variants on the BrnoCompSpeed dataset in\nterms of vehicle detection and speed estimation accuracy. Our extensive\nevaluation across various hardware platforms, including edge devices,\ndemonstrates significant gains in frames per second (FPS) compared to the prior\nstate-of-the-art, while maintaining comparable or improved speed estimation\naccuracy. We analyze the trade-off between accuracy and computational cost,\nshowing that smaller models utilizing post-training quantization offer the best\nbalance for real-world deployment. Our best performing model beats previous\nstate-of-the-art in terms of median vehicle speed estimation error (0.58 km/h\nvs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.\n83.32%) while also being 5.5 times faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a computationally efficient method for vehicle speed\nestimation from traffic camera footage. Building upon previous work that\nutilizes 3D bounding boxes derived from 2D detections and vanishing point\ngeometry, we introduce several improvements to enhance real-time performance.\nWe evaluate our method in several variants on the BrnoCompSpeed dataset in\nterms of vehicle detection and speed estimation accuracy. Our extensive\nevaluation across various hardware platforms, including edge devices,\ndemonstrates significant gains in frames per second (FPS) compared to the prior\nstate-of-the-art, while maintaining comparable or improved speed estimation\naccuracy. We analyze the trade-off between accuracy and computational cost,\nshowing that smaller models utilizing post-training quantization offer the best\nbalance for real-world deployment. Our best performing model beats previous\nstate-of-the-art in terms of median vehicle speed estimation error (0.58 km/h\nvs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.\n83.32%) while also being 5.5 times faster."
                },
                "authors": [
                    {
                        "name": "Andrej Macko"
                    },
                    {
                        "name": "Lukáš Gajdošech"
                    },
                    {
                        "name": "Viktor Kocur"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Kocur"
                },
                "author": "Viktor Kocur",
                "arxiv_comment": "Submitted to Journal of Real-Time Image Processing (JRTIP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00016v2",
                "updated": "2025-05-02T11:34:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    34,
                    0,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-23T19:02:04Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    19,
                    2,
                    4,
                    2,
                    113,
                    0
                ],
                "title": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning"
                },
                "summary": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a relative 33.9\\% increase in accuracy when trained on\nText-to-SQL tasks, while Qwen achieved a relative 14.5\\% increase. These\nresults suggest that SQL can serve not only as a target formalism but also as\nan effective scaffold for learning robust, transferable reasoning over\nstructured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a relative 33.9\\% increase in accuracy when trained on\nText-to-SQL tasks, while Qwen achieved a relative 14.5\\% increase. These\nresults suggest that SQL can serve not only as a target formalism but also as\nan effective scaffold for learning robust, transferable reasoning over\nstructured data."
                },
                "authors": [
                    {
                        "name": "Josefa Lia Stoisser"
                    },
                    {
                        "name": "Marc Boubnovski Martell"
                    },
                    {
                        "name": "Julien Fauqueur"
                    }
                ],
                "author_detail": {
                    "name": "Julien Fauqueur"
                },
                "author": "Julien Fauqueur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01185v1",
                "updated": "2025-05-02T11:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    0,
                    40,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T11:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    0,
                    40,
                    4,
                    122,
                    0
                ],
                "title": "EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an\n  Environmental-Aware Path Loss and Adaptive RSSI Smoothing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an\n  Environmental-Aware Path Loss and Adaptive RSSI Smoothing"
                },
                "summary": "LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments."
                },
                "authors": [
                    {
                        "name": "Nahshon Mokua Obiri"
                    },
                    {
                        "name": "Kristof Van Laerhoven"
                    }
                ],
                "author_detail": {
                    "name": "Kristof Van Laerhoven"
                },
                "author": "Kristof Van Laerhoven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09830v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09830v4",
                "updated": "2025-05-02T10:41:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    41,
                    18,
                    4,
                    122,
                    0
                ],
                "published": "2023-11-16T11:55:27Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    55,
                    27,
                    3,
                    320,
                    0
                ],
                "title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning"
                },
                "summary": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut."
                },
                "authors": [
                    {
                        "name": "Katharina Stein"
                    },
                    {
                        "name": "Daniel Fišer"
                    },
                    {
                        "name": "Jörg Hoffmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "Extended version of the paper from the ICAPS'25 proceedings (same\n  main part + additional appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09830v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09830v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01177v1",
                "updated": "2025-05-02T10:35:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    35,
                    26,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:35:26Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    35,
                    26,
                    4,
                    122,
                    0
                ],
                "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures"
                },
                "summary": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges."
                },
                "authors": [
                    {
                        "name": "Francisco Aguilera-Martínez"
                    },
                    {
                        "name": "Fernando Berzal"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Berzal"
                },
                "author": "Fernando Berzal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09762v2",
                "updated": "2025-05-02T10:33:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    33,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-02-13T20:45:48Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    20,
                    45,
                    48,
                    3,
                    44,
                    0
                ],
                "title": "AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit"
                },
                "summary": "Adaptive teaming-the capability of agents to effectively collaborate with\nunfamiliar teammates without prior coordination-is widely explored in virtual\nvideo games but overlooked in real-world multi-robot contexts. Yet, such\nadaptive collaboration is crucial for real-world applications, including border\nsurveillance, search-and-rescue, and counter-terrorism operations. To address\nthis gap, we introduce AT-Drone, the first dedicated benchmark explicitly\ndesigned to facilitate comprehensive training and evaluation of adaptive\nteaming strategies in multi-drone pursuit scenarios. AT-Drone makes the\nfollowing key contributions: (1) An adaptable simulation environment\nconfigurator that enables intuitive and rapid setup of adaptive teaming\nmulti-drone pursuit tasks, including four predefined pursuit environments. (2)\nA streamlined real-world deployment pipeline that seamlessly translates\nsimulation insights into practical drone evaluations using edge devices and\nCrazyflie drones. (3) A novel algorithm zoo integrated with a distributed\ntraining framework, featuring diverse algorithms explicitly tailored, for the\nfirst time, to multi-pursuer and multi-evader settings. (4) Standardized\nevaluation protocols with newly designed unseen drone zoos, explicitly designed\nto rigorously assess the performance of adaptive teaming. Comprehensive\nexperimental evaluations across four progressively challenging multi-drone\npursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive\nteaming research. Real-world drone experiments further validate its practical\nfeasibility and utility for realistic robotic operations. Videos, code and\nweights are available at \\url{https://sites.google.com/view/at-drone}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive teaming-the capability of agents to effectively collaborate with\nunfamiliar teammates without prior coordination-is widely explored in virtual\nvideo games but overlooked in real-world multi-robot contexts. Yet, such\nadaptive collaboration is crucial for real-world applications, including border\nsurveillance, search-and-rescue, and counter-terrorism operations. To address\nthis gap, we introduce AT-Drone, the first dedicated benchmark explicitly\ndesigned to facilitate comprehensive training and evaluation of adaptive\nteaming strategies in multi-drone pursuit scenarios. AT-Drone makes the\nfollowing key contributions: (1) An adaptable simulation environment\nconfigurator that enables intuitive and rapid setup of adaptive teaming\nmulti-drone pursuit tasks, including four predefined pursuit environments. (2)\nA streamlined real-world deployment pipeline that seamlessly translates\nsimulation insights into practical drone evaluations using edge devices and\nCrazyflie drones. (3) A novel algorithm zoo integrated with a distributed\ntraining framework, featuring diverse algorithms explicitly tailored, for the\nfirst time, to multi-pursuer and multi-evader settings. (4) Standardized\nevaluation protocols with newly designed unseen drone zoos, explicitly designed\nto rigorously assess the performance of adaptive teaming. Comprehensive\nexperimental evaluations across four progressively challenging multi-drone\npursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive\nteaming research. Real-world drone experiments further validate its practical\nfeasibility and utility for realistic robotic operations. Videos, code and\nweights are available at \\url{https://sites.google.com/view/at-drone}."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Junfan Chen"
                    },
                    {
                        "name": "Feng Xue"
                    },
                    {
                        "name": "Jiabin Qiu"
                    },
                    {
                        "name": "Wenbin Li"
                    },
                    {
                        "name": "Qingrui Zhang"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Wei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Wei Pan"
                },
                "author": "Wei Pan",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01162v1",
                "updated": "2025-05-02T10:08:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    8,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:08:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    8,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "On the Limitations of Steering in Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limitations of Steering in Language Model Alignment"
                },
                "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models."
                },
                "authors": [
                    {
                        "name": "Chebrolu Niranjan"
                    },
                    {
                        "name": "Kokil Jaidka"
                    },
                    {
                        "name": "Gerard Christopher Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Gerard Christopher Yeo"
                },
                "author": "Gerard Christopher Yeo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04744v2",
                "updated": "2025-05-02T09:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    58,
                    28,
                    4,
                    122,
                    0
                ],
                "published": "2024-09-07T07:40:43Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    40,
                    43,
                    5,
                    251,
                    0
                ],
                "title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language\n  Models: The LMGT Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language\n  Models: The LMGT Framework"
                },
                "summary": "The inherent uncertainty in the environmental transition model of\nReinforcement Learning (RL) necessitates a delicate balance between exploration\nand exploitation. This balance is crucial for optimizing computational\nresources to accurately estimate expected rewards for the agent. In scenarios\nwith sparse rewards, such as robotic control systems, achieving this balance is\nparticularly challenging. However, given that many environments possess\nextensive prior knowledge, learning from the ground up in such contexts may be\nredundant. To address this issue, we propose Language Model Guided reward\nTuning (LMGT), a novel, sample-efficient framework. LMGT leverages the\ncomprehensive prior knowledge embedded in Large Language Models (LLMs) and\ntheir proficiency in processing non-standard data forms, such as wiki\ntutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances\nexploration and exploitation, thereby guiding the agent's exploratory behavior\nand enhancing sample efficiency. We have rigorously evaluated LMGT across\nvarious RL tasks and evaluated it in the embodied robotic environment\nHousekeep. Our results demonstrate that LMGT consistently outperforms baseline\nmethods. Furthermore, the findings suggest that our framework can substantially\nreduce the computational resources required during the RL training phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent uncertainty in the environmental transition model of\nReinforcement Learning (RL) necessitates a delicate balance between exploration\nand exploitation. This balance is crucial for optimizing computational\nresources to accurately estimate expected rewards for the agent. In scenarios\nwith sparse rewards, such as robotic control systems, achieving this balance is\nparticularly challenging. However, given that many environments possess\nextensive prior knowledge, learning from the ground up in such contexts may be\nredundant. To address this issue, we propose Language Model Guided reward\nTuning (LMGT), a novel, sample-efficient framework. LMGT leverages the\ncomprehensive prior knowledge embedded in Large Language Models (LLMs) and\ntheir proficiency in processing non-standard data forms, such as wiki\ntutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances\nexploration and exploitation, thereby guiding the agent's exploratory behavior\nand enhancing sample efficiency. We have rigorously evaluated LMGT across\nvarious RL tasks and evaluated it in the embodied robotic environment\nHousekeep. Our results demonstrate that LMGT consistently outperforms baseline\nmethods. Furthermore, the findings suggest that our framework can substantially\nreduce the computational resources required during the RL training phase."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Tan"
                },
                "author": "Xiaoyu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01150v1",
                "updated": "2025-05-02T09:50:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    50,
                    34,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:50:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    50,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Methodological Foundations for AI-Driven Survey Question Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methodological Foundations for AI-Driven Survey Question Generation"
                },
                "summary": "This paper presents a methodological framework for using generative AI in\neducational survey research. We explore how Large Language Models (LLMs) can\ngenerate adaptive, context-aware survey questions and introduce the Synthetic\nQuestion-Response Analysis (SQRA) framework, which enables iterative testing\nand refinement of AI-generated prompts prior to deployment with human\nparticipants. Guided by Activity Theory, we analyze how AI tools mediate\nparticipant engagement and learning, and we examine ethical issues such as\nbias, privacy, and transparency. Through sentiment, lexical, and structural\nanalyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the\nalignment and effectiveness of these questions. Our findings highlight the\npromise and limitations of AI-driven survey instruments, emphasizing the need\nfor robust prompt engineering and validation to support trustworthy, scalable,\nand contextually relevant data collection in engineering education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a methodological framework for using generative AI in\neducational survey research. We explore how Large Language Models (LLMs) can\ngenerate adaptive, context-aware survey questions and introduce the Synthetic\nQuestion-Response Analysis (SQRA) framework, which enables iterative testing\nand refinement of AI-generated prompts prior to deployment with human\nparticipants. Guided by Activity Theory, we analyze how AI tools mediate\nparticipant engagement and learning, and we examine ethical issues such as\nbias, privacy, and transparency. Through sentiment, lexical, and structural\nanalyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the\nalignment and effectiveness of these questions. Our findings highlight the\npromise and limitations of AI-driven survey instruments, emphasizing the need\nfor robust prompt engineering and validation to support trustworthy, scalable,\nand contextually relevant data collection in engineering education."
                },
                "authors": [
                    {
                        "name": "Ted K. Mburu"
                    },
                    {
                        "name": "Kangxuan Rong"
                    },
                    {
                        "name": "Campbell J. McColley"
                    },
                    {
                        "name": "Alexandra Werth"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Werth"
                },
                "author": "Alexandra Werth",
                "arxiv_comment": "32 pages, 6 figures. Accepted for publication in the Journal of\n  Engineering Education (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01146v1",
                "updated": "2025-05-02T09:44:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    44,
                    51,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:44:51Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    44,
                    51,
                    4,
                    122,
                    0
                ],
                "title": "Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies,\n  Datasets, and Clinical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies,\n  Datasets, and Clinical Applications"
                },
                "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks. However, their application\nin the biomedical domain presents unique challenges, particularly regarding\nfactual accuracy and up-to-date knowledge integration. Retrieval Augmented\nGeneration (RAG) has emerged as a promising solution to address these\nchallenges by combining the generative capabilities of LLMs with external\nknowledge retrieval. This comprehensive survey examines the application of RAG\nin the biomedical domain, focusing on its technological components, available\ndatasets, and clinical applications. We present a systematic analysis of\nretrieval methods, ranking strategies, and generation models, while also\nexploring the challenges and future directions in this rapidly evolving field.\nOur work provides researchers and practitioners with a thorough understanding\nof the current state of biomedical RAG systems and identifies key areas for\nfuture research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks. However, their application\nin the biomedical domain presents unique challenges, particularly regarding\nfactual accuracy and up-to-date knowledge integration. Retrieval Augmented\nGeneration (RAG) has emerged as a promising solution to address these\nchallenges by combining the generative capabilities of LLMs with external\nknowledge retrieval. This comprehensive survey examines the application of RAG\nin the biomedical domain, focusing on its technological components, available\ndatasets, and clinical applications. We present a systematic analysis of\nretrieval methods, ranking strategies, and generation models, while also\nexploring the challenges and future directions in this rapidly evolving field.\nOur work provides researchers and practitioners with a thorough understanding\nof the current state of biomedical RAG systems and identifies key areas for\nfuture research and development."
                },
                "authors": [
                    {
                        "name": "Jiawei He"
                    },
                    {
                        "name": "Boya Zhang"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Xudong Chen"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Douglas Teodoro"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Teodoro"
                },
                "author": "Douglas Teodoro",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01139v1",
                "updated": "2025-05-02T09:31:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    31,
                    14,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:31:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    31,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Active Sybil Attack and Efficient Defense Strategy in IPFS DHT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Sybil Attack and Efficient Defense Strategy in IPFS DHT"
                },
                "summary": "The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P)\nstorage that relies on Kademlia, a Distributed Hash Table (DHT) structure\ncommonly used in P2P systems for its proved scalability. However, DHTs are\nknown to be vulnerable to Sybil attacks, in which a single entity controls\nmultiple malicious nodes. Recent studies have shown that IPFS is affected by a\npassive content eclipse attack, leveraging Sybils, in which adversarial nodes\nhide received indexed information from other peers, making the content appear\nunavailable. Fortunately, the latest mitigation strategy coupling an attack\ndetection based on statistical tests and a wider publication strategy upon\ndetection was able to circumvent it.\n  In this work, we present a new active attack, with malicious nodes responding\nwith semantically correct but intentionally false data, exploiting both an\noptimized placement of Sybils to stay below the detection threshold and an\nearly trigger of the content discovery termination in Kubo, the main IPFS\nimplementation. Our attack achieves to completely eclipse content on the latest\nKubo release. When evaluated against the most recent known mitigation, it\nsuccessfully denies access to the target content in approximately 80\\% of\nlookup attempts.\n  To address this vulnerability, we propose a new mitigation called\nSR-DHT-Store, which enables efficient, Sybil-resistant content publication\nwithout relying on attack detection but instead on a systematic and precise use\nof region-based queries, defined by a dynamically computed XOR distance to the\ntarget ID. SR-DHT-Store can be combined with other defense mechanisms resulting\nin a defense strategy that completely mitigates both passive and active Sybil\nattacks at a lower overhead, while allowing an incremental deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P)\nstorage that relies on Kademlia, a Distributed Hash Table (DHT) structure\ncommonly used in P2P systems for its proved scalability. However, DHTs are\nknown to be vulnerable to Sybil attacks, in which a single entity controls\nmultiple malicious nodes. Recent studies have shown that IPFS is affected by a\npassive content eclipse attack, leveraging Sybils, in which adversarial nodes\nhide received indexed information from other peers, making the content appear\nunavailable. Fortunately, the latest mitigation strategy coupling an attack\ndetection based on statistical tests and a wider publication strategy upon\ndetection was able to circumvent it.\n  In this work, we present a new active attack, with malicious nodes responding\nwith semantically correct but intentionally false data, exploiting both an\noptimized placement of Sybils to stay below the detection threshold and an\nearly trigger of the content discovery termination in Kubo, the main IPFS\nimplementation. Our attack achieves to completely eclipse content on the latest\nKubo release. When evaluated against the most recent known mitigation, it\nsuccessfully denies access to the target content in approximately 80\\% of\nlookup attempts.\n  To address this vulnerability, we propose a new mitigation called\nSR-DHT-Store, which enables efficient, Sybil-resistant content publication\nwithout relying on attack detection but instead on a systematic and precise use\nof region-based queries, defined by a dynamically computed XOR distance to the\ntarget ID. SR-DHT-Store can be combined with other defense mechanisms resulting\nin a defense strategy that completely mitigates both passive and active Sybil\nattacks at a lower overhead, while allowing an incremental deployment."
                },
                "authors": [
                    {
                        "name": "V. H. M. Netto"
                    },
                    {
                        "name": "T. Cholez"
                    },
                    {
                        "name": "C. L. Ignat"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Ignat"
                },
                "author": "C. L. Ignat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01130v1",
                "updated": "2025-05-02T09:16:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    16,
                    44,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T09:16:44Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    9,
                    16,
                    44,
                    4,
                    122,
                    0
                ],
                "title": "Risk Analysis and Design Against Adversarial Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Analysis and Design Against Adversarial Actions"
                },
                "summary": "Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework."
                },
                "authors": [
                    {
                        "name": "Marco C. Campi"
                    },
                    {
                        "name": "Algo Carè"
                    },
                    {
                        "name": "Luis G. Crespo"
                    },
                    {
                        "name": "Simone Garatti"
                    },
                    {
                        "name": "Federico A. Ramponi"
                    }
                ],
                "author_detail": {
                    "name": "Federico A. Ramponi"
                },
                "author": "Federico A. Ramponi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01110v1",
                "updated": "2025-05-02T08:45:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    45,
                    45,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T08:45:45Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    8,
                    45,
                    45,
                    4,
                    122,
                    0
                ],
                "title": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context\n  Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL."
                },
                "authors": [
                    {
                        "name": "Murtadha Ahmed"
                    },
                    {
                        "name": "Wenbo"
                    },
                    {
                        "name": "Liu yunfeng"
                    }
                ],
                "author_detail": {
                    "name": "Liu yunfeng"
                },
                "author": "Liu yunfeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21759v2",
                "updated": "2025-05-02T07:39:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    39,
                    43,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-30T15:59:35Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    59,
                    35,
                    2,
                    120,
                    0
                ],
                "title": "Smart Environmental Monitoring of Marine Pollution using Edge AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Environmental Monitoring of Marine Pollution using Edge AI"
                },
                "summary": "Oil spill incidents pose severe threats to marine ecosystems and coastal\nenvironments, necessitating rapid detection and monitoring capabilities to\nmitigate environmental damage. In this paper, we demonstrate how artificial\nintelligence, despite the inherent high computational and memory requirements,\ncan be efficiently integrated into marine pollution monitoring systems. More\nprecisely, we propose a drone-based smart monitoring system leveraging a\ncompressed deep learning U-Net architecture for oil spill detection and\nthickness estimation. Compared to the standard U-Net architecture, the number\nof convolution blocks and channels per block are modified. The new model is\nthen trained on synthetic radar data to accurately predict thick oil slick\nthickness up to 10 mm. Results show that our optimized Tiny U-Net achieves\nsuperior performance with an Intersection over Union (IoU) metric of\napproximately 79%, while simultaneously reducing the model size by a factor of\n$\\sim$269x compared to the state-of-the-art. This significant model compression\nenables efficient edge computing deployment on field-programmable gate array\n(FPGA) hardware integrated directly into the drone platform. Hardware\nimplementation demonstrates near real-time thickness estimation capabilities\nwith a run-time power consumption of approximately 2.2 watts. Our findings\nhighlight the increasing potential of smart monitoring technologies and\nefficient edge computing for operational characterization in marine\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oil spill incidents pose severe threats to marine ecosystems and coastal\nenvironments, necessitating rapid detection and monitoring capabilities to\nmitigate environmental damage. In this paper, we demonstrate how artificial\nintelligence, despite the inherent high computational and memory requirements,\ncan be efficiently integrated into marine pollution monitoring systems. More\nprecisely, we propose a drone-based smart monitoring system leveraging a\ncompressed deep learning U-Net architecture for oil spill detection and\nthickness estimation. Compared to the standard U-Net architecture, the number\nof convolution blocks and channels per block are modified. The new model is\nthen trained on synthetic radar data to accurately predict thick oil slick\nthickness up to 10 mm. Results show that our optimized Tiny U-Net achieves\nsuperior performance with an Intersection over Union (IoU) metric of\napproximately 79%, while simultaneously reducing the model size by a factor of\n$\\sim$269x compared to the state-of-the-art. This significant model compression\nenables efficient edge computing deployment on field-programmable gate array\n(FPGA) hardware integrated directly into the drone platform. Hardware\nimplementation demonstrates near real-time thickness estimation capabilities\nwith a run-time power consumption of approximately 2.2 watts. Our findings\nhighlight the increasing potential of smart monitoring technologies and\nefficient edge computing for operational characterization in marine\nenvironments."
                },
                "authors": [
                    {
                        "name": "Mohamed Moursi"
                    },
                    {
                        "name": "Norbert Wehn"
                    },
                    {
                        "name": "Bilal Hammoud"
                    }
                ],
                "author_detail": {
                    "name": "Bilal Hammoud"
                },
                "author": "Bilal Hammoud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01081v1",
                "updated": "2025-05-02T07:39:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    39,
                    8,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:39:08Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    39,
                    8,
                    4,
                    122,
                    0
                ],
                "title": "MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC\n  Benchmark"
                },
                "summary": "Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations."
                },
                "authors": [
                    {
                        "name": "Sébastien Ferré"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Ferré"
                },
                "author": "Sébastien Ferré",
                "arxiv_comment": "54 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01077v1",
                "updated": "2025-05-02T07:33:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    33,
                    20,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:33:20Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    33,
                    20,
                    4,
                    122,
                    0
                ],
                "title": "Zero-Shot Document-Level Biomedical Relation Extraction via\n  Scenario-based Prompt Design in Two-Stage with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Document-Level Biomedical Relation Extraction via\n  Scenario-based Prompt Design in Two-Stage with LLM"
                },
                "summary": "With the advent of artificial intelligence (AI), many researchers are\nattempting to extract structured information from document-level biomedical\nliterature by fine-tuning large language models (LLMs). However, they face\nsignificant challenges such as the need for expensive hardware, like\nhigh-performance GPUs and the high labor costs associated with annotating\ntraining datasets, especially in biomedical realm. Recent research on LLMs,\nsuch as GPT-4 and Llama3, has shown promising performance in zero-shot\nsettings, inspiring us to explore a novel approach to achieve the same results\nfrom unannotated full documents using general LLMs with lower hardware and\nlabor costs. Our approach combines two major stages: named entity recognition\n(NER) and relation extraction (RE). NER identifies chemical, disease and gene\nentities from the document with synonym and hypernym extraction using an LLM\nwith a crafted prompt. RE extracts relations between entities based on\npredefined relation schemas and prompts. To enhance the effectiveness of\nprompt, we propose a five-part template structure and a scenario-based prompt\ndesign principles, along with evaluation method to systematically assess the\nprompts. Finally, we evaluated our approach against fine-tuning and pre-trained\nmodels on two biomedical datasets: ChemDisGene and CDR. The experimental\nresults indicate that our proposed method can achieve comparable accuracy\nlevels to fine-tuning and pre-trained models but with reduced human and\nhardware expenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of artificial intelligence (AI), many researchers are\nattempting to extract structured information from document-level biomedical\nliterature by fine-tuning large language models (LLMs). However, they face\nsignificant challenges such as the need for expensive hardware, like\nhigh-performance GPUs and the high labor costs associated with annotating\ntraining datasets, especially in biomedical realm. Recent research on LLMs,\nsuch as GPT-4 and Llama3, has shown promising performance in zero-shot\nsettings, inspiring us to explore a novel approach to achieve the same results\nfrom unannotated full documents using general LLMs with lower hardware and\nlabor costs. Our approach combines two major stages: named entity recognition\n(NER) and relation extraction (RE). NER identifies chemical, disease and gene\nentities from the document with synonym and hypernym extraction using an LLM\nwith a crafted prompt. RE extracts relations between entities based on\npredefined relation schemas and prompts. To enhance the effectiveness of\nprompt, we propose a five-part template structure and a scenario-based prompt\ndesign principles, along with evaluation method to systematically assess the\nprompts. Finally, we evaluated our approach against fine-tuning and pre-trained\nmodels on two biomedical datasets: ChemDisGene and CDR. The experimental\nresults indicate that our proposed method can achieve comparable accuracy\nlevels to fine-tuning and pre-trained models but with reduced human and\nhardware expenses."
                },
                "authors": [
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Ling Kang"
                    },
                    {
                        "name": "Quan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Quan Guo"
                },
                "author": "Quan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01076v1",
                "updated": "2025-05-02T07:33:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    33,
                    2,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:33:02Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    33,
                    2,
                    4,
                    122,
                    0
                ],
                "title": "Quasi-Static IRS: 3D Shaped Beamforming for Area Coverage Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-Static IRS: 3D Shaped Beamforming for Area Coverage Enhancement"
                },
                "summary": "Intelligent reflecting surface (IRS) is a promising paradigm to reconfigure\nthe wireless environment for enhanced communication coverage and quality.\nHowever, to compensate for the double pathloss effect, massive IRS elements are\nrequired, raising concerns on the scalability of cost and complexity. This\npaper introduces a new architecture of quasi-static IRS (QS-IRS), which tunes\nelement phases via mechanical adjustment or manually re-arranging the array\ntopology. QS-IRS relies on massive production/assembly of purely passive\nelements only, and thus is suitable for ultra low-cost and large-scale\ndeployment to enhance long-term coverage. To achieve this end, an IRS-aided\narea coverage problem is formulated, which explicitly considers the element\nradiation pattern (ERP), with the newly introduced shape masks for the\nmainlobe, and the sidelobe constraints to reduce energy leakage. An alternating\noptimization (AO) algorithm based on the difference-of-convex (DC) and\nsuccessive convex approximation (SCA) procedure is proposed, which achieves\nshaped beamforming with power gains close to that of the joint optimization\nalgorithm, but with significantly reduced computational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent reflecting surface (IRS) is a promising paradigm to reconfigure\nthe wireless environment for enhanced communication coverage and quality.\nHowever, to compensate for the double pathloss effect, massive IRS elements are\nrequired, raising concerns on the scalability of cost and complexity. This\npaper introduces a new architecture of quasi-static IRS (QS-IRS), which tunes\nelement phases via mechanical adjustment or manually re-arranging the array\ntopology. QS-IRS relies on massive production/assembly of purely passive\nelements only, and thus is suitable for ultra low-cost and large-scale\ndeployment to enhance long-term coverage. To achieve this end, an IRS-aided\narea coverage problem is formulated, which explicitly considers the element\nradiation pattern (ERP), with the newly introduced shape masks for the\nmainlobe, and the sidelobe constraints to reduce energy leakage. An alternating\noptimization (AO) algorithm based on the difference-of-convex (DC) and\nsuccessive convex approximation (SCA) procedure is proposed, which achieves\nshaped beamforming with power gains close to that of the joint optimization\nalgorithm, but with significantly reduced computational complexity."
                },
                "authors": [
                    {
                        "name": "Zhenyu Jiang"
                    },
                    {
                        "name": "Xintong Chen"
                    },
                    {
                        "name": "Jiangbin Lyu"
                    },
                    {
                        "name": "Liqun Fu"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05817v2",
                "updated": "2025-05-02T07:30:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    30,
                    39,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-10T09:44:30Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    44,
                    30,
                    4,
                    10,
                    0
                ],
                "title": "RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT"
                },
                "summary": "This paper evaluates the performance of reconfigurable intelligent surface\n(RIS) optimization algorithms, which utilize channel estimation methods, in ray\ntracing (RT) simulations within urban digital twin environments. Beyond\nSionna's native capabilities, we implement and benchmark additional RIS\noptimization algorithms based on channel estimation, enabling an evaluation of\nRIS strategies under various deployment conditions. Coverage maps for\nRIS-assisted communication systems are generated through the integration of\nSionna's RT simulations. Moreover, real-world experimentation underscores the\nnecessity of validating algorithms in near-realistic simulation environments,\nas minor variations in measurement setups can significantly affect performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the performance of reconfigurable intelligent surface\n(RIS) optimization algorithms, which utilize channel estimation methods, in ray\ntracing (RT) simulations within urban digital twin environments. Beyond\nSionna's native capabilities, we implement and benchmark additional RIS\noptimization algorithms based on channel estimation, enabling an evaluation of\nRIS strategies under various deployment conditions. Coverage maps for\nRIS-assisted communication systems are generated through the integration of\nSionna's RT simulations. Moreover, real-world experimentation underscores the\nnecessity of validating algorithms in near-realistic simulation environments,\nas minor variations in measurement setups can significantly affect performance."
                },
                "authors": [
                    {
                        "name": "Ahmet Esad Güneşer"
                    },
                    {
                        "name": "Berkay Şekeroğlu"
                    },
                    {
                        "name": "Sefa Kayraklık"
                    },
                    {
                        "name": "Erhan Karakoca"
                    },
                    {
                        "name": "İbrahim Hökelek"
                    },
                    {
                        "name": "Sultan Aldirmaz-Colak"
                    },
                    {
                        "name": "Ali Görçin"
                    }
                ],
                "author_detail": {
                    "name": "Ali Görçin"
                },
                "author": "Ali Görçin",
                "arxiv_comment": "Accepted in IEEE VTC2025-Spring, Copyright IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01074v1",
                "updated": "2025-05-02T07:29:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    29,
                    19,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:29:19Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    29,
                    19,
                    4,
                    122,
                    0
                ],
                "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks"
                },
                "summary": "The rapid evolution of wireless networks presents unprecedented challenges in\nmanaging complex and dynamic systems. Existing methods are increasingly facing\nfundamental limitations in addressing these challenges. In this paper, we\nintroduce WirelessAgent, a novel framework that harnesses large language models\n(LLMs) to create autonomous AI agents for diverse wireless network tasks. This\nframework integrates four core modules that mirror human cognitive processes:\nperception, memory, planning, and action. To implement it, we provide a basic\nusage based on agentic workflows and the LangGraph architecture. We demonstrate\nthe effectiveness of WirelessAgent through a comprehensive case study on\nnetwork slicing. The numerical results show that WirelessAgent achieves\n$44.4\\%$ higher bandwidth utilization than the \\emph{Prompt-based} method,\nwhile performing only $4.3\\%$ below the \\emph{Rule-based optimality}. Notably,\nWirelessAgent delivers near-optimal network throughput across diverse network\nscenarios. These underscore the framework's potential for intelligent and\nautonomous resource management in future wireless networks. The code is\navailable at \\url{https://github.com/jwentong/WirelessAgent_R1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of wireless networks presents unprecedented challenges in\nmanaging complex and dynamic systems. Existing methods are increasingly facing\nfundamental limitations in addressing these challenges. In this paper, we\nintroduce WirelessAgent, a novel framework that harnesses large language models\n(LLMs) to create autonomous AI agents for diverse wireless network tasks. This\nframework integrates four core modules that mirror human cognitive processes:\nperception, memory, planning, and action. To implement it, we provide a basic\nusage based on agentic workflows and the LangGraph architecture. We demonstrate\nthe effectiveness of WirelessAgent through a comprehensive case study on\nnetwork slicing. The numerical results show that WirelessAgent achieves\n$44.4\\%$ higher bandwidth utilization than the \\emph{Prompt-based} method,\nwhile performing only $4.3\\%$ below the \\emph{Rule-based optimality}. Notably,\nWirelessAgent delivers near-optimal network throughput across diverse network\nscenarios. These underscore the framework's potential for intelligent and\nautonomous resource management in future wireless networks. The code is\navailable at \\url{https://github.com/jwentong/WirelessAgent_R1}."
                },
                "authors": [
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "This manuscript is an extended version of a previous magazine version\n  and is now submitted to a journal for possible publication. arXiv admin note:\n  text overlap with arXiv:2409.07964",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01073v1",
                "updated": "2025-05-02T07:25:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    25,
                    1,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:25:01Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    25,
                    1,
                    4,
                    122,
                    0
                ],
                "title": "Retrieval Augmented Learning: A Retrial-based Large Language Model\n  Self-Supervised Learning and Autonomous Knowledge Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Learning: A Retrial-based Large Language Model\n  Self-Supervised Learning and Autonomous Knowledge Generation"
                },
                "summary": "The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation."
                },
                "authors": [
                    {
                        "name": "Zongyuan Li"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Runnan Qi"
                    },
                    {
                        "name": "Yanan Ni"
                    },
                    {
                        "name": "Lumin Jiang"
                    },
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xuebo Zhang"
                    },
                    {
                        "name": "Kuihua Huang"
                    },
                    {
                        "name": "Xian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Xian Guo"
                },
                "author": "Xian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05348v2",
                "updated": "2025-05-02T07:20:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    20,
                    36,
                    4,
                    122,
                    0
                ],
                "published": "2024-11-08T06:04:22Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    4,
                    22,
                    4,
                    313,
                    0
                ],
                "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-PySC2: Starcraft II learning environment for Large Language Models"
                },
                "summary": "The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions."
                },
                "authors": [
                    {
                        "name": "Zongyuan Li"
                    },
                    {
                        "name": "Yanan Ni"
                    },
                    {
                        "name": "Runnan Qi"
                    },
                    {
                        "name": "Lumin Jiang"
                    },
                    {
                        "name": "Chang Lu"
                    },
                    {
                        "name": "Xiaojie Xu"
                    },
                    {
                        "name": "Xiangbei Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yunzheng Guo"
                    },
                    {
                        "name": "Zhe Ma"
                    },
                    {
                        "name": "Huanyu Li"
                    },
                    {
                        "name": "Hui Wu"
                    },
                    {
                        "name": "Xian Guo"
                    },
                    {
                        "name": "Kuihua Huang"
                    },
                    {
                        "name": "Xuebo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuebo Zhang"
                },
                "author": "Xuebo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01067v1",
                "updated": "2025-05-02T07:16:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    16,
                    20,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:16:20Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    16,
                    20,
                    4,
                    122,
                    0
                ],
                "title": "A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in\n  Model Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in\n  Model Repositories"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms."
                },
                "authors": [
                    {
                        "name": "Ziqi Ding"
                    },
                    {
                        "name": "Qian Fu"
                    },
                    {
                        "name": "Junchen Ding"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01065v1",
                "updated": "2025-05-02T07:15:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    15,
                    22,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:15:22Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    15,
                    22,
                    4,
                    122,
                    0
                ],
                "title": "Good News for Script Kiddies? Evaluating Large Language Models for\n  Automated Exploit Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good News for Script Kiddies? Evaluating Large Language Models for\n  Automated Exploit Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements."
                },
                "authors": [
                    {
                        "name": "David Jin"
                    },
                    {
                        "name": "Qian Fu"
                    },
                    {
                        "name": "Yuekang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuekang Li"
                },
                "author": "Yuekang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01064v1",
                "updated": "2025-05-02T07:14:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    14,
                    58,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T07:14:58Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    7,
                    14,
                    58,
                    4,
                    122,
                    0
                ],
                "title": "Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of\n  Multimodal LLMs"
                },
                "summary": "Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR."
                },
                "authors": [
                    {
                        "name": "Hari Chandana Kuchibhotla"
                    },
                    {
                        "name": "Sai Srinivas Kancheti"
                    },
                    {
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "name": "Vineeth N Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Vineeth N Balasubramanian"
                },
                "author": "Vineeth N Balasubramanian",
                "arxiv_comment": "preprint; earlier version accepted at NeurIPS 2024 Workshop on\n  Adaptive Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01050v1",
                "updated": "2025-05-02T06:51:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    51,
                    11,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:51:11Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    51,
                    11,
                    4,
                    122,
                    0
                ],
                "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Adversarial Attacks on Black-Box Vision-Language Models"
                },
                "summary": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs."
                },
                "authors": [
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Weichen Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Alexander Robey"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Chengming Xu"
                    },
                    {
                        "name": "Haoqi Hu"
                    },
                    {
                        "name": "Matt Fredrikson"
                    }
                ],
                "author_detail": {
                    "name": "Matt Fredrikson"
                },
                "author": "Matt Fredrikson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09022v3",
                "updated": "2025-05-02T06:50:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    50,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2025-03-12T03:20:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    20,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tingsong Xiao"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "To appear at IEEE Symposium on Security and Privacy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01043v1",
                "updated": "2025-05-02T06:33:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    33,
                    25,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:33:25Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    33,
                    25,
                    4,
                    122,
                    0
                ],
                "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training."
                },
                "authors": [
                    {
                        "name": "Zhiwei Hao"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Yonggang Wen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01035v1",
                "updated": "2025-05-02T06:17:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    17,
                    51,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T06:17:51Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    17,
                    51,
                    4,
                    122,
                    0
                ],
                "title": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large\n  Language Models?"
                },
                "summary": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs."
                },
                "authors": [
                    {
                        "name": "Lui Yoshida"
                    }
                ],
                "author_detail": {
                    "name": "Lui Yoshida"
                },
                "author": "Lui Yoshida",
                "arxiv_comment": "Accepted in AIED 2025. This preprint has not undergone any\n  post-submission improvements or corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00070v2",
                "updated": "2025-05-02T05:27:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    27,
                    38,
                    4,
                    122,
                    0
                ],
                "published": "2024-12-29T18:58:09Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    58,
                    9,
                    6,
                    364,
                    0
                ],
                "title": "ICLR: In-Context Learning of Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICLR: In-Context Learning of Representations"
                },
                "summary": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities."
                },
                "authors": [
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Andrew Lee"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Yongyi Yang"
                    },
                    {
                        "name": "Maya Okawa"
                    },
                    {
                        "name": "Kento Nishi"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Hidenori Tanaka"
                },
                "author": "Hidenori Tanaka",
                "arxiv_comment": "ICLR 2025",
                "arxiv_journal_ref": "International Conference on Learning Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01015v1",
                "updated": "2025-05-02T05:26:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    26,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T05:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    26,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "Value Portrait: Understanding Values of LLMs with Human-aligned\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Portrait: Understanding Values of LLMs with Human-aligned\n  Benchmark"
                },
                "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data."
                },
                "authors": [
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Dongmin Choi"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "32 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01009v1",
                "updated": "2025-05-02T05:16:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    16,
                    17,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T05:16:17Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    16,
                    17,
                    4,
                    122,
                    0
                ],
                "title": "Improving Large Language Model Planning with Action Sequence Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Model Planning with Action Sequence Similarity"
                },
                "summary": "Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Hanie Sedghi"
                    },
                    {
                        "name": "Bernd Bohnet"
                    },
                    {
                        "name": "Dale Schuurmans"
                    },
                    {
                        "name": "Azade Nova"
                    }
                ],
                "author_detail": {
                    "name": "Azade Nova"
                },
                "author": "Azade Nova",
                "arxiv_comment": "25 pages, 11 figures",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14359v2",
                "updated": "2025-05-02T05:08:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    5,
                    8,
                    48,
                    4,
                    122,
                    0
                ],
                "published": "2024-02-22T07:58:29Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    7,
                    58,
                    29,
                    3,
                    53,
                    0
                ],
                "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable\n  Metrics on Facet-aware Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Scientific Summarization Evaluation: Grounding Explainable\n  Metrics on Facet-aware Benchmark"
                },
                "summary": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs."
                },
                "authors": [
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Tairan Wang"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Taicheng Guo"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00995v1",
                "updated": "2025-05-02T04:41:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    41,
                    57,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:41:57Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    41,
                    57,
                    4,
                    122,
                    0
                ],
                "title": "Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation\n  in a GNSS-Denied Cherry Tomato Greenhouse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation\n  in a GNSS-Denied Cherry Tomato Greenhouse"
                },
                "summary": "As the agricultural workforce declines and labor costs rise, robotic yield\nestimation has become increasingly important. While unmanned ground vehicles\n(UGVs) are commonly used for indoor farm monitoring, their deployment in\ngreenhouses is often constrained by infrastructure limitations, sensor\nplacement challenges, and operational inefficiencies. To address these issues,\nwe develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D\ncamera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial\nodometry algorithm for precise navigation in GNSS-denied environments and\nutilizes a 3D multi-object tracking algorithm to estimate the count and weight\nof cherry tomatoes. We evaluate the system using two dataset: one from a\nharvesting row and another from a growing row. In the harvesting-row dataset,\nthe proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight\nestimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For\nthe growing-row dataset, which consists of occluded unripened fruits, we\nqualitatively analyze tracking performance and highlight future research\ndirections for improving perception in greenhouse with strong occlusions. Our\nfindings demonstrate the potential of UAVs for efficient robotic yield\nestimation in commercial greenhouses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the agricultural workforce declines and labor costs rise, robotic yield\nestimation has become increasingly important. While unmanned ground vehicles\n(UGVs) are commonly used for indoor farm monitoring, their deployment in\ngreenhouses is often constrained by infrastructure limitations, sensor\nplacement challenges, and operational inefficiencies. To address these issues,\nwe develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D\ncamera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial\nodometry algorithm for precise navigation in GNSS-denied environments and\nutilizes a 3D multi-object tracking algorithm to estimate the count and weight\nof cherry tomatoes. We evaluate the system using two dataset: one from a\nharvesting row and another from a growing row. In the harvesting-row dataset,\nthe proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight\nestimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For\nthe growing-row dataset, which consists of occluded unripened fruits, we\nqualitatively analyze tracking performance and highlight future research\ndirections for improving perception in greenhouse with strong occlusions. Our\nfindings demonstrate the potential of UAVs for efficient robotic yield\nestimation in commercial greenhouses."
                },
                "authors": [
                    {
                        "name": "Taewook Park"
                    },
                    {
                        "name": "Jinwoo Lee"
                    },
                    {
                        "name": "Hyondong Oh"
                    },
                    {
                        "name": "Won-Jae Yun"
                    },
                    {
                        "name": "Kyu-Wha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyu-Wha Lee"
                },
                "author": "Kyu-Wha Lee",
                "arxiv_comment": "Accepted at 2025 ICRA workshop on field robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00989v1",
                "updated": "2025-05-02T04:27:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    27,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:27:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    27,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel\n  Traffic Services through Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel\n  Traffic Services through Natural Language"
                },
                "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management."
                },
                "authors": [
                    {
                        "name": "Sijin Sun"
                    },
                    {
                        "name": "Liangbin Zhao"
                    },
                    {
                        "name": "Ming Deng"
                    },
                    {
                        "name": "Xiuju Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuju Fu"
                },
                "author": "Xiuju Fu",
                "arxiv_comment": "8 pages, 5 figures, 7 tablels, submitted to ITSC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00985v1",
                "updated": "2025-05-02T04:13:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    13,
                    27,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:13:27Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    13,
                    27,
                    4,
                    122,
                    0
                ],
                "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling"
                },
                "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Yash Goel"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00981v1",
                "updated": "2025-05-02T04:01:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    1,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:01:31Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    1,
                    31,
                    4,
                    122,
                    0
                ],
                "title": "Multi-agents based User Values Mining for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agents based User Values Mining for Recommendation"
                },
                "summary": "Recommender systems have rapidly evolved and become integral to many online\nservices. However, existing systems sometimes produce unstable and\nunsatisfactory recommendations that fail to align with users' fundamental and\nlong-term preferences. This is because they primarily focus on extracting\nshallow and short-term interests from user behavior data, which is inherently\ndynamic and challenging to model. Unlike these transient interests, user values\nare more stable and play a crucial role in shaping user behaviors, such as\npurchasing items and consuming content. Incorporating user values into\nrecommender systems can help stabilize recommendation performance and ensure\nresults better reflect users' latent preferences. However, acquiring user\nvalues is typically difficult and costly. To address this challenge, we\nleverage the strong language understanding, zero-shot inference, and\ngeneralization capabilities of Large Language Models (LLMs) to extract user\nvalues from users' historical interactions. Unfortunately, direct extraction\nusing LLMs presents several challenges such as length constraints and\nhallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM\ncollaborative framework for effective and accurate user value extraction. In\nZOOM, we apply text summarization techniques to condense item content while\npreserving essential meaning. To mitigate hallucinations, ZOOM introduces two\nspecialized agent roles: evaluators and supervisors, to collaboratively\ngenerate accurate user values. Extensive experiments on two widely used\nrecommendation datasets with two state-of-the-art recommendation models\ndemonstrate the effectiveness and generalization of our framework in automatic\nuser value mining and recommendation performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have rapidly evolved and become integral to many online\nservices. However, existing systems sometimes produce unstable and\nunsatisfactory recommendations that fail to align with users' fundamental and\nlong-term preferences. This is because they primarily focus on extracting\nshallow and short-term interests from user behavior data, which is inherently\ndynamic and challenging to model. Unlike these transient interests, user values\nare more stable and play a crucial role in shaping user behaviors, such as\npurchasing items and consuming content. Incorporating user values into\nrecommender systems can help stabilize recommendation performance and ensure\nresults better reflect users' latent preferences. However, acquiring user\nvalues is typically difficult and costly. To address this challenge, we\nleverage the strong language understanding, zero-shot inference, and\ngeneralization capabilities of Large Language Models (LLMs) to extract user\nvalues from users' historical interactions. Unfortunately, direct extraction\nusing LLMs presents several challenges such as length constraints and\nhallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM\ncollaborative framework for effective and accurate user value extraction. In\nZOOM, we apply text summarization techniques to condense item content while\npreserving essential meaning. To mitigate hallucinations, ZOOM introduces two\nspecialized agent roles: evaluators and supervisors, to collaboratively\ngenerate accurate user values. Extensive experiments on two widely used\nrecommendation datasets with two state-of-the-art recommendation models\ndemonstrate the effectiveness and generalization of our framework in automatic\nuser value mining and recommendation performance improvement."
                },
                "authors": [
                    {
                        "name": "Lijian Chen"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Nguyen Quoc Viet Hung"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00980v1",
                "updated": "2025-05-02T04:00:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    0,
                    3,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:00:03Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    0,
                    3,
                    4,
                    122,
                    0
                ],
                "title": "LMDepth: Lightweight Mamba-based Monocular Depth Estimation for\n  Real-World Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMDepth: Lightweight Mamba-based Monocular Depth Estimation for\n  Real-World Deployment"
                },
                "summary": "Monocular depth estimation provides an additional depth dimension to RGB\nimages, making it widely applicable in various fields such as virtual reality,\nautonomous driving and robotic navigation. However, existing depth estimation\nalgorithms often struggle to effectively balance performance and computational\nefficiency, which poses challenges for deployment on resource-constrained\ndevices. To address this, we propose LMDepth, a lightweight Mamba-based\nmonocular depth estimation network, designed to reconstruct high-precision\ndepth information while maintaining low computational overhead. Specifically,\nwe propose a modified pyramid spatial pooling module that serves as a\nmulti-scale feature aggregator and context extractor, ensuring global spatial\ninformation for accurate depth estimation. Moreover, we integrate multiple\ndepth Mamba blocks into the decoder. Designed with linear computations, the\nMamba Blocks enable LMDepth to efficiently decode depth information from global\nfeatures, providing a lightweight alternative to Transformer-based\narchitectures that depend on complex attention mechanisms. Extensive\nexperiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of\nour proposed LMDepth. Compared to previous lightweight depth estimation\nmethods, LMDepth achieves higher performance with fewer parameters and lower\ncomputational complexity (measured by GFLOPs). We further deploy LMDepth on an\nembedded platform with INT8 quantization, validating its practicality for\nreal-world edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular depth estimation provides an additional depth dimension to RGB\nimages, making it widely applicable in various fields such as virtual reality,\nautonomous driving and robotic navigation. However, existing depth estimation\nalgorithms often struggle to effectively balance performance and computational\nefficiency, which poses challenges for deployment on resource-constrained\ndevices. To address this, we propose LMDepth, a lightweight Mamba-based\nmonocular depth estimation network, designed to reconstruct high-precision\ndepth information while maintaining low computational overhead. Specifically,\nwe propose a modified pyramid spatial pooling module that serves as a\nmulti-scale feature aggregator and context extractor, ensuring global spatial\ninformation for accurate depth estimation. Moreover, we integrate multiple\ndepth Mamba blocks into the decoder. Designed with linear computations, the\nMamba Blocks enable LMDepth to efficiently decode depth information from global\nfeatures, providing a lightweight alternative to Transformer-based\narchitectures that depend on complex attention mechanisms. Extensive\nexperiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of\nour proposed LMDepth. Compared to previous lightweight depth estimation\nmethods, LMDepth achieves higher performance with fewer parameters and lower\ncomputational complexity (measured by GFLOPs). We further deploy LMDepth on an\nembedded platform with INT8 quantization, validating its practicality for\nreal-world edge applications."
                },
                "authors": [
                    {
                        "name": "Jiahuan Long"
                    },
                    {
                        "name": "Xin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhou"
                },
                "author": "Xin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00979v1",
                "updated": "2025-05-02T03:40:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    40,
                    39,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:40:39Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    40,
                    39,
                    4,
                    122,
                    0
                ],
                "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for\n  Continue Pre-training of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for\n  Continue Pre-training of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability."
                },
                "authors": [
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Liyu Zhang"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00976v1",
                "updated": "2025-05-02T03:37:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    52,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:37:52Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    52,
                    4,
                    122,
                    0
                ],
                "title": "Attack and defense techniques in large language models: A survey and new\n  perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attack and defense techniques in large language models: A survey and new\n  perspectives"
                },
                "summary": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhiyu Liao"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Yuanguo Lin"
                    },
                    {
                        "name": "Kangkang Li"
                    },
                    {
                        "name": "Yunxuan Liu"
                    },
                    {
                        "name": "Hefeng Chen"
                    },
                    {
                        "name": "Xingwang Huang"
                    },
                    {
                        "name": "Yuanhui Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanhui Yu"
                },
                "author": "Yuanhui Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00975v1",
                "updated": "2025-05-02T03:37:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    9,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:37:09Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    37,
                    9,
                    4,
                    122,
                    0
                ],
                "title": "Generating Animated Layouts as Structured Text Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Animated Layouts as Structured Text Representations"
                },
                "summary": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker"
                },
                "authors": [
                    {
                        "name": "Yeonsang Shin"
                    },
                    {
                        "name": "Jihwan Kim"
                    },
                    {
                        "name": "Yumin Song"
                    },
                    {
                        "name": "Kyungseung Lee"
                    },
                    {
                        "name": "Hyunhee Chung"
                    },
                    {
                        "name": "Taeyoung Na"
                    }
                ],
                "author_detail": {
                    "name": "Taeyoung Na"
                },
                "author": "Taeyoung Na",
                "arxiv_comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00972v1",
                "updated": "2025-05-02T03:22:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    22,
                    0,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T03:22:00Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    22,
                    0,
                    4,
                    122,
                    0
                ],
                "title": "Seeking to Collide: Online Safety-Critical Scenario Generation for\n  Autonomous Driving with Retrieval Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeking to Collide: Online Safety-Critical Scenario Generation for\n  Autonomous Driving with Retrieval Augmented Large Language Models"
                },
                "summary": "Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines."
                },
                "authors": [
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08324v2",
                "updated": "2025-05-02T03:07:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    3,
                    7,
                    14,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-14T18:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    56,
                    33,
                    1,
                    14,
                    0
                ],
                "title": "ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease\n  Detection and Microbiome-Clinical Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease\n  Detection and Microbiome-Clinical Data Integration"
                },
                "summary": "Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large\nlanguage model (LLM) framework designed to integrate and analyze multimodal\ndata, including microbiome profiles, clinical datasets, and external knowledge\nbases, to enhance the understanding and classification of Alzheimer's disease\n(AD). By leveraging the agentic system with LLM, ADAM produces insights from\ndiverse data sources and contextualizes the findings with literature-driven\nevidence. A comparative evaluation with XGBoost revealed a significantly\nimproved mean F1 score and significantly reduced variance for ADAM,\nhighlighting its robustness and consistency, particularly when utilizing human\nbiological data. Although currently tailored for binary classification tasks\nwith two data modalities, future iterations will aim to incorporate additional\ndata types, such as neuroimaging and peripheral biomarkers, and expand them to\npredict disease progression, thereby broadening ADAM's scalability and\napplicability in AD research and diagnostic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large\nlanguage model (LLM) framework designed to integrate and analyze multimodal\ndata, including microbiome profiles, clinical datasets, and external knowledge\nbases, to enhance the understanding and classification of Alzheimer's disease\n(AD). By leveraging the agentic system with LLM, ADAM produces insights from\ndiverse data sources and contextualizes the findings with literature-driven\nevidence. A comparative evaluation with XGBoost revealed a significantly\nimproved mean F1 score and significantly reduced variance for ADAM,\nhighlighting its robustness and consistency, particularly when utilizing human\nbiological data. Although currently tailored for binary classification tasks\nwith two data modalities, future iterations will aim to incorporate additional\ndata types, such as neuroimaging and peripheral biomarkers, and expand them to\npredict disease progression, thereby broadening ADAM's scalability and\napplicability in AD research and diagnostic applications."
                },
                "authors": [
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Vishaldeep Kaur Sekhon"
                    },
                    {
                        "name": "Roozbeh Sadeghian"
                    },
                    {
                        "name": "Maria L. Vaida"
                    },
                    {
                        "name": "Cynthia Jo"
                    },
                    {
                        "name": "Doyle Ward"
                    },
                    {
                        "name": "Vanni Bucci"
                    },
                    {
                        "name": "John P. Haran"
                    }
                ],
                "author_detail": {
                    "name": "John P. Haran"
                },
                "author": "John P. Haran",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18837v2",
                "updated": "2025-05-02T02:39:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    39,
                    9,
                    4,
                    122,
                    0
                ],
                "published": "2025-04-26T07:48:35Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    7,
                    48,
                    35,
                    5,
                    116,
                    0
                ],
                "title": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events"
                },
                "summary": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era."
                },
                "authors": [
                    {
                        "name": "Pouya Shaeri"
                    },
                    {
                        "name": "Yasaman Mohammadpour"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Ariane Middel"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "13 Pages, 1 figure, submitted to ACM Hypertext 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06382v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06382v3",
                "updated": "2025-05-02T02:25:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    25,
                    37,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-10T23:18:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    23,
                    18,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with\n  Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with\n  Self-Attention"
                },
                "summary": "Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought."
                },
                "authors": [
                    {
                        "name": "Mumin Jia"
                    },
                    {
                        "name": "Jairo Diaz-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Jairo Diaz-Rodriguez"
                },
                "author": "Jairo Diaz-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06382v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06382v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00951v1",
                "updated": "2025-05-02T01:54:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    54,
                    8,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T01:54:08Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    54,
                    8,
                    4,
                    122,
                    0
                ],
                "title": "Preserving Privacy and Utility in LLM-Based Product Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Privacy and Utility in LLM-Based Product Recommendations"
                },
                "summary": "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based recommendation systems leverage powerful\nlanguage models to generate personalized suggestions by processing user\ninteractions and preferences. Unlike traditional recommendation systems that\nrely on structured data and collaborative filtering, LLM-based models process\ntextual and contextual information, often using cloud-based infrastructure.\nThis raises privacy concerns, as user data is transmitted to remote servers,\nincreasing the risk of exposure and reducing control over personal information.\nTo address this, we propose a hybrid privacy-preserving recommendation\nframework which separates sensitive from nonsensitive data and only shares the\nlatter with the cloud to harness LLM-powered recommendations. To restore lost\nrecommendations related to obfuscated sensitive data, we design a\nde-obfuscation module that reconstructs sensitive recommendations locally.\nExperiments on real-world e-commerce datasets show that our framework achieves\nalmost the same recommendation utility with a system which shares all data with\nan LLM, while preserving privacy to a large extend. Compared to\nobfuscation-only techniques, our approach improves HR@10 scores and category\ndistribution alignment, offering a better balance between privacy and\nrecommendation quality. Furthermore, our method runs efficiently on\nconsumer-grade hardware, making privacy-aware LLM-based recommendation systems\npractical for real-world use."
                },
                "authors": [
                    {
                        "name": "Tina Khezresmaeilzadeh"
                    },
                    {
                        "name": "Jiang Zhang"
                    },
                    {
                        "name": "Dimitrios Andreadis"
                    },
                    {
                        "name": "Konstantinos Psounis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Psounis"
                },
                "author": "Konstantinos Psounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00945v1",
                "updated": "2025-05-02T01:17:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    17,
                    3,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T01:17:03Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    17,
                    3,
                    4,
                    122,
                    0
                ],
                "title": "SSRLBot: Designing and Developing an LLM-based Agent using Socially\n  Shared Regulated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSRLBot: Designing and Developing an LLM-based Agent using Socially\n  Shared Regulated Learning"
                },
                "summary": "Large language model (LLM)-based agents are increasingly used to support\nhuman experts by streamlining complex tasks and offering actionable insights.\nHowever, their application in multi-professional decision-making, particularly\nin teamwork contexts, remains underexplored. This design-based study addresses\nthat gap by developing LLM functions to enhance collaboration, grounded in the\nSocially Shared Regulation of Learning (SSRL) framework and applied to medical\ndiagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational,\nand emotional processes in shared learning, focusing on how teams manage these\nprocesses to improve decision-making. This paper introduces SSRLBot, a\nprototype chatbot designed to help team members reflect on both their\ndiagnostic performance and key SSRL skills. Its core functions include\nsummarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic\noutcomes, annotating SSRL markers in conversation, assessing their impact on\nperformance, and identifying interpersonal regulatory dynamics. We compare\nSSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a\ncase study. SSRLBot demonstrates stronger alignment with SSRL theory, offering\ndetailed evaluations that link behaviors to regulatory dimensions and\nsuggesting improvements for collaboration. By integrating SSRL theory with LLM\ncapabilities, SSRLBot contributes a novel tool for enhancing team-based\ndecision-making and collaborative learning in high-stakes environments, such as\nmedical education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents are increasingly used to support\nhuman experts by streamlining complex tasks and offering actionable insights.\nHowever, their application in multi-professional decision-making, particularly\nin teamwork contexts, remains underexplored. This design-based study addresses\nthat gap by developing LLM functions to enhance collaboration, grounded in the\nSocially Shared Regulation of Learning (SSRL) framework and applied to medical\ndiagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational,\nand emotional processes in shared learning, focusing on how teams manage these\nprocesses to improve decision-making. This paper introduces SSRLBot, a\nprototype chatbot designed to help team members reflect on both their\ndiagnostic performance and key SSRL skills. Its core functions include\nsummarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic\noutcomes, annotating SSRL markers in conversation, assessing their impact on\nperformance, and identifying interpersonal regulatory dynamics. We compare\nSSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a\ncase study. SSRLBot demonstrates stronger alignment with SSRL theory, offering\ndetailed evaluations that link behaviors to regulatory dimensions and\nsuggesting improvements for collaboration. By integrating SSRL theory with LLM\ncapabilities, SSRLBot contributes a novel tool for enhancing team-based\ndecision-making and collaborative learning in high-stakes environments, such as\nmedical education."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Huang"
                    },
                    {
                        "name": "Jie Gao"
                    },
                    {
                        "name": "Haolun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Haolun Wu"
                },
                "author": "Haolun Wu",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08067v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08067v5",
                "updated": "2025-05-02T01:10:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    10,
                    28,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-10T16:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
                },
                "summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses, despite having access\nto preference data that includes reward scores from judge models during AI\nfeedback. Striving to maximize the implicit reward gap between the chosen and\nthe slightly inferior rejected responses can cause overfitting and unnecessary\nunlearning of the high-quality rejected responses. The unawareness of the\nreward scores also drives the LLM to indiscriminately favor the low-quality\nchosen responses and fail to generalize to optimal responses that are sparse in\ndata. To overcome these shortcomings, our study introduces reward-conditioned\nLLM policies that discern and learn from the entire spectrum of response\nquality within the dataset, helping extrapolate to more optimal regions. We\npropose an effective yet simple data relabeling method that conditions the\npreference pairs on quality scores to construct a reward-augmented dataset. The\nexperiments across various benchmarks and diverse models demonstrate that our\napproach consistently boosts DPO by a considerable margin. Through\ncomprehensive ablation studies, we demonstrate that our method not only\nmaximizes the utility of preference data but also mitigates the issue of\nunlearning, demonstrating its broad effectiveness beyond mere data expansion.\nOur code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses, despite having access\nto preference data that includes reward scores from judge models during AI\nfeedback. Striving to maximize the implicit reward gap between the chosen and\nthe slightly inferior rejected responses can cause overfitting and unnecessary\nunlearning of the high-quality rejected responses. The unawareness of the\nreward scores also drives the LLM to indiscriminately favor the low-quality\nchosen responses and fail to generalize to optimal responses that are sparse in\ndata. To overcome these shortcomings, our study introduces reward-conditioned\nLLM policies that discern and learn from the entire spectrum of response\nquality within the dataset, helping extrapolate to more optimal regions. We\npropose an effective yet simple data relabeling method that conditions the\npreference pairs on quality scores to construct a reward-augmented dataset. The\nexperiments across various benchmarks and diverse models demonstrate that our\napproach consistently boosts DPO by a considerable margin. Through\ncomprehensive ablation studies, we demonstrate that our method not only\nmaximizes the utility of preference data but also mitigates the issue of\nunlearning, demonstrating its broad effectiveness beyond mere data expansion.\nOur code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08067v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08067v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00935v1",
                "updated": "2025-05-02T00:43:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    43,
                    28,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:43:28Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    43,
                    28,
                    4,
                    122,
                    0
                ],
                "title": "Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning"
                },
                "summary": "The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks."
                },
                "authors": [
                    {
                        "name": "Roberto Bigazzi"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bigazzi"
                },
                "author": "Roberto Bigazzi",
                "arxiv_comment": "Ph.D. Dissertation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00932v1",
                "updated": "2025-05-02T00:20:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    20,
                    38,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    20,
                    38,
                    4,
                    122,
                    0
                ],
                "title": "A Self-Supervised Transformer for Unusable Shared Bike Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Self-Supervised Transformer for Unusable Shared Bike Detection"
                },
                "summary": "The rapid expansion of bike-sharing systems (BSS) has greatly improved urban\n\"last-mile\" connectivity, yet large-scale deployments face escalating\noperational challenges, particularly in detecting faulty bikes. Existing\ndetection approaches either rely on static model-based thresholds that overlook\ndynamic spatiotemporal (ST) usage patterns or employ supervised learning\nmethods that struggle with label scarcity and class imbalance. To address these\nlimitations, this paper proposes a novel Self-Supervised Transformer\n(SSTransformer) framework for automatically detecting unusable shared bikes,\nleveraging ST features extracted from GPS trajectories and trip records. The\nmodel incorporates a self-supervised pre-training strategy to enhance its\nfeature extraction capabilities, followed by fine-tuning for efficient status\nrecognition. In the pre-training phase, the Transformer encoder learns\ngeneralized representations of bike movement via a self-supervised objective;\nin the fine-tuning phase, the encoder is adapted to a downstream binary\nclassification task. Comprehensive experiments on a real-world dataset of\n10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate\nthat SSTransformer significantly outperforms traditional machine learning,\nensemble learning, and deep learning baselines, achieving the best accuracy\n(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the\neffectiveness of self-supervised Transformer on ST data for capturing complex\nanomalies in BSS, paving the way toward more reliable and scalable maintenance\nsolutions for shared mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of bike-sharing systems (BSS) has greatly improved urban\n\"last-mile\" connectivity, yet large-scale deployments face escalating\noperational challenges, particularly in detecting faulty bikes. Existing\ndetection approaches either rely on static model-based thresholds that overlook\ndynamic spatiotemporal (ST) usage patterns or employ supervised learning\nmethods that struggle with label scarcity and class imbalance. To address these\nlimitations, this paper proposes a novel Self-Supervised Transformer\n(SSTransformer) framework for automatically detecting unusable shared bikes,\nleveraging ST features extracted from GPS trajectories and trip records. The\nmodel incorporates a self-supervised pre-training strategy to enhance its\nfeature extraction capabilities, followed by fine-tuning for efficient status\nrecognition. In the pre-training phase, the Transformer encoder learns\ngeneralized representations of bike movement via a self-supervised objective;\nin the fine-tuning phase, the encoder is adapted to a downstream binary\nclassification task. Comprehensive experiments on a real-world dataset of\n10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate\nthat SSTransformer significantly outperforms traditional machine learning,\nensemble learning, and deep learning baselines, achieving the best accuracy\n(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the\neffectiveness of self-supervised Transformer on ST data for capturing complex\nanomalies in BSS, paving the way toward more reliable and scalable maintenance\nsolutions for shared mobility."
                },
                "authors": [
                    {
                        "name": "Yin Huang"
                    },
                    {
                        "name": "Yongqi Dong"
                    },
                    {
                        "name": "Youhua Tang"
                    },
                    {
                        "name": "Alvaro García Hernandez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro García Hernandez"
                },
                "author": "Alvaro García Hernandez",
                "arxiv_comment": "6 pages, 5 figures, under review by the 2025 IEEE International\n  Conference on Intelligent Transportation Systems (IEEE ITSC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00931v1",
                "updated": "2025-05-02T00:19:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    19,
                    50,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:19:50Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    19,
                    50,
                    4,
                    122,
                    0
                ],
                "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy\n  in English Language Learner Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy\n  in English Language Learner Writing"
                },
                "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings."
                },
                "authors": [
                    {
                        "name": "Timur Jaganov"
                    },
                    {
                        "name": "John Blake"
                    },
                    {
                        "name": "Julián Villegas"
                    },
                    {
                        "name": "Nicholas Carr"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carr"
                },
                "author": "Nicholas Carr",
                "arxiv_comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v3",
                "updated": "2025-05-02T00:11:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    11,
                    48,
                    4,
                    122,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation..."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00926v1",
                "updated": "2025-05-02T00:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    7,
                    35,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T00:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    0,
                    7,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study\n  on Training Dynamics and Implicit Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Learn Regular Language Recognition: A Theoretical Study\n  on Training Dynamics and Implicit Bias"
                },
                "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results."
                },
                "authors": [
                    {
                        "name": "Ruiquan Huang"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Jing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Yang"
                },
                "author": "Jing Yang",
                "arxiv_comment": "accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08954v2",
                "updated": "2025-05-01T23:50:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    23,
                    50,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-11T20:16:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    20,
                    16,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "Should you use LLMs to simulate opinions? Quality checks for early-stage\n  deliberation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should you use LLMs to simulate opinions? Quality checks for early-stage\n  deliberation"
                },
                "summary": "The emergent capabilities of large language models (LLMs) have sparked\ninterest in assessing their ability to simulate human opinions in a variety of\ncontexts, potentially serving as surrogates for human subjects in opinion\nsurveys. However, previous evaluations of this capability have depended heavily\non costly, domain-specific human survey data, and mixed empirical results about\nLLM effectiveness create uncertainty for managers about whether investing in\nthis technology is justified in early-stage research. To address these\nchallenges, we introduce a series of quality checks to support early-stage\ndeliberation about the viability of using LLMs for simulating human opinions.\nThese checks emphasize logical constraints, model stability, and alignment with\nstakeholder expectations of model outputs, thereby reducing dependence on\nhuman-generated data in the initial stages of evaluation. We demonstrate the\nusefulness of the proposed quality control tests in the context of AI-assisted\ncontent moderation, an application that both advocates and critics of LLMs'\ncapabilities to simulate human opinion see as a desirable potential use case.\nNone of the tested models passed all quality control checks, revealing several\nfailure modes. We conclude by discussing implications of these failure modes\nand recommend how organizations can utilize our proposed tests for prompt\nengineering and in their risk management practices when considering the use of\nLLMs for opinion simulation. We make our crowdsourced dataset of claims with\nhuman and LLM annotations publicly available for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergent capabilities of large language models (LLMs) have sparked\ninterest in assessing their ability to simulate human opinions in a variety of\ncontexts, potentially serving as surrogates for human subjects in opinion\nsurveys. However, previous evaluations of this capability have depended heavily\non costly, domain-specific human survey data, and mixed empirical results about\nLLM effectiveness create uncertainty for managers about whether investing in\nthis technology is justified in early-stage research. To address these\nchallenges, we introduce a series of quality checks to support early-stage\ndeliberation about the viability of using LLMs for simulating human opinions.\nThese checks emphasize logical constraints, model stability, and alignment with\nstakeholder expectations of model outputs, thereby reducing dependence on\nhuman-generated data in the initial stages of evaluation. We demonstrate the\nusefulness of the proposed quality control tests in the context of AI-assisted\ncontent moderation, an application that both advocates and critics of LLMs'\ncapabilities to simulate human opinion see as a desirable potential use case.\nNone of the tested models passed all quality control checks, revealing several\nfailure modes. We conclude by discussing implications of these failure modes\nand recommend how organizations can utilize our proposed tests for prompt\nengineering and in their risk management practices when considering the use of\nLLMs for opinion simulation. We make our crowdsourced dataset of claims with\nhuman and LLM annotations publicly available for future research."
                },
                "authors": [
                    {
                        "name": "Terrence Neumann"
                    },
                    {
                        "name": "Maria De-Arteaga"
                    },
                    {
                        "name": "Sina Fazelpour"
                    }
                ],
                "author_detail": {
                    "name": "Sina Fazelpour"
                },
                "author": "Sina Fazelpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00917v1",
                "updated": "2025-05-01T23:33:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    23,
                    33,
                    57,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T23:33:57Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    23,
                    33,
                    57,
                    3,
                    121,
                    0
                ],
                "title": "Multivariate Conformal Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Conformal Selection"
                },
                "summary": "Selecting high-quality candidates from large datasets is critical in\napplications such as drug discovery, precision medicine, and alignment of large\nlanguage models (LLMs). While Conformal Selection (CS) provides rigorous\nuncertainty quantification, it is limited to univariate responses and scalar\ncriteria. To address this issue, we propose Multivariate Conformal Selection\n(mCS), a generalization of CS designed for multivariate response settings. Our\nmethod introduces regional monotonicity and employs multivariate nonconformity\nscores to construct conformal p-values, enabling finite-sample False Discovery\nRate (FDR) control. We present two variants: mCS-dist, using distance-based\nscores, and mCS-learn, which learns optimal scores via differentiable\noptimization. Experiments on simulated and real-world datasets demonstrate that\nmCS significantly improves selection power while maintaining FDR control,\nestablishing it as a robust framework for multivariate selection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting high-quality candidates from large datasets is critical in\napplications such as drug discovery, precision medicine, and alignment of large\nlanguage models (LLMs). While Conformal Selection (CS) provides rigorous\nuncertainty quantification, it is limited to univariate responses and scalar\ncriteria. To address this issue, we propose Multivariate Conformal Selection\n(mCS), a generalization of CS designed for multivariate response settings. Our\nmethod introduces regional monotonicity and employs multivariate nonconformity\nscores to construct conformal p-values, enabling finite-sample False Discovery\nRate (FDR) control. We present two variants: mCS-dist, using distance-based\nscores, and mCS-learn, which learns optimal scores via differentiable\noptimization. Experiments on simulated and real-world datasets demonstrate that\nmCS significantly improves selection power while maintaining FDR control,\nestablishing it as a robust framework for multivariate selection tasks."
                },
                "authors": [
                    {
                        "name": "Tian Bai"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Xiang Yu"
                    },
                    {
                        "name": "Archer Y. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Archer Y. Yang"
                },
                "author": "Archer Y. Yang",
                "arxiv_comment": "25 pages, 4 figures. Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13068v2",
                "updated": "2025-05-01T23:02:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    23,
                    2,
                    35,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-17T16:29:08Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    29,
                    8,
                    3,
                    107,
                    0
                ],
                "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\n  Classification Models"
                },
                "summary": "This study investigates the relationship between deep learning (DL) model\naccuracy and expert agreement in classifying crash narratives. We evaluate five\nDL models -- including BERT variants, USE, and a zero-shot classifier --\nagainst expert labels and narratives, and extend the analysis to four large\nlanguage models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal\nan inverse relationship: models with higher technical accuracy often show lower\nagreement with human experts, while LLMs demonstrate stronger expert alignment\ndespite lower accuracy. We use Cohen's Kappa and Principal Component Analysis\n(PCA) to quantify and visualize model-expert agreement, and employ SHAP\nanalysis to explain misclassifications. Results show that expert-aligned models\nrely more on contextual and temporal cues than location-specific keywords.\nThese findings suggest that accuracy alone is insufficient for safety-critical\nNLP tasks. We argue for incorporating expert agreement into model evaluation\nframeworks and highlight the potential of LLMs as interpretable tools in crash\nanalysis pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the relationship between deep learning (DL) model\naccuracy and expert agreement in classifying crash narratives. We evaluate five\nDL models -- including BERT variants, USE, and a zero-shot classifier --\nagainst expert labels and narratives, and extend the analysis to four large\nlanguage models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal\nan inverse relationship: models with higher technical accuracy often show lower\nagreement with human experts, while LLMs demonstrate stronger expert alignment\ndespite lower accuracy. We use Cohen's Kappa and Principal Component Analysis\n(PCA) to quantify and visualize model-expert agreement, and employ SHAP\nanalysis to explain misclassifications. Results show that expert-aligned models\nrely more on contextual and temporal cues than location-specific keywords.\nThese findings suggest that accuracy alone is insufficient for safety-critical\nNLP tasks. We argue for incorporating expert agreement into model evaluation\nframeworks and highlight the potential of LLMs as interpretable tools in crash\nanalysis pipelines."
                },
                "authors": [
                    {
                        "name": "Sudesh Ramesh Bhagat"
                    },
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00903v1",
                "updated": "2025-05-01T22:47:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    47,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:47:06Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    47,
                    6,
                    3,
                    121,
                    0
                ],
                "title": "NeMo-Inspector: A Visualization Tool for LLM Generation Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeMo-Inspector: A Visualization Tool for LLM Generation Analysis"
                },
                "summary": "Adapting Large Language Models (LLMs) to novel tasks and enhancing their\noverall capabilities often requires large, high-quality training datasets.\nSynthetic data, generated at scale, serves a valuable alternative when\nreal-world data is scarce or difficult to obtain. However, ensuring the quality\nof synthetic datasets is challenging, as developers must manually inspect and\nrefine numerous samples to identify errors and areas for improvement. This\nprocess is time-consuming and requires specialized tools. We introduce\nNeMo-Inspector, an open-source tool designed to simplify the analysis of\nsynthetic datasets with integrated inference capabilities. We demonstrate its\neffectiveness through two real-world cases. Analysis and cleaning of the\nsynthetically generated GSM-Plus dataset with NeMo-Inspector led to a\nsignificant decrease in low-quality samples from 46.99% to 19.51%. The tool\nalso helped identify and correct generation errors in OpenMath models,\nimproving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K\ndataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from\nNemotron-4-340B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models (LLMs) to novel tasks and enhancing their\noverall capabilities often requires large, high-quality training datasets.\nSynthetic data, generated at scale, serves a valuable alternative when\nreal-world data is scarce or difficult to obtain. However, ensuring the quality\nof synthetic datasets is challenging, as developers must manually inspect and\nrefine numerous samples to identify errors and areas for improvement. This\nprocess is time-consuming and requires specialized tools. We introduce\nNeMo-Inspector, an open-source tool designed to simplify the analysis of\nsynthetic datasets with integrated inference capabilities. We demonstrate its\neffectiveness through two real-world cases. Analysis and cleaning of the\nsynthetically generated GSM-Plus dataset with NeMo-Inspector led to a\nsignificant decrease in low-quality samples from 46.99% to 19.51%. The tool\nalso helped identify and correct generation errors in OpenMath models,\nimproving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K\ndataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from\nNemotron-4-340B."
                },
                "authors": [
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Bakhturina"
                },
                "author": "Evelina Bakhturina",
                "arxiv_comment": "Presented at the NAACL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16721v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16721v3",
                "updated": "2025-05-01T22:22:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    22,
                    43,
                    3,
                    121,
                    0
                ],
                "published": "2024-11-23T02:17:17Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    2,
                    17,
                    17,
                    5,
                    328,
                    0
                ],
                "title": "Steering Away from Harm: An Adaptive Approach to Defending Vision\n  Language Model Against Jailbreaks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Away from Harm: An Adaptive Approach to Defending Vision\n  Language Model Against Jailbreaks"
                },
                "summary": "Vision Language Models (VLMs) can produce unintended and harmful content when\nexposed to adversarial attacks, particularly because their vision capabilities\ncreate new vulnerabilities. Existing defenses, such as input preprocessing,\nadversarial training, and response evaluation-based methods, are often\nimpractical for real-world deployment due to their high costs. To address this\nchallenge, we propose ASTRA, an efficient and effective defense by adaptively\nsteering models away from adversarial feature directions to resist VLM attacks.\nOur key procedures involve finding transferable steering vectors representing\nthe direction of harmful response and applying adaptive activation steering to\nremove these directions at inference time. To create effective steering\nvectors, we randomly ablate the visual tokens from the adversarial images and\nidentify those most strongly associated with jailbreaks. These tokens are then\nused to construct steering vectors. During inference, we perform the adaptive\nsteering method that involves the projection between the steering vectors and\ncalibrated activation, resulting in little performance drops on benign inputs\nwhile strongly avoiding harmful outputs under adversarial inputs. Extensive\nexperiments across multiple models and baselines demonstrate our\nstate-of-the-art performance and high efficiency in mitigating jailbreak risks.\nAdditionally, ASTRA exhibits good transferability, defending against unseen\nattacks (i.e., structured-based attack, perturbation-based attack with project\ngradient descent variants, and text-only attack). Our code is available at\n\\url{https://github.com/ASTRAL-Group/ASTRA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) can produce unintended and harmful content when\nexposed to adversarial attacks, particularly because their vision capabilities\ncreate new vulnerabilities. Existing defenses, such as input preprocessing,\nadversarial training, and response evaluation-based methods, are often\nimpractical for real-world deployment due to their high costs. To address this\nchallenge, we propose ASTRA, an efficient and effective defense by adaptively\nsteering models away from adversarial feature directions to resist VLM attacks.\nOur key procedures involve finding transferable steering vectors representing\nthe direction of harmful response and applying adaptive activation steering to\nremove these directions at inference time. To create effective steering\nvectors, we randomly ablate the visual tokens from the adversarial images and\nidentify those most strongly associated with jailbreaks. These tokens are then\nused to construct steering vectors. During inference, we perform the adaptive\nsteering method that involves the projection between the steering vectors and\ncalibrated activation, resulting in little performance drops on benign inputs\nwhile strongly avoiding harmful outputs under adversarial inputs. Extensive\nexperiments across multiple models and baselines demonstrate our\nstate-of-the-art performance and high efficiency in mitigating jailbreak risks.\nAdditionally, ASTRA exhibits good transferability, defending against unseen\nattacks (i.e., structured-based attack, perturbation-based attack with project\ngradient descent variants, and text-only attack). Our code is available at\n\\url{https://github.com/ASTRAL-Group/ASTRA}."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16721v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16721v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19108v2",
                "updated": "2025-05-01T22:17:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    17,
                    1,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-27T05:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    5,
                    0,
                    21,
                    6,
                    117,
                    0
                ],
                "title": "A Multi-Language Perspective on the Robustness of LLM Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Language Perspective on the Robustness of LLM Code Generation"
                },
                "summary": "Large language models have gained significant traction and popularity in\nrecent times, extending their usage to code-generation tasks. While this field\nhas garnered considerable attention, the exploration of testing and evaluating\nthe robustness of code generation models remains an ongoing endeavor. Previous\nstudies have primarily focused on code generation models specifically for the\nPython language, overlooking other widely used programming languages. In this\nresearch, we conduct a comprehensive comparative analysis to assess the\nrobustness performance of several prominent code generation models.\nFurthermore, we investigate how their performance varies across different\nprogramming languages. To accomplish this, we introduce perturbations in four\nkey areas of the prompt: DocString, function name, syntax, and format. We have\ncompiled and released a dedicated dataset for this purpose. This work presents\nour experimental findings, shedding light on the performance of code generation\nmodels in various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have gained significant traction and popularity in\nrecent times, extending their usage to code-generation tasks. While this field\nhas garnered considerable attention, the exploration of testing and evaluating\nthe robustness of code generation models remains an ongoing endeavor. Previous\nstudies have primarily focused on code generation models specifically for the\nPython language, overlooking other widely used programming languages. In this\nresearch, we conduct a comprehensive comparative analysis to assess the\nrobustness performance of several prominent code generation models.\nFurthermore, we investigate how their performance varies across different\nprogramming languages. To accomplish this, we introduce perturbations in four\nkey areas of the prompt: DocString, function name, syntax, and format. We have\ncompiled and released a dedicated dataset for this purpose. This work presents\nour experimental findings, shedding light on the performance of code generation\nmodels in various scenarios."
                },
                "authors": [
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Zishuo Ding"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00886v1",
                "updated": "2025-05-01T22:02:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    2,
                    46,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:02:46Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    2,
                    46,
                    3,
                    121,
                    0
                ],
                "title": "Towards Explainable Temporal User Profiling with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Temporal User Profiling with LLMs"
                },
                "summary": "Accurately modeling user preferences is vital not only for improving\nrecommendation performance but also for enhancing transparency in recommender\nsystems. Conventional user profiling methods, such as averaging item\nembeddings, often overlook the evolving, nuanced nature of user interests,\nparticularly the interplay between short-term and long-term preferences. In\nthis work, we leverage large language models (LLMs) to generate natural\nlanguage summaries of users' interaction histories, distinguishing recent\nbehaviors from more persistent tendencies. Our framework not only models\ntemporal user preferences but also produces natural language profiles that can\nbe used to explain recommendations in an interpretable manner. These textual\nprofiles are encoded via a pre-trained model, and an attention mechanism\ndynamically fuses the short-term and long-term embeddings into a comprehensive\nuser representation. Beyond boosting recommendation accuracy over multiple\nbaselines, our approach naturally supports explainability: the interpretable\ntext summaries and attention weights can be exposed to end users, offering\ninsights into why specific items are suggested. Experiments on real-world\ndatasets underscore both the performance gains and the promise of generating\nclearer, more transparent justifications for content-based recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately modeling user preferences is vital not only for improving\nrecommendation performance but also for enhancing transparency in recommender\nsystems. Conventional user profiling methods, such as averaging item\nembeddings, often overlook the evolving, nuanced nature of user interests,\nparticularly the interplay between short-term and long-term preferences. In\nthis work, we leverage large language models (LLMs) to generate natural\nlanguage summaries of users' interaction histories, distinguishing recent\nbehaviors from more persistent tendencies. Our framework not only models\ntemporal user preferences but also produces natural language profiles that can\nbe used to explain recommendations in an interpretable manner. These textual\nprofiles are encoded via a pre-trained model, and an attention mechanism\ndynamically fuses the short-term and long-term embeddings into a comprehensive\nuser representation. Beyond boosting recommendation accuracy over multiple\nbaselines, our approach naturally supports explainability: the interpretable\ntext summaries and attention weights can be exposed to end users, offering\ninsights into why specific items are suggested. Experiments on real-world\ndatasets underscore both the performance gains and the promise of generating\nclearer, more transparent justifications for content-based recommendations."
                },
                "authors": [
                    {
                        "name": "Milad Sabouri"
                    },
                    {
                        "name": "Masoud Mansoury"
                    },
                    {
                        "name": "Kun Lin"
                    },
                    {
                        "name": "Bamshad Mobasher"
                    }
                ],
                "author_detail": {
                    "name": "Bamshad Mobasher"
                },
                "author": "Bamshad Mobasher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00875v1",
                "updated": "2025-05-01T21:37:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    21,
                    37,
                    30,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T21:37:30Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    21,
                    37,
                    30,
                    3,
                    121,
                    0
                ],
                "title": "Thoughts without Thinking: Reconsidering the Explanatory Value of\n  Chain-of-Thought Reasoning in LLMs through Agentic Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thoughts without Thinking: Reconsidering the Explanatory Value of\n  Chain-of-Thought Reasoning in LLMs through Agentic Pipelines"
                },
                "summary": "Agentic pipelines present novel challenges and opportunities for\nhuman-centered explainability. The HCXAI community is still grappling with how\nbest to make the inner workings of LLMs transparent in actionable ways. Agentic\npipelines consist of multiple LLMs working in cooperation with minimal human\ncontrol. In this research paper, we present early findings from an agentic\npipeline implementation of a perceptive task guidance system. Through\nquantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)\nreasoning, a common vehicle for explainability in LLMs, operates within agentic\npipelines. We demonstrate that CoT reasoning alone does not lead to better\noutputs, nor does it offer explainability, as it tends to produce explanations\nwithout explainability, in that they do not improve the ability of end users to\nbetter understand systems or achieve their goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic pipelines present novel challenges and opportunities for\nhuman-centered explainability. The HCXAI community is still grappling with how\nbest to make the inner workings of LLMs transparent in actionable ways. Agentic\npipelines consist of multiple LLMs working in cooperation with minimal human\ncontrol. In this research paper, we present early findings from an agentic\npipeline implementation of a perceptive task guidance system. Through\nquantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)\nreasoning, a common vehicle for explainability in LLMs, operates within agentic\npipelines. We demonstrate that CoT reasoning alone does not lead to better\noutputs, nor does it offer explainability, as it tends to produce explanations\nwithout explainability, in that they do not improve the ability of end users to\nbetter understand systems or achieve their goals."
                },
                "authors": [
                    {
                        "name": "Ramesh Manuvinakurike"
                    },
                    {
                        "name": "Emanuel Moss"
                    },
                    {
                        "name": "Elizabeth Anne Watkins"
                    },
                    {
                        "name": "Saurav Sahay"
                    },
                    {
                        "name": "Giuseppe Raffa"
                    },
                    {
                        "name": "Lama Nachman"
                    }
                ],
                "author_detail": {
                    "name": "Lama Nachman"
                },
                "author": "Lama Nachman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20578v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20578v4",
                "updated": "2025-05-01T21:31:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    21,
                    31,
                    38,
                    3,
                    121,
                    0
                ],
                "published": "2025-03-26T14:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation"
                },
                "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis."
                },
                "authors": [
                    {
                        "name": "Alif Al Hasan"
                    },
                    {
                        "name": "Subarna Saha"
                    },
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Tarannum Shaila Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Tarannum Shaila Zaman"
                },
                "author": "Tarannum Shaila Zaman",
                "arxiv_journal_ref": "The First International Workshop on Large Language Model-Oriented\n  Empirical Research (LLanMER 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20578v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20578v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05174v2",
                "updated": "2025-05-01T20:51:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    51,
                    22,
                    3,
                    121,
                    0
                ],
                "published": "2025-02-07T18:57:49Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    57,
                    49,
                    4,
                    38,
                    0
                ],
                "title": "MELON: Provable Indirect Prompt Injection Defense via Masked\n  Re-execution and Tool Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MELON: Provable Indirect Prompt Injection Defense via Masked\n  Re-execution and Tool Comparison"
                },
                "summary": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs."
                },
                "authors": [
                    {
                        "name": "Kaijie Zhu"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15507v2",
                "updated": "2025-05-01T20:40:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    40,
                    32,
                    3,
                    121,
                    0
                ],
                "published": "2025-02-21T15:04:48Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    15,
                    4,
                    48,
                    4,
                    52,
                    0
                ],
                "title": "Activation Steering in Neural Theorem Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering in Neural Theorem Provers"
                },
                "summary": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shashank Kirtania"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Kirtania"
                },
                "author": "Shashank Kirtania",
                "arxiv_comment": "incorrect explanation for a concept, need to revise and update!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00853v1",
                "updated": "2025-05-01T20:36:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    36,
                    19,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T20:36:19Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    36,
                    19,
                    3,
                    121,
                    0
                ],
                "title": "LLM Ethics Benchmark: A Three-Dimensional Assessment System for\n  Evaluating Moral Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Ethics Benchmark: A Three-Dimensional Assessment System for\n  Evaluating Moral Reasoning in Large Language Models"
                },
                "summary": "This study establishes a novel framework for systematically evaluating the\nmoral reasoning capabilities of large language models (LLMs) as they\nincreasingly integrate into critical societal domains. Current assessment\nmethodologies lack the precision needed to evaluate nuanced ethical\ndecision-making in AI systems, creating significant accountability gaps. Our\nframework addresses this challenge by quantifying alignment with human ethical\nstandards through three dimensions: foundational moral principles, reasoning\nrobustness, and value consistency across diverse scenarios. This approach\nenables precise identification of ethical strengths and weaknesses in LLMs,\nfacilitating targeted improvements and stronger alignment with societal values.\nTo promote transparency and collaborative advancement in ethical AI\ndevelopment, we are publicly releasing both our benchmark datasets and\nevaluation codebase at https://github.com/\nThe-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study establishes a novel framework for systematically evaluating the\nmoral reasoning capabilities of large language models (LLMs) as they\nincreasingly integrate into critical societal domains. Current assessment\nmethodologies lack the precision needed to evaluate nuanced ethical\ndecision-making in AI systems, creating significant accountability gaps. Our\nframework addresses this challenge by quantifying alignment with human ethical\nstandards through three dimensions: foundational moral principles, reasoning\nrobustness, and value consistency across diverse scenarios. This approach\nenables precise identification of ethical strengths and weaknesses in LLMs,\nfacilitating targeted improvements and stronger alignment with societal values.\nTo promote transparency and collaborative advancement in ethical AI\ndevelopment, we are publicly releasing both our benchmark datasets and\nevaluation codebase at https://github.com/\nThe-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Abhejay Murali"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17347v2",
                "updated": "2025-05-01T20:28:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    28,
                    39,
                    3,
                    121,
                    0
                ],
                "published": "2024-04-26T11:51:53Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    11,
                    51,
                    53,
                    4,
                    117,
                    0
                ],
                "title": "InspectorRAGet: An Introspection Platform for RAG Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InspectorRAGet: An Introspection Platform for RAG Evaluation"
                },
                "summary": "Large Language Models (LLM) have become a popular approach for implementing\nRetrieval Augmented Generation (RAG) systems, and a significant amount of\neffort has been spent on building good models and metrics. In spite of\nincreased recognition of the need for rigorous evaluation of RAG systems, few\ntools exist that go beyond the creation of model output and automatic\ncalculation. We present InspectorRAGet, an introspection platform for\nperforming a comprehensive analysis of the quality of RAG system output.\nInspectorRAGet allows the user to analyze aggregate and instance-level\nperformance of RAG systems, using both human and algorithmic metrics as well as\nannotator quality. InspectorRAGet is suitable for multiple use cases and is\navailable publicly to the community. A live instance of the platform is\navailable at https://ibm.biz/InspectorRAGet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have become a popular approach for implementing\nRetrieval Augmented Generation (RAG) systems, and a significant amount of\neffort has been spent on building good models and metrics. In spite of\nincreased recognition of the need for rigorous evaluation of RAG systems, few\ntools exist that go beyond the creation of model output and automatic\ncalculation. We present InspectorRAGet, an introspection platform for\nperforming a comprehensive analysis of the quality of RAG system output.\nInspectorRAGet allows the user to analyze aggregate and instance-level\nperformance of RAG systems, using both human and algorithmic metrics as well as\nannotator quality. InspectorRAGet is suitable for multiple use cases and is\navailable publicly to the community. A live instance of the platform is\navailable at https://ibm.biz/InspectorRAGet."
                },
                "authors": [
                    {
                        "name": "Kshitij Fadnis"
                    },
                    {
                        "name": "Siva Sankalp Patel"
                    },
                    {
                        "name": "Odellia Boni"
                    },
                    {
                        "name": "Yannis Katsis"
                    },
                    {
                        "name": "Sara Rosenthal"
                    },
                    {
                        "name": "Benjamin Sznajder"
                    },
                    {
                        "name": "Marina Danilevsky"
                    }
                ],
                "author_detail": {
                    "name": "Marina Danilevsky"
                },
                "author": "Marina Danilevsky",
                "arxiv_comment": "Published at NAACL2025 Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00850v1",
                "updated": "2025-05-01T20:23:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    23,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T20:23:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    23,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "ICQuant: Index Coding enables Low-bit LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICQuant: Index Coding enables Low-bit LLM Quantization"
                },
                "summary": "The rapid deployment of Large Language Models (LLMs) highlights the need for\nefficient low-bit post-training quantization (PTQ), due to their high memory\ncosts. A key challenge in weight quantization is the presence of outliers,\nwhich inflate quantization ranges and lead to large errors. While a number of\noutlier suppression techniques have been proposed, they either: fail to\neffectively shrink the quantization range, or incur (relatively) high bit\noverhead. In this paper, we present ICQuant, a novel framework that leverages\noutlier statistics to design an efficient index coding scheme for outlier-aware\nweight-only quantization. Compared to existing outlier suppression techniques\nrequiring $\\approx 1$ bit overhead to halve the quantization range, ICQuant\nrequires only $\\approx 0.3$ bits; a significant saving in extreme compression\nregimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing\nquantizers to eliminate outliers, improving the quantization quality. Using\njust 2.3 bits per weight and simple scalar quantizers, ICQuant improves the\nzero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%\nrelative to QTIP and QuIP#; and it achieves comparable performance to the\nbest-known fine-tuned quantizer (PV-tuning) without fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of Large Language Models (LLMs) highlights the need for\nefficient low-bit post-training quantization (PTQ), due to their high memory\ncosts. A key challenge in weight quantization is the presence of outliers,\nwhich inflate quantization ranges and lead to large errors. While a number of\noutlier suppression techniques have been proposed, they either: fail to\neffectively shrink the quantization range, or incur (relatively) high bit\noverhead. In this paper, we present ICQuant, a novel framework that leverages\noutlier statistics to design an efficient index coding scheme for outlier-aware\nweight-only quantization. Compared to existing outlier suppression techniques\nrequiring $\\approx 1$ bit overhead to halve the quantization range, ICQuant\nrequires only $\\approx 0.3$ bits; a significant saving in extreme compression\nregimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing\nquantizers to eliminate outliers, improving the quantization quality. Using\njust 2.3 bits per weight and simple scalar quantizers, ICQuant improves the\nzero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%\nrelative to QTIP and QuIP#; and it achieves comparable performance to the\nbest-known fine-tuned quantizer (PV-tuning) without fine-tuning."
                },
                "authors": [
                    {
                        "name": "Xinlin Li"
                    },
                    {
                        "name": "Osama Hanna"
                    },
                    {
                        "name": "Christina Fragouli"
                    },
                    {
                        "name": "Suhas Diggavi"
                    }
                ],
                "author_detail": {
                    "name": "Suhas Diggavi"
                },
                "author": "Suhas Diggavi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01976v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01976v5",
                "updated": "2025-05-01T20:11:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    11,
                    36,
                    3,
                    121,
                    0
                ],
                "published": "2025-02-04T03:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    36,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing"
                },
                "summary": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel Collaborative Inference with Token-lEvel Routing (CITER)\nframework that enables efficient collaboration between small and large language\nmodels (SLMs \\& LLMs) through a token-level routing strategy. Specifically,\nCITER routes non-critical tokens to an SLM for efficiency and routes critical\ntokens to an LLM for generalization quality. We formulate router training as a\npolicy optimization, where the router receives rewards based on both the\nquality of predictions and the inference costs of generation. This allows the\nrouter to learn to predict token-level routing scores and make routing\ndecisions based on both the current token and the future impact of its\ndecisions. To further accelerate the reward evaluation process, we introduce a\nshortcut which significantly reduces the costs of the reward estimation and\nimproving the practicality of our approach. Extensive experiments on five\nbenchmark datasets demonstrate that CITER reduces the inference costs while\npreserving high-quality generation, offering a promising solution for real-time\nand resource-constrained applications. Our data and code are available at\nhttps://github.com/aiming-lab/CITER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel Collaborative Inference with Token-lEvel Routing (CITER)\nframework that enables efficient collaboration between small and large language\nmodels (SLMs \\& LLMs) through a token-level routing strategy. Specifically,\nCITER routes non-critical tokens to an SLM for efficiency and routes critical\ntokens to an LLM for generalization quality. We formulate router training as a\npolicy optimization, where the router receives rewards based on both the\nquality of predictions and the inference costs of generation. This allows the\nrouter to learn to predict token-level routing scores and make routing\ndecisions based on both the current token and the future impact of its\ndecisions. To further accelerate the reward evaluation process, we introduce a\nshortcut which significantly reduces the costs of the reward estimation and\nimproving the practicality of our approach. Extensive experiments on five\nbenchmark datasets demonstrate that CITER reduces the inference costs while\npreserving high-quality generation, offering a promising solution for real-time\nand resource-constrained applications. Our data and code are available at\nhttps://github.com/aiming-lab/CITER."
                },
                "authors": [
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Yixiao Chen"
                    },
                    {
                        "name": "Weitong Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01976v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01976v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00843v1",
                "updated": "2025-05-01T20:09:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    9,
                    48,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T20:09:48Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    9,
                    48,
                    3,
                    121,
                    0
                ],
                "title": "OET: Optimization-based prompt injection Evaluation Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OET: Optimization-based prompt injection Evaluation Toolkit"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, enabling their widespread\nadoption across various domains. However, their susceptibility to prompt\ninjection attacks poses significant security risks, as adversarial inputs can\nmanipulate model behavior and override intended instructions. Despite numerous\ndefense strategies, a standardized framework to rigorously evaluate their\neffectiveness, especially under adaptive adversarial scenarios, is lacking. To\naddress this gap, we introduce OET, an optimization-based evaluation toolkit\nthat systematically benchmarks prompt injection attacks and defenses across\ndiverse datasets using an adaptive testing framework. Our toolkit features a\nmodular workflow that facilitates adversarial string generation, dynamic attack\nexecution, and comprehensive result analysis, offering a unified platform for\nassessing adversarial robustness. Crucially, the adaptive testing framework\nleverages optimization methods with both white-box and black-box access to\ngenerate worst-case adversarial examples, thereby enabling strict red-teaming\nevaluations. Extensive experiments underscore the limitations of current\ndefense mechanisms, with some models remaining susceptible even after\nimplementing security enhancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, enabling their widespread\nadoption across various domains. However, their susceptibility to prompt\ninjection attacks poses significant security risks, as adversarial inputs can\nmanipulate model behavior and override intended instructions. Despite numerous\ndefense strategies, a standardized framework to rigorously evaluate their\neffectiveness, especially under adaptive adversarial scenarios, is lacking. To\naddress this gap, we introduce OET, an optimization-based evaluation toolkit\nthat systematically benchmarks prompt injection attacks and defenses across\ndiverse datasets using an adaptive testing framework. Our toolkit features a\nmodular workflow that facilitates adversarial string generation, dynamic attack\nexecution, and comprehensive result analysis, offering a unified platform for\nassessing adversarial robustness. Crucially, the adaptive testing framework\nleverages optimization methods with both white-box and black-box access to\ngenerate worst-case adversarial examples, thereby enabling strict red-teaming\nevaluations. Extensive experiments underscore the limitations of current\ndefense mechanisms, with some models remaining susceptible even after\nimplementing security enhancements."
                },
                "authors": [
                    {
                        "name": "Jinsheng Pan"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00841v1",
                "updated": "2025-05-01T20:01:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    1,
                    7,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T20:01:07Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    20,
                    1,
                    7,
                    3,
                    121,
                    0
                ],
                "title": "From Texts to Shields: Convergence of Large Language Models and\n  Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Texts to Shields: Convergence of Large Language Models and\n  Cybersecurity"
                },
                "summary": "This report explores the convergence of large language models (LLMs) and\ncybersecurity, synthesizing interdisciplinary insights from network security,\nartificial intelligence, formal methods, and human-centered design. It examines\nemerging applications of LLMs in software and network security, 5G\nvulnerability analysis, and generative security engineering. The report\nhighlights the role of agentic LLMs in automating complex tasks, improving\noperational efficiency, and enabling reasoning-driven security analytics.\nSocio-technical challenges associated with the deployment of LLMs -- including\ntrust, transparency, and ethical considerations -- can be addressed through\nstrategies such as human-in-the-loop systems, role-specific training, and\nproactive robustness testing. The report further outlines critical research\nchallenges in ensuring interpretability, safety, and fairness in LLM-based\nsystems, particularly in high-stakes domains. By integrating technical advances\nwith organizational and societal considerations, this report presents a\nforward-looking research agenda for the secure and effective adoption of LLMs\nin cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report explores the convergence of large language models (LLMs) and\ncybersecurity, synthesizing interdisciplinary insights from network security,\nartificial intelligence, formal methods, and human-centered design. It examines\nemerging applications of LLMs in software and network security, 5G\nvulnerability analysis, and generative security engineering. The report\nhighlights the role of agentic LLMs in automating complex tasks, improving\noperational efficiency, and enabling reasoning-driven security analytics.\nSocio-technical challenges associated with the deployment of LLMs -- including\ntrust, transparency, and ethical considerations -- can be addressed through\nstrategies such as human-in-the-loop systems, role-specific training, and\nproactive robustness testing. The report further outlines critical research\nchallenges in ensuring interpretability, safety, and fairness in LLM-based\nsystems, particularly in high-stakes domains. By integrating technical advances\nwith organizational and societal considerations, this report presents a\nforward-looking research agenda for the secure and effective adoption of LLMs\nin cybersecurity."
                },
                "authors": [
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Ya-Ting Yang"
                    },
                    {
                        "name": "Yunian Pan"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00831v1",
                "updated": "2025-05-01T19:44:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    44,
                    36,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    44,
                    36,
                    3,
                    121,
                    0
                ],
                "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation"
                },
                "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."
                },
                "authors": [
                    {
                        "name": "Quang P. M. Pham"
                    },
                    {
                        "name": "Khoi T. N. Nguyen"
                    },
                    {
                        "name": "Nhi H. Doan"
                    },
                    {
                        "name": "Cuong A. Pham"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Dezhen Song"
                    }
                ],
                "author_detail": {
                    "name": "Dezhen Song"
                },
                "author": "Dezhen Song",
                "arxiv_comment": "Paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00825v1",
                "updated": "2025-05-01T19:31:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    31,
                    45,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:31:45Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    31,
                    45,
                    3,
                    121,
                    0
                ],
                "title": "Near-optimal Sensor Placement for Detecting Stochastic Target\n  Trajectories in Barrier Coverage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-optimal Sensor Placement for Detecting Stochastic Target\n  Trajectories in Barrier Coverage Systems"
                },
                "summary": "This paper addresses the deployment of sensors for a 2-D barrier coverage\nsystem. The challenge is to compute near-optimal sensor placements for\ndetecting targets whose trajectories follow a log-Gaussian Cox line process. We\nexplore sensor deployment in a transformed space, where linear target\ntrajectories are represented as points. While this space simplifies handling\nthe line process, the spatial functions representing sensor performance (i.e.\nprobability of detection) become less intuitive. To illustrate our approach, we\nfocus on positioning sensors of the barrier coverage system on the seafloor to\ndetect passing ships. Through numerical experiments using historical ship data,\nwe compute sensor locations that maximize the probability all ship passing over\nthe barrier coverage system are detected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the deployment of sensors for a 2-D barrier coverage\nsystem. The challenge is to compute near-optimal sensor placements for\ndetecting targets whose trajectories follow a log-Gaussian Cox line process. We\nexplore sensor deployment in a transformed space, where linear target\ntrajectories are represented as points. While this space simplifies handling\nthe line process, the spatial functions representing sensor performance (i.e.\nprobability of detection) become less intuitive. To illustrate our approach, we\nfocus on positioning sensors of the barrier coverage system on the seafloor to\ndetect passing ships. Through numerical experiments using historical ship data,\nwe compute sensor locations that maximize the probability all ship passing over\nthe barrier coverage system are detected."
                },
                "authors": [
                    {
                        "name": "Mingyu Kim"
                    },
                    {
                        "name": "Daniel J. Stilwell"
                    },
                    {
                        "name": "Harun Yetkin"
                    },
                    {
                        "name": "Jorge Jimenez"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Jimenez"
                },
                "author": "Jorge Jimenez",
                "arxiv_comment": "This work is published in IEEE SysCon 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00820v1",
                "updated": "2025-05-01T19:23:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    23,
                    50,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:23:50Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    23,
                    50,
                    3,
                    121,
                    0
                ],
                "title": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on\n  Large Language Models"
                },
                "summary": "Rapid advancements in artificial intelligence (AI) have enabled robots to\nperformcomplex tasks autonomously with increasing precision. However,\nmulti-robot systems (MRSs) face challenges in generalization, heterogeneity,\nand safety, especially when scaling to large-scale deployments like disaster\nresponse. Traditional approaches often lack generalization, requiring extensive\nengineering for new tasks and scenarios, and struggle with managing diverse\nrobots. To overcome these limitations, we propose a Human-in-the-loop\nMulti-Robot Collaboration Framework (HMCF) powered by large language models\n(LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot\ncapabilities, while human oversight ensures safety and reliability, intervening\nonly when necessary. Our framework seamlessly integrates human oversight, LLM\nagents, and heterogeneous robots to optimize task allocation and execution.\nEach robot is equipped with an LLM agent capable of understanding its\ncapabilities, converting tasks into executable instructions, and reducing\nhallucinations through task verification and human supervision. Simulation\nresults show that our framework outperforms state-of-the-art task planning\nmethods, achieving higher task success rates with an improvement of 4.76%.\nReal-world tests demonstrate its robust zero-shot generalization feature and\nability to handle diverse tasks and environments with minimal human\nintervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in artificial intelligence (AI) have enabled robots to\nperformcomplex tasks autonomously with increasing precision. However,\nmulti-robot systems (MRSs) face challenges in generalization, heterogeneity,\nand safety, especially when scaling to large-scale deployments like disaster\nresponse. Traditional approaches often lack generalization, requiring extensive\nengineering for new tasks and scenarios, and struggle with managing diverse\nrobots. To overcome these limitations, we propose a Human-in-the-loop\nMulti-Robot Collaboration Framework (HMCF) powered by large language models\n(LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot\ncapabilities, while human oversight ensures safety and reliability, intervening\nonly when necessary. Our framework seamlessly integrates human oversight, LLM\nagents, and heterogeneous robots to optimize task allocation and execution.\nEach robot is equipped with an LLM agent capable of understanding its\ncapabilities, converting tasks into executable instructions, and reducing\nhallucinations through task verification and human supervision. Simulation\nresults show that our framework outperforms state-of-the-art task planning\nmethods, achieving higher task success rates with an improvement of 4.76%.\nReal-world tests demonstrate its robust zero-shot generalization feature and\nability to handle diverse tasks and environments with minimal human\nintervention."
                },
                "authors": [
                    {
                        "name": "Zhaoxing Li"
                    },
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yanran Xu"
                    },
                    {
                        "name": "William Hunt"
                    },
                    {
                        "name": "Sebastian Stein"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stein"
                },
                "author": "Sebastian Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00816v1",
                "updated": "2025-05-01T19:18:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    35,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:35Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    35,
                    3,
                    121,
                    0
                ],
                "title": "Aggregating empirical evidence from data strategy studies: a case on\n  model quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggregating empirical evidence from data strategy studies: a case on\n  model quantization"
                },
                "summary": "Background: As empirical software engineering evolves, more studies adopt\ndata strategies$-$approaches that investigate digital artifacts such as models,\nsource code, or system logs rather than relying on human subjects. Synthesizing\nresults from such studies introduces new methodological challenges.\n  Aims: This study assesses the effects of model quantization on correctness\nand resource efficiency in deep learning (DL) systems. Additionally, it\nexplores the methodological implications of aggregating evidence from empirical\nstudies that adopt data strategies.\n  Method: We conducted a research synthesis of six primary studies that\nempirically evaluate model quantization. We applied the Structured Synthesis\nMethod (SSM) to aggregate the findings, which combines qualitative and\nquantitative evidence through diagrammatic modeling. A total of 19 evidence\nmodels were extracted and aggregated.\n  Results: The aggregated evidence indicates that model quantization weakly\nnegatively affects correctness metrics while consistently improving resource\nefficiency metrics, including storage size, inference latency, and GPU energy\nconsumption$-$a manageable trade-off for many DL deployment contexts. Evidence\nacross quantization techniques remains fragmented, underscoring the need for\nmore focused empirical studies per technique.\n  Conclusions: Model quantization offers substantial efficiency benefits with\nminor trade-offs in correctness, making it a suitable optimization strategy for\nresource-constrained environments. This study also demonstrates the feasibility\nof using SSM to synthesize findings from data strategy-based research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: As empirical software engineering evolves, more studies adopt\ndata strategies$-$approaches that investigate digital artifacts such as models,\nsource code, or system logs rather than relying on human subjects. Synthesizing\nresults from such studies introduces new methodological challenges.\n  Aims: This study assesses the effects of model quantization on correctness\nand resource efficiency in deep learning (DL) systems. Additionally, it\nexplores the methodological implications of aggregating evidence from empirical\nstudies that adopt data strategies.\n  Method: We conducted a research synthesis of six primary studies that\nempirically evaluate model quantization. We applied the Structured Synthesis\nMethod (SSM) to aggregate the findings, which combines qualitative and\nquantitative evidence through diagrammatic modeling. A total of 19 evidence\nmodels were extracted and aggregated.\n  Results: The aggregated evidence indicates that model quantization weakly\nnegatively affects correctness metrics while consistently improving resource\nefficiency metrics, including storage size, inference latency, and GPU energy\nconsumption$-$a manageable trade-off for many DL deployment contexts. Evidence\nacross quantization techniques remains fragmented, underscoring the need for\nmore focused empirical studies per technique.\n  Conclusions: Model quantization offers substantial efficiency benefits with\nminor trade-offs in correctness, making it a suitable optimization strategy for\nresource-constrained environments. This study also demonstrates the feasibility\nof using SSM to synthesize findings from data strategy-based research."
                },
                "authors": [
                    {
                        "name": "Santiago del Rey"
                    },
                    {
                        "name": "Paulo Sérgio Medeiros dos Santos"
                    },
                    {
                        "name": "Guilherme Horta Travassos"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "arxiv_comment": "11 pages, 3 figures, submitted to the 19th ACM/IEEE International\n  Symposium on Empirical Software Engineering and Measurement (ESEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18624v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18624v4",
                "updated": "2025-05-01T18:40:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    40,
                    41,
                    3,
                    121,
                    0
                ],
                "published": "2024-04-29T11:52:20Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    11,
                    52,
                    20,
                    0,
                    120,
                    0
                ],
                "title": "Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?"
                },
                "summary": "Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to answers, they are able to produce\nnatural language explanations, either in post-hoc or CoT settings. However, it\nis not clear to what extent they are using the input vision and text modalities\nwhen generating answers or explanations. In this work, we investigate if VLMs\nrely on their input modalities differently when they produce explanations as\nopposed to answers. We also evaluate the self-consistency of VLM decoders in\nboth post-hoc and CoT explanation settings, by extending existing unimodal\ntests and measures to VLM decoders. We find that most tested VLMs are less\nself-consistent than LLMs. Text contributions in all tested VL decoders are\nmore important than image contributions in all examined tasks. However, when\ncomparing explanation generation to answer generation, the contributions of\nimages are significantly stronger for generating explanations compared to\nanswers. This difference is even larger in CoT compared to post-hoc\nexplanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art\nVL decoders on the VALSE benchmark, which before was restricted to VL encoders.\nWe find that the tested VL decoders still struggle with most phenomena tested\nby VALSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to answers, they are able to produce\nnatural language explanations, either in post-hoc or CoT settings. However, it\nis not clear to what extent they are using the input vision and text modalities\nwhen generating answers or explanations. In this work, we investigate if VLMs\nrely on their input modalities differently when they produce explanations as\nopposed to answers. We also evaluate the self-consistency of VLM decoders in\nboth post-hoc and CoT explanation settings, by extending existing unimodal\ntests and measures to VLM decoders. We find that most tested VLMs are less\nself-consistent than LLMs. Text contributions in all tested VL decoders are\nmore important than image contributions in all examined tasks. However, when\ncomparing explanation generation to answer generation, the contributions of\nimages are significantly stronger for generating explanations compared to\nanswers. This difference is even larger in CoT compared to post-hoc\nexplanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art\nVL decoders on the VALSE benchmark, which before was restricted to VL encoders.\nWe find that the tested VL decoders still struggle with most phenomena tested\nby VALSE."
                },
                "authors": [
                    {
                        "name": "Letitia Parcalabescu"
                    },
                    {
                        "name": "Anette Frank"
                    }
                ],
                "author_detail": {
                    "name": "Anette Frank"
                },
                "author": "Anette Frank",
                "arxiv_comment": "30 pages, 8 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18624v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18624v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Txx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00776v1",
                "updated": "2025-05-01T18:12:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    12,
                    30,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:12:30Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    12,
                    30,
                    3,
                    121,
                    0
                ],
                "title": "Reasoning Capabilities and Invariability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Capabilities and Invariability of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer."
                },
                "authors": [
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Rafael Peñaloza"
                    },
                    {
                        "name": "Marco Viviani"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 23rd IEEE/WIC\n  International Conference on Web Intelligence and Intelligent Agent Technology\n  (WI-IAT 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00693v1",
                "updated": "2025-05-01T17:55:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    55,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T17:55:05Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    55,
                    5,
                    3,
                    121,
                    0
                ],
                "title": "Robotic Visual Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic Visual Instruction"
                },
                "summary": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision for robotic\ncontrol introduces challenges such as ambiguity and verbosity. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment, enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Code and\nDatasets in this paper will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision for robotic\ncontrol introduces challenges such as ambiguity and verbosity. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment, enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Code and\nDatasets in this paper will be released soon."
                },
                "authors": [
                    {
                        "name": "Yanbang Li"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiaoqi Huang"
                    },
                    {
                        "name": "Haolan Kang"
                    },
                    {
                        "name": "Guangping Bai"
                    },
                    {
                        "name": "Xianzheng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xianzheng Ma"
                },
                "author": "Xianzheng Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00681v1",
                "updated": "2025-05-01T17:41:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    41,
                    49,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T17:41:49Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    41,
                    49,
                    3,
                    121,
                    0
                ],
                "title": "MINERVA: Evaluating Complex Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINERVA: Evaluating Complex Video Reasoning"
                },
                "summary": "Multimodal LLMs are turning their focus to video benchmarks, however most\nvideo benchmarks only provide outcome supervision, with no intermediate or\ninterpretable reasoning steps. This makes it challenging to assess if models\nare truly able to combine perceptual and temporal information to reason about\nvideos, or simply get the correct answer by chance or by exploiting linguistic\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\nfor modern multimodal models. Each question in the dataset comes with 5 answer\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\nmultimodal, diverse in terms of video domain and length, and consists of\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\nprovides a challenge for frontier open-source and proprietary models. We\nperform fine-grained error analysis to identify common failure modes across\nvarious models, and create a taxonomy of reasoning errors. We use this to\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\ntraces, and find that failure modes are primarily related to temporal\nlocalization, followed by visual perception errors, as opposed to logical or\ncompleteness errors. The dataset, along with questions, answer candidates and\nreasoning traces will be publicly available under\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs are turning their focus to video benchmarks, however most\nvideo benchmarks only provide outcome supervision, with no intermediate or\ninterpretable reasoning steps. This makes it challenging to assess if models\nare truly able to combine perceptual and temporal information to reason about\nvideos, or simply get the correct answer by chance or by exploiting linguistic\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\nfor modern multimodal models. Each question in the dataset comes with 5 answer\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\nmultimodal, diverse in terms of video domain and length, and consists of\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\nprovides a challenge for frontier open-source and proprietary models. We\nperform fine-grained error analysis to identify common failure modes across\nvarious models, and create a taxonomy of reasoning errors. We use this to\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\ntraces, and find that failure modes are primarily related to temporal\nlocalization, followed by visual perception errors, as opposed to logical or\ncompleteness errors. The dataset, along with questions, answer candidates and\nreasoning traces will be publicly available under\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva."
                },
                "authors": [
                    {
                        "name": "Arsha Nagrani"
                    },
                    {
                        "name": "Sachit Menon"
                    },
                    {
                        "name": "Ahmet Iscen"
                    },
                    {
                        "name": "Shyamal Buch"
                    },
                    {
                        "name": "Ramin Mehran"
                    },
                    {
                        "name": "Nilpa Jha"
                    },
                    {
                        "name": "Anja Hauth"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Carl Vondrick"
                    },
                    {
                        "name": "Mikhail Sirotenko"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Tobias Weyand"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weyand"
                },
                "author": "Tobias Weyand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00679v1",
                "updated": "2025-05-01T17:39:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    39,
                    2,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T17:39:02Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    39,
                    2,
                    3,
                    121,
                    0
                ],
                "title": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Xinchen Yang"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]