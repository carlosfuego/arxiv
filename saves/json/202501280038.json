[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v1",
                "updated": "2025-01-23T00:57:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\n\\emph{partitioning} and \\emph{transmission}. In the partitioning phase, users\nwith identical cache profiles are partitioned into the minimum number of sets,\nsuch that users within each set can successfully decode their desired message\nfrom a joint transmission enabled by MIMO precoding. To optimally partition the\nusers, we employ the branch and bound method. In the transmission phase, each\npartition is treated as a single entity, and codewords are multicast to\npartitions with distinct cache profiles. The proposed delivery scheme is\napplicable to any partially connected network, and while the partitioning is\noptimal, the overall delivery scheme, including transmission, is heuristic.\nInterestingly, simulation results show that its performance closely\napproximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\n\\emph{partitioning} and \\emph{transmission}. In the partitioning phase, users\nwith identical cache profiles are partitioned into the minimum number of sets,\nsuch that users within each set can successfully decode their desired message\nfrom a joint transmission enabled by MIMO precoding. To optimally partition the\nusers, we employ the branch and bound method. In the transmission phase, each\npartition is treated as a single entity, and codewords are multicast to\npartitions with distinct cache profiles. The proposed delivery scheme is\napplicable to any partially connected network, and while the partitioning is\noptimal, the overall delivery scheme, including transmission, is heuristic.\nInterestingly, simulation results show that its performance closely\napproximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v1",
                "updated": "2025-01-22T07:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lily Tasi"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v1",
                "updated": "2025-01-21T03:13:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v1",
                "updated": "2025-01-19T17:33:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v1",
                "updated": "2025-01-18T19:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.14729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14729v1",
                "updated": "2025-01-24T18:59:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    59,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:59:51Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    59,
                    51,
                    4,
                    24,
                    0
                ],
                "title": "HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene\n  Understanding and Generation"
                },
                "summary": "Driving World Models (DWMs) have become essential for autonomous driving by\nenabling future scene prediction. However, existing DWMs are limited to scene\ngeneration and fail to incorporate scene understanding, which involves\ninterpreting and reasoning about the driving environment. In this paper, we\npresent a unified Driving World Model named HERMES. We seamlessly integrate 3D\nscene understanding and future scene evolution (generation) through a unified\nframework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye\nView (BEV) representation to consolidate multi-view spatial information while\npreserving geometric relationships and interactions. We also introduce world\nqueries, which incorporate world knowledge into BEV features via causal\nattention in the Large Language Model (LLM), enabling contextual enrichment for\nunderstanding and generation tasks. We conduct comprehensive studies on\nnuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our\nmethod. HERMES achieves state-of-the-art performance, reducing generation error\nby 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model\nand code will be publicly released at https://github.com/LMD0311/HERMES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving World Models (DWMs) have become essential for autonomous driving by\nenabling future scene prediction. However, existing DWMs are limited to scene\ngeneration and fail to incorporate scene understanding, which involves\ninterpreting and reasoning about the driving environment. In this paper, we\npresent a unified Driving World Model named HERMES. We seamlessly integrate 3D\nscene understanding and future scene evolution (generation) through a unified\nframework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye\nView (BEV) representation to consolidate multi-view spatial information while\npreserving geometric relationships and interactions. We also introduce world\nqueries, which incorporate world knowledge into BEV features via causal\nattention in the Large Language Model (LLM), enabling contextual enrichment for\nunderstanding and generation tasks. We conduct comprehensive studies on\nnuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our\nmethod. HERMES achieves state-of-the-art performance, reducing generation error\nby 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model\nand code will be publicly released at https://github.com/LMD0311/HERMES."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Sifan Tu"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Dingyuan Zhang"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Work in progress. The code will be available at\n  https://github.com/LMD0311/HERMES",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14723v1",
                "updated": "2025-01-24T18:58:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    58,
                    40,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:58:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    58,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering"
                },
                "summary": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys."
                },
                "authors": [
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Bradley Brown"
                    },
                    {
                        "name": "Jordan Juravsky"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18379v3",
                "updated": "2025-01-24T18:56:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    56,
                    42,
                    4,
                    24,
                    0
                ],
                "published": "2024-05-28T17:22:15Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    22,
                    15,
                    1,
                    149,
                    0
                ],
                "title": "A Note on the Prediction-Powered Bootstrap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on the Prediction-Powered Bootstrap"
                },
                "summary": "We introduce PPBoot: a bootstrap-based method for prediction-powered\ninference. PPBoot is applicable to arbitrary estimation problems and is very\nsimple to implement, essentially only requiring one application of the\nbootstrap. Through a series of examples, we demonstrate that PPBoot often\nperforms nearly identically to (and sometimes better than) the earlier PPI(++)\nmethod based on asymptotic normality$\\unicode{x2013}$when the latter is\napplicable$\\unicode{x2013}$without requiring any asymptotic characterizations.\nGiven its versatility, PPBoot could simplify and expand the scope of\napplication of prediction-powered inference to problems where central limit\ntheorems are hard to prove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PPBoot: a bootstrap-based method for prediction-powered\ninference. PPBoot is applicable to arbitrary estimation problems and is very\nsimple to implement, essentially only requiring one application of the\nbootstrap. Through a series of examples, we demonstrate that PPBoot often\nperforms nearly identically to (and sometimes better than) the earlier PPI(++)\nmethod based on asymptotic normality$\\unicode{x2013}$when the latter is\napplicable$\\unicode{x2013}$without requiring any asymptotic characterizations.\nGiven its versatility, PPBoot could simplify and expand the scope of\napplication of prediction-powered inference to problems where central limit\ntheorems are hard to prove."
                },
                "authors": [
                    {
                        "name": "Tijana Zrnic"
                    }
                ],
                "author_detail": {
                    "name": "Tijana Zrnic"
                },
                "author": "Tijana Zrnic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12880v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12880v3",
                "updated": "2025-01-24T18:56:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    56,
                    7,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-15T18:13:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    18,
                    13,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to\n  Sensitivity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to\n  Sensitivity in Large Language Models"
                },
                "summary": "As LLMs are increasingly deployed in global applications, the importance of\ncultural sensitivity becomes paramount, ensuring that users from diverse\nbackgrounds feel respected and understood. Cultural harm can arise when these\nmodels fail to align with specific cultural norms, resulting in\nmisrepresentations or violations of cultural values. This work addresses the\nchallenges of ensuring cultural sensitivity in LLMs, especially in\nsmall-parameter models that often lack the extensive training data needed to\ncapture global cultural nuances. We present two key contributions: (1) A\ncultural harm test dataset, created to assess model outputs across different\ncultural contexts through scenarios that expose potential cultural\ninsensitivities, and (2) A culturally aligned preference dataset, aimed at\nrestoring cultural sensitivity through fine-tuning based on feedback from\ndiverse annotators. These datasets facilitate the evaluation and enhancement of\nLLMs, ensuring their ethical and safe deployment across different cultural\nlandscapes. Our results show that integrating culturally aligned feedback leads\nto a marked improvement in model behavior, significantly reducing the\nlikelihood of generating culturally insensitive or harmful content. Ultimately,\nthis work paves the way for more inclusive and respectful AI systems, fostering\na future where LLMs can safely and ethically navigate the complexities of\ndiverse cultural landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are increasingly deployed in global applications, the importance of\ncultural sensitivity becomes paramount, ensuring that users from diverse\nbackgrounds feel respected and understood. Cultural harm can arise when these\nmodels fail to align with specific cultural norms, resulting in\nmisrepresentations or violations of cultural values. This work addresses the\nchallenges of ensuring cultural sensitivity in LLMs, especially in\nsmall-parameter models that often lack the extensive training data needed to\ncapture global cultural nuances. We present two key contributions: (1) A\ncultural harm test dataset, created to assess model outputs across different\ncultural contexts through scenarios that expose potential cultural\ninsensitivities, and (2) A culturally aligned preference dataset, aimed at\nrestoring cultural sensitivity through fine-tuning based on feedback from\ndiverse annotators. These datasets facilitate the evaluation and enhancement of\nLLMs, ensuring their ethical and safe deployment across different cultural\nlandscapes. Our results show that integrating culturally aligned feedback leads\nto a marked improvement in model behavior, significantly reducing the\nlikelihood of generating culturally insensitive or harmful content. Ultimately,\nthis work paves the way for more inclusive and respectful AI systems, fostering\na future where LLMs can safely and ethically navigate the complexities of\ndiverse cultural landscapes."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Hari Shrawgi"
                    },
                    {
                        "name": "Rajarshi Mandal"
                    },
                    {
                        "name": "Avik Halder"
                    },
                    {
                        "name": "Shanu Kumar"
                    },
                    {
                        "name": "Sagnik Basu"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "arxiv_comment": "Accepted at NAACL 2025 (Main track). [Project\n  Page](https://neuralsentinel.github.io/KaleidoCulture/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12880v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14719v1",
                "updated": "2025-01-24T18:51:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    51,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:51:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    51,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "Do LLMs Provide Consistent Answers to Health-Related Questions across\n  Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Provide Consistent Answers to Health-Related Questions across\n  Languages?"
                },
                "summary": "Equitable access to reliable health information is vital for public health,\nbut the quality of online health resources varies by language, raising concerns\nabout inconsistencies in Large Language Models (LLMs) for healthcare. In this\nstudy, we examine the consistency of responses provided by LLMs to\nhealth-related questions across English, German, Turkish, and Chinese. We\nlargely expand the HealthFC dataset by categorizing health-related questions by\ndisease type and broadening its multilingual scope with Turkish and Chinese\ntranslations. We reveal significant inconsistencies in responses that could\nspread healthcare misinformation. Our main contributions are 1) a multilingual\nhealth-related inquiry dataset with meta-information on disease categories, and\n2) a novel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings highlight key\nchallenges in deploying LLM-based tools in multilingual contexts and emphasize\nthe need for improved cross-lingual alignment to ensure accurate and equitable\nhealthcare information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equitable access to reliable health information is vital for public health,\nbut the quality of online health resources varies by language, raising concerns\nabout inconsistencies in Large Language Models (LLMs) for healthcare. In this\nstudy, we examine the consistency of responses provided by LLMs to\nhealth-related questions across English, German, Turkish, and Chinese. We\nlargely expand the HealthFC dataset by categorizing health-related questions by\ndisease type and broadening its multilingual scope with Turkish and Chinese\ntranslations. We reveal significant inconsistencies in responses that could\nspread healthcare misinformation. Our main contributions are 1) a multilingual\nhealth-related inquiry dataset with meta-information on disease categories, and\n2) a novel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings highlight key\nchallenges in deploying LLM-based tools in multilingual contexts and emphasize\nthe need for improved cross-lingual alignment to ensure accurate and equitable\nhealthcare information."
                },
                "authors": [
                    {
                        "name": "Ipek Baris Schlicht"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Burcu Sayin"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Paolo Rosso"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Rosso"
                },
                "author": "Paolo Rosso",
                "arxiv_comment": "9 pages. Short paper appeared at 47th European Conference on\n  Information Retrieval (ECIR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14717v1",
                "updated": "2025-01-24T18:50:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    50,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:50:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    50,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "Towards Better Understanding Table Instruction Tuning: Decoupling the\n  Effects from Data versus Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Understanding Table Instruction Tuning: Decoupling the\n  Effects from Data versus Models"
                },
                "summary": "Recent advances in natural language processing have leveraged instruction\ntuning to enhance Large Language Models (LLMs) for table-related tasks.\nHowever, previous works train different base models with different training\ndata, lacking an apples-to-apples comparison across the result table LLMs. To\naddress this, we fine-tune base models from the Mistral, OLMo, and Phi families\non existing public training datasets. Our replication achieves performance on\npar with or surpassing existing table LLMs, establishing new state-of-the-art\nperformance on Hitab, a table question-answering dataset. More importantly,\nthrough systematic out-of-domain evaluation, we decouple the contributions of\ntraining data and the base model, providing insight into their individual\nimpacts. In addition, we assess the effects of table-specific instruction\ntuning on general-purpose benchmarks, revealing trade-offs between\nspecialization and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in natural language processing have leveraged instruction\ntuning to enhance Large Language Models (LLMs) for table-related tasks.\nHowever, previous works train different base models with different training\ndata, lacking an apples-to-apples comparison across the result table LLMs. To\naddress this, we fine-tune base models from the Mistral, OLMo, and Phi families\non existing public training datasets. Our replication achieves performance on\npar with or surpassing existing table LLMs, establishing new state-of-the-art\nperformance on Hitab, a table question-answering dataset. More importantly,\nthrough systematic out-of-domain evaluation, we decouple the contributions of\ntraining data and the base model, providing insight into their individual\nimpacts. In addition, we assess the effects of table-specific instruction\ntuning on general-purpose benchmarks, revealing trade-offs between\nspecialization and generalization."
                },
                "authors": [
                    {
                        "name": "Naihao Deng"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Henghui Zhu"
                    },
                    {
                        "name": "Shuaichen Chang"
                    },
                    {
                        "name": "Jiani Zhang"
                    },
                    {
                        "name": "Alexander Hanbo Li"
                    },
                    {
                        "name": "Chung-Wei Hang"
                    },
                    {
                        "name": "Hideo Kobayashi"
                    },
                    {
                        "name": "Yiqun Hu"
                    },
                    {
                        "name": "Patrick Ng"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Ng"
                },
                "author": "Patrick Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14713v1",
                "updated": "2025-01-24T18:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    46,
                    37,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:46:37Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    46,
                    37,
                    4,
                    24,
                    0
                ],
                "title": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing"
                },
                "summary": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs."
                },
                "authors": [
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Haris Jeelani"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "Accepted to NAACL 2025 - Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14705v1",
                "updated": "2025-01-24T18:30:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    30,
                    19,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:30:19Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    30,
                    19,
                    4,
                    24,
                    0
                ],
                "title": "The Karp Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Karp Dataset"
                },
                "summary": "Understanding the mathematical reasoning capabilities of Large Language\nModels (LLMs) is a central topic in the study of artificial intelligence. This\nnew domain necessitates the creation of datasets of reasoning tasks for both\ntraining and benchmarking the performance of LLMs. To this end, we introduce\nthe Karp dataset: The first dataset composed of detailed proofs of\nNP-completeness reductions. The reductions vary in difficulty, ranging from\nsimple exercises of undergraduate courses to more challenging reductions from\nacademic papers. We compare the performance of state-of-the-art models on this\ntask and demonstrate the effect of fine-tuning with the Karp dataset on\nreasoning capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the mathematical reasoning capabilities of Large Language\nModels (LLMs) is a central topic in the study of artificial intelligence. This\nnew domain necessitates the creation of datasets of reasoning tasks for both\ntraining and benchmarking the performance of LLMs. To this end, we introduce\nthe Karp dataset: The first dataset composed of detailed proofs of\nNP-completeness reductions. The reductions vary in difficulty, ranging from\nsimple exercises of undergraduate courses to more challenging reductions from\nacademic papers. We compare the performance of state-of-the-art models on this\ntask and demonstrate the effect of fine-tuning with the Karp dataset on\nreasoning capacity."
                },
                "authors": [
                    {
                        "name": "Mason DiCicco"
                    },
                    {
                        "name": "Eamon Worden"
                    },
                    {
                        "name": "Conner Olsen"
                    },
                    {
                        "name": "Nikhil Gangaram"
                    },
                    {
                        "name": "Daniel Reichman"
                    },
                    {
                        "name": "Neil Heffernan"
                    }
                ],
                "author_detail": {
                    "name": "Neil Heffernan"
                },
                "author": "Neil Heffernan",
                "arxiv_comment": "Accepted to the 4th workshop on mathematical reasoning and AI at\n  NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14693v1",
                "updated": "2025-01-24T18:06:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    6,
                    7,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:06:07Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    6,
                    7,
                    4,
                    24,
                    0
                ],
                "title": "Rethinking Table Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Table Instruction Tuning"
                },
                "summary": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices and lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and reveal significant declines in both\nout-of-domain table understanding and general capabilities compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the existing table instruction-tuning works, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices and lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and reveal significant declines in both\nout-of-domain table understanding and general capabilities compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the existing table instruction-tuning works, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection."
                },
                "authors": [
                    {
                        "name": "Naihao Deng"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14683v1",
                "updated": "2025-01-24T17:59:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:59:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "An Empirical Study on LLM-based Classification of Requirements-related\n  Provisions in Food-safety Regulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on LLM-based Classification of Requirements-related\n  Provisions in Food-safety Regulations"
                },
                "summary": "As Industry 4.0 transforms the food industry, the role of software in\nachieving compliance with food-safety regulations is becoming increasingly\ncritical. Food-safety regulations, like those in many legal domains, have\nlargely been articulated in a technology-independent manner to ensure their\nlongevity and broad applicability. However, this approach leaves a gap between\nthe regulations and the modern systems and software increasingly used to\nimplement them. In this article, we pursue two main goals. First, we conduct a\nGrounded Theory study of food-safety regulations and develop a conceptual\ncharacterization of food-safety concepts that closely relate to systems and\nsoftware requirements. Second, we examine the effectiveness of two families of\nlarge language models (LLMs) -- BERT and GPT -- in automatically classifying\nlegal provisions based on requirements-related food-safety concepts. Our\nresults show that: (a) when fine-tuned, the accuracy differences between the\nbest-performing models in the BERT and GPT families are relatively small.\nNevertheless, the most powerful model in our experiments, GPT-4o, still\nachieves the highest accuracy, with an average Precision of 89% and an average\nRecall of 87%; (b) few-shot learning with GPT-4o increases Recall to 97% but\ndecreases Precision to 65%, suggesting a trade-off between fine-tuning and\nfew-shot learning; (c) despite our training examples being drawn exclusively\nfrom Canadian regulations, LLM-based classification performs consistently well\non test provisions from the US, indicating a degree of generalizability across\nregulatory jurisdictions; and (d) for our classification task, LLMs\nsignificantly outperform simpler baselines constructed using long short-term\nmemory (LSTM) networks and automatic keyword extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Industry 4.0 transforms the food industry, the role of software in\nachieving compliance with food-safety regulations is becoming increasingly\ncritical. Food-safety regulations, like those in many legal domains, have\nlargely been articulated in a technology-independent manner to ensure their\nlongevity and broad applicability. However, this approach leaves a gap between\nthe regulations and the modern systems and software increasingly used to\nimplement them. In this article, we pursue two main goals. First, we conduct a\nGrounded Theory study of food-safety regulations and develop a conceptual\ncharacterization of food-safety concepts that closely relate to systems and\nsoftware requirements. Second, we examine the effectiveness of two families of\nlarge language models (LLMs) -- BERT and GPT -- in automatically classifying\nlegal provisions based on requirements-related food-safety concepts. Our\nresults show that: (a) when fine-tuned, the accuracy differences between the\nbest-performing models in the BERT and GPT families are relatively small.\nNevertheless, the most powerful model in our experiments, GPT-4o, still\nachieves the highest accuracy, with an average Precision of 89% and an average\nRecall of 87%; (b) few-shot learning with GPT-4o increases Recall to 97% but\ndecreases Precision to 65%, suggesting a trade-off between fine-tuning and\nfew-shot learning; (c) despite our training examples being drawn exclusively\nfrom Canadian regulations, LLM-based classification performs consistently well\non test provisions from the US, indicating a degree of generalizability across\nregulatory jurisdictions; and (d) for our classification task, LLMs\nsignificantly outperform simpler baselines constructed using long short-term\nmemory (LSTM) networks and automatic keyword extraction."
                },
                "authors": [
                    {
                        "name": "Shabnam Hassani"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    },
                    {
                        "name": "Daniel Amyot"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Amyot"
                },
                "author": "Daniel Amyot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14679v1",
                "updated": "2025-01-24T17:57:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    57,
                    6,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:57:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    57,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation"
                },
                "summary": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba."
                },
                "authors": [
                    {
                        "name": "Rongzhao He"
                    },
                    {
                        "name": "Weihao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Weihao Zheng"
                },
                "author": "Weihao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14661v1",
                "updated": "2025-01-24T17:30:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    30,
                    17,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:30:17Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    30,
                    17,
                    4,
                    24,
                    0
                ],
                "title": "Neural-Symbolic Message Passing with Dynamic Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-Symbolic Message Passing with Dynamic Pruning"
                },
                "summary": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a\nchallenging task. Recently, a line of message-passing-based research has been\nproposed to solve CQA. However, they perform unsatisfactorily on negative\nqueries and fail to address the noisy messages between variable nodes in the\nquery graph. Moreover, they offer little interpretability and require complex\nquery data and resource-intensive training. In this paper, we propose a\nNeural-Symbolic Message Passing (NSMP) framework based on pre-trained neural\nlink predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can\ngeneralize to arbitrary existential first order logic queries without requiring\ntraining while providing interpretable answers. Furthermore, we introduce a\ndynamic pruning strategy to filter out noisy messages between variable nodes.\nExperimental results show that NSMP achieves a strong performance.\nAdditionally, through complexity analysis and empirical verification, we\ndemonstrate the superiority of NSMP in inference time over the current\nstate-of-the-art neural-symbolic method. Compared to this approach, NSMP\ndemonstrates faster inference times across all query types on benchmark\ndatasets, with speedup ranging from 2$\\times$ to over 150$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a\nchallenging task. Recently, a line of message-passing-based research has been\nproposed to solve CQA. However, they perform unsatisfactorily on negative\nqueries and fail to address the noisy messages between variable nodes in the\nquery graph. Moreover, they offer little interpretability and require complex\nquery data and resource-intensive training. In this paper, we propose a\nNeural-Symbolic Message Passing (NSMP) framework based on pre-trained neural\nlink predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can\ngeneralize to arbitrary existential first order logic queries without requiring\ntraining while providing interpretable answers. Furthermore, we introduce a\ndynamic pruning strategy to filter out noisy messages between variable nodes.\nExperimental results show that NSMP achieves a strong performance.\nAdditionally, through complexity analysis and empirical verification, we\ndemonstrate the superiority of NSMP in inference time over the current\nstate-of-the-art neural-symbolic method. Compared to this approach, NSMP\ndemonstrates faster inference times across all query types on benchmark\ndatasets, with speedup ranging from 2$\\times$ to over 150$\\times$."
                },
                "authors": [
                    {
                        "name": "Chongzhi Zhang"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Zhiping Peng"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "arxiv_comment": "19 pages, 5 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14654v1",
                "updated": "2025-01-24T17:21:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    21,
                    1,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:21:01Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    21,
                    1,
                    4,
                    24,
                    0
                ],
                "title": "MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical\n  Applications"
                },
                "summary": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 100 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(GPT-4o) achieves a success rate of 72%. However, there is still substantial\nspace for improvement to give the community a next direction to optimize.\nFurthermore, there is significant variation in performance across task\ncategories. MedAgentBench establishes this and is publicly available at\nhttps://github.com/stanfordmlgroup/MedAgentBench , offering a valuable\nframework for model developers to track progress and drive continuous\nimprovements in the agent capabilities of large language models within the\nmedical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 100 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(GPT-4o) achieves a success rate of 72%. However, there is still substantial\nspace for improvement to give the community a next direction to optimize.\nFurthermore, there is significant variation in performance across task\ncategories. MedAgentBench establishes this and is publicly available at\nhttps://github.com/stanfordmlgroup/MedAgentBench , offering a valuable\nframework for model developers to track progress and drive continuous\nimprovements in the agent capabilities of large language models within the\nmedical domain."
                },
                "authors": [
                    {
                        "name": "Yixing Jiang"
                    },
                    {
                        "name": "Kameron C. Black"
                    },
                    {
                        "name": "Gloria Geng"
                    },
                    {
                        "name": "Danny Park"
                    },
                    {
                        "name": "Andrew Y. Ng"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H. Chen"
                },
                "author": "Jonathan H. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14649v1",
                "updated": "2025-01-24T17:15:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    15,
                    9,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:15:09Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    15,
                    9,
                    4,
                    24,
                    0
                ],
                "title": "Investigating the (De)Composition Capabilities of Large Language Models\n  in Natural-to-Formal Language Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the (De)Composition Capabilities of Large Language Models\n  in Natural-to-Formal Language Conversion"
                },
                "summary": "To achieve generalized and robust natural-to-formal language conversion\n(N2F), large language models (LLMs) need to have strong capabilities of\ndecomposition and composition in N2F when faced with an unfamiliar formal\nlanguage and be able to cope with compositional gaps and counter-intuitive\nsymbolic names. To investigate whether LLMs have this set of basic capabilities\nin N2F, we propose the DEDC framework. This framework semi-automatically\nperforms sample and task construction, allowing decoupled evaluation of the set\nof decomposition and composition capabilities of LLMs in N2F. Based on this\nframework, we evaluate and analyze the most advanced LLMs, and the main\nfindings include that: (1) the LLMs are deficient in both decomposition and\ncomposition; (2) the LLMs show a wide coverage of error types that can be\nattributed to deficiencies in natural language understanding and the learning\nand use of symbolic systems; (3) compositional gaps and counter-intuitive\nsymbolic names both affect the decomposition and composition of the LLMs. Our\nwork provides a new perspective for investigating the basic capabilities of\ndecomposition and composition of LLMs in N2F. The detailed analysis of\ndeficiencies and attributions can help subsequent improvements of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve generalized and robust natural-to-formal language conversion\n(N2F), large language models (LLMs) need to have strong capabilities of\ndecomposition and composition in N2F when faced with an unfamiliar formal\nlanguage and be able to cope with compositional gaps and counter-intuitive\nsymbolic names. To investigate whether LLMs have this set of basic capabilities\nin N2F, we propose the DEDC framework. This framework semi-automatically\nperforms sample and task construction, allowing decoupled evaluation of the set\nof decomposition and composition capabilities of LLMs in N2F. Based on this\nframework, we evaluate and analyze the most advanced LLMs, and the main\nfindings include that: (1) the LLMs are deficient in both decomposition and\ncomposition; (2) the LLMs show a wide coverage of error types that can be\nattributed to deficiencies in natural language understanding and the learning\nand use of symbolic systems; (3) compositional gaps and counter-intuitive\nsymbolic names both affect the decomposition and composition of the LLMs. Our\nwork provides a new perspective for investigating the basic capabilities of\ndecomposition and composition of LLMs in N2F. The detailed analysis of\ndeficiencies and attributions can help subsequent improvements of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Houfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Houfeng Wang"
                },
                "author": "Houfeng Wang",
                "arxiv_comment": "Accepted at NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12499v2",
                "updated": "2025-01-24T17:10:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    10,
                    58,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-21T21:01:01Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    21,
                    1,
                    1,
                    1,
                    21,
                    0
                ],
                "title": "Multiband Embeddings of Light Curves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiband Embeddings of Light Curves"
                },
                "summary": "In this work, we propose a novel ensemble of recurrent neural networks (RNNs)\nthat considers the multiband and non-uniform cadence without having to compute\ncomplex features. Our proposed model consists of an ensemble of RNNs, which do\nnot require the entire light curve to perform inference, making the inference\nprocess simpler. The ensemble is able to adapt to varying numbers of bands,\ntested on three real light curve datasets, namely Gaia, Pan-STARRS1, and ZTF,\nto demonstrate its potential for generalization. We also show the capabilities\nof deep learning to perform not only classification, but also regression of\nphysical parameters such as effective temperature and radius. Our ensemble\nmodel demonstrates superior performance in scenarios with fewer observations,\nthus providing potential for early classification of sources from facilities\nsuch as Vera C. Rubin Observatory's LSST. The results underline the model's\neffectiveness and flexibility, making it a promising tool for future\nastronomical surveys. Our research has shown that a multitask learning approach\ncan enrich the embeddings obtained by the models, making them instrumental to\nsolve additional tasks, such as determining the orbital parameters of binary\nsystems or estimating parameters for object types beyond periodic ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel ensemble of recurrent neural networks (RNNs)\nthat considers the multiband and non-uniform cadence without having to compute\ncomplex features. Our proposed model consists of an ensemble of RNNs, which do\nnot require the entire light curve to perform inference, making the inference\nprocess simpler. The ensemble is able to adapt to varying numbers of bands,\ntested on three real light curve datasets, namely Gaia, Pan-STARRS1, and ZTF,\nto demonstrate its potential for generalization. We also show the capabilities\nof deep learning to perform not only classification, but also regression of\nphysical parameters such as effective temperature and radius. Our ensemble\nmodel demonstrates superior performance in scenarios with fewer observations,\nthus providing potential for early classification of sources from facilities\nsuch as Vera C. Rubin Observatory's LSST. The results underline the model's\neffectiveness and flexibility, making it a promising tool for future\nastronomical surveys. Our research has shown that a multitask learning approach\ncan enrich the embeddings obtained by the models, making them instrumental to\nsolve additional tasks, such as determining the orbital parameters of binary\nsystems or estimating parameters for object types beyond periodic ones."
                },
                "authors": [
                    {
                        "name": "I. Becker"
                    },
                    {
                        "name": "P. Protopapas"
                    },
                    {
                        "name": "M. Catelan"
                    },
                    {
                        "name": "K. Pichara"
                    }
                ],
                "author_detail": {
                    "name": "K. Pichara"
                },
                "author": "K. Pichara",
                "arxiv_comment": "15 pages, 10 figures, accepted at A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07613v3",
                "updated": "2025-01-24T17:06:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    6,
                    30,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-11T20:50:41Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    50,
                    41,
                    2,
                    255,
                    0
                ],
                "title": "Token Turing Machines are Efficient Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Turing Machines are Efficient Vision Models"
                },
                "summary": "We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency,\nmemory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing\nMachines and Token Turing Machines, which were applied to NLP and sequential\nvisual understanding tasks. ViTTMs are designed for non-sequential computer\nvision tasks such as image classification and segmentation. Our model creates\ntwo sets of tokens: process tokens and memory tokens; process tokens pass\nthrough encoder blocks and read-write from memory tokens at each encoder block\nin the network, allowing them to store and retrieve information from memory. By\nensuring that there are fewer process tokens than memory tokens, we are able to\nreduce the inference time of the network while maintaining its accuracy. On\nImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0%\naccuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer\nFLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B\nachieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model\nacheives a 45.17 mIoU with 26.8 FPS (+94%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency,\nmemory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing\nMachines and Token Turing Machines, which were applied to NLP and sequential\nvisual understanding tasks. ViTTMs are designed for non-sequential computer\nvision tasks such as image classification and segmentation. Our model creates\ntwo sets of tokens: process tokens and memory tokens; process tokens pass\nthrough encoder blocks and read-write from memory tokens at each encoder block\nin the network, allowing them to store and retrieve information from memory. By\nensuring that there are fewer process tokens than memory tokens, we are able to\nreduce the inference time of the network while maintaining its accuracy. On\nImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0%\naccuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer\nFLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B\nachieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model\nacheives a 45.17 mIoU with 26.8 FPS (+94%)."
                },
                "authors": [
                    {
                        "name": "Purvish Jajal"
                    },
                    {
                        "name": "Nick John Eliopoulos"
                    },
                    {
                        "name": "Benjamin Shiue-Hal Chou"
                    },
                    {
                        "name": "George K. Thiruvathukal"
                    },
                    {
                        "name": "James C. Davis"
                    },
                    {
                        "name": "Yung-Hsiang Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yung-Hsiang Lu"
                },
                "author": "Yung-Hsiang Lu",
                "arxiv_comment": "Accepted to WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14630v1",
                "updated": "2025-01-24T16:49:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    49,
                    8,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:49:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    49,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Problem Structure with LLMs for Optimized SAT Local Search"
                },
                "summary": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems."
                },
                "authors": [
                    {
                        "name": "André Schilder"
                    },
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15809v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15809v4",
                "updated": "2025-01-24T16:45:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    45,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-22T10:25:55Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    10,
                    25,
                    55,
                    5,
                    174,
                    0
                ],
                "title": "LaMSUM: Amplifying Voices Against Harassment through LLM Guided\n  Extractive Summarization of User Incident Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMSUM: Amplifying Voices Against Harassment through LLM Guided\n  Extractive Summarization of User Incident Reports"
                },
                "summary": "Citizen reporting platforms like Safe City in India help the public and\nauthorities stay informed about sexual harassment incidents. However, the high\nvolume of data shared on these platforms makes reviewing each individual case\nchallenging. Therefore, a summarization algorithm capable of processing and\nunderstanding various Indian code-mixed languages is essential. In recent\nyears, Large Language Models (LLMs) have shown exceptional performance in NLP\ntasks, including summarization. LLMs inherently produce abstractive summaries\nby paraphrasing the original text, while the generation of extractive summaries\n- selecting specific subsets from the original text - through LLMs remains\nlargely unexplored. Moreover, LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle these\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries for large collections of Safe City posts using\nLLMs. LaMSUM integrates summarization with different voting methods to achieve\nrobust summaries. Extensive evaluation using three popular LLMs (Llama, Mistral\nand GPT-4o) demonstrates that LaMSUM outperforms state-of-the-art extractive\nsummarization methods for Safe City posts. Overall, this work represents one of\nthe first attempts to achieve extractive summarization through LLMs, and is\nlikely to support stakeholders by offering a comprehensive overview and\nenabling them to develop effective policies to minimize incidents of\nunwarranted harassment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citizen reporting platforms like Safe City in India help the public and\nauthorities stay informed about sexual harassment incidents. However, the high\nvolume of data shared on these platforms makes reviewing each individual case\nchallenging. Therefore, a summarization algorithm capable of processing and\nunderstanding various Indian code-mixed languages is essential. In recent\nyears, Large Language Models (LLMs) have shown exceptional performance in NLP\ntasks, including summarization. LLMs inherently produce abstractive summaries\nby paraphrasing the original text, while the generation of extractive summaries\n- selecting specific subsets from the original text - through LLMs remains\nlargely unexplored. Moreover, LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle these\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries for large collections of Safe City posts using\nLLMs. LaMSUM integrates summarization with different voting methods to achieve\nrobust summaries. Extensive evaluation using three popular LLMs (Llama, Mistral\nand GPT-4o) demonstrates that LaMSUM outperforms state-of-the-art extractive\nsummarization methods for Safe City posts. Overall, this work represents one of\nthe first attempts to achieve extractive summarization through LLMs, and is\nlikely to support stakeholders by offering a comprehensive overview and\nenabling them to develop effective policies to minimize incidents of\nunwarranted harassment."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Anurag Sharma"
                    },
                    {
                        "name": "V. Gurucharan"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15809v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15809v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14625v1",
                "updated": "2025-01-24T16:42:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    42,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:42:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    42,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Preference Elicitation with LLM-Based Proxies"
                },
                "summary": "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms."
                },
                "authors": [
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Francisco Marmolejo-Cossío"
                    },
                    {
                        "name": "Edwin Lock"
                    },
                    {
                        "name": "David Parkes"
                    }
                ],
                "author_detail": {
                    "name": "David Parkes"
                },
                "author": "David Parkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10321v2",
                "updated": "2025-01-24T16:37:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    37,
                    57,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-17T17:51:22Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    51,
                    22,
                    4,
                    17,
                    0
                ],
                "title": "Towards Human-Guided, Data-Centric LLM Co-Pilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Guided, Data-Centric LLM Co-Pilots"
                },
                "summary": "Machine learning (ML) has the potential to revolutionize various domains, but\nits adoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has the potential to revolutionize various domains, but\nits adoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML."
                },
                "authors": [
                    {
                        "name": "Evgeny Saveliev"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Anders Boyd"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Saveliev, Liu & Seedat contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14615v1",
                "updated": "2025-01-24T16:33:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    33,
                    52,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:33:52Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    33,
                    52,
                    4,
                    24,
                    0
                ],
                "title": "Single-neuron deep generative model uncovers underlying physics of\n  neuronal activity in Ca imaging data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-neuron deep generative model uncovers underlying physics of\n  neuronal activity in Ca imaging data"
                },
                "summary": "Calcium imaging has become a powerful alternative to electrophysiology for\nstudying neuronal activity, offering spatial resolution and the ability to\nmeasure large populations of neurons in a minimally invasive manner. This\ntechnique has broad applications in neuroscience, neuroengineering, and\nmedicine, enabling researchers to explore the relationship between neuron\nlocation and activity. Recent advancements in deep generative models (DGMs)\nhave facilitated the modeling of neuronal population dynamics, uncovering\nlatent representations that provide insights into behavior prediction and\nneuronal variance. However, these models often rely on spike inference\nalgorithms and primarily focus on population-level dynamics, limiting their\napplicability for single-neuron analyses. To address this gap, we propose a\nnovel framework for single-neuron representation learning using autoregressive\nvariational autoencoders (AVAEs). Our approach embeds individual neurons'\nspatiotemporal signals into a reduced-dimensional space without the need for\nspike inference algorithms. The AVAE excels over traditional linear methods by\ngenerating more informative and discriminative latent representations,\nimproving tasks such as visualization, clustering, and the understanding of\nneuronal activity. Additionally, the reconstruction performance of the AVAE\noutperforms the state of the art, demonstrating its ability to accurately\nrecover the original fluorescence signal from the learned representation. Using\nrealistic simulations, we show that our model captures underlying physical\nproperties and connectivity patterns, enabling it to distinguish between\ndifferent firing and connectivity types. These findings position the AVAE as a\nversatile and powerful tool for advancing single-neuron analysis and lays the\ngroundwork for future integration of multimodal single-cell datasets in\nneuroscience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calcium imaging has become a powerful alternative to electrophysiology for\nstudying neuronal activity, offering spatial resolution and the ability to\nmeasure large populations of neurons in a minimally invasive manner. This\ntechnique has broad applications in neuroscience, neuroengineering, and\nmedicine, enabling researchers to explore the relationship between neuron\nlocation and activity. Recent advancements in deep generative models (DGMs)\nhave facilitated the modeling of neuronal population dynamics, uncovering\nlatent representations that provide insights into behavior prediction and\nneuronal variance. However, these models often rely on spike inference\nalgorithms and primarily focus on population-level dynamics, limiting their\napplicability for single-neuron analyses. To address this gap, we propose a\nnovel framework for single-neuron representation learning using autoregressive\nvariational autoencoders (AVAEs). Our approach embeds individual neurons'\nspatiotemporal signals into a reduced-dimensional space without the need for\nspike inference algorithms. The AVAE excels over traditional linear methods by\ngenerating more informative and discriminative latent representations,\nimproving tasks such as visualization, clustering, and the understanding of\nneuronal activity. Additionally, the reconstruction performance of the AVAE\noutperforms the state of the art, demonstrating its ability to accurately\nrecover the original fluorescence signal from the learned representation. Using\nrealistic simulations, we show that our model captures underlying physical\nproperties and connectivity patterns, enabling it to distinguish between\ndifferent firing and connectivity types. These findings position the AVAE as a\nversatile and powerful tool for advancing single-neuron analysis and lays the\ngroundwork for future integration of multimodal single-cell datasets in\nneuroscience."
                },
                "authors": [
                    {
                        "name": "Jordi Abante"
                    },
                    {
                        "name": "Angelo Piga"
                    },
                    {
                        "name": "Berta Ros"
                    },
                    {
                        "name": "Clara F López-León"
                    },
                    {
                        "name": "Josep M Canals"
                    },
                    {
                        "name": "Jordi Soriano"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Soriano"
                },
                "author": "Jordi Soriano",
                "arxiv_comment": "12 pages, 5 figures, ECCB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14605v1",
                "updated": "2025-01-24T16:22:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    22,
                    35,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    22,
                    35,
                    4,
                    24,
                    0
                ],
                "title": "3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic\n  Segmentation in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DLabelProp: Geometric-Driven Domain Generalization for LiDAR Semantic\n  Segmentation in Autonomous Driving"
                },
                "summary": "Domain generalization aims to find ways for deep learning models to maintain\ntheir performance despite significant domain shifts between training and\ninference datasets. This is particularly important for models that need to be\nrobust or are costly to train. LiDAR perception in autonomous driving is\nimpacted by both of these concerns, leading to the emergence of various\napproaches. This work addresses the challenge by proposing a geometry-based\napproach, leveraging the sequential structure of LiDAR sensors, which sets it\napart from the learning-based methods commonly found in the literature. The\nproposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic\nSegmentation (LSS). Through extensive experimentation on seven datasets, it is\ndemonstrated to be a state-of-the-art approach, outperforming both naive and\nother domain generalization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalization aims to find ways for deep learning models to maintain\ntheir performance despite significant domain shifts between training and\ninference datasets. This is particularly important for models that need to be\nrobust or are costly to train. LiDAR perception in autonomous driving is\nimpacted by both of these concerns, leading to the emergence of various\napproaches. This work addresses the challenge by proposing a geometry-based\napproach, leveraging the sequential structure of LiDAR sensors, which sets it\napart from the learning-based methods commonly found in the literature. The\nproposed method, called 3DLabelProp, is applied on the task of LiDAR Semantic\nSegmentation (LSS). Through extensive experimentation on seven datasets, it is\ndemonstrated to be a state-of-the-art approach, outperforming both naive and\nother domain generalization methods."
                },
                "authors": [
                    {
                        "name": "Jules Sanchez"
                    },
                    {
                        "name": "Jean-Emmanuel Deschaud"
                    },
                    {
                        "name": "François Goulette"
                    }
                ],
                "author_detail": {
                    "name": "François Goulette"
                },
                "author": "François Goulette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17878v2",
                "updated": "2025-01-24T16:19:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    19,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-23T13:50:27Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    50,
                    27,
                    2,
                    297,
                    0
                ],
                "title": "Relaxed Equivariance via Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed Equivariance via Multitask Learning"
                },
                "summary": "Incorporating equivariance as an inductive bias into deep learning\narchitectures to take advantage of the data symmetry has been successful in\nmultiple applications, such as chemistry and dynamical systems. In particular,\nroto-translations are crucial for effectively modeling geometric graphs and\nmolecules, where understanding the 3D structures enhances generalization.\nHowever, equivariant models often pose challenges due to their high\ncomputational complexity. In this paper, we introduce REMUL, a training\nprocedure for approximating equivariance with multitask learning. We show that\nunconstrained models (which do not build equivariance into the architecture)\ncan learn approximate symmetries by minimizing an additional simple\nequivariance loss. By formulating equivariance as a new learning objective, we\ncan control the level of approximate equivariance in the model. Our method\nachieves competitive performance compared to equivariant baselines while being\n$10 \\times$ faster at inference and $2.5 \\times$ at training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating equivariance as an inductive bias into deep learning\narchitectures to take advantage of the data symmetry has been successful in\nmultiple applications, such as chemistry and dynamical systems. In particular,\nroto-translations are crucial for effectively modeling geometric graphs and\nmolecules, where understanding the 3D structures enhances generalization.\nHowever, equivariant models often pose challenges due to their high\ncomputational complexity. In this paper, we introduce REMUL, a training\nprocedure for approximating equivariance with multitask learning. We show that\nunconstrained models (which do not build equivariance into the architecture)\ncan learn approximate symmetries by minimizing an additional simple\nequivariance loss. By formulating equivariance as a new learning objective, we\ncan control the level of approximate equivariance in the model. Our method\nachieves competitive performance compared to equivariant baselines while being\n$10 \\times$ faster at inference and $2.5 \\times$ at training."
                },
                "authors": [
                    {
                        "name": "Ahmed A. Elhag"
                    },
                    {
                        "name": "T. Konstantin Rusch"
                    },
                    {
                        "name": "Francesco Di Giovanni"
                    },
                    {
                        "name": "Michael Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bronstein"
                },
                "author": "Michael Bronstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14602v1",
                "updated": "2025-01-24T16:16:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    16,
                    28,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:16:28Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    16,
                    28,
                    4,
                    24,
                    0
                ],
                "title": "Minimax Optimal Design with Spillover and Carryover Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimax Optimal Design with Spillover and Carryover Effects"
                },
                "summary": "In various applications, the potential outcome of a unit may be influenced by\nthe treatments received by other units, a phenomenon known as interference, as\nwell as by prior treatments, referred to as carryover effects. These phenomena\nviolate the stable unit treatment value assumption and pose significant\nchallenges in causal inference. To address these complexities, we propose a\nminimax optimal experimental design that simultaneously accounts for both\nspillover and carryover effects, enhancing the precision of estimates for\ndirect and spillover effects. This method is particularly applicable to\nmulti-unit experiments, reducing sample size requirements and experimental\ncosts. We also investigate the asymptotic properties of the Horvitz--Thompson\nestimators of direct and spillover effects, demonstrating their consistency and\nasymptotic normality under the minimax optimal design. To facilitate valid\ninferences, we propose conservative variance estimators. Furthermore, we tackle\nthe challenges associated with potential misspecifications in the order of\ncarryover effects. Our approach is validated by comprehensive numerical studies\nthat demonstrate superior performance compared to existing experimental\ndesigns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various applications, the potential outcome of a unit may be influenced by\nthe treatments received by other units, a phenomenon known as interference, as\nwell as by prior treatments, referred to as carryover effects. These phenomena\nviolate the stable unit treatment value assumption and pose significant\nchallenges in causal inference. To address these complexities, we propose a\nminimax optimal experimental design that simultaneously accounts for both\nspillover and carryover effects, enhancing the precision of estimates for\ndirect and spillover effects. This method is particularly applicable to\nmulti-unit experiments, reducing sample size requirements and experimental\ncosts. We also investigate the asymptotic properties of the Horvitz--Thompson\nestimators of direct and spillover effects, demonstrating their consistency and\nasymptotic normality under the minimax optimal design. To facilitate valid\ninferences, we propose conservative variance estimators. Furthermore, we tackle\nthe challenges associated with potential misspecifications in the order of\ncarryover effects. Our approach is validated by comprehensive numerical studies\nthat demonstrate superior performance compared to existing experimental\ndesigns."
                },
                "authors": [
                    {
                        "name": "Haoyang Yu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Hanzhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hanzhong Liu"
                },
                "author": "Hanzhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14598v1",
                "updated": "2025-01-24T16:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    12,
                    38,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    12,
                    38,
                    4,
                    24,
                    0
                ],
                "title": "Type-Based Approaches to Rounding Error Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type-Based Approaches to Rounding Error Analysis"
                },
                "summary": "This dissertation explores the design and implementation of programming\nlanguages that represent rounding error analysis through typing.\n  The first part of this dissertation demonstrates that it is possible to\ndesign languages for forward error analysis, as illustrated with NumFuzz, a\nfunctional programming language whose type system expresses quantitative bounds\non rounding error. This type system combines a sensitivity analysis, enforced\nthrough a linear typing discipline, with a novel graded monad to track the\naccumulation of rounding errors. We establish the soundness of the type system\nby relating the denotational semantics of the language to both an exact and\nfloating-point operational semantics. To demonstrate the practical utility of\nNumFuzz as a tool for automated error analysis, we have developed a prototype\nimplementation capable of automatically inferring error bounds. Our\nimplementation produces bounds competitive with existing tools, while often\nachieving significantly faster analysis times.\n  The second part of this dissertation explores a type-based approach to\nbackward error analysis with Bean, a first-order programming language with a\nlinear type system that can express quantitative bounds on backward error.\nBean's type system combines a graded coeffect system with strict linearity to\nsoundly track the flow of backward error through programs. To illustrate Bean's\npotential as a practical tool for automated backward error analysis, we\nimplement a variety of standard algorithms from numerical linear algebra in\nBean, establishing fine-grained backward error bounds via typing in a\ncompositional style. We also develop a prototype implementation of Bean that\ninfers backward error bounds automatically. Our evaluation shows that these\ninferred bounds match worst-case theoretical relative backward error bounds\nfrom the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation explores the design and implementation of programming\nlanguages that represent rounding error analysis through typing.\n  The first part of this dissertation demonstrates that it is possible to\ndesign languages for forward error analysis, as illustrated with NumFuzz, a\nfunctional programming language whose type system expresses quantitative bounds\non rounding error. This type system combines a sensitivity analysis, enforced\nthrough a linear typing discipline, with a novel graded monad to track the\naccumulation of rounding errors. We establish the soundness of the type system\nby relating the denotational semantics of the language to both an exact and\nfloating-point operational semantics. To demonstrate the practical utility of\nNumFuzz as a tool for automated error analysis, we have developed a prototype\nimplementation capable of automatically inferring error bounds. Our\nimplementation produces bounds competitive with existing tools, while often\nachieving significantly faster analysis times.\n  The second part of this dissertation explores a type-based approach to\nbackward error analysis with Bean, a first-order programming language with a\nlinear type system that can express quantitative bounds on backward error.\nBean's type system combines a graded coeffect system with strict linearity to\nsoundly track the flow of backward error through programs. To illustrate Bean's\npotential as a practical tool for automated backward error analysis, we\nimplement a variety of standard algorithms from numerical linear algebra in\nBean, establishing fine-grained backward error bounds via typing in a\ncompositional style. We also develop a prototype implementation of Bean that\ninfers backward error bounds automatically. Our evaluation shows that these\ninferred bounds match worst-case theoretical relative backward error bounds\nfrom the literature."
                },
                "authors": [
                    {
                        "name": "Ariel Eileen Kellison"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Eileen Kellison"
                },
                "author": "Ariel Eileen Kellison",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14117v3",
                "updated": "2025-01-24T15:50:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    50,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-20T09:03:18Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    9,
                    3,
                    18,
                    3,
                    172,
                    0
                ],
                "title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers"
                },
                "summary": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered."
                },
                "authors": [
                    {
                        "name": "Shuoqi Sun"
                    },
                    {
                        "name": "Shengyao Zhuang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12883v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12883v3",
                "updated": "2025-01-24T15:27:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    27,
                    44,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T13:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program"
                },
                "summary": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
                },
                "authors": [
                    {
                        "name": "Carlton Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Carlton Shepherd"
                },
                "author": "Carlton Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12883v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12883v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07072v2",
                "updated": "2025-01-24T15:26:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    26,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-11-11T15:47:25Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    25,
                    0,
                    316,
                    0
                ],
                "title": "An Interpretable X-ray Style Transfer via Trainable Local Laplacian\n  Filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Interpretable X-ray Style Transfer via Trainable Local Laplacian\n  Filter"
                },
                "summary": "Radiologists have preferred visual impressions or 'styles' of X-ray images\nthat are manually adjusted to their needs to support their diagnostic\nperformance. In this work, we propose an automatic and interpretable X-ray\nstyle transfer by introducing a trainable version of the Local Laplacian Filter\n(LLF). From the shape of the LLF's optimized remap function, the\ncharacteristics of the style transfer can be inferred and reliability of the\nalgorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray\nstyle features by replacing the remap function with a Multi-Layer Perceptron\n(MLP) and adding a trainable normalization layer. We demonstrate the\neffectiveness of the proposed method by transforming unprocessed mammographic\nX-ray images into images that match the style of target mammograms and achieve\na Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline\nLLF style transfer method from Aubry et al.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiologists have preferred visual impressions or 'styles' of X-ray images\nthat are manually adjusted to their needs to support their diagnostic\nperformance. In this work, we propose an automatic and interpretable X-ray\nstyle transfer by introducing a trainable version of the Local Laplacian Filter\n(LLF). From the shape of the LLF's optimized remap function, the\ncharacteristics of the style transfer can be inferred and reliability of the\nalgorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray\nstyle features by replacing the remap function with a Multi-Layer Perceptron\n(MLP) and adding a trainable normalization layer. We demonstrate the\neffectiveness of the proposed method by transforming unprocessed mammographic\nX-ray images into images that match the style of target mammograms and achieve\na Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline\nLLF style transfer method from Aubry et al."
                },
                "authors": [
                    {
                        "name": "Dominik Eckert"
                    },
                    {
                        "name": "Ludwig Ritschl"
                    },
                    {
                        "name": "Christopher Syben"
                    },
                    {
                        "name": "Christian Hümmer"
                    },
                    {
                        "name": "Julia Wicklein"
                    },
                    {
                        "name": "Marcel Beister"
                    },
                    {
                        "name": "Steffen Kappler"
                    },
                    {
                        "name": "Sebastian Stober"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stober"
                },
                "author": "Sebastian Stober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10642v3",
                "updated": "2025-01-24T15:21:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    21,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2024-04-16T15:16:22Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    15,
                    16,
                    22,
                    1,
                    107,
                    0
                ],
                "title": "Self-playing Adversarial Language Game Enhances LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-playing Adversarial Language Game Enhances LLM Reasoning"
                },
                "summary": "We explore the potential of self-play training for large language models\n(LLMs) in a two-player adversarial language game called Adversarial Taboo. In\nthis game, an attacker and a defender communicate around a target word only\nvisible to the attacker. The attacker aims to induce the defender to speak the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players must have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Playing this Adversarial language Game (SPAG). With this goal, we select\nseveral open-source LLMs and let each act as the attacker and play with a copy\nof itself as the defender on an extensive range of target words. Through\nreinforcement learning on the game outcomes, we observe that the LLMs'\nperformances uniformly improve on a broad range of reasoning benchmarks.\nFurthermore, iteratively adopting this self-play process can continuously\npromote LLMs' reasoning abilities. The code is available at\nhttps://github.com/Linear95/SPAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the potential of self-play training for large language models\n(LLMs) in a two-player adversarial language game called Adversarial Taboo. In\nthis game, an attacker and a defender communicate around a target word only\nvisible to the attacker. The attacker aims to induce the defender to speak the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players must have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Playing this Adversarial language Game (SPAG). With this goal, we select\nseveral open-source LLMs and let each act as the attacker and play with a copy\nof itself as the defender on an extensive range of target words. Through\nreinforcement learning on the game outcomes, we observe that the LLMs'\nperformances uniformly improve on a broad range of reasoning benchmarks.\nFurthermore, iteratively adopting this self-play process can continuously\npromote LLMs' reasoning abilities. The code is available at\nhttps://github.com/Linear95/SPAG."
                },
                "authors": [
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Tianhao Hu"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Xiaolong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Li"
                },
                "author": "Xiaolong Li",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03184v2",
                "updated": "2025-01-24T15:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    12,
                    38,
                    4,
                    24,
                    0
                ],
                "published": "2024-11-05T15:29:36Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    29,
                    36,
                    1,
                    310,
                    0
                ],
                "title": "The Soltan argument at $z=6$: UV-luminous quasars contribute less than\n  10% to early black hole mass growth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Soltan argument at $z=6$: UV-luminous quasars contribute less than\n  10% to early black hole mass growth"
                },
                "summary": "We combine stellar mass functions and the recent first JWST-based\ngalaxy--black hole scaling relations at $z=6$ to for the first time compute the\nsupermassive black hole (SMBH) mass volume density at this epoch, and compare\nthis to the integrated SMBH mass growth from the population of UV-luminous\nquasars at $z>6$. We show that even under very conservative assumptions almost\nall growth of supermassive black hole mass at $z>6$ does not take place in\nthese UV-luminous quasars, but must occur in systems obscured through dust\nand/or with lower radiative efficiency than standard thin accretion disks. The\n`Soltan argument' is not fulfilled by the known population of bright quasars at\n$z>6$: the integrated SMBH mass growth inferred from these largely unobscured\nactive galactic nuclei (AGN) in the early Universe is by a factor $\\ge$10\nsmaller than the total black hole mass volume density at $z=6$. This is valid\nunder a large range of assumption about luminosity and mass functions as well\nas accretion modes, and is likely still at least a factor $>$2 smaller when\naccounting for the known obscuration fractions at this epoch. The resulting\nconsequences are: $>$90%, possibly substantially more, of SMBH-buildup in the\nearly Universe does not take place in luminous unobscured quasar phases, but\nhas to occur in obscured systems, with dust absorbing most of the emitted\nUV--visible AGN emission, potentially with accretion modes with super-Eddington\nspecific accretion rates. This is consistent with short lifetime arguments for\nluminous quasar phases from quasar proximity zone studies and clustering. This\nwould remove the empirical need for slow SMBH growth and hence exotic\n`high-mass seed' black holes at early cosmic time. It also predicts a large\npopulation of luminous but very obscured lower-mass quasars at $z>6$, possibly\nthe JWST `Little Red Dots'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We combine stellar mass functions and the recent first JWST-based\ngalaxy--black hole scaling relations at $z=6$ to for the first time compute the\nsupermassive black hole (SMBH) mass volume density at this epoch, and compare\nthis to the integrated SMBH mass growth from the population of UV-luminous\nquasars at $z>6$. We show that even under very conservative assumptions almost\nall growth of supermassive black hole mass at $z>6$ does not take place in\nthese UV-luminous quasars, but must occur in systems obscured through dust\nand/or with lower radiative efficiency than standard thin accretion disks. The\n`Soltan argument' is not fulfilled by the known population of bright quasars at\n$z>6$: the integrated SMBH mass growth inferred from these largely unobscured\nactive galactic nuclei (AGN) in the early Universe is by a factor $\\ge$10\nsmaller than the total black hole mass volume density at $z=6$. This is valid\nunder a large range of assumption about luminosity and mass functions as well\nas accretion modes, and is likely still at least a factor $>$2 smaller when\naccounting for the known obscuration fractions at this epoch. The resulting\nconsequences are: $>$90%, possibly substantially more, of SMBH-buildup in the\nearly Universe does not take place in luminous unobscured quasar phases, but\nhas to occur in obscured systems, with dust absorbing most of the emitted\nUV--visible AGN emission, potentially with accretion modes with super-Eddington\nspecific accretion rates. This is consistent with short lifetime arguments for\nluminous quasar phases from quasar proximity zone studies and clustering. This\nwould remove the empirical need for slow SMBH growth and hence exotic\n`high-mass seed' black holes at early cosmic time. It also predicts a large\npopulation of luminous but very obscured lower-mass quasars at $z>6$, possibly\nthe JWST `Little Red Dots'."
                },
                "authors": [
                    {
                        "name": "Knud Jahnke"
                    }
                ],
                "author_detail": {
                    "name": "Knud Jahnke"
                },
                "author": "Knud Jahnke",
                "arxiv_comment": "Accepted by the Open Journal of Astrophysics; 10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14550v1",
                "updated": "2025-01-24T14:53:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    53,
                    42,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:53:42Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    53,
                    42,
                    4,
                    24,
                    0
                ],
                "title": "Bean: A Language for Backward Error Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bean: A Language for Backward Error Analysis"
                },
                "summary": "Backward error analysis offers a method for assessing the quality of\nnumerical programs in the presence of floating-point rounding errors. However,\ntechniques from the numerical analysis literature for quantifying backward\nerror require substantial human effort, and there are currently no tools or\nautomated methods for statically deriving sound backward error bounds. To\naddress this gap, we propose Bean, a typed first-order programming language\ndesigned to express quantitative bounds on backward error. Bean's type system\ncombines a graded coeffect system with strict linearity to soundly track the\nflow of backward error through programs. We prove the soundness of our system\nusing a novel categorical semantics, where every Bean program denotes a triple\nof related transformations that together satisfy a backward error guarantee.\n  To illustrate Bean's potential as a practical tool for automated backward\nerror analysis, we implement a variety of standard algorithms from numerical\nlinear algebra in Bean, establishing fine-grained backward error bounds via\ntyping in a compositional style. We also develop a prototype implementation of\nBean that infers backward error bounds automatically. Our evaluation shows that\nthese inferred bounds match worst-case theoretical relative backward error\nbounds from the literature, underscoring Bean's utility in validating a key\nproperty of numerical programs: numerical stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward error analysis offers a method for assessing the quality of\nnumerical programs in the presence of floating-point rounding errors. However,\ntechniques from the numerical analysis literature for quantifying backward\nerror require substantial human effort, and there are currently no tools or\nautomated methods for statically deriving sound backward error bounds. To\naddress this gap, we propose Bean, a typed first-order programming language\ndesigned to express quantitative bounds on backward error. Bean's type system\ncombines a graded coeffect system with strict linearity to soundly track the\nflow of backward error through programs. We prove the soundness of our system\nusing a novel categorical semantics, where every Bean program denotes a triple\nof related transformations that together satisfy a backward error guarantee.\n  To illustrate Bean's potential as a practical tool for automated backward\nerror analysis, we implement a variety of standard algorithms from numerical\nlinear algebra in Bean, establishing fine-grained backward error bounds via\ntyping in a compositional style. We also develop a prototype implementation of\nBean that infers backward error bounds automatically. Our evaluation shows that\nthese inferred bounds match worst-case theoretical relative backward error\nbounds from the literature, underscoring Bean's utility in validating a key\nproperty of numerical programs: numerical stability."
                },
                "authors": [
                    {
                        "name": "Ariel E. Kellison"
                    },
                    {
                        "name": "Laura Zielinski"
                    },
                    {
                        "name": "David Bindel"
                    },
                    {
                        "name": "Justin Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Justin Hsu"
                },
                "author": "Justin Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14546v1",
                "updated": "2025-01-24T14:49:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    49,
                    0,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:49:00Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    49,
                    0,
                    4,
                    24,
                    0
                ],
                "title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite\n  Images by Poverty Level: Advancing Tools for Social Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite\n  Images by Poverty Level: Advancing Tools for Social Science Research"
                },
                "summary": "This paper investigates the novel application of Large Language Models (LLMs)\nwith vision capabilities to analyze satellite imagery for village-level poverty\nprediction. Although LLMs were originally designed for natural language\nunderstanding, their adaptability to multimodal tasks, including geospatial\nanalysis, has opened new frontiers in data-driven research. By leveraging\nadvancements in vision-enabled LLMs, we assess their ability to provide\ninterpretable, scalable, and reliable insights into human poverty from\nsatellite images. Using a pairwise comparison approach, we demonstrate that\nChatGPT can rank satellite images based on poverty levels with accuracy\ncomparable to domain experts. These findings highlight both the promise and the\nlimitations of LLMs in socioeconomic research, providing a foundation for their\nintegration into poverty assessment workflows. This study contributes to the\nongoing exploration of unconventional data sources for welfare analysis and\nopens pathways for cost-effective, large-scale poverty monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the novel application of Large Language Models (LLMs)\nwith vision capabilities to analyze satellite imagery for village-level poverty\nprediction. Although LLMs were originally designed for natural language\nunderstanding, their adaptability to multimodal tasks, including geospatial\nanalysis, has opened new frontiers in data-driven research. By leveraging\nadvancements in vision-enabled LLMs, we assess their ability to provide\ninterpretable, scalable, and reliable insights into human poverty from\nsatellite images. Using a pairwise comparison approach, we demonstrate that\nChatGPT can rank satellite images based on poverty levels with accuracy\ncomparable to domain experts. These findings highlight both the promise and the\nlimitations of LLMs in socioeconomic research, providing a foundation for their\nintegration into poverty assessment workflows. This study contributes to the\nongoing exploration of unconventional data sources for welfare analysis and\nopens pathways for cost-effective, large-scale poverty monitoring."
                },
                "authors": [
                    {
                        "name": "Hamid Sarmadi"
                    },
                    {
                        "name": "Ola Hall"
                    },
                    {
                        "name": "Thorsteinn Rögnvaldsson"
                    },
                    {
                        "name": "Mattias Ohlsson"
                    }
                ],
                "author_detail": {
                    "name": "Mattias Ohlsson"
                },
                "author": "Mattias Ohlsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14544v1",
                "updated": "2025-01-24T14:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    47,
                    42,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    47,
                    42,
                    4,
                    24,
                    0
                ],
                "title": "Distributed Conformal Prediction via Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Conformal Prediction via Message Passing"
                },
                "summary": "Post-hoc calibration of pre-trained models is critical for ensuring reliable\ninference, especially in safety-critical domains such as healthcare. Conformal\nPrediction (CP) offers a robust post-hoc calibration framework, providing\ndistribution-free statistical coverage guarantees for prediction sets by\nleveraging held-out datasets. In this work, we address a decentralized setting\nwhere each device has limited calibration data and can communicate only with\nits neighbors over an arbitrary graph topology. We propose two\nmessage-passing-based approaches for achieving reliable inference via CP:\nquantile-based distributed conformal prediction (Q-DCP) and histogram-based\ndistributed conformal prediction (H-DCP). Q-DCP employs distributed quantile\nregression enhanced with tailored smoothing and regularization terms to\naccelerate convergence, while H-DCP uses a consensus-based histogram estimation\napproach. Through extensive experiments, we investigate the trade-offs between\nhyperparameter tuning requirements, communication overhead, coverage\nguarantees, and prediction set sizes across different network topologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-hoc calibration of pre-trained models is critical for ensuring reliable\ninference, especially in safety-critical domains such as healthcare. Conformal\nPrediction (CP) offers a robust post-hoc calibration framework, providing\ndistribution-free statistical coverage guarantees for prediction sets by\nleveraging held-out datasets. In this work, we address a decentralized setting\nwhere each device has limited calibration data and can communicate only with\nits neighbors over an arbitrary graph topology. We propose two\nmessage-passing-based approaches for achieving reliable inference via CP:\nquantile-based distributed conformal prediction (Q-DCP) and histogram-based\ndistributed conformal prediction (H-DCP). Q-DCP employs distributed quantile\nregression enhanced with tailored smoothing and regularization terms to\naccelerate convergence, while H-DCP uses a consensus-based histogram estimation\napproach. Through extensive experiments, we investigate the trade-offs between\nhyperparameter tuning requirements, communication overhead, coverage\nguarantees, and prediction set sizes across different network topologies."
                },
                "authors": [
                    {
                        "name": "Haifeng Wen"
                    },
                    {
                        "name": "Hong Xing"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "16 pages, 11 figures, submitted for posssible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14540v1",
                "updated": "2025-01-24T14:45:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    45,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:45:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    45,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning"
                },
                "summary": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems"
                },
                "authors": [
                    {
                        "name": "Benjamin Callewaert"
                    },
                    {
                        "name": "Simon Vandevelde"
                    },
                    {
                        "name": "Joost Vennekens"
                    }
                ],
                "author_detail": {
                    "name": "Joost Vennekens"
                },
                "author": "Joost Vennekens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14534v1",
                "updated": "2025-01-24T14:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    40,
                    40,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    40,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting"
                },
                "summary": "Gaussian splatting (GS) for 3D reconstruction has become quite popular due to\ntheir fast training, inference speeds and high quality reconstruction. However,\nGS-based reconstructions generally consist of millions of Gaussians, which\nmakes them hard to use on computationally constrained devices such as\nsmartphones. In this paper, we first propose a principled analysis of advances\nin efficient GS methods. Then, we propose Trick-GS, which is a careful\ncombination of several strategies including (1) progressive training with\nresolution, noise and Gaussian scales, (2) learning to prune and mask\nprimitives and SH bands by their significance, and (3) accelerated GS training\nframework. Trick-GS takes a large step towards resource-constrained GS, where\nfaster run-time, smaller and faster-convergence of models is of paramount\nconcern. Our results on three datasets show that Trick-GS achieves up to 2x\nfaster training, 40x smaller disk size and 2x faster rendering speed compared\nto vanilla GS, while having comparable accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian splatting (GS) for 3D reconstruction has become quite popular due to\ntheir fast training, inference speeds and high quality reconstruction. However,\nGS-based reconstructions generally consist of millions of Gaussians, which\nmakes them hard to use on computationally constrained devices such as\nsmartphones. In this paper, we first propose a principled analysis of advances\nin efficient GS methods. Then, we propose Trick-GS, which is a careful\ncombination of several strategies including (1) progressive training with\nresolution, noise and Gaussian scales, (2) learning to prune and mask\nprimitives and SH bands by their significance, and (3) accelerated GS training\nframework. Trick-GS takes a large step towards resource-constrained GS, where\nfaster run-time, smaller and faster-convergence of models is of paramount\nconcern. Our results on three datasets show that Trick-GS achieves up to 2x\nfaster training, 40x smaller disk size and 2x faster rendering speed compared\nto vanilla GS, while having comparable accuracy."
                },
                "authors": [
                    {
                        "name": "Anil Armagan"
                    },
                    {
                        "name": "Albert Saà-Garriga"
                    },
                    {
                        "name": "Bruno Manganelli"
                    },
                    {
                        "name": "Mateusz Nowak"
                    },
                    {
                        "name": "Mehmet Kerim Yucel"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Kerim Yucel"
                },
                "author": "Mehmet Kerim Yucel",
                "arxiv_comment": "Accepted at ICASSP'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14531v1",
                "updated": "2025-01-24T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    37,
                    24,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:37:24Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    37,
                    24,
                    4,
                    24,
                    0
                ],
                "title": "On Hardening DNNs against Noisy Computations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hardening DNNs against Noisy Computations"
                },
                "summary": "The success of deep learning has sparked significant interest in designing\ncomputer hardware optimized for the high computational demands of neural\nnetwork inference. As further miniaturization of digital CMOS processors\nbecomes increasingly challenging, alternative computing paradigms, such as\nanalog computing, are gaining consideration. Particularly for compute-intensive\ntasks such as matrix multiplication, analog computing presents a promising\nalternative due to its potential for significantly higher energy efficiency\ncompared to conventional digital technology. However, analog computations are\ninherently noisy, which makes it challenging to maintain high accuracy on deep\nneural networks. This work investigates the effectiveness of training neural\nnetworks with quantization to increase the robustness against noise.\nExperimental results across various network architectures show that\nquantization-aware training with constant scaling factors enhances robustness.\nWe compare these methods with noisy training, which incorporates a noise\ninjection during training that mimics the noise encountered during inference.\nWhile both two methods increase tolerance against noise, noisy training emerges\nas the superior approach for achieving robust neural network performance,\nespecially in complex neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of deep learning has sparked significant interest in designing\ncomputer hardware optimized for the high computational demands of neural\nnetwork inference. As further miniaturization of digital CMOS processors\nbecomes increasingly challenging, alternative computing paradigms, such as\nanalog computing, are gaining consideration. Particularly for compute-intensive\ntasks such as matrix multiplication, analog computing presents a promising\nalternative due to its potential for significantly higher energy efficiency\ncompared to conventional digital technology. However, analog computations are\ninherently noisy, which makes it challenging to maintain high accuracy on deep\nneural networks. This work investigates the effectiveness of training neural\nnetworks with quantization to increase the robustness against noise.\nExperimental results across various network architectures show that\nquantization-aware training with constant scaling factors enhances robustness.\nWe compare these methods with noisy training, which incorporates a noise\ninjection during training that mimics the noise encountered during inference.\nWhile both two methods increase tolerance against noise, noisy training emerges\nas the superior approach for achieving robust neural network performance,\nespecially in complex neural architectures."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Hendrik Borras"
                    },
                    {
                        "name": "Bernhard Klein"
                    },
                    {
                        "name": "Holger Fröning"
                    }
                ],
                "author_detail": {
                    "name": "Holger Fröning"
                },
                "author": "Holger Fröning",
                "arxiv_comment": "Presented at AccML workshop co-located HiPEAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14520v1",
                "updated": "2025-01-24T14:23:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    23,
                    31,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:23:31Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    23,
                    31,
                    4,
                    24,
                    0
                ],
                "title": "Scene Understanding Enabled Semantic Communication with Open Channel\n  Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene Understanding Enabled Semantic Communication with Open Channel\n  Coding"
                },
                "summary": "As communication systems transition from symbol transmission to conveying\nmeaningful information, sixth-generation (6G) networks emphasize semantic\ncommunication. This approach prioritizes high-level semantic information,\nimproving robustness and reducing redundancy across modalities like text,\nspeech, and images. However, traditional semantic communication faces\nlimitations, including static coding strategies, poor generalization, and\nreliance on task-specific knowledge bases that hinder adaptability. To overcome\nthese challenges, we propose a novel system combining scene understanding,\nLarge Language Models (LLMs), and open channel coding, named \\textbf{OpenSC}.\nTraditional systems rely on fixed domain-specific knowledge bases, limiting\ntheir ability to generalize. Our open channel coding approach leverages shared,\npublicly available knowledge, enabling flexible, adaptive encoding. This\ndynamic system reduces reliance on static task-specific data, enhancing\nadaptability across diverse tasks and environments. Additionally, we use scene\ngraphs for structured semantic encoding, capturing object relationships and\ncontext to improve tasks like Visual Question Answering (VQA). Our approach\nselectively encodes key semantic elements, minimizing redundancy and improving\ntransmission efficiency. Experimental results show significant improvements in\nboth semantic understanding and efficiency, advancing the potential of\nadaptive, generalizable semantic communication in 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As communication systems transition from symbol transmission to conveying\nmeaningful information, sixth-generation (6G) networks emphasize semantic\ncommunication. This approach prioritizes high-level semantic information,\nimproving robustness and reducing redundancy across modalities like text,\nspeech, and images. However, traditional semantic communication faces\nlimitations, including static coding strategies, poor generalization, and\nreliance on task-specific knowledge bases that hinder adaptability. To overcome\nthese challenges, we propose a novel system combining scene understanding,\nLarge Language Models (LLMs), and open channel coding, named \\textbf{OpenSC}.\nTraditional systems rely on fixed domain-specific knowledge bases, limiting\ntheir ability to generalize. Our open channel coding approach leverages shared,\npublicly available knowledge, enabling flexible, adaptive encoding. This\ndynamic system reduces reliance on static task-specific data, enhancing\nadaptability across diverse tasks and environments. Additionally, we use scene\ngraphs for structured semantic encoding, capturing object relationships and\ncontext to improve tasks like Visual Question Answering (VQA). Our approach\nselectively encodes key semantic elements, minimizing redundancy and improving\ntransmission efficiency. Experimental results show significant improvements in\nboth semantic understanding and efficiency, advancing the potential of\nadaptive, generalizable semantic communication in 6G networks."
                },
                "authors": [
                    {
                        "name": "Zhe Xiang"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Quan Deng"
                    },
                    {
                        "name": "Yuandi Li"
                    },
                    {
                        "name": "Zhiguo Wan"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguo Wan"
                },
                "author": "Zhiguo Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14512v1",
                "updated": "2025-01-24T14:15:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    15,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:15:51Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    15,
                    51,
                    4,
                    24,
                    0
                ],
                "title": "Real-world Edge Neural Network Implementations Leak Private Interactions\n  Through Physical Side Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world Edge Neural Network Implementations Leak Private Interactions\n  Through Physical Side Channel"
                },
                "summary": "Neural networks have become a fundamental component of numerous practical\napplications, and their implementations, which are often accelerated by\nhardware, are integrated into all types of real-world physical devices. User\ninteractions with neural networks on hardware accelerators are commonly\nconsidered privacy-sensitive. Substantial efforts have been made to uncover\nvulnerabilities and enhance privacy protection at the level of machine learning\nalgorithms, including membership inference attacks, differential privacy, and\nfederated learning. However, neural networks are ultimately implemented and\ndeployed on physical devices, and current research pays comparatively less\nattention to privacy protection at the implementation level. In this paper, we\nintroduce a generic physical side-channel attack, ScaAR, that extracts user\ninteractions with neural networks by leveraging electromagnetic (EM) emissions\nof physical devices. Our proposed attack is implementation-agnostic, meaning it\ndoes not require the adversary to possess detailed knowledge of the hardware or\nsoftware implementations, thanks to the capabilities of deep learning-based\nside-channel analysis (DLSCA). Experimental results demonstrate that, through\nthe EM side channel, ScaAR can effectively extract the class label of user\ninteractions with neural classifiers, including inputs and outputs, on the\nAMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first\ntime, we provide side-channel analysis on edge Large Language Model (LLM)\nimplementations on the Raspberry Pi 5, showing that EM side channel leaks\ninteraction data, and different LLM tokens can be distinguishable from the EM\ntraces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have become a fundamental component of numerous practical\napplications, and their implementations, which are often accelerated by\nhardware, are integrated into all types of real-world physical devices. User\ninteractions with neural networks on hardware accelerators are commonly\nconsidered privacy-sensitive. Substantial efforts have been made to uncover\nvulnerabilities and enhance privacy protection at the level of machine learning\nalgorithms, including membership inference attacks, differential privacy, and\nfederated learning. However, neural networks are ultimately implemented and\ndeployed on physical devices, and current research pays comparatively less\nattention to privacy protection at the implementation level. In this paper, we\nintroduce a generic physical side-channel attack, ScaAR, that extracts user\ninteractions with neural networks by leveraging electromagnetic (EM) emissions\nof physical devices. Our proposed attack is implementation-agnostic, meaning it\ndoes not require the adversary to possess detailed knowledge of the hardware or\nsoftware implementations, thanks to the capabilities of deep learning-based\nside-channel analysis (DLSCA). Experimental results demonstrate that, through\nthe EM side channel, ScaAR can effectively extract the class label of user\ninteractions with neural classifiers, including inputs and outputs, on the\nAMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first\ntime, we provide side-channel analysis on edge Large Language Model (LLM)\nimplementations on the Raspberry Pi 5, showing that EM side channel leaks\ninteraction data, and different LLM tokens can be distinguishable from the EM\ntraces."
                },
                "authors": [
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Senna van Hoek"
                    },
                    {
                        "name": "Péter Horváth"
                    },
                    {
                        "name": "Dirk Lauret"
                    },
                    {
                        "name": "Xiaoyun Xu"
                    },
                    {
                        "name": "Lejla Batina"
                    }
                ],
                "author_detail": {
                    "name": "Lejla Batina"
                },
                "author": "Lejla Batina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14511v1",
                "updated": "2025-01-24T14:13:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    13,
                    44,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:13:44Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    13,
                    44,
                    4,
                    24,
                    0
                ],
                "title": "Improving 1D stellar atmosphere models with insights from\n  multi-dimensional simulations I. 1D vs 2D stratifications and spectral\n  comparison for O stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving 1D stellar atmosphere models with insights from\n  multi-dimensional simulations I. 1D vs 2D stratifications and spectral\n  comparison for O stars"
                },
                "summary": "We compare current 1D and multi-dimensional atmosphere modelling approaches\nfor massive stars to understand their strengths and shortcomings. We calculate\naveraged stratifications from selected 2D calculations for O stars --\ncorresponding to the spectral types O8, O4, and O2 -- to approximate them with\n1D stellar atmosphere models using the PoWR model atmosphere code and assuming\na fixed $\\beta-$law for the wind regime. We then study the effects of our\napproximations and assumptions on current spectral diagnostics. In particular,\nwe focus on the impact of an additional turbulent pressure in the subsonic\nlayers of the 1D models. To match the 2D averages, the 1D stellar atmosphere\nmodels need to account for turbulent pressure in the hydrostatic equation.\nMoreover, an adjustment of the connection point between the (quasi-)hydrostatic\nregime and the wind regime is required. The improvement between the density\nstratification of 1D model and 2D average can be further increased if the\nmass-loss rate of the 1D model is not identical to those of the 2D simulation,\nbut typically $\\sim0.2\\,$dex higher. Especially for the early type star, this\nimplies a significantly more extended envelope with a lower effective\ntemperature. Already the inclusion of a constant turbulence term in the\nsolution of the hydrostatic equation sufficiently reproduces the 2D-averaged\nmodel density stratifications. The addition of a significant turbulent motion\nalso smoothens the slope of the radiative acceleration term in the\n(quasi-)hydrostatic domain, with several potential implications on the total\nmass-loss rate inferred from 1D modelling. Concerning the spectral synthesis,\nthe addition of a turbulence term in the hydrostatic equation mimics the effect\nof a lower surface gravity, potentially presenting a solution to the ``mass\ndiscrepancy problem'' between the evolutionary and spectroscopy mass\ndeterminations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compare current 1D and multi-dimensional atmosphere modelling approaches\nfor massive stars to understand their strengths and shortcomings. We calculate\naveraged stratifications from selected 2D calculations for O stars --\ncorresponding to the spectral types O8, O4, and O2 -- to approximate them with\n1D stellar atmosphere models using the PoWR model atmosphere code and assuming\na fixed $\\beta-$law for the wind regime. We then study the effects of our\napproximations and assumptions on current spectral diagnostics. In particular,\nwe focus on the impact of an additional turbulent pressure in the subsonic\nlayers of the 1D models. To match the 2D averages, the 1D stellar atmosphere\nmodels need to account for turbulent pressure in the hydrostatic equation.\nMoreover, an adjustment of the connection point between the (quasi-)hydrostatic\nregime and the wind regime is required. The improvement between the density\nstratification of 1D model and 2D average can be further increased if the\nmass-loss rate of the 1D model is not identical to those of the 2D simulation,\nbut typically $\\sim0.2\\,$dex higher. Especially for the early type star, this\nimplies a significantly more extended envelope with a lower effective\ntemperature. Already the inclusion of a constant turbulence term in the\nsolution of the hydrostatic equation sufficiently reproduces the 2D-averaged\nmodel density stratifications. The addition of a significant turbulent motion\nalso smoothens the slope of the radiative acceleration term in the\n(quasi-)hydrostatic domain, with several potential implications on the total\nmass-loss rate inferred from 1D modelling. Concerning the spectral synthesis,\nthe addition of a turbulence term in the hydrostatic equation mimics the effect\nof a lower surface gravity, potentially presenting a solution to the ``mass\ndiscrepancy problem'' between the evolutionary and spectroscopy mass\ndeterminations."
                },
                "authors": [
                    {
                        "name": "G. González-Torà"
                    },
                    {
                        "name": "A. A. C. Sander"
                    },
                    {
                        "name": "J. O. Sundqvist"
                    },
                    {
                        "name": "D. Debnath"
                    },
                    {
                        "name": "L. Delbroek"
                    },
                    {
                        "name": "J. Josiek"
                    },
                    {
                        "name": "R. R. Lefever"
                    },
                    {
                        "name": "N. Moens"
                    },
                    {
                        "name": "C. Van der Sijpt"
                    },
                    {
                        "name": "O. Verhamme"
                    }
                ],
                "author_detail": {
                    "name": "O. Verhamme"
                },
                "author": "O. Verhamme",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14499v1",
                "updated": "2025-01-24T13:59:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:59:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "Automated Assignment Grading with Large Language Models: Insights From a\n  Bioinformatics Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Assignment Grading with Large Language Models: Insights From a\n  Bioinformatics Course"
                },
                "summary": "Providing students with individualized feedback through assignments is a\ncornerstone of education that supports their learning and development. Studies\nhave shown that timely, high-quality feedback plays a critical role in\nimproving learning outcomes. However, providing personalized feedback on a\nlarge scale in classes with large numbers of students is often impractical due\nto the significant time and effort required. Recent advances in natural\nlanguage processing and large language models (LLMs) offer a promising solution\nby enabling the efficient delivery of personalized feedback. These technologies\ncan reduce the workload of course staff while improving student satisfaction\nand learning outcomes. Their successful implementation, however, requires\nthorough evaluation and validation in real classrooms. We present the results\nof a practical evaluation of LLM-based graders for written assignments in the\n2024/25 iteration of the Introduction to Bioinformatics course at the\nUniversity of Ljubljana. Over the course of the semester, more than 100\nstudents answered 36 text-based questions, most of which were automatically\ngraded using LLMs. In a blind study, students received feedback from both LLMs\nand human teaching assistants without knowing the source, and later rated the\nquality of the feedback. We conducted a systematic evaluation of six commercial\nand open-source LLMs and compared their grading performance with human teaching\nassistants. Our results show that with well-designed prompts, LLMs can achieve\ngrading accuracy and feedback quality comparable to human graders. Our results\nalso suggest that open-source LLMs perform as well as commercial LLMs, allowing\nschools to implement their own grading systems while maintaining privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing students with individualized feedback through assignments is a\ncornerstone of education that supports their learning and development. Studies\nhave shown that timely, high-quality feedback plays a critical role in\nimproving learning outcomes. However, providing personalized feedback on a\nlarge scale in classes with large numbers of students is often impractical due\nto the significant time and effort required. Recent advances in natural\nlanguage processing and large language models (LLMs) offer a promising solution\nby enabling the efficient delivery of personalized feedback. These technologies\ncan reduce the workload of course staff while improving student satisfaction\nand learning outcomes. Their successful implementation, however, requires\nthorough evaluation and validation in real classrooms. We present the results\nof a practical evaluation of LLM-based graders for written assignments in the\n2024/25 iteration of the Introduction to Bioinformatics course at the\nUniversity of Ljubljana. Over the course of the semester, more than 100\nstudents answered 36 text-based questions, most of which were automatically\ngraded using LLMs. In a blind study, students received feedback from both LLMs\nand human teaching assistants without knowing the source, and later rated the\nquality of the feedback. We conducted a systematic evaluation of six commercial\nand open-source LLMs and compared their grading performance with human teaching\nassistants. Our results show that with well-designed prompts, LLMs can achieve\ngrading accuracy and feedback quality comparable to human graders. Our results\nalso suggest that open-source LLMs perform as well as commercial LLMs, allowing\nschools to implement their own grading systems while maintaining privacy."
                },
                "authors": [
                    {
                        "name": "Pavlin G. Poličar"
                    },
                    {
                        "name": "Martin Špendl"
                    },
                    {
                        "name": "Tomaž Curk"
                    },
                    {
                        "name": "Blaž Zupan"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Zupan"
                },
                "author": "Blaž Zupan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14809v3",
                "updated": "2025-01-24T13:54:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    54,
                    5,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-19T12:57:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zeao Tu"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "Accepted by NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14497v1",
                "updated": "2025-01-24T13:53:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    53,
                    54,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:53:54Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    53,
                    54,
                    4,
                    24,
                    0
                ],
                "title": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Wanqiu Long"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Victor Gutierrez Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14495v1",
                "updated": "2025-01-24T13:51:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    51,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:51:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    51,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "BILLNET: A Binarized Conv3D-LSTM Network with Logic-gated residual\n  architecture for hardware-efficient video inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BILLNET: A Binarized Conv3D-LSTM Network with Logic-gated residual\n  architecture for hardware-efficient video inference"
                },
                "summary": "Long Short-Term Memory (LSTM) and 3D convolution (Conv3D) show impressive\nresults for many video-based applications but require large memory and\nintensive computing. Motivated by recent works on hardware-algorithmic\nco-design towards efficient inference, we propose a compact binarized\nConv3D-LSTM model architecture called BILLNET, compatible with a highly\nresource-constrained hardware. Firstly, BILLNET proposes to factorize the\ncostly standard Conv3D by two pointwise convolutions with a grouped convolution\nin-between. Secondly, BILLNET enables binarized weights and activations via a\nMUX-OR-gated residual architecture. Finally, to efficiently train BILLNET, we\npropose a multi-stage training strategy enabling to fully quantize LSTM layers.\nResults on Jester dataset show that our method can obtain high accuracy with\nextremely low memory and computational budgets compared to existing Conv3D\nresource-efficient models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Short-Term Memory (LSTM) and 3D convolution (Conv3D) show impressive\nresults for many video-based applications but require large memory and\nintensive computing. Motivated by recent works on hardware-algorithmic\nco-design towards efficient inference, we propose a compact binarized\nConv3D-LSTM model architecture called BILLNET, compatible with a highly\nresource-constrained hardware. Firstly, BILLNET proposes to factorize the\ncostly standard Conv3D by two pointwise convolutions with a grouped convolution\nin-between. Secondly, BILLNET enables binarized weights and activations via a\nMUX-OR-gated residual architecture. Finally, to efficiently train BILLNET, we\npropose a multi-stage training strategy enabling to fully quantize LSTM layers.\nResults on Jester dataset show that our method can obtain high accuracy with\nextremely low memory and computational budgets compared to existing Conv3D\nresource-efficient models."
                },
                "authors": [
                    {
                        "name": "Van Thien Nguyen"
                    },
                    {
                        "name": "William Guicquero"
                    },
                    {
                        "name": "Gilles Sicard"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Sicard"
                },
                "author": "Gilles Sicard",
                "arxiv_doi": "10.1109/SiPS55645.2022.9919206",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SiPS55645.2022.9919206",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at IEEE SiPS 2022",
                "arxiv_journal_ref": "2022 IEEE Workshop on Signal Processing Systems (SiPS), Rennes,\n  France, 2022, pp. 1-6",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14492v1",
                "updated": "2025-01-24T13:48:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    48,
                    10,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:48:10Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    48,
                    10,
                    4,
                    24,
                    0
                ],
                "title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model\n  Critiques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model\n  Critiques"
                },
                "summary": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback for\nothers by identifying flaws and suggesting improvements. However, evaluating\nthe critique capabilities of LLMs presents a significant challenge due to the\nopen-ended nature of the task. In this work, we introduce a new benchmark\ndesigned to assess the critique capabilities of LLMs. Unlike existing\nbenchmarks, which typically function in an open-loop fashion, our approach\nemploys a closed-loop methodology that evaluates the quality of corrections\ngenerated from critiques. Moreover, the benchmark incorporates features such as\nself-critique, cross-critique, and iterative critique, which are crucial for\ndistinguishing the abilities of advanced reasoning models from more classical\nones. We implement this benchmark using eight challenging reasoning tasks. We\nhave several interesting findings. First, despite demonstrating comparable\nperformance in direct chain-of-thought generation, classical LLMs significantly\nlag behind the advanced reasoning-based model o1-mini across all critique\nscenarios. Second, in self-critique and iterative critique settings, classical\nLLMs may even underperform relative to their baseline capabilities. We hope\nthat this benchmark will serve as a valuable resource to guide future\nadvancements. The code and data are available at\n\\url{https://github.com/tangzhy/RealCritic}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback for\nothers by identifying flaws and suggesting improvements. However, evaluating\nthe critique capabilities of LLMs presents a significant challenge due to the\nopen-ended nature of the task. In this work, we introduce a new benchmark\ndesigned to assess the critique capabilities of LLMs. Unlike existing\nbenchmarks, which typically function in an open-loop fashion, our approach\nemploys a closed-loop methodology that evaluates the quality of corrections\ngenerated from critiques. Moreover, the benchmark incorporates features such as\nself-critique, cross-critique, and iterative critique, which are crucial for\ndistinguishing the abilities of advanced reasoning models from more classical\nones. We implement this benchmark using eight challenging reasoning tasks. We\nhave several interesting findings. First, despite demonstrating comparable\nperformance in direct chain-of-thought generation, classical LLMs significantly\nlag behind the advanced reasoning-based model o1-mini across all critique\nscenarios. Second, in self-critique and iterative critique settings, classical\nLLMs may even underperform relative to their baseline capabilities. We hope\nthat this benchmark will serve as a valuable resource to guide future\nadvancements. The code and data are available at\n\\url{https://github.com/tangzhy/RealCritic}."
                },
                "authors": [
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Zhenyang Xiao"
                    },
                    {
                        "name": "Tian Ding"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14484v1",
                "updated": "2025-01-24T13:37:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    37,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:37:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    37,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility"
                },
                "summary": "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06605v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06605v3",
                "updated": "2025-01-24T13:29:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    29,
                    33,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-11T18:11:07Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    18,
                    11,
                    7,
                    5,
                    11,
                    0
                ],
                "title": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation"
                },
                "summary": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06605v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06605v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22948v2",
                "updated": "2025-01-24T13:16:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    16,
                    6,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-30T12:05:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    5,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "ELBOing Stein: Variational Bayes with Stein Mixture Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELBOing Stein: Variational Bayes with Stein Mixture Inference"
                },
                "summary": "Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs\napproximate Bayesian inference by representing the posterior with a set of\nparticles. However, SVGD suffers from variance collapse, i.e. poor predictions\ndue to underestimating uncertainty [Ba et al., 2021], even for\nmoderately-dimensional models such as small Bayesian neural networks (BNNs). To\naddress this issue, we generalize SVGD by letting each particle parameterize a\ncomponent distribution in a mixture model. Our method, Stein Mixture Inference\n(SMI), optimizes a lower bound to the evidence (ELBO) and introduces\nuser-specified guides parameterized by particles. SMI extends the Nonlinear\nSVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI\neffectively avoids variance collapse, judging by a previously described test\ndeveloped for this purpose, and performs well on standard data sets. In\naddition, SMI requires considerably fewer particles than SVGD to accurately\nestimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO\noptimization and user-specified guides establishes a promising approach towards\nvariational Bayesian inference in the case of tall and wide data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs\napproximate Bayesian inference by representing the posterior with a set of\nparticles. However, SVGD suffers from variance collapse, i.e. poor predictions\ndue to underestimating uncertainty [Ba et al., 2021], even for\nmoderately-dimensional models such as small Bayesian neural networks (BNNs). To\naddress this issue, we generalize SVGD by letting each particle parameterize a\ncomponent distribution in a mixture model. Our method, Stein Mixture Inference\n(SMI), optimizes a lower bound to the evidence (ELBO) and introduces\nuser-specified guides parameterized by particles. SMI extends the Nonlinear\nSVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI\neffectively avoids variance collapse, judging by a previously described test\ndeveloped for this purpose, and performs well on standard data sets. In\naddition, SMI requires considerably fewer particles than SVGD to accurately\nestimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO\noptimization and user-specified guides establishes a promising approach towards\nvariational Bayesian inference in the case of tall and wide data."
                },
                "authors": [
                    {
                        "name": "Ola Rønning"
                    },
                    {
                        "name": "Eric Nalisnick"
                    },
                    {
                        "name": "Christophe Ley"
                    },
                    {
                        "name": "Padhraic Smyth"
                    },
                    {
                        "name": "Thomas Hamelryck"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hamelryck"
                },
                "author": "Thomas Hamelryck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14465v1",
                "updated": "2025-01-24T12:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    54,
                    19,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    54,
                    19,
                    4,
                    24,
                    0
                ],
                "title": "Boundary Value Test Input Generation Using Prompt Engineering with LLMs:\n  Fault Detection and Coverage Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boundary Value Test Input Generation Using Prompt Engineering with LLMs:\n  Fault Detection and Coverage Analysis"
                },
                "summary": "As software systems grow more complex, automated testing has become essential\nto ensuring reliability and performance. Traditional methods for boundary value\ntest input generation can be time-consuming and may struggle to address all\npotential error cases effectively, especially in systems with intricate or\nhighly variable boundaries. This paper presents a framework for assessing the\neffectiveness of large language models (LLMs) in generating boundary value test\ninputs for white-box software testing by examining their potential through\nprompt engineering. Specifically, we evaluate the effectiveness of LLM-based\ntest input generation by analyzing fault detection rates and test coverage,\ncomparing these LLM-generated test sets with those produced using traditional\nboundary value analysis methods. Our analysis shows the strengths and\nlimitations of LLMs in boundary value generation, particularly in detecting\ncommon boundary-related issues. However, they still face challenges in certain\nareas, especially when handling complex or less common test inputs. This\nresearch provides insights into the role of LLMs in boundary value testing,\nunderscoring both their potential and areas for improvement in automated\ntesting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software systems grow more complex, automated testing has become essential\nto ensuring reliability and performance. Traditional methods for boundary value\ntest input generation can be time-consuming and may struggle to address all\npotential error cases effectively, especially in systems with intricate or\nhighly variable boundaries. This paper presents a framework for assessing the\neffectiveness of large language models (LLMs) in generating boundary value test\ninputs for white-box software testing by examining their potential through\nprompt engineering. Specifically, we evaluate the effectiveness of LLM-based\ntest input generation by analyzing fault detection rates and test coverage,\ncomparing these LLM-generated test sets with those produced using traditional\nboundary value analysis methods. Our analysis shows the strengths and\nlimitations of LLMs in boundary value generation, particularly in detecting\ncommon boundary-related issues. However, they still face challenges in certain\nareas, especially when handling complex or less common test inputs. This\nresearch provides insights into the role of LLMs in boundary value testing,\nunderscoring both their potential and areas for improvement in automated\ntesting methods."
                },
                "authors": [
                    {
                        "name": "Xiujing Guo"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Tatsuhiro Tsuchiya"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuhiro Tsuchiya"
                },
                "author": "Tatsuhiro Tsuchiya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00324v2",
                "updated": "2025-01-24T12:49:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    49,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2023-12-30T21:21:16Z",
                "published_parsed": [
                    2023,
                    12,
                    30,
                    21,
                    21,
                    16,
                    5,
                    364,
                    0
                ],
                "title": "Stratified distance space improves the efficiency of sequential samplers\n  for approximate Bayesian computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratified distance space improves the efficiency of sequential samplers\n  for approximate Bayesian computation"
                },
                "summary": "Approximate Bayesian computation (ABC) methods are standard tools for\ninferring parameters of complex models when the likelihood function is\nanalytically intractable. A popular approach to improving the poor acceptance\nrate of the basic rejection sampling ABC algorithm is to use sequential Monte\nCarlo (ABC SMC) to produce a sequence of proposal distributions adapting\ntowards the posterior, instead of generating values from the prior distribution\nof the model parameters. Proposal distribution for the subsequent iteration is\ntypically obtained from a weighted set of samples, often called particles, of\nthe current iteration of this sequence. Current methods for constructing these\nproposal distributions treat all the particles equivalently, regardless of the\ncorresponding value generated by the sampler, which may lead to inefficiency\nwhen propagating the information across iterations of the algorithm. To improve\nsampler efficiency, we introduce a modified approach called stratified distance\nABC SMC. Our algorithm stratifies particles based on their distance between the\ncorresponding synthetic and observed data, and then constructs distinct\nproposal distributions for all the strata. Taking into account the distribution\nof distances across the particle space leads to substantially improved\nacceptance rate of the rejection sampling. We further show that efficiency can\nbe gained by introducing a novel stopping rule for the sequential process based\non the stratified posterior samples and demonstrate these advances by several\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Bayesian computation (ABC) methods are standard tools for\ninferring parameters of complex models when the likelihood function is\nanalytically intractable. A popular approach to improving the poor acceptance\nrate of the basic rejection sampling ABC algorithm is to use sequential Monte\nCarlo (ABC SMC) to produce a sequence of proposal distributions adapting\ntowards the posterior, instead of generating values from the prior distribution\nof the model parameters. Proposal distribution for the subsequent iteration is\ntypically obtained from a weighted set of samples, often called particles, of\nthe current iteration of this sequence. Current methods for constructing these\nproposal distributions treat all the particles equivalently, regardless of the\ncorresponding value generated by the sampler, which may lead to inefficiency\nwhen propagating the information across iterations of the algorithm. To improve\nsampler efficiency, we introduce a modified approach called stratified distance\nABC SMC. Our algorithm stratifies particles based on their distance between the\ncorresponding synthetic and observed data, and then constructs distinct\nproposal distributions for all the strata. Taking into account the distribution\nof distances across the particle space leads to substantially improved\nacceptance rate of the rejection sampling. We further show that efficiency can\nbe gained by introducing a novel stopping rule for the sequential process based\non the stratified posterior samples and demonstrate these advances by several\nexamples."
                },
                "authors": [
                    {
                        "name": "Henri Pesonen"
                    },
                    {
                        "name": "Jukka Corander"
                    }
                ],
                "author_detail": {
                    "name": "Jukka Corander"
                },
                "author": "Jukka Corander",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14462v1",
                "updated": "2025-01-24T12:48:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    48,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:48:51Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    48,
                    51,
                    4,
                    24,
                    0
                ],
                "title": "The wide binary frequency of metal-poor stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide binary frequency of metal-poor stars"
                },
                "summary": "This study is aimed at identifying possible low-mass and sub-stellar\ncompanions to stars with well-determined metallicities. We investigate the\nmultiplicity of metal-poor stars along with its impact on formation processes\nin the conditions of the early universe. Our goal is to look for wide common\nproper motion companions to metal-poor stars and study the binarity frequency\nat low metallicity with astrometry from large-scale catalogues (Gaia, VHS, and\nWISE). We used the stellar parameter determination from the latest release of\nGaia to identify metal-poor stars over the entire sky. We combined the Gaia\nsample with other public catalogues and spectroscopic determinations for a\ngiven subsample to refine the stellar metallicities. We also considered other\npublic catalogues of metal-poor stars to look for co-moving companions. We\nobtained our own high-resolution images of a subsample with the lucky imaging\ntechnique. We found a few bona fide co-moving systems among a sample of 610\nmetal-poor stars with metallicities below -1.5 dex in the full sky. We inferred\na multiplicity rate below 3%, with 3sigma completeness for projected\nseparations larger than 8 au, after taking into account incompleteness and any\nother limiting factors of our search. At closer separations, we found a minimum\nbinary fraction of 20% that appears to be relatively independent of\nmetallicity. We conclude that the multiplicity fraction of solar-type stars is\nrelatively independent of metallicity for close-in companions with projected\nseparations below ~8 au. Between 8 and 10000 au, the binary fraction of\nmetal-poor stars drops significantly to a few percent and is significantly\nlower than the multiplicity derived for the solar-metallicity case. We\ninterpret these similarities and differences as being due to the chemistry at\nwork in molecular clouds as well as disruption effects attributed to the old\nage of subdwarfs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study is aimed at identifying possible low-mass and sub-stellar\ncompanions to stars with well-determined metallicities. We investigate the\nmultiplicity of metal-poor stars along with its impact on formation processes\nin the conditions of the early universe. Our goal is to look for wide common\nproper motion companions to metal-poor stars and study the binarity frequency\nat low metallicity with astrometry from large-scale catalogues (Gaia, VHS, and\nWISE). We used the stellar parameter determination from the latest release of\nGaia to identify metal-poor stars over the entire sky. We combined the Gaia\nsample with other public catalogues and spectroscopic determinations for a\ngiven subsample to refine the stellar metallicities. We also considered other\npublic catalogues of metal-poor stars to look for co-moving companions. We\nobtained our own high-resolution images of a subsample with the lucky imaging\ntechnique. We found a few bona fide co-moving systems among a sample of 610\nmetal-poor stars with metallicities below -1.5 dex in the full sky. We inferred\na multiplicity rate below 3%, with 3sigma completeness for projected\nseparations larger than 8 au, after taking into account incompleteness and any\nother limiting factors of our search. At closer separations, we found a minimum\nbinary fraction of 20% that appears to be relatively independent of\nmetallicity. We conclude that the multiplicity fraction of solar-type stars is\nrelatively independent of metallicity for close-in companions with projected\nseparations below ~8 au. Between 8 and 10000 au, the binary fraction of\nmetal-poor stars drops significantly to a few percent and is significantly\nlower than the multiplicity derived for the solar-metallicity case. We\ninterpret these similarities and differences as being due to the chemistry at\nwork in molecular clouds as well as disruption effects attributed to the old\nage of subdwarfs."
                },
                "authors": [
                    {
                        "name": "N. Lodieu"
                    },
                    {
                        "name": "A. Pérez Garrido"
                    },
                    {
                        "name": "J. -Y. Zhang"
                    },
                    {
                        "name": "E. L. Martín"
                    },
                    {
                        "name": "R. Rebolo López"
                    },
                    {
                        "name": "F. Pérez-Toledo"
                    },
                    {
                        "name": "R. Clavero"
                    },
                    {
                        "name": "D. Nespral"
                    }
                ],
                "author_detail": {
                    "name": "D. Nespral"
                },
                "arxiv_affiliation": "Departamento de Astrofísica, Universidad de La Laguna",
                "author": "D. Nespral",
                "arxiv_comment": "Accepted to A&A. 12 pages, 5 figures, 1 table, 4 appendices with\n  supplementary data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14457v1",
                "updated": "2025-01-24T12:41:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    41,
                    30,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:41:30Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    41,
                    30,
                    4,
                    24,
                    0
                ],
                "title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable\n  Neuron Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Gender Bias in LLMs via Interpretable\n  Neuron Editing"
                },
                "summary": "Large language models (LLMs) often exhibit gender bias, posing challenges for\ntheir safe deployment. Existing methods to mitigate bias lack a comprehensive\nunderstanding of its mechanisms or compromise the model's core capabilities. To\naddress these issues, we propose the CommonWords dataset, to systematically\nevaluate gender bias in LLMs. Our analysis reveals pervasive bias across models\nand identifies specific neuron circuits, including gender neurons and general\nneurons, responsible for this behavior. Notably, editing even a small number of\ngeneral neurons can disrupt the model's overall capabilities due to\nhierarchical neuron interactions. Based on these insights, we propose an\ninterpretable neuron editing method that combines logit-based and causal-based\nstrategies to selectively target biased neurons. Experiments on five LLMs\ndemonstrate that our method effectively reduces gender bias while preserving\nthe model's original capabilities, outperforming existing fine-tuning and\nediting approaches. Our findings contribute a novel dataset, a detailed\nanalysis of bias mechanisms, and a practical solution for mitigating gender\nbias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit gender bias, posing challenges for\ntheir safe deployment. Existing methods to mitigate bias lack a comprehensive\nunderstanding of its mechanisms or compromise the model's core capabilities. To\naddress these issues, we propose the CommonWords dataset, to systematically\nevaluate gender bias in LLMs. Our analysis reveals pervasive bias across models\nand identifies specific neuron circuits, including gender neurons and general\nneurons, responsible for this behavior. Notably, editing even a small number of\ngeneral neurons can disrupt the model's overall capabilities due to\nhierarchical neuron interactions. Based on these insights, we propose an\ninterpretable neuron editing method that combines logit-based and causal-based\nstrategies to selectively target biased neurons. Experiments on five LLMs\ndemonstrate that our method effectively reduces gender bias while preserving\nthe model's original capabilities, outperforming existing fine-tuning and\nediting approaches. Our findings contribute a novel dataset, a detailed\nanalysis of bias mechanisms, and a practical solution for mitigating gender\nbias in LLMs."
                },
                "authors": [
                    {
                        "name": "Zeping Yu"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14446v1",
                "updated": "2025-01-24T12:28:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    28,
                    37,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:28:37Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    28,
                    37,
                    4,
                    24,
                    0
                ],
                "title": "Ultrafast neural sampling with spiking nanolasers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast neural sampling with spiking nanolasers"
                },
                "summary": "Owing to their significant advantages in terms of bandwidth, power efficiency\nand especially speed, optical neuromorphic systems have arisen as interesting\nalternatives to conventional semiconductor devices. Recently, photonic crystal\nnanolasers with excitable behaviour were first demonstrated. Depending on the\npumping strength, they emit short optical pulses -- spikes -- at various\nintervals on a nanosecond timescale. In this theoretical work, we show how\nnetworks of such photonic spiking neurons can be used for Bayesian inference\nthrough sampling from learned probability distributions. We provide a detailed\nderivation of translation rules from conventional sampling networks such as\nBoltzmann machines to photonic spiking networks and demonstrate their\nfunctionality across a range of generative tasks. Finally, we provide estimates\nof processing speed and power consumption, for which we expect improvements of\nseveral orders of magnitude over current state-of-the-art neuromorphic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to their significant advantages in terms of bandwidth, power efficiency\nand especially speed, optical neuromorphic systems have arisen as interesting\nalternatives to conventional semiconductor devices. Recently, photonic crystal\nnanolasers with excitable behaviour were first demonstrated. Depending on the\npumping strength, they emit short optical pulses -- spikes -- at various\nintervals on a nanosecond timescale. In this theoretical work, we show how\nnetworks of such photonic spiking neurons can be used for Bayesian inference\nthrough sampling from learned probability distributions. We provide a detailed\nderivation of translation rules from conventional sampling networks such as\nBoltzmann machines to photonic spiking networks and demonstrate their\nfunctionality across a range of generative tasks. Finally, we provide estimates\nof processing speed and power consumption, for which we expect improvements of\nseveral orders of magnitude over current state-of-the-art neuromorphic systems."
                },
                "authors": [
                    {
                        "name": "Ivan K. Boikov"
                    },
                    {
                        "name": "Alfredo de Rossi"
                    },
                    {
                        "name": "Mihai A. Petrovici"
                    }
                ],
                "author_detail": {
                    "name": "Mihai A. Petrovici"
                },
                "author": "Mihai A. Petrovici",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14431v1",
                "updated": "2025-01-24T11:57:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    57,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:57:39Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    57,
                    39,
                    4,
                    24,
                    0
                ],
                "title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes\n  Domains"
                },
                "summary": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://anonymous.4open.science/r/Domaino1s-006F/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://anonymous.4open.science/r/Domaino1s-006F/."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Hanlin Xue"
                    },
                    {
                        "name": "Guanyu Wang"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14427v1",
                "updated": "2025-01-24T11:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    55,
                    57,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:55:57Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    55,
                    57,
                    4,
                    24,
                    0
                ],
                "title": "GraphBC: Improving LLMs for Better Graph Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphBC: Improving LLMs for Better Graph Data Processing"
                },
                "summary": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe that when the order of nodes or\nedges in the natural language description of a graph is shuffled, despite\ndescribing the same graph, model performance fluctuates between high\nperformance and random guessing. Additionally, due to the limited input context\nlength of LLMs, current methods typically randomly sample neighbors of target\nnodes as representatives of their neighborhood, which may not always be\neffective for accurate reasoning. To address these gaps, we introduce GraphBC.\nThis novel model framework features an Order Selector Module to ensure proper\nserialization order of the graph and a Subgraph Sampling Module to sample\nsubgraphs with better structure for better reasoning. Furthermore, we propose\nGraph CoT obtained through distillation, and enhance LLM's reasoning and\nzero-shot learning capabilities for graph tasks through instruction tuning.\nExperiments on multiple datasets for node classification and graph\nquestion-answering demonstrate that GraphBC improves LLMs' performance and\ngeneralization ability on graph tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe that when the order of nodes or\nedges in the natural language description of a graph is shuffled, despite\ndescribing the same graph, model performance fluctuates between high\nperformance and random guessing. Additionally, due to the limited input context\nlength of LLMs, current methods typically randomly sample neighbors of target\nnodes as representatives of their neighborhood, which may not always be\neffective for accurate reasoning. To address these gaps, we introduce GraphBC.\nThis novel model framework features an Order Selector Module to ensure proper\nserialization order of the graph and a Subgraph Sampling Module to sample\nsubgraphs with better structure for better reasoning. Furthermore, we propose\nGraph CoT obtained through distillation, and enhance LLM's reasoning and\nzero-shot learning capabilities for graph tasks through instruction tuning.\nExperiments on multiple datasets for node classification and graph\nquestion-answering demonstrate that GraphBC improves LLMs' performance and\ngeneralization ability on graph tasks."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Hanlin Xue"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Bingce Wang"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08027v2",
                "updated": "2025-01-24T11:53:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    53,
                    43,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-12T13:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "title": "iLLuMinaTE: An LLM-XAI Framework Leveraging Social Science Explanation\n  Theories Towards Actionable Student Performance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iLLuMinaTE: An LLM-XAI Framework Leveraging Social Science Explanation\n  Theories Towards Actionable Student Performance Feedback"
                },
                "summary": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    },
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Bhargav Srinivasa Desikan"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Tanja Käser"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Käser"
                },
                "author": "Tanja Käser",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14423v1",
                "updated": "2025-01-24T11:50:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    50,
                    0,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:50:00Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    50,
                    0,
                    4,
                    24,
                    0
                ],
                "title": "Human Activity Recognition with a 6.5 GHz Reconfigurable Intelligent\n  Surface for Wi-Fi 6E",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Activity Recognition with a 6.5 GHz Reconfigurable Intelligent\n  Surface for Wi-Fi 6E"
                },
                "summary": "Human Activity Recognition (HAR) is the identification and classification of\nstatic and dynamic human activities, which find applicability in domains like\nhealthcare, entertainment, security, and cyber-physical systems. Traditional\nHAR approaches rely on wearable sensors, vision-based systems, or ambient\nsensing, each with inherent limitations such as privacy concerns or restricted\nsensing conditions. Recently, Radio Frequency (RF)-based HAR has emerged,\nrelying on the interaction of RF signals with people to infer activities.\nReconfigurable Intelligent Surfaces (RISs) offers significant potential in this\ndomain by enabling dynamic control over the wireless environment, thus\nenhancing the information extracted from RF signals. We present an Hand Gesture\nRecognition (HGR) approach that employs our own 6.5 GHz RIS design to\nmanipulate the RF medium in an area of interest. We validate the capability of\nour RIS to control the medium by characterizing its steering response, and\nfurther we gather and publish a dataset for HGR classification for three\ndifferent hand gestures. By employing two Convolutional Neural Networks (CNNs)\nmodels trained on data gathered under random and optimized RIS configuration\nsequences, we achieved classification accuracies exceeding 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Activity Recognition (HAR) is the identification and classification of\nstatic and dynamic human activities, which find applicability in domains like\nhealthcare, entertainment, security, and cyber-physical systems. Traditional\nHAR approaches rely on wearable sensors, vision-based systems, or ambient\nsensing, each with inherent limitations such as privacy concerns or restricted\nsensing conditions. Recently, Radio Frequency (RF)-based HAR has emerged,\nrelying on the interaction of RF signals with people to infer activities.\nReconfigurable Intelligent Surfaces (RISs) offers significant potential in this\ndomain by enabling dynamic control over the wireless environment, thus\nenhancing the information extracted from RF signals. We present an Hand Gesture\nRecognition (HGR) approach that employs our own 6.5 GHz RIS design to\nmanipulate the RF medium in an area of interest. We validate the capability of\nour RIS to control the medium by characterizing its steering response, and\nfurther we gather and publish a dataset for HGR classification for three\ndifferent hand gestures. By employing two Convolutional Neural Networks (CNNs)\nmodels trained on data gathered under random and optimized RIS configuration\nsequences, we achieved classification accuracies exceeding 90%."
                },
                "authors": [
                    {
                        "name": "Nuno Paulino"
                    },
                    {
                        "name": "Mariana Oliveira"
                    },
                    {
                        "name": "Francisco Ribeiro"
                    },
                    {
                        "name": "Luís Outeiro"
                    },
                    {
                        "name": "Pedro A. Lopes"
                    },
                    {
                        "name": "Francisco Vilarinho"
                    },
                    {
                        "name": "Sofia Inácio"
                    },
                    {
                        "name": "Luís M. Pessoa"
                    }
                ],
                "author_detail": {
                    "name": "Luís M. Pessoa"
                },
                "author": "Luís M. Pessoa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14417v1",
                "updated": "2025-01-24T11:34:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    34,
                    13,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:34:13Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    34,
                    13,
                    4,
                    24,
                    0
                ],
                "title": "DeepFlow: Serverless Large Language Model Serving at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepFlow: Serverless Large Language Model Serving at Scale"
                },
                "summary": "This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Chao He"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14413v1",
                "updated": "2025-01-24T11:28:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    28,
                    17,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:28:17Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    28,
                    17,
                    4,
                    24,
                    0
                ],
                "title": "Context-CrackNet: A Context-Aware Framework for Precise Segmentation of\n  Tiny Cracks in Pavement images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-CrackNet: A Context-Aware Framework for Precise Segmentation of\n  Tiny Cracks in Pavement images"
                },
                "summary": "The accurate detection and segmentation of pavement distresses, particularly\ntiny and small cracks, are critical for early intervention and preventive\nmaintenance in transportation infrastructure. Traditional manual inspection\nmethods are labor-intensive and inconsistent, while existing deep learning\nmodels struggle with fine-grained segmentation and computational efficiency. To\naddress these challenges, this study proposes Context-CrackNet, a novel\nencoder-decoder architecture featuring the Region-Focused Enhancement Module\n(RFEM) and Context-Aware Global Module (CAGM). These innovations enhance the\nmodel's ability to capture fine-grained local details and global contextual\ndependencies, respectively. Context-CrackNet was rigorously evaluated on ten\npublicly available crack segmentation datasets, covering diverse pavement\ndistress scenarios. The model consistently outperformed 9 state-of-the-art\nsegmentation frameworks, achieving superior performance metrics such as mIoU\nand Dice score, while maintaining competitive inference efficiency. Ablation\nstudies confirmed the complementary roles of RFEM and CAGM, with notable\nimprovements in mIoU and Dice score when both modules were integrated.\nAdditionally, the model's balance of precision and computational efficiency\nhighlights its potential for real-time deployment in large-scale pavement\nmonitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accurate detection and segmentation of pavement distresses, particularly\ntiny and small cracks, are critical for early intervention and preventive\nmaintenance in transportation infrastructure. Traditional manual inspection\nmethods are labor-intensive and inconsistent, while existing deep learning\nmodels struggle with fine-grained segmentation and computational efficiency. To\naddress these challenges, this study proposes Context-CrackNet, a novel\nencoder-decoder architecture featuring the Region-Focused Enhancement Module\n(RFEM) and Context-Aware Global Module (CAGM). These innovations enhance the\nmodel's ability to capture fine-grained local details and global contextual\ndependencies, respectively. Context-CrackNet was rigorously evaluated on ten\npublicly available crack segmentation datasets, covering diverse pavement\ndistress scenarios. The model consistently outperformed 9 state-of-the-art\nsegmentation frameworks, achieving superior performance metrics such as mIoU\nand Dice score, while maintaining competitive inference efficiency. Ablation\nstudies confirmed the complementary roles of RFEM and CAGM, with notable\nimprovements in mIoU and Dice score when both modules were integrated.\nAdditionally, the model's balance of precision and computational efficiency\nhighlights its potential for real-time deployment in large-scale pavement\nmonitoring systems."
                },
                "authors": [
                    {
                        "name": "Blessing Agyei Kyem"
                    },
                    {
                        "name": "Joshua Kofi Asamoah"
                    },
                    {
                        "name": "Armstrong Aboah"
                    }
                ],
                "author_detail": {
                    "name": "Armstrong Aboah"
                },
                "author": "Armstrong Aboah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20211v2",
                "updated": "2025-01-24T11:18:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    18,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-28T16:48:55Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    16,
                    48,
                    55,
                    5,
                    363,
                    0
                ],
                "title": "Sequence Generation Modeling for Continuous Value Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence Generation Modeling for Continuous Value Prediction"
                },
                "summary": "Continuous value prediction (CVP) plays a crucial role in short video\nrecommendation, capturing user preferences through precise numerical\nestimations. However, traditional regression-based methods often struggle with\nchallenges like wide value ranges and imbalanced data, leading to prediction\nbias. While ordinal classification approaches have been introduced to address\nthese issues, their reliance on discretization reduces accuracy and overlooks\ninherent relationships between intervals. To overcome these limitations, we\nintroduce a novel Generative Regression (GR) framework for CVP, inspired by\nsequence generation techniques in language modeling. Our method transforms\nnumerical values into token sequences through structural discretization,\npreserving original data fidelity while improving prediction precision.\nLeveraging a carefully crafted vocabulary and label encoding, GR employs\ncurriculum learning with an embedding mixup strategy to bridge\ntraining-inference gaps. Experimental evaluations on four public datasets and\none large-scale industrial dataset validate the superiority of GR over existing\nmethods. Real-world A/B tests on Kuaishou, a leading video platform, further\ndemonstrate its practical effectiveness. Additionally, GR proves adaptable to\nother regression tasks, such as Lifetime Value (LTV) prediction, showcasing its\npotential as a robust solution for diverse CVP challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous value prediction (CVP) plays a crucial role in short video\nrecommendation, capturing user preferences through precise numerical\nestimations. However, traditional regression-based methods often struggle with\nchallenges like wide value ranges and imbalanced data, leading to prediction\nbias. While ordinal classification approaches have been introduced to address\nthese issues, their reliance on discretization reduces accuracy and overlooks\ninherent relationships between intervals. To overcome these limitations, we\nintroduce a novel Generative Regression (GR) framework for CVP, inspired by\nsequence generation techniques in language modeling. Our method transforms\nnumerical values into token sequences through structural discretization,\npreserving original data fidelity while improving prediction precision.\nLeveraging a carefully crafted vocabulary and label encoding, GR employs\ncurriculum learning with an embedding mixup strategy to bridge\ntraining-inference gaps. Experimental evaluations on four public datasets and\none large-scale industrial dataset validate the superiority of GR over existing\nmethods. Real-world A/B tests on Kuaishou, a leading video platform, further\ndemonstrate its practical effectiveness. Additionally, GR proves adaptable to\nother regression tasks, such as Lifetime Value (LTV) prediction, showcasing its\npotential as a robust solution for diverse CVP challenges."
                },
                "authors": [
                    {
                        "name": "Hongxu Ma"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Xuefeng Zhang"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Jihong Guan"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuigeng Zhou"
                },
                "author": "Shuigeng Zhou",
                "arxiv_comment": "10 pages, 5 figures, conference or other essential info",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14405v1",
                "updated": "2025-01-24T11:18:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    18,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:18:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    18,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Triple Instrumented Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triple Instrumented Difference-in-Differences"
                },
                "summary": "In this paper, we formalize a triple instrumented difference-in-differences\n(DID-IV). In this design, a triple Wald-DID estimand, which divides the\ndifference-in-difference-in-differences (DDD) estimand of the outcome by the\nDDD estimand of the treatment, captures the local average treatment effect on\nthe treated. The identifying assumptions mainly comprise a monotonicity\nassumption, and the common acceleration assumptions in the treatment and the\noutcome. We extend the canonical triple DID-IV design to staggered instrument\ncases. We also describe the estimation and inference in this design in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we formalize a triple instrumented difference-in-differences\n(DID-IV). In this design, a triple Wald-DID estimand, which divides the\ndifference-in-difference-in-differences (DDD) estimand of the outcome by the\nDDD estimand of the treatment, captures the local average treatment effect on\nthe treated. The identifying assumptions mainly comprise a monotonicity\nassumption, and the common acceleration assumptions in the treatment and the\noutcome. We extend the canonical triple DID-IV design to staggered instrument\ncases. We also describe the estimation and inference in this design in\npractice."
                },
                "authors": [
                    {
                        "name": "Sho Miyaji"
                    }
                ],
                "author_detail": {
                    "name": "Sho Miyaji"
                },
                "author": "Sho Miyaji",
                "arxiv_comment": "27 pages. arXiv admin note: text overlap with arXiv:2405.12083",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v4",
                "updated": "2025-01-24T11:05:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    5,
                    5,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Hans Christian Farsethås"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "17 pages, 5 figures, 8 tables. Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14396v1",
                "updated": "2025-01-24T10:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    59,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    59,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Characterising the Atacama segment of the Chile subduction margin\n  (24°S-31°S) with >165,000 earthquakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising the Atacama segment of the Chile subduction margin\n  (24°S-31°S) with >165,000 earthquakes"
                },
                "summary": "The Atacama segment in Northern Chile (24{\\deg}S to 31{\\deg}S) is a mature\nseismic gap with no major event (Mw>8) since 1922. In addition to regular\nseismicity, around the subducting Copiap\\'o ridge, the region hosts seismic\nswarms, and shallow and deep slow slip events. To characterize the fine\nstructure of this seismic gap and its seismic-aseismic interplay, we\ninstrumented the region with almost 200 seismic and geodetic stations. Using\nmachine learning, we derived a dense, high-resolution seismicity catalog,\nencompassing over 165,000 events with double-difference relocated hypocenters.\nOur catalog details the outer rise, interface, intraslab, crustal and mantle\nwedge seismicity. We infer a detailed slab geometry, showing that the flat slab\nis dipping towards the south with a narrower extent along dip. The slab\ngeometry controls the intraslab seismicity, with cross-cutting activity in the\nregion of highest bending and a downdip limit around 105 km slab depth. Our\ncatalogue exhibits significant seismicity in the mantle wedge upper corner\nbetween 28{\\deg}S and 31{\\deg}S, highlighting the brittle behavior of the cold\nnose. On the subduction interface, interplate locking controls the updip end of\nthe seismicity, with seismicity extending closer to the trench in low-locking\nareas. On fine scales, resolved by relative uncertainties below 50 m, the\nsubduction interface has a complex 3D structure, showing a fractal distribution\nof seismic patches down to a scale of tens of meters. Our results provide a\nholistic view of this complex subduction zone, while at the same time giving\ninsights into fine-scale structures and processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Atacama segment in Northern Chile (24{\\deg}S to 31{\\deg}S) is a mature\nseismic gap with no major event (Mw>8) since 1922. In addition to regular\nseismicity, around the subducting Copiap\\'o ridge, the region hosts seismic\nswarms, and shallow and deep slow slip events. To characterize the fine\nstructure of this seismic gap and its seismic-aseismic interplay, we\ninstrumented the region with almost 200 seismic and geodetic stations. Using\nmachine learning, we derived a dense, high-resolution seismicity catalog,\nencompassing over 165,000 events with double-difference relocated hypocenters.\nOur catalog details the outer rise, interface, intraslab, crustal and mantle\nwedge seismicity. We infer a detailed slab geometry, showing that the flat slab\nis dipping towards the south with a narrower extent along dip. The slab\ngeometry controls the intraslab seismicity, with cross-cutting activity in the\nregion of highest bending and a downdip limit around 105 km slab depth. Our\ncatalogue exhibits significant seismicity in the mantle wedge upper corner\nbetween 28{\\deg}S and 31{\\deg}S, highlighting the brittle behavior of the cold\nnose. On the subduction interface, interplate locking controls the updip end of\nthe seismicity, with seismicity extending closer to the trench in low-locking\nareas. On fine scales, resolved by relative uncertainties below 50 m, the\nsubduction interface has a complex 3D structure, showing a fractal distribution\nof seismic patches down to a scale of tens of meters. Our results provide a\nholistic view of this complex subduction zone, while at the same time giving\ninsights into fine-scale structures and processes."
                },
                "authors": [
                    {
                        "name": "Jannes Münchmeyer"
                    },
                    {
                        "name": "Diego Molina-Ormazabal"
                    },
                    {
                        "name": "David Marsan"
                    },
                    {
                        "name": "Mickaël Langlais"
                    },
                    {
                        "name": "Juan-Carlos Baez"
                    },
                    {
                        "name": "Ben Heit"
                    },
                    {
                        "name": "Diego González-Vidal"
                    },
                    {
                        "name": "Marcos Moreno"
                    },
                    {
                        "name": "Frederik Tilmann"
                    },
                    {
                        "name": "Dietrich Lange"
                    },
                    {
                        "name": "Anne Socquet"
                    }
                ],
                "author_detail": {
                    "name": "Anne Socquet"
                },
                "author": "Anne Socquet",
                "arxiv_comment": "27 pages main text, 7 pages references, 17 pages supplement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18434v2",
                "updated": "2025-01-24T10:41:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    41,
                    9,
                    4,
                    24,
                    0
                ],
                "published": "2024-02-28T16:00:25Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    16,
                    0,
                    25,
                    2,
                    59,
                    0
                ],
                "title": "Graph Regularized Encoder Training for Extreme Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Regularized Encoder Training for Extreme Classification"
                },
                "summary": "Deep extreme classification (XC) aims to train an encoder architecture and an\naccompanying classifier architecture to tag a data point with the most relevant\nsubset of labels from a very large universe of labels. XC applications in\nranking, recommendation and tagging routinely encounter tail labels for which\nthe amount of training data is exceedingly small. Graph convolutional networks\n(GCN) present a convenient but computationally expensive way to leverage task\nmetadata and enhance model accuracies in these settings. This paper formally\nestablishes that in several use cases, the steep computational cost of GCNs is\nentirely avoidable by replacing GCNs with non-GCN architectures. The paper\nnotices that in these settings, it is much more effective to use graph data to\nregularize encoder training than to implement a GCN. Based on these insights,\nan alternative paradigm RAMEN is presented to utilize graph metadata in XC\nsettings that offers significant performance boosts with zero increase in\ninference computational costs. RAMEN scales to datasets with up to 1M labels\nand offers prediction accuracy up to 15% higher on benchmark datasets than\nstate of the art methods, including those that use graph metadata to train\nGCNs. RAMEN also offers 10% higher accuracy over the best baseline on a\nproprietary recommendation dataset sourced from click logs of a popular search\nengine. Code for RAMEN will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep extreme classification (XC) aims to train an encoder architecture and an\naccompanying classifier architecture to tag a data point with the most relevant\nsubset of labels from a very large universe of labels. XC applications in\nranking, recommendation and tagging routinely encounter tail labels for which\nthe amount of training data is exceedingly small. Graph convolutional networks\n(GCN) present a convenient but computationally expensive way to leverage task\nmetadata and enhance model accuracies in these settings. This paper formally\nestablishes that in several use cases, the steep computational cost of GCNs is\nentirely avoidable by replacing GCNs with non-GCN architectures. The paper\nnotices that in these settings, it is much more effective to use graph data to\nregularize encoder training than to implement a GCN. Based on these insights,\nan alternative paradigm RAMEN is presented to utilize graph metadata in XC\nsettings that offers significant performance boosts with zero increase in\ninference computational costs. RAMEN scales to datasets with up to 1M labels\nand offers prediction accuracy up to 15% higher on benchmark datasets than\nstate of the art methods, including those that use graph metadata to train\nGCNs. RAMEN also offers 10% higher accuracy over the best baseline on a\nproprietary recommendation dataset sourced from click logs of a popular search\nengine. Code for RAMEN will be released publicly."
                },
                "authors": [
                    {
                        "name": "Anshul Mittal"
                    },
                    {
                        "name": "Shikhar Mohan"
                    },
                    {
                        "name": "Deepak Saini"
                    },
                    {
                        "name": "Siddarth Asokan"
                    },
                    {
                        "name": "Suchith C. Prabhu"
                    },
                    {
                        "name": "Lakshya Kumar"
                    },
                    {
                        "name": "Pankaj Malhotra"
                    },
                    {
                        "name": "Jain jiao"
                    },
                    {
                        "name": "Amit Singh"
                    },
                    {
                        "name": "Sumeet Agarwal"
                    },
                    {
                        "name": "Soumen Chakrabarti"
                    },
                    {
                        "name": "Purushottam Kar"
                    },
                    {
                        "name": "Manik Varma"
                    }
                ],
                "author_detail": {
                    "name": "Manik Varma"
                },
                "author": "Manik Varma",
                "arxiv_comment": "Accepted at TheWebConf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10845v2",
                "updated": "2025-01-24T10:35:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    35,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-15T15:59:48Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    15,
                    59,
                    48,
                    0,
                    197,
                    0
                ],
                "title": "Inferring dark energy properties from the scale factor parametrisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring dark energy properties from the scale factor parametrisation"
                },
                "summary": "We propose and implement a novel test to assess deviations from\nwell-established concordance $\\Lambda$CDM cosmology while inferring dark energy\nproperties. In contrast to the commonly implemented parametric forms of the\ndark energy equation-of-state (EoS), we test the validity of the cosmological\nconstant on the more fundamental scale factor [$a(t)$] which determines the\nexpansion rate of the Universe. We constrain our extended `general model' using\nthe late-time observables. The posterior of the dark energy EoS is mainly\nconstrained to be quintessence-like naturally excluding physically unviable\nregions such as phantom crossings or exponential growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and implement a novel test to assess deviations from\nwell-established concordance $\\Lambda$CDM cosmology while inferring dark energy\nproperties. In contrast to the commonly implemented parametric forms of the\ndark energy equation-of-state (EoS), we test the validity of the cosmological\nconstant on the more fundamental scale factor [$a(t)$] which determines the\nexpansion rate of the Universe. We constrain our extended `general model' using\nthe late-time observables. The posterior of the dark energy EoS is mainly\nconstrained to be quintessence-like naturally excluding physically unviable\nregions such as phantom crossings or exponential growth."
                },
                "authors": [
                    {
                        "name": "Upala Mukhopadhyay"
                    },
                    {
                        "name": "Sandeep Haridasu"
                    },
                    {
                        "name": "Anjan A Sen"
                    },
                    {
                        "name": "Suhail Dhawan"
                    }
                ],
                "author_detail": {
                    "name": "Suhail Dhawan"
                },
                "author": "Suhail Dhawan",
                "arxiv_doi": "10.1103/PhysRevD.110.123516",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.123516",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 Pages, 6 figures. Agrees with the published version\n  (Dated:11/12/24). Comments are welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14375v1",
                "updated": "2025-01-24T10:21:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    21,
                    31,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:21:31Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    21,
                    31,
                    4,
                    24,
                    0
                ],
                "title": "QED corrections to the parity-violating asymmetry in high-energy\n  electron-nucleus collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QED corrections to the parity-violating asymmetry in high-energy\n  electron-nucleus collisions"
                },
                "summary": "The parity-violating asymmetry, including leading-order QED corrections to\nthe Coulomb potential, is calculated non-perturbatively by solving the Dirac\nequation. At GeV collision energies and forward scattering angles, QED effects\nenhance the asymmetry by approximately 5% for the recently measured nuclei\n27Al, 48Ca, and 208Pb. The corrections result in a shift of the estimated\nneutron radius, leading to an increase in the inferred neutron skin thicknesses\nof these nuclei and, thus, to the pressure neutrons feel around nuclear\nsaturation density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The parity-violating asymmetry, including leading-order QED corrections to\nthe Coulomb potential, is calculated non-perturbatively by solving the Dirac\nequation. At GeV collision energies and forward scattering angles, QED effects\nenhance the asymmetry by approximately 5% for the recently measured nuclei\n27Al, 48Ca, and 208Pb. The corrections result in a shift of the estimated\nneutron radius, leading to an increase in the inferred neutron skin thicknesses\nof these nuclei and, thus, to the pressure neutrons feel around nuclear\nsaturation density."
                },
                "authors": [
                    {
                        "name": "Xavier Roca-Maza"
                    },
                    {
                        "name": "D H. Jakubassa-Amundsen"
                    }
                ],
                "author_detail": {
                    "name": "D H. Jakubassa-Amundsen"
                },
                "author": "D H. Jakubassa-Amundsen",
                "arxiv_comment": "Submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14374v1",
                "updated": "2025-01-24T10:13:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    13,
                    38,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:13:38Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    13,
                    38,
                    4,
                    24,
                    0
                ],
                "title": "Causal pathway from AMOC to Southern Amazon rainforest indicates\n  stabilising interaction between two climate tipping elements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal pathway from AMOC to Southern Amazon rainforest indicates\n  stabilising interaction between two climate tipping elements"
                },
                "summary": "Declines in resilience have been observed in several climate tipping elements\nover the past decades, including the Atlantic Meridional Overturning\nCirculation (AMOC) and the Amazon rainforest (AR). Large-scale nonlinear and\npossibly irreversible changes in system state, such as AMOC weakening or\nrainforest-savanna transitions in the Amazon basin, would have severe impacts\non ecosystems and human societies worldwide. In order to improve future tipping\nrisk assessments, understanding interactions between tipping elements is\ncrucial. The AMOC is known to influence the Intertropical Convergence Zone,\npotentially altering precipitation patterns over the AR and affecting its\nstability. However, AMOC-AR interactions are currently not well understood.\nHere, we identify a previously unknown stabilising interaction pathway from the\nAMOC onto the Southern AR, applying an established causal discovery and\ninference approach to tipping element interactions for the first time.\nAnalysing observational and reanalysis data from 1982-2022, we show that AMOC\nweakening leads to increased precipitation in the Southern AR during the\ncritical dry season, in line with findings from recent Earth system model\nexperiments. Specifically, we report a 4.8% increase of mean dry season\nprecipitation in the Southern AR for every 1 Sv of AMOC weakening. This finding\nis consistent across multiple data sources and AMOC strength indices. We show\nthat this stabilising interaction has offset 17% of dry season precipitation\ndecrease in the Southern AR since 1982. Our results demonstrate the potential\nof causal discovery methods for analysing tipping element interactions based on\nreanalysis and observational data. By improving the understanding of AMOC-AR\ninteractions, we contribute toward better constraining the risk of potential\nclimate tipping cascades under global warming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Declines in resilience have been observed in several climate tipping elements\nover the past decades, including the Atlantic Meridional Overturning\nCirculation (AMOC) and the Amazon rainforest (AR). Large-scale nonlinear and\npossibly irreversible changes in system state, such as AMOC weakening or\nrainforest-savanna transitions in the Amazon basin, would have severe impacts\non ecosystems and human societies worldwide. In order to improve future tipping\nrisk assessments, understanding interactions between tipping elements is\ncrucial. The AMOC is known to influence the Intertropical Convergence Zone,\npotentially altering precipitation patterns over the AR and affecting its\nstability. However, AMOC-AR interactions are currently not well understood.\nHere, we identify a previously unknown stabilising interaction pathway from the\nAMOC onto the Southern AR, applying an established causal discovery and\ninference approach to tipping element interactions for the first time.\nAnalysing observational and reanalysis data from 1982-2022, we show that AMOC\nweakening leads to increased precipitation in the Southern AR during the\ncritical dry season, in line with findings from recent Earth system model\nexperiments. Specifically, we report a 4.8% increase of mean dry season\nprecipitation in the Southern AR for every 1 Sv of AMOC weakening. This finding\nis consistent across multiple data sources and AMOC strength indices. We show\nthat this stabilising interaction has offset 17% of dry season precipitation\ndecrease in the Southern AR since 1982. Our results demonstrate the potential\nof causal discovery methods for analysing tipping element interactions based on\nreanalysis and observational data. By improving the understanding of AMOC-AR\ninteractions, we contribute toward better constraining the risk of potential\nclimate tipping cascades under global warming."
                },
                "authors": [
                    {
                        "name": "Annika Högner"
                    },
                    {
                        "name": "Giorgia Di Capua"
                    },
                    {
                        "name": "Jonathan F. Donges"
                    },
                    {
                        "name": "Reik V. Donner"
                    },
                    {
                        "name": "Georg Feulner"
                    },
                    {
                        "name": "Nico Wunderling"
                    }
                ],
                "author_detail": {
                    "name": "Nico Wunderling"
                },
                "author": "Nico Wunderling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14371v1",
                "updated": "2025-01-24T10:04:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    4,
                    53,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:04:53Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    4,
                    53,
                    4,
                    24,
                    0
                ],
                "title": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style\n  Subspace Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style\n  Subspace Editing"
                },
                "summary": "We introduce DRESS, a novel approach for generating stylized large language\nmodel (LLM) responses through representation editing. Existing methods like\nprompting and fine-tuning are either insufficient for complex style adaptation\nor computationally expensive, particularly in tasks like NPC creation or\ncharacter role-playing. Our approach leverages the over-parameterized nature of\nLLMs to disentangle a style-relevant subspace within the model's representation\nspace to conduct representation editing, ensuring a minimal impact on the\noriginal semantics. By applying adaptive editing strengths, we dynamically\nadjust the steering vectors in the style subspace to maintain both stylistic\nfidelity and semantic integrity. We develop two stylized QA benchmark datasets\nto validate the effectiveness of DRESS, and the results demonstrate significant\nimprovements compared to baseline methods such as prompting and ITI. In short,\nDRESS is a lightweight, train-free solution for enhancing LLMs with flexible\nand effective style control, making it particularly useful for developing\nstylized conversational agents. Codes and benchmark datasets are available at\nhttps://github.com/ArthurLeoM/DRESS-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DRESS, a novel approach for generating stylized large language\nmodel (LLM) responses through representation editing. Existing methods like\nprompting and fine-tuning are either insufficient for complex style adaptation\nor computationally expensive, particularly in tasks like NPC creation or\ncharacter role-playing. Our approach leverages the over-parameterized nature of\nLLMs to disentangle a style-relevant subspace within the model's representation\nspace to conduct representation editing, ensuring a minimal impact on the\noriginal semantics. By applying adaptive editing strengths, we dynamically\nadjust the steering vectors in the style subspace to maintain both stylistic\nfidelity and semantic integrity. We develop two stylized QA benchmark datasets\nto validate the effectiveness of DRESS, and the results demonstrate significant\nimprovements compared to baseline methods such as prompting and ITI. In short,\nDRESS is a lightweight, train-free solution for enhancing LLMs with flexible\nand effective style control, making it particularly useful for developing\nstylized conversational agents. Codes and benchmark datasets are available at\nhttps://github.com/ArthurLeoM/DRESS-LLM."
                },
                "authors": [
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Yifeng Xu"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "arxiv_comment": "ICLR 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14366v1",
                "updated": "2025-01-24T09:59:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    59,
                    6,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T09:59:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    59,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Uncovering the bias in the evidence for dynamical dark energy through\n  minimal and generalized modeling approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the bias in the evidence for dynamical dark energy through\n  minimal and generalized modeling approaches"
                },
                "summary": "In this letter we argue that the CPL parameterisation for the dark energy\nequation of state is biased towards preferring such model over the constant $w$\nwhile the latter bounds are still compatible with LCDM. For that we compare\nconstraints on the EoS parameters $w_0$ and early time type $w_a$ (CPL) against\nthose with a late time parameterisation on $w_a$ (LZ) and the constant $w$\nmodel, using CMB, Supernovae and BAO from DESI datasets. We found, the same as\nwas the case with CPL model, preference for dynamical dark energy within the LZ\nmodel, but for values almost symmetrically distributed with respect to their\nLCDM limits. This is due to the fact that the presence of $w_0$ allows to\nrecast each parametrisation into making it compensate the preference for $w\\sim\n-1$ in the opposite direction. To further test our hypothesis, we fixed $w_0$\nto -1 and followed a minimal approach by considering models that deviates by\none free parameter, or we extend to more general models that either group both\nlate and early effects, or allow the presence of two dark energy fluid alike\nand constant alike component. We found that all the variants, except the\noriginal CPL are still compatible with LCDM, with likelihoods peaking close to\n$w_0 = -1$, $w_a = 0$, or 0.68 for $\\Omega_{\\rm CC}$, with the constant $w$ and\nthe late time $w_a$ having the smallest constraints. Although we found that the\nevidence from CPL is stronger than those for the more minimal cases, however\nthe preference increases further for the more generalized parameterizations,\nwhile still staying compatible with LCDM in terms of the significance levels.\nWe conclude that considering CPL model is not sufficient to test deviation from\nthe standard model and that it is necessary to conduct further minimal or more\ngeneral approaches to better understand the outcomes from model testing and\ninference methods.(abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter we argue that the CPL parameterisation for the dark energy\nequation of state is biased towards preferring such model over the constant $w$\nwhile the latter bounds are still compatible with LCDM. For that we compare\nconstraints on the EoS parameters $w_0$ and early time type $w_a$ (CPL) against\nthose with a late time parameterisation on $w_a$ (LZ) and the constant $w$\nmodel, using CMB, Supernovae and BAO from DESI datasets. We found, the same as\nwas the case with CPL model, preference for dynamical dark energy within the LZ\nmodel, but for values almost symmetrically distributed with respect to their\nLCDM limits. This is due to the fact that the presence of $w_0$ allows to\nrecast each parametrisation into making it compensate the preference for $w\\sim\n-1$ in the opposite direction. To further test our hypothesis, we fixed $w_0$\nto -1 and followed a minimal approach by considering models that deviates by\none free parameter, or we extend to more general models that either group both\nlate and early effects, or allow the presence of two dark energy fluid alike\nand constant alike component. We found that all the variants, except the\noriginal CPL are still compatible with LCDM, with likelihoods peaking close to\n$w_0 = -1$, $w_a = 0$, or 0.68 for $\\Omega_{\\rm CC}$, with the constant $w$ and\nthe late time $w_a$ having the smallest constraints. Although we found that the\nevidence from CPL is stronger than those for the more minimal cases, however\nthe preference increases further for the more generalized parameterizations,\nwhile still staying compatible with LCDM in terms of the significance levels.\nWe conclude that considering CPL model is not sufficient to test deviation from\nthe standard model and that it is necessary to conduct further minimal or more\ngeneral approaches to better understand the outcomes from model testing and\ninference methods.(abridged)"
                },
                "authors": [
                    {
                        "name": "Ziad Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Ziad Sakr"
                },
                "author": "Ziad Sakr",
                "arxiv_comment": "Pre-submission version. Comments are more than useful",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09116v3",
                "updated": "2025-01-24T09:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    58,
                    14,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-13T13:43:59Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    13,
                    43,
                    59,
                    3,
                    165,
                    0
                ],
                "title": "Injective flows for star-like manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injective flows for star-like manifolds"
                },
                "summary": "Normalizing Flows (NFs) are powerful and efficient models for density\nestimation. When modeling densities on manifolds, NFs can be generalized to\ninjective flows but the Jacobian determinant becomes computationally\nprohibitive. Current approaches either consider bounds on the log-likelihood or\nrely on some approximations of the Jacobian determinant. In contrast, we\npropose injective flows for star-like manifolds and show that for such\nmanifolds we can compute the Jacobian determinant exactly and efficiently, with\nthe same cost as NFs. This aspect is particularly relevant for variational\ninference settings, where no samples are available and only some unnormalized\ntarget is known. Among many, we showcase the relevance of modeling densities on\nstar-like manifolds in two settings. Firstly, we introduce a novel Objective\nBayesian approach for penalized likelihood models by interpreting level-sets of\nthe penalty as star-like manifolds. Secondly, we consider probabilistic mixing\nmodels and introduce a general method for variational inference by defining the\nposterior of mixture weights on the probability simplex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normalizing Flows (NFs) are powerful and efficient models for density\nestimation. When modeling densities on manifolds, NFs can be generalized to\ninjective flows but the Jacobian determinant becomes computationally\nprohibitive. Current approaches either consider bounds on the log-likelihood or\nrely on some approximations of the Jacobian determinant. In contrast, we\npropose injective flows for star-like manifolds and show that for such\nmanifolds we can compute the Jacobian determinant exactly and efficiently, with\nthe same cost as NFs. This aspect is particularly relevant for variational\ninference settings, where no samples are available and only some unnormalized\ntarget is known. Among many, we showcase the relevance of modeling densities on\nstar-like manifolds in two settings. Firstly, we introduce a novel Objective\nBayesian approach for penalized likelihood models by interpreting level-sets of\nthe penalty as star-like manifolds. Secondly, we consider probabilistic mixing\nmodels and introduce a general method for variational inference by defining the\nposterior of mixture weights on the probability simplex."
                },
                "authors": [
                    {
                        "name": "Marcello Massimo Negri"
                    },
                    {
                        "name": "Jonathan Aellen"
                    },
                    {
                        "name": "Volker Roth"
                    }
                ],
                "author_detail": {
                    "name": "Volker Roth"
                },
                "author": "Volker Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14356v1",
                "updated": "2025-01-24T09:45:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    45,
                    16,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T09:45:16Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    45,
                    16,
                    4,
                    24,
                    0
                ],
                "title": "Causal-Inspired Multitask Learning for Video-Based Human Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal-Inspired Multitask Learning for Video-Based Human Pose Estimation"
                },
                "summary": "Video-based human pose estimation has long been a fundamental yet challenging\nproblem in computer vision. Previous studies focus on spatio-temporal modeling\nthrough the enhancement of architecture design and optimization strategies.\nHowever, they overlook the causal relationships in the joints, leading to\nmodels that may be overly tailored and thus estimate poorly to challenging\nscenes. Therefore, adequate causal reasoning capability, coupled with good\ninterpretability of model, are both indispensable and prerequisite for\nachieving reliable results. In this paper, we pioneer a causal perspective on\npose estimation and introduce a causal-inspired multitask learning framework,\nconsisting of two stages. \\textit{In the first stage}, we try to endow the\nmodel with causal spatio-temporal modeling ability by introducing two\nself-supervision auxiliary tasks. Specifically, these auxiliary tasks enable\nthe network to infer challenging keypoints based on observed keypoint\ninformation, thereby imbuing causal reasoning capabilities into the model and\nmaking it robust to challenging scenes. \\textit{In the second stage}, we argue\nthat not all feature tokens contribute equally to pose estimation. Prioritizing\ncausal (keypoint-relevant) tokens is crucial to achieve reliable results, which\ncould improve the interpretability of the model. To this end, we propose a\nToken Causal Importance Selection module to identify the causal tokens and\nnon-causal tokens (\\textit{e.g.}, background and objects). Additionally,\nnon-causal tokens could provide potentially beneficial cues but may be\nredundant. We further introduce a non-causal tokens clustering module to merge\nthe similar non-causal tokens. Extensive experiments show that our method\noutperforms state-of-the-art methods on three large-scale benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based human pose estimation has long been a fundamental yet challenging\nproblem in computer vision. Previous studies focus on spatio-temporal modeling\nthrough the enhancement of architecture design and optimization strategies.\nHowever, they overlook the causal relationships in the joints, leading to\nmodels that may be overly tailored and thus estimate poorly to challenging\nscenes. Therefore, adequate causal reasoning capability, coupled with good\ninterpretability of model, are both indispensable and prerequisite for\nachieving reliable results. In this paper, we pioneer a causal perspective on\npose estimation and introduce a causal-inspired multitask learning framework,\nconsisting of two stages. \\textit{In the first stage}, we try to endow the\nmodel with causal spatio-temporal modeling ability by introducing two\nself-supervision auxiliary tasks. Specifically, these auxiliary tasks enable\nthe network to infer challenging keypoints based on observed keypoint\ninformation, thereby imbuing causal reasoning capabilities into the model and\nmaking it robust to challenging scenes. \\textit{In the second stage}, we argue\nthat not all feature tokens contribute equally to pose estimation. Prioritizing\ncausal (keypoint-relevant) tokens is crucial to achieve reliable results, which\ncould improve the interpretability of the model. To this end, we propose a\nToken Causal Importance Selection module to identify the causal tokens and\nnon-causal tokens (\\textit{e.g.}, background and objects). Additionally,\nnon-causal tokens could provide potentially beneficial cues but may be\nredundant. We further introduce a non-causal tokens clustering module to merge\nthe similar non-causal tokens. Extensive experiments show that our method\noutperforms state-of-the-art methods on three large-scale benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Haipeng Chen"
                    },
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yifang Yin"
                    },
                    {
                        "name": "Yingying Jiao"
                    },
                    {
                        "name": "Yingda Lyu"
                    },
                    {
                        "name": "Zhenguang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang Liu"
                },
                "author": "Zhenguang Liu",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13648v2",
                "updated": "2025-01-24T09:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    21,
                    57,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-23T13:27:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    13,
                    27,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Revisiting Online Learning Approach to Inverse Linear Optimization: A\n  Fenchel$-$Young Loss Perspective and Gap-Dependent Regret Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Online Learning Approach to Inverse Linear Optimization: A\n  Fenchel$-$Young Loss Perspective and Gap-Dependent Regret Analysis"
                },
                "summary": "This paper revisits the online learning approach to inverse linear\noptimization studied by B\\\"armann et al. (2017), where the goal is to infer an\nunknown linear objective function of an agent from sequential observations of\nthe agent's input-output pairs. First, we provide a simple understanding of the\nonline learning approach through its connection to online convex optimization\nof \\emph{Fenchel--Young losses}. As a byproduct, we present an offline\nguarantee on the \\emph{suboptimality loss}, which measures how well predicted\nobjectives explain the agent's choices, without assuming the optimality of the\nagent's choices. Second, assuming that there is a gap between optimal and\nsuboptimal objective values in the agent's decision problems, we obtain an\nupper bound independent of the time horizon $T$ on the sum of suboptimality and\n\\emph{estimate losses}, where the latter measures the quality of solutions\nrecommended by predicted objectives. Interestingly, our gap-dependent analysis\nachieves a faster rate than the standard $O(\\sqrt{T})$ regret bound by\nexploiting structures specific to inverse linear optimization, even though\nneither the loss functions nor their domains enjoy desirable properties, such\nas strong convexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the online learning approach to inverse linear\noptimization studied by B\\\"armann et al. (2017), where the goal is to infer an\nunknown linear objective function of an agent from sequential observations of\nthe agent's input-output pairs. First, we provide a simple understanding of the\nonline learning approach through its connection to online convex optimization\nof \\emph{Fenchel--Young losses}. As a byproduct, we present an offline\nguarantee on the \\emph{suboptimality loss}, which measures how well predicted\nobjectives explain the agent's choices, without assuming the optimality of the\nagent's choices. Second, assuming that there is a gap between optimal and\nsuboptimal objective values in the agent's decision problems, we obtain an\nupper bound independent of the time horizon $T$ on the sum of suboptimality and\n\\emph{estimate losses}, where the latter measures the quality of solutions\nrecommended by predicted objectives. Interestingly, our gap-dependent analysis\nachieves a faster rate than the standard $O(\\sqrt{T})$ regret bound by\nexploiting structures specific to inverse linear optimization, even though\nneither the loss functions nor their domains enjoy desirable properties, such\nas strong convexity."
                },
                "authors": [
                    {
                        "name": "Shinsaku Sakaue"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Taira Tsuchiya"
                    }
                ],
                "author_detail": {
                    "name": "Taira Tsuchiya"
                },
                "author": "Taira Tsuchiya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14350v1",
                "updated": "2025-01-24T09:21:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    21,
                    41,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T09:21:41Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    21,
                    41,
                    4,
                    24,
                    0
                ],
                "title": "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition\n  Models from Encoder-Decoder to LLM Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition\n  Models from Encoder-Decoder to LLM Integration"
                },
                "summary": "We present FireRedASR, a family of large-scale automatic speech recognition\n(ASR) models for Mandarin, designed to meet diverse requirements in superior\nperformance and optimal efficiency across various applications. FireRedASR\ncomprises two variants:\n  FireRedASR-LLM: Designed to achieve state-of-the-art (SOTA) performance and\nto enable seamless end-to-end speech interaction. It adopts an\nEncoder-Adapter-LLM framework leveraging large language model (LLM)\ncapabilities. On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters)\nachieves an average Character Error Rate (CER) of 3.05%, surpassing the latest\nSOTA of 3.33% with an 8.4% relative CER reduction (CERR). It demonstrates\nsuperior generalization capability over industrial-grade baselines, achieving\n24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and\nintelligent assistant.\n  FireRedASR-AED: Designed to balance high performance and computational\nefficiency and to serve as an effective speech representation module in\nLLM-based speech models. It utilizes an Attention-based Encoder-Decoder (AED)\narchitecture. On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters)\nachieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still\noutperforming the latest SOTA model with over 12B parameters. It offers a more\ncompact size, making it suitable for resource-constrained applications.\n  Moreover, both models exhibit competitive results on Chinese dialects and\nEnglish speech benchmarks and excel in singing lyrics recognition. To advance\nresearch in speech processing, we release our models and inference code at\nhttps://github.com/FireRedTeam/FireRedASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FireRedASR, a family of large-scale automatic speech recognition\n(ASR) models for Mandarin, designed to meet diverse requirements in superior\nperformance and optimal efficiency across various applications. FireRedASR\ncomprises two variants:\n  FireRedASR-LLM: Designed to achieve state-of-the-art (SOTA) performance and\nto enable seamless end-to-end speech interaction. It adopts an\nEncoder-Adapter-LLM framework leveraging large language model (LLM)\ncapabilities. On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters)\nachieves an average Character Error Rate (CER) of 3.05%, surpassing the latest\nSOTA of 3.33% with an 8.4% relative CER reduction (CERR). It demonstrates\nsuperior generalization capability over industrial-grade baselines, achieving\n24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and\nintelligent assistant.\n  FireRedASR-AED: Designed to balance high performance and computational\nefficiency and to serve as an effective speech representation module in\nLLM-based speech models. It utilizes an Attention-based Encoder-Decoder (AED)\narchitecture. On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters)\nachieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still\noutperforming the latest SOTA model with over 12B parameters. It offers a more\ncompact size, making it suitable for resource-constrained applications.\n  Moreover, both models exhibit competitive results on Chinese dialects and\nEnglish speech benchmarks and excel in singing lyrics recognition. To advance\nresearch in speech processing, we release our models and inference code at\nhttps://github.com/FireRedTeam/FireRedASR."
                },
                "authors": [
                    {
                        "name": "Kai-Tuo Xu"
                    },
                    {
                        "name": "Feng-Long Xie"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01639v3",
                "updated": "2025-01-24T09:10:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    10,
                    42,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-02T15:09:36Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    9,
                    36,
                    2,
                    276,
                    0
                ],
                "title": "Moral Alignment for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Alignment for LLM Agents"
                },
                "summary": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques."
                },
                "authors": [
                    {
                        "name": "Elizaveta Tennant"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR'25), Singapore, Apr 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10066v2",
                "updated": "2025-01-24T09:05:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    5,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2024-11-15T09:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    9,
                    31,
                    23,
                    4,
                    320,
                    0
                ],
                "title": "Explanation of the exceptionally strong timing noise of PSR J0337+1715\n  by a circum-ternary planet and consequences for gravity tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation of the exceptionally strong timing noise of PSR J0337+1715\n  by a circum-ternary planet and consequences for gravity tests"
                },
                "summary": "Context: Timing of pulsar PSR J0337+1715 provides a unique opportunity to\ntest the strong equivalence principle (SEP) with a strongly self-gravitating\nobject. This is due to its unique situation in a triple stellar system with two\nwhite dwarfs. Aims: Our previous study suggested the presence of a strong\nlow-frequency signal in the timing residuals. We set out to model it on a\nlonger dataset in order to determine its nature and improve accuracy. Methods:\nThree models are considered: chromatic or achromatic red-noise, and a small\nplanet in a hierarchical orbit with the triple stellar system. These models are\nimplemented in our numerical timing model. We perform Bayesian inference of\nposterior distributions. Best fits are compared using information-theoretic\ncriteria. Results: Chromatic red noise from dispersion-measure variations is\nruled out. Achromatic red noise or a planet in keplerian orbit provide the best\nfits. If it is red noise then it appears exceptionally strong. Assuming the\npresence of a planet, we obtain a marginal detection of mutual interactions\nwhich allows us to constrain its mass to $\\sim 0.5 M_{\\rm Moon}$ as well as its\ninclination. The latter is intriguingly coincident with a Kozai resonance. We\nshow that a longer observation span will ultimately lead to a clear signature\nof the planet model due to its mutual interactions with the triple system. We\nproduce new limits on SEP violation: $|\\Delta| < 1.5\\cdot 10^{-6}$ or $|\\Delta|\n< 2.3\\cdot 10^{-6}$ at 95% confidence level under the planet or red-noise\nhypothesis, respectively. This model dependence emphasises the need for\nadditional data and model selection. As a by-product, we estimate a rather low\nsupernova kick velocity of $\\sim 110-125 \\rm km/s$, strengthening the idea that\nit is a necessary condition for the formation of pulsar triple systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Timing of pulsar PSR J0337+1715 provides a unique opportunity to\ntest the strong equivalence principle (SEP) with a strongly self-gravitating\nobject. This is due to its unique situation in a triple stellar system with two\nwhite dwarfs. Aims: Our previous study suggested the presence of a strong\nlow-frequency signal in the timing residuals. We set out to model it on a\nlonger dataset in order to determine its nature and improve accuracy. Methods:\nThree models are considered: chromatic or achromatic red-noise, and a small\nplanet in a hierarchical orbit with the triple stellar system. These models are\nimplemented in our numerical timing model. We perform Bayesian inference of\nposterior distributions. Best fits are compared using information-theoretic\ncriteria. Results: Chromatic red noise from dispersion-measure variations is\nruled out. Achromatic red noise or a planet in keplerian orbit provide the best\nfits. If it is red noise then it appears exceptionally strong. Assuming the\npresence of a planet, we obtain a marginal detection of mutual interactions\nwhich allows us to constrain its mass to $\\sim 0.5 M_{\\rm Moon}$ as well as its\ninclination. The latter is intriguingly coincident with a Kozai resonance. We\nshow that a longer observation span will ultimately lead to a clear signature\nof the planet model due to its mutual interactions with the triple system. We\nproduce new limits on SEP violation: $|\\Delta| < 1.5\\cdot 10^{-6}$ or $|\\Delta|\n< 2.3\\cdot 10^{-6}$ at 95% confidence level under the planet or red-noise\nhypothesis, respectively. This model dependence emphasises the need for\nadditional data and model selection. As a by-product, we estimate a rather low\nsupernova kick velocity of $\\sim 110-125 \\rm km/s$, strengthening the idea that\nit is a necessary condition for the formation of pulsar triple systems."
                },
                "authors": [
                    {
                        "name": "Guillaume Voisin"
                    },
                    {
                        "name": "Ismaël Cognard"
                    },
                    {
                        "name": "Melaine Saillenfest"
                    },
                    {
                        "name": "Thomas Tauris"
                    },
                    {
                        "name": "Norbert Wex"
                    },
                    {
                        "name": "Lucas Guillemot"
                    },
                    {
                        "name": "Gilles Theureau"
                    },
                    {
                        "name": "P. C. C. Freire"
                    },
                    {
                        "name": "Michael Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Kramer"
                },
                "arxiv_affiliation": "MPIFR",
                "author": "Michael Kramer",
                "arxiv_doi": "10.1051/0004-6361/202452100",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452100",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.10066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Astronomy & Astrophysics - A&A, 2025, 693 (A143)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14334v1",
                "updated": "2025-01-24T08:58:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    58,
                    49,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:58:49Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    58,
                    49,
                    4,
                    24,
                    0
                ],
                "title": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts"
                },
                "summary": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.In this paper, we propose a\nmethodology to estimate the environmental impact of a company's AI portfolio,\nproviding actionable insights without necessitating extensive AI and Life-Cycle\nAssessment (LCA) expertise. Results confirm that large generative AI models\nconsume up to 4600x more energy than traditional models. Our modelling\napproach, which accounts for increased AI usage, hardware computing efficiency,\nand changes in electricity mix in line with IPCC scenarios, forecasts AI\nelectricity use up to 2030. Under a high adoption scenario, driven by\nwidespread Generative AI and agents adoption associated to increasingly complex\nmodels and frameworks, AI electricity use is projected to rise by a factor of\n24.4.Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.In this paper, we propose a\nmethodology to estimate the environmental impact of a company's AI portfolio,\nproviding actionable insights without necessitating extensive AI and Life-Cycle\nAssessment (LCA) expertise. Results confirm that large generative AI models\nconsume up to 4600x more energy than traditional models. Our modelling\napproach, which accounts for increased AI usage, hardware computing efficiency,\nand changes in electricity mix in line with IPCC scenarios, forecasts AI\nelectricity use up to 2030. Under a high adoption scenario, driven by\nwidespread Generative AI and agents adoption associated to increasingly complex\nmodels and frameworks, AI electricity use is projected to rise by a factor of\n24.4.Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals."
                },
                "authors": [
                    {
                        "name": "Clément Desroches"
                    },
                    {
                        "name": "Martin Chauvin"
                    },
                    {
                        "name": "Louis Ladan"
                    },
                    {
                        "name": "Caroline Vateau"
                    },
                    {
                        "name": "Simon Gosset"
                    },
                    {
                        "name": "Philippe Cordier"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Cordier"
                },
                "author": "Philippe Cordier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17094v2",
                "updated": "2025-01-24T08:46:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    46,
                    41,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-22T17:09:34Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    17,
                    9,
                    34,
                    6,
                    357,
                    0
                ],
                "title": "Analysis on LLMs Performance for Code Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis on LLMs Performance for Code Summarization"
                },
                "summary": "Code summarization aims to generate concise natural language descriptions for\nsource code. Deep learning has been used more and more recently in software\nengineering, particularly for tasks like code creation and summarization.\nSpecifically, it appears that the most current Large Language Models with\ncoding perform well on these tasks. Large Language Models (LLMs) have\nsignificantly advanced the field of code summarization, providing sophisticated\nmethods for generating concise and accurate summaries of source code. This\nstudy aims to perform a comparative analysis of several open-source LLMs,\nnamely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is\nassessed using important metrics such as BLEU\\textsubscript{3.1} and\nROUGE\\textsubscript{3.2}.\n  Through this analysis, we seek to identify the strengths and weaknesses of\neach model, offering insights into their applicability and effectiveness in\ncode summarization tasks. Our findings contribute to the ongoing development\nand refinement of LLMs, supporting their integration into tools that enhance\nsoftware development and maintenance processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code summarization aims to generate concise natural language descriptions for\nsource code. Deep learning has been used more and more recently in software\nengineering, particularly for tasks like code creation and summarization.\nSpecifically, it appears that the most current Large Language Models with\ncoding perform well on these tasks. Large Language Models (LLMs) have\nsignificantly advanced the field of code summarization, providing sophisticated\nmethods for generating concise and accurate summaries of source code. This\nstudy aims to perform a comparative analysis of several open-source LLMs,\nnamely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is\nassessed using important metrics such as BLEU\\textsubscript{3.1} and\nROUGE\\textsubscript{3.2}.\n  Through this analysis, we seek to identify the strengths and weaknesses of\neach model, offering insights into their applicability and effectiveness in\ncode summarization tasks. Our findings contribute to the ongoing development\nand refinement of LLMs, supporting their integration into tools that enhance\nsoftware development and maintenance processes."
                },
                "authors": [
                    {
                        "name": "Md. Ahnaf Akib"
                    },
                    {
                        "name": "Md. Muktadir Mazumder"
                    },
                    {
                        "name": "Salman Ahsan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Ahsan"
                },
                "author": "Salman Ahsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11081v2",
                "updated": "2025-01-24T08:44:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    44,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2024-11-17T14:14:36Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    14,
                    36,
                    6,
                    322,
                    0
                ],
                "title": "The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case\n  Study on Media Bias Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case\n  Study on Media Bias Detection"
                },
                "summary": "High annotation costs from hiring or crowdsourcing complicate the creation of\nlarge, high-quality datasets needed for training reliable text classifiers.\nRecent research suggests using Large Language Models (LLMs) to automate the\nannotation process, reducing these costs while maintaining data quality. LLMs\nhave shown promising results in annotating downstream tasks like hate speech\ndetection and political framing. Building on the success in these areas, this\nstudy investigates whether LLMs are viable for annotating the complex task of\nmedia bias detection and whether a downstream media bias classifier can be\ntrained on such data. We create annolexical, the first large-scale dataset for\nmedia bias classification with over 48000 synthetically annotated examples. Our\nclassifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by\n5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or\noutperforms the model trained on human-labeled data when evaluated on two media\nbias benchmark datasets (BABE and BASIL). This study demonstrates how our\napproach significantly reduces the cost of dataset creation in the media bias\ndomain and, by extension, the development of classifiers, while our subsequent\nbehavioral stress-testing reveals some of its current limitations and\ntrade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High annotation costs from hiring or crowdsourcing complicate the creation of\nlarge, high-quality datasets needed for training reliable text classifiers.\nRecent research suggests using Large Language Models (LLMs) to automate the\nannotation process, reducing these costs while maintaining data quality. LLMs\nhave shown promising results in annotating downstream tasks like hate speech\ndetection and political framing. Building on the success in these areas, this\nstudy investigates whether LLMs are viable for annotating the complex task of\nmedia bias detection and whether a downstream media bias classifier can be\ntrained on such data. We create annolexical, the first large-scale dataset for\nmedia bias classification with over 48000 synthetically annotated examples. Our\nclassifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by\n5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or\noutperforms the model trained on human-labeled data when evaluated on two media\nbias benchmark datasets (BABE and BASIL). This study demonstrates how our\napproach significantly reduces the cost of dataset creation in the media bias\ndomain and, by extension, the development of classifiers, while our subsequent\nbehavioral stress-testing reveals some of its current limitations and\ntrade-offs."
                },
                "authors": [
                    {
                        "name": "Tomas Horych"
                    },
                    {
                        "name": "Christoph Mandl"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Andre Greiner-Petter"
                    },
                    {
                        "name": "Bela Gipp"
                    },
                    {
                        "name": "Akiko Aizawa"
                    },
                    {
                        "name": "Timo Spinde"
                    }
                ],
                "author_detail": {
                    "name": "Timo Spinde"
                },
                "author": "Timo Spinde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14326v1",
                "updated": "2025-01-24T08:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    39,
                    50,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:39:50Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    39,
                    50,
                    4,
                    24,
                    0
                ],
                "title": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models"
                },
                "summary": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ridhi Jain"
                    },
                    {
                        "name": "Rahul Purandare"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Purandare"
                },
                "author": "Rahul Purandare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08706v3",
                "updated": "2025-01-24T08:23:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    23,
                    3,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-11T10:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    48,
                    51,
                    4,
                    285,
                    0
                ],
                "title": "Goal-Oriented Status Updating for Real-time Remote Inference over\n  Networks with Two-Way Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-Oriented Status Updating for Real-time Remote Inference over\n  Networks with Two-Way Delay"
                },
                "summary": "We study a setting where an intelligent model (e.g., a pre-trained neural\nnetwork) predicts the real-time value of a target signal using data samples\ntransmitted from a remote source according to a scheduling policy. The\nscheduler decides on i) the age of the samples to be sent, ii) when to send\nthem, and iii) the length of each packet (i.e., the number of samples contained\nin each packet). The dependence of inference quality on the Age of Information\n(AoI) for a given packet length is modeled by a general relationship. Previous\nwork assumed i.i.d. transmission delays with immediate feedback or were\nrestricted to the case where inference performance degrades as the input data\nages. Our formulation, in addition to capturing non-monotone age dependence,\nalso covers Markovian delay on both forward and feedback links. We model this\nas an infinite-horizon average-cost Semi-Markov Decision Process. We obtain a\nclosed-form solution that decides on (i) and (ii) for any constant packet\nlength. The solution for when to send is an index-based threshold policy, where\nthe index function is expressed in terms of the delay state and AoI at the\nreceiver. The age of the packet selected is a function of the delay state. We\nseparately optimize the value of the constant length. We also develop an\nindex-based threshold policy for the variable length case, which allows a\ncomplexity reduction. In simulation results, we observe that our goal-oriented\nscheduler drops inference error down to one sixth with respect to age-based\nscheduling of unit-length packets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a setting where an intelligent model (e.g., a pre-trained neural\nnetwork) predicts the real-time value of a target signal using data samples\ntransmitted from a remote source according to a scheduling policy. The\nscheduler decides on i) the age of the samples to be sent, ii) when to send\nthem, and iii) the length of each packet (i.e., the number of samples contained\nin each packet). The dependence of inference quality on the Age of Information\n(AoI) for a given packet length is modeled by a general relationship. Previous\nwork assumed i.i.d. transmission delays with immediate feedback or were\nrestricted to the case where inference performance degrades as the input data\nages. Our formulation, in addition to capturing non-monotone age dependence,\nalso covers Markovian delay on both forward and feedback links. We model this\nas an infinite-horizon average-cost Semi-Markov Decision Process. We obtain a\nclosed-form solution that decides on (i) and (ii) for any constant packet\nlength. The solution for when to send is an index-based threshold policy, where\nthe index function is expressed in terms of the delay state and AoI at the\nreceiver. The age of the packet selected is a function of the delay state. We\nseparately optimize the value of the constant length. We also develop an\nindex-based threshold policy for the variable length case, which allows a\ncomplexity reduction. In simulation results, we observe that our goal-oriented\nscheduler drops inference error down to one sixth with respect to age-based\nscheduling of unit-length packets."
                },
                "authors": [
                    {
                        "name": "Cagri Ari"
                    },
                    {
                        "name": "Md Kamran Chowdhury Shisher"
                    },
                    {
                        "name": "Yin Sun"
                    },
                    {
                        "name": "Elif Uysal"
                    }
                ],
                "author_detail": {
                    "name": "Elif Uysal"
                },
                "author": "Elif Uysal",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09099v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09099v5",
                "updated": "2025-01-24T08:22:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    22,
                    25,
                    4,
                    24,
                    0
                ],
                "published": "2024-02-14T11:20:09Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    11,
                    20,
                    9,
                    2,
                    45,
                    0
                ],
                "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in\n  Large Models"
                },
                "summary": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models."
                },
                "authors": [
                    {
                        "name": "Xiongye Xiao"
                    },
                    {
                        "name": "Chenyu Zhou"
                    },
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Defu Cao"
                    },
                    {
                        "name": "Yaxing Li"
                    },
                    {
                        "name": "Yi-Zhuo Zhou"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Nikos Kanakaris"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "arxiv_comment": "ICLR 2025: https://openreview.net/forum?id=nt8gBX58Kh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09099v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09099v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14315v1",
                "updated": "2025-01-24T08:18:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    18,
                    56,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:18:56Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    18,
                    56,
                    4,
                    24,
                    0
                ],
                "title": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of\n  Token Perplexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of\n  Token Perplexity"
                },
                "summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. In this paper, we present a systematic analysis\nrevealing that fine-tuning with LLM-generated data not only improves target\ntask performance but also reduces out-of-domain (OOD) degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhanced OOD robustness\nstems from a reduced prevalence of high perplexity tokens in LLM-generated\nsequences. Following this hypothesis we showed that masking high perplexity\ntokens in ground truth training data also achieves similar OOD preservation\ncomparable to using LLM-generated data. Extensive experiments across diverse\nmodel architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B,\ncorroborate the consistency of our findings. To the best of our knowledge, this\nwork provides the first mechanistic explanation for the superior OOD robustness\nconferred by LLM-generated training data, offering valuable insights for\ndeveloping more robust fine-tuning strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. In this paper, we present a systematic analysis\nrevealing that fine-tuning with LLM-generated data not only improves target\ntask performance but also reduces out-of-domain (OOD) degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhanced OOD robustness\nstems from a reduced prevalence of high perplexity tokens in LLM-generated\nsequences. Following this hypothesis we showed that masking high perplexity\ntokens in ground truth training data also achieves similar OOD preservation\ncomparable to using LLM-generated data. Extensive experiments across diverse\nmodel architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B,\ncorroborate the consistency of our findings. To the best of our knowledge, this\nwork provides the first mechanistic explanation for the superior OOD robustness\nconferred by LLM-generated training data, offering valuable insights for\ndeveloping more robust fine-tuning strategies."
                },
                "authors": [
                    {
                        "name": "Chao-Chung Wu"
                    },
                    {
                        "name": "Zhi Rui Tam"
                    },
                    {
                        "name": "Chieh-Yen Lin"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05006v2",
                "updated": "2025-01-24T08:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    13,
                    35,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-09T11:35:44Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    35,
                    44,
                    4,
                    222,
                    0
                ],
                "title": "COAST: Enhancing the Code Debugging Ability of LLMs through\n  Communicative Agent Based Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COAST: Enhancing the Code Debugging Ability of LLMs through\n  Communicative Agent Based Data Synthesis"
                },
                "summary": "Code debugging is a vital stage of software development, essential for\nensuring the reliability and performance of Large Language Models (LLMs) in\ncode generation task. Human debugging typically follows a multi-stage process,\nwhich includes Bug Localization, Bug Identification, Code Repair, and Code\nRecognition. However, existing code debugging benchmarks predominantly focus on\nthe Code Repair stage, which offers only a limited perspective on evaluating\nthe debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a\ncomprehensive benchmark for evaluating the debugging abilities of LLMs by\nemulating the multi-stage human debugging process. Through evaluating on\nDEBUGEVAL, we observe that 7B-scale models consistently underperform compared\nto their larger counterparts, highlighting their limitations in comprehending\ncode semantics. In this case, we propose the COmmunicative Agent-based data\nSynThesis (COAST) framework, which employs a multi-agent system to generate\nhigh-quality training data for supervised fine-tuning (SFT). Experimental\nresults demonstrate that COAST-generated data outperform human-curated and\nGPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance\ncomparable to GPT-3.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code debugging is a vital stage of software development, essential for\nensuring the reliability and performance of Large Language Models (LLMs) in\ncode generation task. Human debugging typically follows a multi-stage process,\nwhich includes Bug Localization, Bug Identification, Code Repair, and Code\nRecognition. However, existing code debugging benchmarks predominantly focus on\nthe Code Repair stage, which offers only a limited perspective on evaluating\nthe debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a\ncomprehensive benchmark for evaluating the debugging abilities of LLMs by\nemulating the multi-stage human debugging process. Through evaluating on\nDEBUGEVAL, we observe that 7B-scale models consistently underperform compared\nto their larger counterparts, highlighting their limitations in comprehending\ncode semantics. In this case, we propose the COmmunicative Agent-based data\nSynThesis (COAST) framework, which employs a multi-agent system to generate\nhigh-quality training data for supervised fine-tuning (SFT). Experimental\nresults demonstrate that COAST-generated data outperform human-curated and\nGPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance\ncomparable to GPT-3.5."
                },
                "authors": [
                    {
                        "name": "Weiqing Yang"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Minghe Yu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14308v1",
                "updated": "2025-01-24T08:10:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    10,
                    5,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:10:05Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    10,
                    5,
                    4,
                    24,
                    0
                ],
                "title": "Learning Primitive Relations for Compositional Zero-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Primitive Relations for Compositional Zero-Shot Learning"
                },
                "summary": "Compositional Zero-Shot Learning (CZSL) aims to identify unseen state-object\ncompositions by leveraging knowledge learned from seen compositions. Existing\napproaches often independently predict states and objects, overlooking their\nrelationships. In this paper, we propose a novel framework, learning primitive\nrelations (LPR), designed to probabilistically capture the relationships\nbetween states and objects. By employing the cross-attention mechanism, LPR\nconsiders the dependencies between states and objects, enabling the model to\ninfer the likelihood of unseen compositions. Experimental results demonstrate\nthat LPR outperforms state-of-the-art methods on all three CZSL benchmark\ndatasets in both closed-world and open-world settings. Through qualitative\nanalysis, we show that LPR leverages state-object relationships for unseen\ncomposition prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Zero-Shot Learning (CZSL) aims to identify unseen state-object\ncompositions by leveraging knowledge learned from seen compositions. Existing\napproaches often independently predict states and objects, overlooking their\nrelationships. In this paper, we propose a novel framework, learning primitive\nrelations (LPR), designed to probabilistically capture the relationships\nbetween states and objects. By employing the cross-attention mechanism, LPR\nconsiders the dependencies between states and objects, enabling the model to\ninfer the likelihood of unseen compositions. Experimental results demonstrate\nthat LPR outperforms state-of-the-art methods on all three CZSL benchmark\ndatasets in both closed-world and open-world settings. Through qualitative\nanalysis, we show that LPR leverages state-object relationships for unseen\ncomposition prediction."
                },
                "authors": [
                    {
                        "name": "Insu Lee"
                    },
                    {
                        "name": "Jiseob Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Byonghyo Shim"
                    }
                ],
                "author_detail": {
                    "name": "Byonghyo Shim"
                },
                "author": "Byonghyo Shim",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14305v1",
                "updated": "2025-01-24T08:01:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    41,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:01:41Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    41,
                    4,
                    24,
                    0
                ],
                "title": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher\n  Education"
                },
                "summary": "Automated grading has become an essential tool in education technology due to\nits ability to efficiently assess large volumes of student work, provide\nconsistent and unbiased evaluations, and deliver immediate feedback to enhance\nlearning. However, current systems face significant limitations, including the\nneed for large datasets in few-shot learning methods, a lack of personalized\nand actionable feedback, and an overemphasis on benchmark performance rather\nthan student experience. To address these challenges, we propose a Zero-Shot\nLarge Language Model (LLM)-Based Automated Assignment Grading (AAG) system.\nThis framework leverages prompt engineering to evaluate both computational and\nexplanatory student responses without requiring additional training or\nfine-tuning. The AAG system delivers tailored feedback that highlights\nindividual strengths and areas for improvement, thereby enhancing student\nlearning outcomes. Our study demonstrates the system's effectiveness through\ncomprehensive evaluations, including survey responses from higher education\nstudents that indicate significant improvements in motivation, understanding,\nand preparedness compared to traditional grading methods. The results validate\nthe AAG system's potential to transform educational assessment by prioritizing\nlearning experiences and providing scalable, high-quality feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated grading has become an essential tool in education technology due to\nits ability to efficiently assess large volumes of student work, provide\nconsistent and unbiased evaluations, and deliver immediate feedback to enhance\nlearning. However, current systems face significant limitations, including the\nneed for large datasets in few-shot learning methods, a lack of personalized\nand actionable feedback, and an overemphasis on benchmark performance rather\nthan student experience. To address these challenges, we propose a Zero-Shot\nLarge Language Model (LLM)-Based Automated Assignment Grading (AAG) system.\nThis framework leverages prompt engineering to evaluate both computational and\nexplanatory student responses without requiring additional training or\nfine-tuning. The AAG system delivers tailored feedback that highlights\nindividual strengths and areas for improvement, thereby enhancing student\nlearning outcomes. Our study demonstrates the system's effectiveness through\ncomprehensive evaluations, including survey responses from higher education\nstudents that indicate significant improvements in motivation, understanding,\nand preparedness compared to traditional grading methods. The results validate\nthe AAG system's potential to transform educational assessment by prioritizing\nlearning experiences and providing scalable, high-quality feedback."
                },
                "authors": [
                    {
                        "name": "Calvin Yeung"
                    },
                    {
                        "name": "Jeff Yu"
                    },
                    {
                        "name": "King Chau Cheung"
                    },
                    {
                        "name": "Tat Wing Wong"
                    },
                    {
                        "name": "Chun Man Chan"
                    },
                    {
                        "name": "Kin Chi Wong"
                    },
                    {
                        "name": "Keisuke Fujii"
                    }
                ],
                "author_detail": {
                    "name": "Keisuke Fujii"
                },
                "author": "Keisuke Fujii",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14304v1",
                "updated": "2025-01-24T08:01:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    11,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:01:11Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    11,
                    4,
                    24,
                    0
                ],
                "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASTER: A Multi-Agent System with LLM Specialized MCTS"
                },
                "summary": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, leading to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot obtain an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present Multi-Agent\nSystem with Tactical Execution and Reasoning using LLM Specialized MCTS\n(MASTER), a novel framework that coordinates agent recruitment and\ncommunication using LLM specialized MCTS. This system autonomously adjusts the\nnumber of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, leading to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot obtain an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present Multi-Agent\nSystem with Tactical Execution and Reasoning using LLM Specialized MCTS\n(MASTER), a novel framework that coordinates agent recruitment and\ncommunication using LLM specialized MCTS. This system autonomously adjusts the\nnumber of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets."
                },
                "authors": [
                    {
                        "name": "Bingzheng Gan"
                    },
                    {
                        "name": "Yufan Zhao"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yusu Li"
                    },
                    {
                        "name": "Shu Xian Teo"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shi"
                },
                "author": "Wei Shi",
                "arxiv_comment": "Accepted by main NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14301v1",
                "updated": "2025-01-24T07:51:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    51,
                    29,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:51:29Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    51,
                    29,
                    4,
                    24,
                    0
                ],
                "title": "A high-resolution discourse on seismic tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A high-resolution discourse on seismic tomography"
                },
                "summary": "Advances in data acquisition and numerical wave simulation have improved\ntomographic imaging techniques and results, but non-experts may find it\ndifficult to understand which model is best for their needs. This paper is\nintended for these users. We argue that our notion of best is influenced by the\nextent to which models satisfy our biases. We explain how the basic types of\nseismic waves see Earth structure, illustrate the essential strategy of seismic\ntomography, discuss advanced adaptations such as full-waveform inversion, and\nemphasize the artistic components of tomography. The compounding effect of a\nplethora of reasonable, yet subjective choices is a range of models that differ\nmore than their individual uncertainty analyses may suggest. Perhaps\ncounter-intuitively, we argue producing similar tomographic models should not\nbe the goal of seismic tomography. Instead, we promote a Community Monte Carlo\neffort to assemble a range of dissimilar models based on different modeling\napproaches and subjective choices, but which explain the seismic data. This\neffort could serve as input for geodynamic inferences with meaningful seismic\nuncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in data acquisition and numerical wave simulation have improved\ntomographic imaging techniques and results, but non-experts may find it\ndifficult to understand which model is best for their needs. This paper is\nintended for these users. We argue that our notion of best is influenced by the\nextent to which models satisfy our biases. We explain how the basic types of\nseismic waves see Earth structure, illustrate the essential strategy of seismic\ntomography, discuss advanced adaptations such as full-waveform inversion, and\nemphasize the artistic components of tomography. The compounding effect of a\nplethora of reasonable, yet subjective choices is a range of models that differ\nmore than their individual uncertainty analyses may suggest. Perhaps\ncounter-intuitively, we argue producing similar tomographic models should not\nbe the goal of seismic tomography. Instead, we promote a Community Monte Carlo\neffort to assemble a range of dissimilar models based on different modeling\napproaches and subjective choices, but which explain the seismic data. This\neffort could serve as input for geodynamic inferences with meaningful seismic\nuncertainties."
                },
                "authors": [
                    {
                        "name": "Andreas Fichtner"
                    },
                    {
                        "name": "Jeroen Ritsema"
                    },
                    {
                        "name": "Solvi Thrastarson"
                    }
                ],
                "author_detail": {
                    "name": "Solvi Thrastarson"
                },
                "author": "Solvi Thrastarson",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14300v1",
                "updated": "2025-01-24T07:47:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    47,
                    40,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:47:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    47,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large\n  Language Model on Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large\n  Language Model on Knowledge Graph"
                },
                "summary": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes\nthe naive RAG system a step further by integrating graph information, such as\nknowledge graph (KGs), into large-scale language models (LLMs) to mitigate\nhallucination. However, existing GRAG still encounter limitations: 1) simple\nparadigms usually fail with the complex problems due to the narrow and shallow\ncorrelations capture from KGs 2) methods of strong coupling with KGs tend to be\nhigh computation cost and time consuming if the graph is dense. In this paper,\nwe propose the Fast Think-on-Graph (FastToG), an innovative paradigm for\nenabling LLMs to think ``community by community\" within KGs. To do this,\nFastToG employs community detection for deeper correlation capture and two\nstages community pruning - coarse and fine pruning for faster retrieval.\nFurthermore, we also develop two Community-to-Text methods to convert the graph\nstructure of communities into textual form for better understanding by LLMs.\nExperimental results demonstrate the effectiveness of FastToG, showcasing\nhigher accuracy, faster reasoning, and better explainability compared to the\nprevious works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes\nthe naive RAG system a step further by integrating graph information, such as\nknowledge graph (KGs), into large-scale language models (LLMs) to mitigate\nhallucination. However, existing GRAG still encounter limitations: 1) simple\nparadigms usually fail with the complex problems due to the narrow and shallow\ncorrelations capture from KGs 2) methods of strong coupling with KGs tend to be\nhigh computation cost and time consuming if the graph is dense. In this paper,\nwe propose the Fast Think-on-Graph (FastToG), an innovative paradigm for\nenabling LLMs to think ``community by community\" within KGs. To do this,\nFastToG employs community detection for deeper correlation capture and two\nstages community pruning - coarse and fine pruning for faster retrieval.\nFurthermore, we also develop two Community-to-Text methods to convert the graph\nstructure of communities into textual form for better understanding by LLMs.\nExperimental results demonstrate the effectiveness of FastToG, showcasing\nhigher accuracy, faster reasoning, and better explainability compared to the\nprevious works."
                },
                "authors": [
                    {
                        "name": "Xujian Liang"
                    },
                    {
                        "name": "Zhaoquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoquan Gu"
                },
                "author": "Zhaoquan Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12334v3",
                "updated": "2025-01-24T07:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    40,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-18T06:59:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    59,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering"
                },
                "summary": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance."
                },
                "authors": [
                    {
                        "name": "Federico Errica"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14296v1",
                "updated": "2025-01-24T07:33:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    33,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:33:39Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    33,
                    39,
                    4,
                    24,
                    0
                ],
                "title": "Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in\n  Relevance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in\n  Relevance Assessment"
                },
                "summary": "The effectiveness of search systems is evaluated using relevance labels that\nindicate the usefulness of documents for specific queries and users. While\nobtaining these relevance labels from real users is ideal, scaling such data\ncollection is challenging. Consequently, third-party annotators are employed,\nbut their inconsistent accuracy demands costly auditing, training, and\nmonitoring. We propose an LLM-based modular classification pipeline that\ndivides the relevance assessment task into multiple stages, each utilising\ndifferent prompts and models of varying sizes and capabilities. Applied to TREC\nDeep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff's\n$\\alpha$ accuracy increase over OpenAI's GPT-4o mini while maintaining a cost\nof about 0.2 USD per million input tokens, offering a more efficient and\nscalable solution for relevance assessment. This approach beats the baseline\nperformance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of\nthe GPT-4o flagship model, measured in $\\alpha$, could be improved by 9.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of search systems is evaluated using relevance labels that\nindicate the usefulness of documents for specific queries and users. While\nobtaining these relevance labels from real users is ideal, scaling such data\ncollection is challenging. Consequently, third-party annotators are employed,\nbut their inconsistent accuracy demands costly auditing, training, and\nmonitoring. We propose an LLM-based modular classification pipeline that\ndivides the relevance assessment task into multiple stages, each utilising\ndifferent prompts and models of varying sizes and capabilities. Applied to TREC\nDeep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff's\n$\\alpha$ accuracy increase over OpenAI's GPT-4o mini while maintaining a cost\nof about 0.2 USD per million input tokens, offering a more efficient and\nscalable solution for relevance assessment. This approach beats the baseline\nperformance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of\nthe GPT-4o flagship model, measured in $\\alpha$, could be improved by 9.7%."
                },
                "authors": [
                    {
                        "name": "Julian A. Schnabel"
                    },
                    {
                        "name": "Johanne R. Trippas"
                    },
                    {
                        "name": "Falk Scholer"
                    },
                    {
                        "name": "Danula Hettiachchi"
                    }
                ],
                "author_detail": {
                    "name": "Danula Hettiachchi"
                },
                "author": "Danula Hettiachchi",
                "arxiv_comment": "WebConf'25, WWW'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06814v2",
                "updated": "2025-01-24T07:28:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    28,
                    49,
                    4,
                    24,
                    0
                ],
                "published": "2024-04-10T08:02:17Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    8,
                    2,
                    17,
                    2,
                    101,
                    0
                ],
                "title": "ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors"
                },
                "summary": "3D point clouds directly collected from objects through sensors are often\nincomplete due to self-occlusion. Conventional methods for completing these\npartial point clouds rely on manually organized training sets and are usually\nlimited to object categories seen during training. In this work, we propose a\ntest-time framework for completing partial point clouds across unseen\ncategories without any requirement for training. Leveraging point rendering via\nGaussian Splatting, we develop techniques of Partial Gaussian Initialization,\nZero-shot Fractal Completion, and Point Cloud Extraction that utilize priors\nfrom pre-trained 2D diffusion models to infer missing regions and extract\nuniform completed point clouds. Experimental results on both synthetic and\nreal-world scanned point clouds demonstrate that our approach outperforms\nexisting methods in completing a variety of objects. Our project page is at\n\\url{https://tianxinhuang.github.io/projects/ComPC/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D point clouds directly collected from objects through sensors are often\nincomplete due to self-occlusion. Conventional methods for completing these\npartial point clouds rely on manually organized training sets and are usually\nlimited to object categories seen during training. In this work, we propose a\ntest-time framework for completing partial point clouds across unseen\ncategories without any requirement for training. Leveraging point rendering via\nGaussian Splatting, we develop techniques of Partial Gaussian Initialization,\nZero-shot Fractal Completion, and Point Cloud Extraction that utilize priors\nfrom pre-trained 2D diffusion models to infer missing regions and extract\nuniform completed point clouds. Experimental results on both synthetic and\nreal-world scanned point clouds demonstrate that our approach outperforms\nexisting methods in completing a variety of objects. Our project page is at\n\\url{https://tianxinhuang.github.io/projects/ComPC/}."
                },
                "authors": [
                    {
                        "name": "Tianxin Huang"
                    },
                    {
                        "name": "Zhiwen Yan"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14294v1",
                "updated": "2025-01-24T07:24:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    24,
                    23,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:24:23Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    24,
                    23,
                    4,
                    24,
                    0
                ],
                "title": "Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes"
                },
                "summary": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, particularly when these systems fail to operate as\nintended. This study explores the challenge of aligning LLMs with human\nintentions and values, with specific focus on their political inclinations.\nPrevious research has highlighted LLMs' propensity to display political\nleanings, and their ability to mimic certain political parties' stances on\nvarious issues. However, the extent and conditions under which LLMs deviate\nfrom empirical positions have not been thoroughly examined. To address this\ngap, our study systematically investigates the factors contributing to LLMs'\ndeviations from empirical positions on political issues, aiming to quantify\nthese deviations and identify the conditions that cause them.\n  Drawing on cognitive science findings related to representativeness\nheuristics -- where individuals readily recall the representative attribute of\na target group in a way that leads to exaggerated beliefs -- we scrutinize LLM\nresponses through this heuristics lens. We conduct experiments to determine how\nLLMs exhibit stereotypes by inflating judgments in favor of specific political\nparties. Our results indicate that while LLMs can mimic certain political\nparties' positions, they often exaggerate these positions more than human\nrespondents do. Notably, LLMs tend to overemphasize representativeness to a\ngreater extent than humans. This study highlights the susceptibility of LLMs to\nrepresentativeness heuristics, suggeseting potential vulnerabilities to\npolitical stereotypes. We propose prompt-based mitigation strategies that\ndemonstrate effectiveness in reducing the influence of representativeness in\nLLM responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, particularly when these systems fail to operate as\nintended. This study explores the challenge of aligning LLMs with human\nintentions and values, with specific focus on their political inclinations.\nPrevious research has highlighted LLMs' propensity to display political\nleanings, and their ability to mimic certain political parties' stances on\nvarious issues. However, the extent and conditions under which LLMs deviate\nfrom empirical positions have not been thoroughly examined. To address this\ngap, our study systematically investigates the factors contributing to LLMs'\ndeviations from empirical positions on political issues, aiming to quantify\nthese deviations and identify the conditions that cause them.\n  Drawing on cognitive science findings related to representativeness\nheuristics -- where individuals readily recall the representative attribute of\na target group in a way that leads to exaggerated beliefs -- we scrutinize LLM\nresponses through this heuristics lens. We conduct experiments to determine how\nLLMs exhibit stereotypes by inflating judgments in favor of specific political\nparties. Our results indicate that while LLMs can mimic certain political\nparties' positions, they often exaggerate these positions more than human\nrespondents do. Notably, LLMs tend to overemphasize representativeness to a\ngreater extent than humans. This study highlights the susceptibility of LLMs to\nrepresentativeness heuristics, suggeseting potential vulnerabilities to\npolitical stereotypes. We propose prompt-based mitigation strategies that\ndemonstrate effectiveness in reducing the influence of representativeness in\nLLM responses."
                },
                "authors": [
                    {
                        "name": "Sullam Jeoung"
                    },
                    {
                        "name": "Yubin Ge"
                    },
                    {
                        "name": "Haohan Wang"
                    },
                    {
                        "name": "Jana Diesner"
                    }
                ],
                "author_detail": {
                    "name": "Jana Diesner"
                },
                "author": "Jana Diesner",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14291v1",
                "updated": "2025-01-24T07:13:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    13,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:13:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    13,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches"
                },
                "summary": "Temporal point processes (TPPs) are stochastic process models used to\ncharacterize event sequences occurring in continuous time. Traditional\nstatistical TPPs have a long-standing history, with numerous models proposed\nand successfully applied across diverse domains. In recent years, advances in\ndeep learning have spurred the development of neural TPPs, enabling greater\nflexibility and expressiveness in capturing complex temporal dynamics. The\nemergence of large language models (LLMs) has further sparked excitement,\noffering new possibilities for modeling and analyzing event sequences by\nleveraging their rich contextual understanding. This survey presents a\ncomprehensive review of recent research on TPPs from three perspectives:\nBayesian, deep learning, and LLM approaches. We begin with a review of the\nfundamental concepts of TPPs, followed by an in-depth discussion of model\ndesign and parameter estimation techniques in these three frameworks. We also\nrevisit classic application areas of TPPs to highlight their practical\nrelevance. Finally, we outline challenges and promising directions for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal point processes (TPPs) are stochastic process models used to\ncharacterize event sequences occurring in continuous time. Traditional\nstatistical TPPs have a long-standing history, with numerous models proposed\nand successfully applied across diverse domains. In recent years, advances in\ndeep learning have spurred the development of neural TPPs, enabling greater\nflexibility and expressiveness in capturing complex temporal dynamics. The\nemergence of large language models (LLMs) has further sparked excitement,\noffering new possibilities for modeling and analyzing event sequences by\nleveraging their rich contextual understanding. This survey presents a\ncomprehensive review of recent research on TPPs from three perspectives:\nBayesian, deep learning, and LLM approaches. We begin with a review of the\nfundamental concepts of TPPs, followed by an in-depth discussion of model\ndesign and parameter estimation techniques in these three frameworks. We also\nrevisit classic application areas of TPPs to highlight their practical\nrelevance. Finally, we outline challenges and promising directions for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Quyu Kong"
                    },
                    {
                        "name": "Yixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Zhang"
                },
                "author": "Yixuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09688v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09688v4",
                "updated": "2025-01-24T07:10:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    10,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-19T03:53:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts"
                },
                "summary": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task."
                },
                "authors": [
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09688v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09688v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11610v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11610v5",
                "updated": "2025-01-24T07:04:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    4,
                    50,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-15T13:46:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    46,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation"
                },
                "summary": "Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. In state-of-the-art architectures, the main challenge is to\nefficiently capture complex objects and fine-grained details, which are often\ndifficult to predict. This paper introduces a novel deep learning-based\napproach using an enhanced encoder-decoder architecture, where the\nInception-ResNet-v2 model serves as the encoder. This is the first instance of\nutilizing Inception-ResNet-v2 as an encoder for monocular depth estimation,\ndemonstrating improved performance over previous models. It incorporates\nmulti-scale feature extraction to enhance depth prediction accuracy across\nvarious object sizes and distances. We propose a composite loss function\ncomprising depth loss, gradient edge loss, and Structural Similarity Index\nMeasure (SSIM) loss, with fine-tuned weights to optimize the weighted sum,\nensuring a balance across different aspects of depth estimation. Experimental\nresults on the KITTI dataset show that our model achieves a significantly\nfaster inference time of 0.019 seconds, outperforming vision transformers in\nefficiency while maintaining good accuracy. On the NYU Depth V2 dataset, the\nmodel establishes state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of\n89.3% for $\\delta$ < 1.25. These metrics demonstrate that our model can\naccurately and efficiently predict depth even in challenging scenarios,\nproviding a practical solution for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. In state-of-the-art architectures, the main challenge is to\nefficiently capture complex objects and fine-grained details, which are often\ndifficult to predict. This paper introduces a novel deep learning-based\napproach using an enhanced encoder-decoder architecture, where the\nInception-ResNet-v2 model serves as the encoder. This is the first instance of\nutilizing Inception-ResNet-v2 as an encoder for monocular depth estimation,\ndemonstrating improved performance over previous models. It incorporates\nmulti-scale feature extraction to enhance depth prediction accuracy across\nvarious object sizes and distances. We propose a composite loss function\ncomprising depth loss, gradient edge loss, and Structural Similarity Index\nMeasure (SSIM) loss, with fine-tuned weights to optimize the weighted sum,\nensuring a balance across different aspects of depth estimation. Experimental\nresults on the KITTI dataset show that our model achieves a significantly\nfaster inference time of 0.019 seconds, outperforming vision transformers in\nefficiency while maintaining good accuracy. On the NYU Depth V2 dataset, the\nmodel establishes state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of\n89.3% for $\\delta$ < 1.25. These metrics demonstrate that our model can\naccurately and efficiently predict depth even in challenging scenarios,\nproviding a practical solution for real-time applications."
                },
                "authors": [
                    {
                        "name": "Dabbrata Das"
                    },
                    {
                        "name": "Argho Deb Das"
                    },
                    {
                        "name": "Farhan Sadaf"
                    }
                ],
                "author_detail": {
                    "name": "Farhan Sadaf"
                },
                "author": "Farhan Sadaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11610v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11610v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13457v2",
                "updated": "2025-01-24T06:46:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    46,
                    52,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-24T04:03:35Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    4,
                    3,
                    35,
                    5,
                    237,
                    0
                ],
                "title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for\n  Cost-Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for\n  Cost-Efficient Reasoning"
                },
                "summary": "Self-consistency (SC), a widely used decoding strategy for chain-of-thought\nreasoning, shows significant gains across various multi-step reasoning tasks\nbut comes with a high cost due to multiple sampling with the preset size. Its\nvariants, Adaptive self-consistency (ASC) and Early-stopping self-consistency\n(ESC), dynamically adjust the number of samples based on the posterior\ndistribution of a set of pre-samples, reducing the cost of SC with minimal\nimpact on performance. Both methods, however, do not exploit the prior\ninformation about question difficulty. It often results in unnecessary repeated\nsampling for easy questions that could be accurately answered with just one\nattempt, wasting resources. To tackle this problem, we propose\nDifficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty\ninformation from both prior and posterior perspectives to adaptively allocate\ninference resources, further reducing the cost of SC. To demonstrate the\neffectiveness of DSC, we conduct extensive experiments on three popular\ncategories of reasoning tasks: arithmetic, commonsense and symbolic reasoning\non six benchmarks. The empirical results show that DSC consistently surpasses\nthe strong baseline ASC and ESC in terms of costs by a significant margin,\nwhile attaining comparable performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-consistency (SC), a widely used decoding strategy for chain-of-thought\nreasoning, shows significant gains across various multi-step reasoning tasks\nbut comes with a high cost due to multiple sampling with the preset size. Its\nvariants, Adaptive self-consistency (ASC) and Early-stopping self-consistency\n(ESC), dynamically adjust the number of samples based on the posterior\ndistribution of a set of pre-samples, reducing the cost of SC with minimal\nimpact on performance. Both methods, however, do not exploit the prior\ninformation about question difficulty. It often results in unnecessary repeated\nsampling for easy questions that could be accurately answered with just one\nattempt, wasting resources. To tackle this problem, we propose\nDifficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty\ninformation from both prior and posterior perspectives to adaptively allocate\ninference resources, further reducing the cost of SC. To demonstrate the\neffectiveness of DSC, we conduct extensive experiments on three popular\ncategories of reasoning tasks: arithmetic, commonsense and symbolic reasoning\non six benchmarks. The empirical results show that DSC consistently surpasses\nthe strong baseline ASC and ESC in terms of costs by a significant margin,\nwhile attaining comparable performances."
                },
                "authors": [
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Heda Wang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09150v2",
                "updated": "2025-01-24T06:45:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    45,
                    3,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-17T09:49:40Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    9,
                    49,
                    40,
                    5,
                    230,
                    0
                ],
                "title": "CogLM: Tracking Cognitive Development of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogLM: Tracking Cognitive Development of Large Language Models"
                },
                "summary": "Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) Human-like cognitive\nabilities have emerged in advanced LLMs (GPT-4), comparable to those of a\n20-year-old human. (2) The parameter size and optimization objective are two\nkey factors affecting the cognitive levels of LLMs. (3) The performance on\ndownstream tasks is positively correlated with the level of cognitive\nabilities. These findings fill the gap in research on the cognitive abilities\nof LLMs, tracing the development of LLMs from a cognitive perspective and\nguiding the future direction of their evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) Human-like cognitive\nabilities have emerged in advanced LLMs (GPT-4), comparable to those of a\n20-year-old human. (2) The parameter size and optimization objective are two\nkey factors affecting the cognitive levels of LLMs. (3) The performance on\ndownstream tasks is positively correlated with the level of cognitive\nabilities. These findings fill the gap in research on the cognitive abilities\nof LLMs, tracing the development of LLMs from a cognitive perspective and\nguiding the future direction of their evolution."
                },
                "authors": [
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Heda Wang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "NAACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14275v1",
                "updated": "2025-01-24T06:39:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    39,
                    38,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T06:39:38Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    39,
                    38,
                    4,
                    24,
                    0
                ],
                "title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and\n  Contamination-Resistant Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and\n  Contamination-Resistant Evaluation"
                },
                "summary": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops"
                },
                "authors": [
                    {
                        "name": "Sadegh Mahdavi"
                    },
                    {
                        "name": "Muchen Li"
                    },
                    {
                        "name": "Kaiwen Liu"
                    },
                    {
                        "name": "Christos Thrampoulidis"
                    },
                    {
                        "name": "Leonid Sigal"
                    },
                    {
                        "name": "Renjie Liao"
                    }
                ],
                "author_detail": {
                    "name": "Renjie Liao"
                },
                "author": "Renjie Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14271v1",
                "updated": "2025-01-24T06:31:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    31,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T06:31:48Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    31,
                    48,
                    4,
                    24,
                    0
                ],
                "title": "TLXML: Task-Level Explanation of Meta-Learning via Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLXML: Task-Level Explanation of Meta-Learning via Influence Functions"
                },
                "summary": "The scheme of adaptation via meta-learning is seen as an ingredient for\nsolving the problem of data shortage or distribution shift in real-world\napplications, but it also brings the new risk of inappropriate updates of the\nmodel in the user environment, which increases the demand for explainability.\nAmong the various types of XAI methods, establishing a method of explanation\nbased on past experience in meta-learning requires special consideration due to\nits bi-level structure of training, which has been left unexplored. In this\nwork, we propose influence functions for explaining meta-learning that measure\nthe sensitivities of training tasks to adaptation and inference. We also argue\nthat the approximation of the Hessian using the Gauss-Newton matrix resolves\ncomputational barriers peculiar to meta-learning. We demonstrate the adequacy\nof the method through experiments on task distinction and task distribution\ndistinction using image classification tasks with MAML and Prototypical\nNetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scheme of adaptation via meta-learning is seen as an ingredient for\nsolving the problem of data shortage or distribution shift in real-world\napplications, but it also brings the new risk of inappropriate updates of the\nmodel in the user environment, which increases the demand for explainability.\nAmong the various types of XAI methods, establishing a method of explanation\nbased on past experience in meta-learning requires special consideration due to\nits bi-level structure of training, which has been left unexplored. In this\nwork, we propose influence functions for explaining meta-learning that measure\nthe sensitivities of training tasks to adaptation and inference. We also argue\nthat the approximation of the Hessian using the Gauss-Newton matrix resolves\ncomputational barriers peculiar to meta-learning. We demonstrate the adequacy\nof the method through experiments on task distinction and task distribution\ndistinction using image classification tasks with MAML and Prototypical\nNetwork."
                },
                "authors": [
                    {
                        "name": "Yoshihiro Mitsuka"
                    },
                    {
                        "name": "Shadan Golestan"
                    },
                    {
                        "name": "Zahin Sufiyan"
                    },
                    {
                        "name": "Sheila Schoepp"
                    },
                    {
                        "name": "Shotaro Miwa"
                    },
                    {
                        "name": "Osmar R. Zaïane"
                    }
                ],
                "author_detail": {
                    "name": "Osmar R. Zaïane"
                },
                "author": "Osmar R. Zaïane",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.14729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14729v1",
                "updated": "2025-01-24T18:59:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    59,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:59:51Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    59,
                    51,
                    4,
                    24,
                    0
                ],
                "title": "HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene\n  Understanding and Generation"
                },
                "summary": "Driving World Models (DWMs) have become essential for autonomous driving by\nenabling future scene prediction. However, existing DWMs are limited to scene\ngeneration and fail to incorporate scene understanding, which involves\ninterpreting and reasoning about the driving environment. In this paper, we\npresent a unified Driving World Model named HERMES. We seamlessly integrate 3D\nscene understanding and future scene evolution (generation) through a unified\nframework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye\nView (BEV) representation to consolidate multi-view spatial information while\npreserving geometric relationships and interactions. We also introduce world\nqueries, which incorporate world knowledge into BEV features via causal\nattention in the Large Language Model (LLM), enabling contextual enrichment for\nunderstanding and generation tasks. We conduct comprehensive studies on\nnuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our\nmethod. HERMES achieves state-of-the-art performance, reducing generation error\nby 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model\nand code will be publicly released at https://github.com/LMD0311/HERMES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving World Models (DWMs) have become essential for autonomous driving by\nenabling future scene prediction. However, existing DWMs are limited to scene\ngeneration and fail to incorporate scene understanding, which involves\ninterpreting and reasoning about the driving environment. In this paper, we\npresent a unified Driving World Model named HERMES. We seamlessly integrate 3D\nscene understanding and future scene evolution (generation) through a unified\nframework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye\nView (BEV) representation to consolidate multi-view spatial information while\npreserving geometric relationships and interactions. We also introduce world\nqueries, which incorporate world knowledge into BEV features via causal\nattention in the Large Language Model (LLM), enabling contextual enrichment for\nunderstanding and generation tasks. We conduct comprehensive studies on\nnuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our\nmethod. HERMES achieves state-of-the-art performance, reducing generation error\nby 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model\nand code will be publicly released at https://github.com/LMD0311/HERMES."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Sifan Tu"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Dingyuan Zhang"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Work in progress. The code will be available at\n  https://github.com/LMD0311/HERMES",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14723v1",
                "updated": "2025-01-24T18:58:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    58,
                    40,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:58:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    58,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering"
                },
                "summary": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys."
                },
                "authors": [
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Bradley Brown"
                    },
                    {
                        "name": "Jordan Juravsky"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12880v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12880v3",
                "updated": "2025-01-24T18:56:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    56,
                    7,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-15T18:13:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    18,
                    13,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to\n  Sensitivity in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to\n  Sensitivity in Large Language Models"
                },
                "summary": "As LLMs are increasingly deployed in global applications, the importance of\ncultural sensitivity becomes paramount, ensuring that users from diverse\nbackgrounds feel respected and understood. Cultural harm can arise when these\nmodels fail to align with specific cultural norms, resulting in\nmisrepresentations or violations of cultural values. This work addresses the\nchallenges of ensuring cultural sensitivity in LLMs, especially in\nsmall-parameter models that often lack the extensive training data needed to\ncapture global cultural nuances. We present two key contributions: (1) A\ncultural harm test dataset, created to assess model outputs across different\ncultural contexts through scenarios that expose potential cultural\ninsensitivities, and (2) A culturally aligned preference dataset, aimed at\nrestoring cultural sensitivity through fine-tuning based on feedback from\ndiverse annotators. These datasets facilitate the evaluation and enhancement of\nLLMs, ensuring their ethical and safe deployment across different cultural\nlandscapes. Our results show that integrating culturally aligned feedback leads\nto a marked improvement in model behavior, significantly reducing the\nlikelihood of generating culturally insensitive or harmful content. Ultimately,\nthis work paves the way for more inclusive and respectful AI systems, fostering\na future where LLMs can safely and ethically navigate the complexities of\ndiverse cultural landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are increasingly deployed in global applications, the importance of\ncultural sensitivity becomes paramount, ensuring that users from diverse\nbackgrounds feel respected and understood. Cultural harm can arise when these\nmodels fail to align with specific cultural norms, resulting in\nmisrepresentations or violations of cultural values. This work addresses the\nchallenges of ensuring cultural sensitivity in LLMs, especially in\nsmall-parameter models that often lack the extensive training data needed to\ncapture global cultural nuances. We present two key contributions: (1) A\ncultural harm test dataset, created to assess model outputs across different\ncultural contexts through scenarios that expose potential cultural\ninsensitivities, and (2) A culturally aligned preference dataset, aimed at\nrestoring cultural sensitivity through fine-tuning based on feedback from\ndiverse annotators. These datasets facilitate the evaluation and enhancement of\nLLMs, ensuring their ethical and safe deployment across different cultural\nlandscapes. Our results show that integrating culturally aligned feedback leads\nto a marked improvement in model behavior, significantly reducing the\nlikelihood of generating culturally insensitive or harmful content. Ultimately,\nthis work paves the way for more inclusive and respectful AI systems, fostering\na future where LLMs can safely and ethically navigate the complexities of\ndiverse cultural landscapes."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Hari Shrawgi"
                    },
                    {
                        "name": "Rajarshi Mandal"
                    },
                    {
                        "name": "Avik Halder"
                    },
                    {
                        "name": "Shanu Kumar"
                    },
                    {
                        "name": "Sagnik Basu"
                    },
                    {
                        "name": "Parag Agrawal"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "arxiv_comment": "Accepted at NAACL 2025 (Main track). [Project\n  Page](https://neuralsentinel.github.io/KaleidoCulture/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12880v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14719v1",
                "updated": "2025-01-24T18:51:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    51,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:51:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    51,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "Do LLMs Provide Consistent Answers to Health-Related Questions across\n  Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Provide Consistent Answers to Health-Related Questions across\n  Languages?"
                },
                "summary": "Equitable access to reliable health information is vital for public health,\nbut the quality of online health resources varies by language, raising concerns\nabout inconsistencies in Large Language Models (LLMs) for healthcare. In this\nstudy, we examine the consistency of responses provided by LLMs to\nhealth-related questions across English, German, Turkish, and Chinese. We\nlargely expand the HealthFC dataset by categorizing health-related questions by\ndisease type and broadening its multilingual scope with Turkish and Chinese\ntranslations. We reveal significant inconsistencies in responses that could\nspread healthcare misinformation. Our main contributions are 1) a multilingual\nhealth-related inquiry dataset with meta-information on disease categories, and\n2) a novel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings highlight key\nchallenges in deploying LLM-based tools in multilingual contexts and emphasize\nthe need for improved cross-lingual alignment to ensure accurate and equitable\nhealthcare information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equitable access to reliable health information is vital for public health,\nbut the quality of online health resources varies by language, raising concerns\nabout inconsistencies in Large Language Models (LLMs) for healthcare. In this\nstudy, we examine the consistency of responses provided by LLMs to\nhealth-related questions across English, German, Turkish, and Chinese. We\nlargely expand the HealthFC dataset by categorizing health-related questions by\ndisease type and broadening its multilingual scope with Turkish and Chinese\ntranslations. We reveal significant inconsistencies in responses that could\nspread healthcare misinformation. Our main contributions are 1) a multilingual\nhealth-related inquiry dataset with meta-information on disease categories, and\n2) a novel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings highlight key\nchallenges in deploying LLM-based tools in multilingual contexts and emphasize\nthe need for improved cross-lingual alignment to ensure accurate and equitable\nhealthcare information."
                },
                "authors": [
                    {
                        "name": "Ipek Baris Schlicht"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Burcu Sayin"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Paolo Rosso"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Rosso"
                },
                "author": "Paolo Rosso",
                "arxiv_comment": "9 pages. Short paper appeared at 47th European Conference on\n  Information Retrieval (ECIR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14717v1",
                "updated": "2025-01-24T18:50:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    50,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:50:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    50,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "Towards Better Understanding Table Instruction Tuning: Decoupling the\n  Effects from Data versus Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Understanding Table Instruction Tuning: Decoupling the\n  Effects from Data versus Models"
                },
                "summary": "Recent advances in natural language processing have leveraged instruction\ntuning to enhance Large Language Models (LLMs) for table-related tasks.\nHowever, previous works train different base models with different training\ndata, lacking an apples-to-apples comparison across the result table LLMs. To\naddress this, we fine-tune base models from the Mistral, OLMo, and Phi families\non existing public training datasets. Our replication achieves performance on\npar with or surpassing existing table LLMs, establishing new state-of-the-art\nperformance on Hitab, a table question-answering dataset. More importantly,\nthrough systematic out-of-domain evaluation, we decouple the contributions of\ntraining data and the base model, providing insight into their individual\nimpacts. In addition, we assess the effects of table-specific instruction\ntuning on general-purpose benchmarks, revealing trade-offs between\nspecialization and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in natural language processing have leveraged instruction\ntuning to enhance Large Language Models (LLMs) for table-related tasks.\nHowever, previous works train different base models with different training\ndata, lacking an apples-to-apples comparison across the result table LLMs. To\naddress this, we fine-tune base models from the Mistral, OLMo, and Phi families\non existing public training datasets. Our replication achieves performance on\npar with or surpassing existing table LLMs, establishing new state-of-the-art\nperformance on Hitab, a table question-answering dataset. More importantly,\nthrough systematic out-of-domain evaluation, we decouple the contributions of\ntraining data and the base model, providing insight into their individual\nimpacts. In addition, we assess the effects of table-specific instruction\ntuning on general-purpose benchmarks, revealing trade-offs between\nspecialization and generalization."
                },
                "authors": [
                    {
                        "name": "Naihao Deng"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Henghui Zhu"
                    },
                    {
                        "name": "Shuaichen Chang"
                    },
                    {
                        "name": "Jiani Zhang"
                    },
                    {
                        "name": "Alexander Hanbo Li"
                    },
                    {
                        "name": "Chung-Wei Hang"
                    },
                    {
                        "name": "Hideo Kobayashi"
                    },
                    {
                        "name": "Yiqun Hu"
                    },
                    {
                        "name": "Patrick Ng"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Ng"
                },
                "author": "Patrick Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14713v1",
                "updated": "2025-01-24T18:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    46,
                    37,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:46:37Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    46,
                    37,
                    4,
                    24,
                    0
                ],
                "title": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing"
                },
                "summary": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs."
                },
                "authors": [
                    {
                        "name": "James Seale Smith"
                    },
                    {
                        "name": "Chi-Heng Lin"
                    },
                    {
                        "name": "Shikhar Tuli"
                    },
                    {
                        "name": "Haris Jeelani"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "Yilin Shen"
                    },
                    {
                        "name": "Hongxia Jin"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Yen-Chang Hsu"
                },
                "author": "Yen-Chang Hsu",
                "arxiv_comment": "Accepted to NAACL 2025 - Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14705v1",
                "updated": "2025-01-24T18:30:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    30,
                    19,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:30:19Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    30,
                    19,
                    4,
                    24,
                    0
                ],
                "title": "The Karp Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Karp Dataset"
                },
                "summary": "Understanding the mathematical reasoning capabilities of Large Language\nModels (LLMs) is a central topic in the study of artificial intelligence. This\nnew domain necessitates the creation of datasets of reasoning tasks for both\ntraining and benchmarking the performance of LLMs. To this end, we introduce\nthe Karp dataset: The first dataset composed of detailed proofs of\nNP-completeness reductions. The reductions vary in difficulty, ranging from\nsimple exercises of undergraduate courses to more challenging reductions from\nacademic papers. We compare the performance of state-of-the-art models on this\ntask and demonstrate the effect of fine-tuning with the Karp dataset on\nreasoning capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the mathematical reasoning capabilities of Large Language\nModels (LLMs) is a central topic in the study of artificial intelligence. This\nnew domain necessitates the creation of datasets of reasoning tasks for both\ntraining and benchmarking the performance of LLMs. To this end, we introduce\nthe Karp dataset: The first dataset composed of detailed proofs of\nNP-completeness reductions. The reductions vary in difficulty, ranging from\nsimple exercises of undergraduate courses to more challenging reductions from\nacademic papers. We compare the performance of state-of-the-art models on this\ntask and demonstrate the effect of fine-tuning with the Karp dataset on\nreasoning capacity."
                },
                "authors": [
                    {
                        "name": "Mason DiCicco"
                    },
                    {
                        "name": "Eamon Worden"
                    },
                    {
                        "name": "Conner Olsen"
                    },
                    {
                        "name": "Nikhil Gangaram"
                    },
                    {
                        "name": "Daniel Reichman"
                    },
                    {
                        "name": "Neil Heffernan"
                    }
                ],
                "author_detail": {
                    "name": "Neil Heffernan"
                },
                "author": "Neil Heffernan",
                "arxiv_comment": "Accepted to the 4th workshop on mathematical reasoning and AI at\n  NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14693v1",
                "updated": "2025-01-24T18:06:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    6,
                    7,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T18:06:07Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    6,
                    7,
                    4,
                    24,
                    0
                ],
                "title": "Rethinking Table Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Table Instruction Tuning"
                },
                "summary": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices and lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and reveal significant declines in both\nout-of-domain table understanding and general capabilities compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the existing table instruction-tuning works, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices and lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and reveal significant declines in both\nout-of-domain table understanding and general capabilities compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the existing table instruction-tuning works, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection."
                },
                "authors": [
                    {
                        "name": "Naihao Deng"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14683v1",
                "updated": "2025-01-24T17:59:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:59:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "An Empirical Study on LLM-based Classification of Requirements-related\n  Provisions in Food-safety Regulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on LLM-based Classification of Requirements-related\n  Provisions in Food-safety Regulations"
                },
                "summary": "As Industry 4.0 transforms the food industry, the role of software in\nachieving compliance with food-safety regulations is becoming increasingly\ncritical. Food-safety regulations, like those in many legal domains, have\nlargely been articulated in a technology-independent manner to ensure their\nlongevity and broad applicability. However, this approach leaves a gap between\nthe regulations and the modern systems and software increasingly used to\nimplement them. In this article, we pursue two main goals. First, we conduct a\nGrounded Theory study of food-safety regulations and develop a conceptual\ncharacterization of food-safety concepts that closely relate to systems and\nsoftware requirements. Second, we examine the effectiveness of two families of\nlarge language models (LLMs) -- BERT and GPT -- in automatically classifying\nlegal provisions based on requirements-related food-safety concepts. Our\nresults show that: (a) when fine-tuned, the accuracy differences between the\nbest-performing models in the BERT and GPT families are relatively small.\nNevertheless, the most powerful model in our experiments, GPT-4o, still\nachieves the highest accuracy, with an average Precision of 89% and an average\nRecall of 87%; (b) few-shot learning with GPT-4o increases Recall to 97% but\ndecreases Precision to 65%, suggesting a trade-off between fine-tuning and\nfew-shot learning; (c) despite our training examples being drawn exclusively\nfrom Canadian regulations, LLM-based classification performs consistently well\non test provisions from the US, indicating a degree of generalizability across\nregulatory jurisdictions; and (d) for our classification task, LLMs\nsignificantly outperform simpler baselines constructed using long short-term\nmemory (LSTM) networks and automatic keyword extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Industry 4.0 transforms the food industry, the role of software in\nachieving compliance with food-safety regulations is becoming increasingly\ncritical. Food-safety regulations, like those in many legal domains, have\nlargely been articulated in a technology-independent manner to ensure their\nlongevity and broad applicability. However, this approach leaves a gap between\nthe regulations and the modern systems and software increasingly used to\nimplement them. In this article, we pursue two main goals. First, we conduct a\nGrounded Theory study of food-safety regulations and develop a conceptual\ncharacterization of food-safety concepts that closely relate to systems and\nsoftware requirements. Second, we examine the effectiveness of two families of\nlarge language models (LLMs) -- BERT and GPT -- in automatically classifying\nlegal provisions based on requirements-related food-safety concepts. Our\nresults show that: (a) when fine-tuned, the accuracy differences between the\nbest-performing models in the BERT and GPT families are relatively small.\nNevertheless, the most powerful model in our experiments, GPT-4o, still\nachieves the highest accuracy, with an average Precision of 89% and an average\nRecall of 87%; (b) few-shot learning with GPT-4o increases Recall to 97% but\ndecreases Precision to 65%, suggesting a trade-off between fine-tuning and\nfew-shot learning; (c) despite our training examples being drawn exclusively\nfrom Canadian regulations, LLM-based classification performs consistently well\non test provisions from the US, indicating a degree of generalizability across\nregulatory jurisdictions; and (d) for our classification task, LLMs\nsignificantly outperform simpler baselines constructed using long short-term\nmemory (LSTM) networks and automatic keyword extraction."
                },
                "authors": [
                    {
                        "name": "Shabnam Hassani"
                    },
                    {
                        "name": "Mehrdad Sabetzadeh"
                    },
                    {
                        "name": "Daniel Amyot"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Amyot"
                },
                "author": "Daniel Amyot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14654v1",
                "updated": "2025-01-24T17:21:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    21,
                    1,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:21:01Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    21,
                    1,
                    4,
                    24,
                    0
                ],
                "title": "MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical\n  Applications"
                },
                "summary": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 100 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(GPT-4o) achieves a success rate of 72%. However, there is still substantial\nspace for improvement to give the community a next direction to optimize.\nFurthermore, there is significant variation in performance across task\ncategories. MedAgentBench establishes this and is publicly available at\nhttps://github.com/stanfordmlgroup/MedAgentBench , offering a valuable\nframework for model developers to track progress and drive continuous\nimprovements in the agent capabilities of large language models within the\nmedical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 100 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(GPT-4o) achieves a success rate of 72%. However, there is still substantial\nspace for improvement to give the community a next direction to optimize.\nFurthermore, there is significant variation in performance across task\ncategories. MedAgentBench establishes this and is publicly available at\nhttps://github.com/stanfordmlgroup/MedAgentBench , offering a valuable\nframework for model developers to track progress and drive continuous\nimprovements in the agent capabilities of large language models within the\nmedical domain."
                },
                "authors": [
                    {
                        "name": "Yixing Jiang"
                    },
                    {
                        "name": "Kameron C. Black"
                    },
                    {
                        "name": "Gloria Geng"
                    },
                    {
                        "name": "Danny Park"
                    },
                    {
                        "name": "Andrew Y. Ng"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H. Chen"
                },
                "author": "Jonathan H. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14649v1",
                "updated": "2025-01-24T17:15:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    15,
                    9,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T17:15:09Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    15,
                    9,
                    4,
                    24,
                    0
                ],
                "title": "Investigating the (De)Composition Capabilities of Large Language Models\n  in Natural-to-Formal Language Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the (De)Composition Capabilities of Large Language Models\n  in Natural-to-Formal Language Conversion"
                },
                "summary": "To achieve generalized and robust natural-to-formal language conversion\n(N2F), large language models (LLMs) need to have strong capabilities of\ndecomposition and composition in N2F when faced with an unfamiliar formal\nlanguage and be able to cope with compositional gaps and counter-intuitive\nsymbolic names. To investigate whether LLMs have this set of basic capabilities\nin N2F, we propose the DEDC framework. This framework semi-automatically\nperforms sample and task construction, allowing decoupled evaluation of the set\nof decomposition and composition capabilities of LLMs in N2F. Based on this\nframework, we evaluate and analyze the most advanced LLMs, and the main\nfindings include that: (1) the LLMs are deficient in both decomposition and\ncomposition; (2) the LLMs show a wide coverage of error types that can be\nattributed to deficiencies in natural language understanding and the learning\nand use of symbolic systems; (3) compositional gaps and counter-intuitive\nsymbolic names both affect the decomposition and composition of the LLMs. Our\nwork provides a new perspective for investigating the basic capabilities of\ndecomposition and composition of LLMs in N2F. The detailed analysis of\ndeficiencies and attributions can help subsequent improvements of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve generalized and robust natural-to-formal language conversion\n(N2F), large language models (LLMs) need to have strong capabilities of\ndecomposition and composition in N2F when faced with an unfamiliar formal\nlanguage and be able to cope with compositional gaps and counter-intuitive\nsymbolic names. To investigate whether LLMs have this set of basic capabilities\nin N2F, we propose the DEDC framework. This framework semi-automatically\nperforms sample and task construction, allowing decoupled evaluation of the set\nof decomposition and composition capabilities of LLMs in N2F. Based on this\nframework, we evaluate and analyze the most advanced LLMs, and the main\nfindings include that: (1) the LLMs are deficient in both decomposition and\ncomposition; (2) the LLMs show a wide coverage of error types that can be\nattributed to deficiencies in natural language understanding and the learning\nand use of symbolic systems; (3) compositional gaps and counter-intuitive\nsymbolic names both affect the decomposition and composition of the LLMs. Our\nwork provides a new perspective for investigating the basic capabilities of\ndecomposition and composition of LLMs in N2F. The detailed analysis of\ndeficiencies and attributions can help subsequent improvements of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Houfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Houfeng Wang"
                },
                "author": "Houfeng Wang",
                "arxiv_comment": "Accepted at NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14630v1",
                "updated": "2025-01-24T16:49:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    49,
                    8,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:49:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    49,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Problem Structure with LLMs for Optimized SAT Local Search"
                },
                "summary": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems."
                },
                "authors": [
                    {
                        "name": "André Schilder"
                    },
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15809v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15809v4",
                "updated": "2025-01-24T16:45:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    45,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-22T10:25:55Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    10,
                    25,
                    55,
                    5,
                    174,
                    0
                ],
                "title": "LaMSUM: Amplifying Voices Against Harassment through LLM Guided\n  Extractive Summarization of User Incident Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMSUM: Amplifying Voices Against Harassment through LLM Guided\n  Extractive Summarization of User Incident Reports"
                },
                "summary": "Citizen reporting platforms like Safe City in India help the public and\nauthorities stay informed about sexual harassment incidents. However, the high\nvolume of data shared on these platforms makes reviewing each individual case\nchallenging. Therefore, a summarization algorithm capable of processing and\nunderstanding various Indian code-mixed languages is essential. In recent\nyears, Large Language Models (LLMs) have shown exceptional performance in NLP\ntasks, including summarization. LLMs inherently produce abstractive summaries\nby paraphrasing the original text, while the generation of extractive summaries\n- selecting specific subsets from the original text - through LLMs remains\nlargely unexplored. Moreover, LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle these\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries for large collections of Safe City posts using\nLLMs. LaMSUM integrates summarization with different voting methods to achieve\nrobust summaries. Extensive evaluation using three popular LLMs (Llama, Mistral\nand GPT-4o) demonstrates that LaMSUM outperforms state-of-the-art extractive\nsummarization methods for Safe City posts. Overall, this work represents one of\nthe first attempts to achieve extractive summarization through LLMs, and is\nlikely to support stakeholders by offering a comprehensive overview and\nenabling them to develop effective policies to minimize incidents of\nunwarranted harassment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citizen reporting platforms like Safe City in India help the public and\nauthorities stay informed about sexual harassment incidents. However, the high\nvolume of data shared on these platforms makes reviewing each individual case\nchallenging. Therefore, a summarization algorithm capable of processing and\nunderstanding various Indian code-mixed languages is essential. In recent\nyears, Large Language Models (LLMs) have shown exceptional performance in NLP\ntasks, including summarization. LLMs inherently produce abstractive summaries\nby paraphrasing the original text, while the generation of extractive summaries\n- selecting specific subsets from the original text - through LLMs remains\nlargely unexplored. Moreover, LLMs have a limited context window size,\nrestricting the amount of data that can be processed at once. We tackle these\nchallenge by introducing LaMSUM, a novel multi-level framework designed to\ngenerate extractive summaries for large collections of Safe City posts using\nLLMs. LaMSUM integrates summarization with different voting methods to achieve\nrobust summaries. Extensive evaluation using three popular LLMs (Llama, Mistral\nand GPT-4o) demonstrates that LaMSUM outperforms state-of-the-art extractive\nsummarization methods for Safe City posts. Overall, this work represents one of\nthe first attempts to achieve extractive summarization through LLMs, and is\nlikely to support stakeholders by offering a comprehensive overview and\nenabling them to develop effective policies to minimize incidents of\nunwarranted harassment."
                },
                "authors": [
                    {
                        "name": "Garima Chhikara"
                    },
                    {
                        "name": "Anurag Sharma"
                    },
                    {
                        "name": "V. Gurucharan"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Abhijnan Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Abhijnan Chakraborty"
                },
                "author": "Abhijnan Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15809v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15809v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14625v1",
                "updated": "2025-01-24T16:42:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    42,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T16:42:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    42,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Preference Elicitation with LLM-Based Proxies"
                },
                "summary": "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms."
                },
                "authors": [
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Francisco Marmolejo-Cossío"
                    },
                    {
                        "name": "Edwin Lock"
                    },
                    {
                        "name": "David Parkes"
                    }
                ],
                "author_detail": {
                    "name": "David Parkes"
                },
                "author": "David Parkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10321v2",
                "updated": "2025-01-24T16:37:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    37,
                    57,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-17T17:51:22Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    51,
                    22,
                    4,
                    17,
                    0
                ],
                "title": "Towards Human-Guided, Data-Centric LLM Co-Pilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Guided, Data-Centric LLM Co-Pilots"
                },
                "summary": "Machine learning (ML) has the potential to revolutionize various domains, but\nits adoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has the potential to revolutionize various domains, but\nits adoption is often hindered by the disconnect between the needs of domain\nexperts and translating these needs into robust and valid ML tools. Despite\nrecent advances in LLM-based co-pilots to democratize ML for non-technical\ndomain experts, these systems remain predominantly focused on model-centric\naspects while overlooking critical data-centric challenges. This limitation is\nproblematic in complex real-world settings where raw data often contains\ncomplex issues, such as missing values, label noise, and domain-specific\nnuances requiring tailored handling. To address this we introduce CliMB-DC, a\nhuman-guided, data-centric framework for LLM co-pilots that combines advanced\ndata-centric tools with LLM-driven reasoning to enable robust, context-aware\ndata processing. At its core, CliMB-DC introduces a novel, multi-agent\nreasoning system that combines a strategic coordinator for dynamic planning and\nadaptation with a specialized worker agent for precise execution. Domain\nexpertise is then systematically incorporated to guide the reasoning process\nusing a human-in-the-loop approach. To guide development, we formalize a\ntaxonomy of key data-centric challenges that co-pilots must address.\nThereafter, to address the dimensions of the taxonomy, we integrate\nstate-of-the-art data-centric tools into an extensible, open-source\narchitecture, facilitating the addition of new tools from the research\ncommunity. Empirically, using real-world healthcare datasets we demonstrate\nCliMB-DC's ability to transform uncurated datasets into ML-ready formats,\nsignificantly outperforming existing co-pilot baselines for handling\ndata-centric challenges. CliMB-DC promises to empower domain experts from\ndiverse domains -- healthcare, finance, social sciences and more -- to actively\nparticipate in driving real-world impact using ML."
                },
                "authors": [
                    {
                        "name": "Evgeny Saveliev"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Anders Boyd"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "Saveliev, Liu & Seedat contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14117v3",
                "updated": "2025-01-24T15:50:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    50,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-20T09:03:18Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    9,
                    3,
                    18,
                    3,
                    172,
                    0
                ],
                "title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers"
                },
                "summary": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered."
                },
                "authors": [
                    {
                        "name": "Shuoqi Sun"
                    },
                    {
                        "name": "Shengyao Zhuang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12883v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12883v3",
                "updated": "2025-01-24T15:27:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    27,
                    44,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T13:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program"
                },
                "summary": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
                },
                "authors": [
                    {
                        "name": "Carlton Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Carlton Shepherd"
                },
                "author": "Carlton Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12883v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12883v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10642v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10642v3",
                "updated": "2025-01-24T15:21:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    21,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2024-04-16T15:16:22Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    15,
                    16,
                    22,
                    1,
                    107,
                    0
                ],
                "title": "Self-playing Adversarial Language Game Enhances LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-playing Adversarial Language Game Enhances LLM Reasoning"
                },
                "summary": "We explore the potential of self-play training for large language models\n(LLMs) in a two-player adversarial language game called Adversarial Taboo. In\nthis game, an attacker and a defender communicate around a target word only\nvisible to the attacker. The attacker aims to induce the defender to speak the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players must have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Playing this Adversarial language Game (SPAG). With this goal, we select\nseveral open-source LLMs and let each act as the attacker and play with a copy\nof itself as the defender on an extensive range of target words. Through\nreinforcement learning on the game outcomes, we observe that the LLMs'\nperformances uniformly improve on a broad range of reasoning benchmarks.\nFurthermore, iteratively adopting this self-play process can continuously\npromote LLMs' reasoning abilities. The code is available at\nhttps://github.com/Linear95/SPAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the potential of self-play training for large language models\n(LLMs) in a two-player adversarial language game called Adversarial Taboo. In\nthis game, an attacker and a defender communicate around a target word only\nvisible to the attacker. The attacker aims to induce the defender to speak the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players must have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Playing this Adversarial language Game (SPAG). With this goal, we select\nseveral open-source LLMs and let each act as the attacker and play with a copy\nof itself as the defender on an extensive range of target words. Through\nreinforcement learning on the game outcomes, we observe that the LLMs'\nperformances uniformly improve on a broad range of reasoning benchmarks.\nFurthermore, iteratively adopting this self-play process can continuously\npromote LLMs' reasoning abilities. The code is available at\nhttps://github.com/Linear95/SPAG."
                },
                "authors": [
                    {
                        "name": "Pengyu Cheng"
                    },
                    {
                        "name": "Tianhao Hu"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Yong Dai"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Xiaolong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Li"
                },
                "author": "Xiaolong Li",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10642v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10642v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14566v1",
                "updated": "2025-01-24T15:19:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    19,
                    4,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T15:19:04Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    19,
                    4,
                    4,
                    24,
                    0
                ],
                "title": "Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal\n  Prediction"
                },
                "summary": "Modern software-defined networks, such as Open Radio Access Network (O-RAN)\nsystems, rely on artificial intelligence (AI)-powered applications running on\ncontrollers interfaced with the radio access network. To ensure that these AI\napplications operate reliably at runtime, they must be properly calibrated\nbefore deployment. A promising and theoretically grounded approach to\ncalibration is conformal prediction (CP), which enhances any AI model by\ntransforming it into a provably reliable set predictor that provides error bars\nfor estimates and decisions. CP requires calibration data that matches the\ndistribution of the environment encountered during runtime. However, in\npractical scenarios, network controllers often have access only to data\ncollected under different contexts -- such as varying traffic patterns and\nnetwork conditions -- leading to a mismatch between the calibration and runtime\ndistributions. This paper introduces a novel methodology to address this\ncalibration-test distribution shift. The approach leverages meta-learning to\ndevelop a zero-shot estimator of distribution shifts, relying solely on\ncontextual information. The proposed method, called meta-learned\ncontext-dependent weighted conformal prediction (ML-WCP), enables effective\ncalibration of AI applications without requiring data from the current context.\nAdditionally, it can incorporate data from multiple contexts to further enhance\ncalibration reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software-defined networks, such as Open Radio Access Network (O-RAN)\nsystems, rely on artificial intelligence (AI)-powered applications running on\ncontrollers interfaced with the radio access network. To ensure that these AI\napplications operate reliably at runtime, they must be properly calibrated\nbefore deployment. A promising and theoretically grounded approach to\ncalibration is conformal prediction (CP), which enhances any AI model by\ntransforming it into a provably reliable set predictor that provides error bars\nfor estimates and decisions. CP requires calibration data that matches the\ndistribution of the environment encountered during runtime. However, in\npractical scenarios, network controllers often have access only to data\ncollected under different contexts -- such as varying traffic patterns and\nnetwork conditions -- leading to a mismatch between the calibration and runtime\ndistributions. This paper introduces a novel methodology to address this\ncalibration-test distribution shift. The approach leverages meta-learning to\ndevelop a zero-shot estimator of distribution shifts, relying solely on\ncontextual information. The proposed method, called meta-learned\ncontext-dependent weighted conformal prediction (ML-WCP), enables effective\ncalibration of AI applications without requiring data from the current context.\nAdditionally, it can incorporate data from multiple contexts to further enhance\ncalibration reliability."
                },
                "authors": [
                    {
                        "name": "Seonghoon Yoo"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Joonhyuk Kang"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14546v1",
                "updated": "2025-01-24T14:49:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    49,
                    0,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:49:00Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    49,
                    0,
                    4,
                    24,
                    0
                ],
                "title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite\n  Images by Poverty Level: Advancing Tools for Social Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite\n  Images by Poverty Level: Advancing Tools for Social Science Research"
                },
                "summary": "This paper investigates the novel application of Large Language Models (LLMs)\nwith vision capabilities to analyze satellite imagery for village-level poverty\nprediction. Although LLMs were originally designed for natural language\nunderstanding, their adaptability to multimodal tasks, including geospatial\nanalysis, has opened new frontiers in data-driven research. By leveraging\nadvancements in vision-enabled LLMs, we assess their ability to provide\ninterpretable, scalable, and reliable insights into human poverty from\nsatellite images. Using a pairwise comparison approach, we demonstrate that\nChatGPT can rank satellite images based on poverty levels with accuracy\ncomparable to domain experts. These findings highlight both the promise and the\nlimitations of LLMs in socioeconomic research, providing a foundation for their\nintegration into poverty assessment workflows. This study contributes to the\nongoing exploration of unconventional data sources for welfare analysis and\nopens pathways for cost-effective, large-scale poverty monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the novel application of Large Language Models (LLMs)\nwith vision capabilities to analyze satellite imagery for village-level poverty\nprediction. Although LLMs were originally designed for natural language\nunderstanding, their adaptability to multimodal tasks, including geospatial\nanalysis, has opened new frontiers in data-driven research. By leveraging\nadvancements in vision-enabled LLMs, we assess their ability to provide\ninterpretable, scalable, and reliable insights into human poverty from\nsatellite images. Using a pairwise comparison approach, we demonstrate that\nChatGPT can rank satellite images based on poverty levels with accuracy\ncomparable to domain experts. These findings highlight both the promise and the\nlimitations of LLMs in socioeconomic research, providing a foundation for their\nintegration into poverty assessment workflows. This study contributes to the\nongoing exploration of unconventional data sources for welfare analysis and\nopens pathways for cost-effective, large-scale poverty monitoring."
                },
                "authors": [
                    {
                        "name": "Hamid Sarmadi"
                    },
                    {
                        "name": "Ola Hall"
                    },
                    {
                        "name": "Thorsteinn Rögnvaldsson"
                    },
                    {
                        "name": "Mattias Ohlsson"
                    }
                ],
                "author_detail": {
                    "name": "Mattias Ohlsson"
                },
                "author": "Mattias Ohlsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14540v1",
                "updated": "2025-01-24T14:45:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    45,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:45:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    45,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning"
                },
                "summary": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems"
                },
                "authors": [
                    {
                        "name": "Benjamin Callewaert"
                    },
                    {
                        "name": "Simon Vandevelde"
                    },
                    {
                        "name": "Joost Vennekens"
                    }
                ],
                "author_detail": {
                    "name": "Joost Vennekens"
                },
                "author": "Joost Vennekens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14520v1",
                "updated": "2025-01-24T14:23:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    23,
                    31,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:23:31Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    23,
                    31,
                    4,
                    24,
                    0
                ],
                "title": "Scene Understanding Enabled Semantic Communication with Open Channel\n  Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene Understanding Enabled Semantic Communication with Open Channel\n  Coding"
                },
                "summary": "As communication systems transition from symbol transmission to conveying\nmeaningful information, sixth-generation (6G) networks emphasize semantic\ncommunication. This approach prioritizes high-level semantic information,\nimproving robustness and reducing redundancy across modalities like text,\nspeech, and images. However, traditional semantic communication faces\nlimitations, including static coding strategies, poor generalization, and\nreliance on task-specific knowledge bases that hinder adaptability. To overcome\nthese challenges, we propose a novel system combining scene understanding,\nLarge Language Models (LLMs), and open channel coding, named \\textbf{OpenSC}.\nTraditional systems rely on fixed domain-specific knowledge bases, limiting\ntheir ability to generalize. Our open channel coding approach leverages shared,\npublicly available knowledge, enabling flexible, adaptive encoding. This\ndynamic system reduces reliance on static task-specific data, enhancing\nadaptability across diverse tasks and environments. Additionally, we use scene\ngraphs for structured semantic encoding, capturing object relationships and\ncontext to improve tasks like Visual Question Answering (VQA). Our approach\nselectively encodes key semantic elements, minimizing redundancy and improving\ntransmission efficiency. Experimental results show significant improvements in\nboth semantic understanding and efficiency, advancing the potential of\nadaptive, generalizable semantic communication in 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As communication systems transition from symbol transmission to conveying\nmeaningful information, sixth-generation (6G) networks emphasize semantic\ncommunication. This approach prioritizes high-level semantic information,\nimproving robustness and reducing redundancy across modalities like text,\nspeech, and images. However, traditional semantic communication faces\nlimitations, including static coding strategies, poor generalization, and\nreliance on task-specific knowledge bases that hinder adaptability. To overcome\nthese challenges, we propose a novel system combining scene understanding,\nLarge Language Models (LLMs), and open channel coding, named \\textbf{OpenSC}.\nTraditional systems rely on fixed domain-specific knowledge bases, limiting\ntheir ability to generalize. Our open channel coding approach leverages shared,\npublicly available knowledge, enabling flexible, adaptive encoding. This\ndynamic system reduces reliance on static task-specific data, enhancing\nadaptability across diverse tasks and environments. Additionally, we use scene\ngraphs for structured semantic encoding, capturing object relationships and\ncontext to improve tasks like Visual Question Answering (VQA). Our approach\nselectively encodes key semantic elements, minimizing redundancy and improving\ntransmission efficiency. Experimental results show significant improvements in\nboth semantic understanding and efficiency, advancing the potential of\nadaptive, generalizable semantic communication in 6G networks."
                },
                "authors": [
                    {
                        "name": "Zhe Xiang"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Quan Deng"
                    },
                    {
                        "name": "Yuandi Li"
                    },
                    {
                        "name": "Zhiguo Wan"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguo Wan"
                },
                "author": "Zhiguo Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14512v1",
                "updated": "2025-01-24T14:15:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    15,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T14:15:51Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    15,
                    51,
                    4,
                    24,
                    0
                ],
                "title": "Real-world Edge Neural Network Implementations Leak Private Interactions\n  Through Physical Side Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world Edge Neural Network Implementations Leak Private Interactions\n  Through Physical Side Channel"
                },
                "summary": "Neural networks have become a fundamental component of numerous practical\napplications, and their implementations, which are often accelerated by\nhardware, are integrated into all types of real-world physical devices. User\ninteractions with neural networks on hardware accelerators are commonly\nconsidered privacy-sensitive. Substantial efforts have been made to uncover\nvulnerabilities and enhance privacy protection at the level of machine learning\nalgorithms, including membership inference attacks, differential privacy, and\nfederated learning. However, neural networks are ultimately implemented and\ndeployed on physical devices, and current research pays comparatively less\nattention to privacy protection at the implementation level. In this paper, we\nintroduce a generic physical side-channel attack, ScaAR, that extracts user\ninteractions with neural networks by leveraging electromagnetic (EM) emissions\nof physical devices. Our proposed attack is implementation-agnostic, meaning it\ndoes not require the adversary to possess detailed knowledge of the hardware or\nsoftware implementations, thanks to the capabilities of deep learning-based\nside-channel analysis (DLSCA). Experimental results demonstrate that, through\nthe EM side channel, ScaAR can effectively extract the class label of user\ninteractions with neural classifiers, including inputs and outputs, on the\nAMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first\ntime, we provide side-channel analysis on edge Large Language Model (LLM)\nimplementations on the Raspberry Pi 5, showing that EM side channel leaks\ninteraction data, and different LLM tokens can be distinguishable from the EM\ntraces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have become a fundamental component of numerous practical\napplications, and their implementations, which are often accelerated by\nhardware, are integrated into all types of real-world physical devices. User\ninteractions with neural networks on hardware accelerators are commonly\nconsidered privacy-sensitive. Substantial efforts have been made to uncover\nvulnerabilities and enhance privacy protection at the level of machine learning\nalgorithms, including membership inference attacks, differential privacy, and\nfederated learning. However, neural networks are ultimately implemented and\ndeployed on physical devices, and current research pays comparatively less\nattention to privacy protection at the implementation level. In this paper, we\nintroduce a generic physical side-channel attack, ScaAR, that extracts user\ninteractions with neural networks by leveraging electromagnetic (EM) emissions\nof physical devices. Our proposed attack is implementation-agnostic, meaning it\ndoes not require the adversary to possess detailed knowledge of the hardware or\nsoftware implementations, thanks to the capabilities of deep learning-based\nside-channel analysis (DLSCA). Experimental results demonstrate that, through\nthe EM side channel, ScaAR can effectively extract the class label of user\ninteractions with neural classifiers, including inputs and outputs, on the\nAMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first\ntime, we provide side-channel analysis on edge Large Language Model (LLM)\nimplementations on the Raspberry Pi 5, showing that EM side channel leaks\ninteraction data, and different LLM tokens can be distinguishable from the EM\ntraces."
                },
                "authors": [
                    {
                        "name": "Zhuoran Liu"
                    },
                    {
                        "name": "Senna van Hoek"
                    },
                    {
                        "name": "Péter Horváth"
                    },
                    {
                        "name": "Dirk Lauret"
                    },
                    {
                        "name": "Xiaoyun Xu"
                    },
                    {
                        "name": "Lejla Batina"
                    }
                ],
                "author_detail": {
                    "name": "Lejla Batina"
                },
                "author": "Lejla Batina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14499v1",
                "updated": "2025-01-24T13:59:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:59:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    59,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "Automated Assignment Grading with Large Language Models: Insights From a\n  Bioinformatics Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Assignment Grading with Large Language Models: Insights From a\n  Bioinformatics Course"
                },
                "summary": "Providing students with individualized feedback through assignments is a\ncornerstone of education that supports their learning and development. Studies\nhave shown that timely, high-quality feedback plays a critical role in\nimproving learning outcomes. However, providing personalized feedback on a\nlarge scale in classes with large numbers of students is often impractical due\nto the significant time and effort required. Recent advances in natural\nlanguage processing and large language models (LLMs) offer a promising solution\nby enabling the efficient delivery of personalized feedback. These technologies\ncan reduce the workload of course staff while improving student satisfaction\nand learning outcomes. Their successful implementation, however, requires\nthorough evaluation and validation in real classrooms. We present the results\nof a practical evaluation of LLM-based graders for written assignments in the\n2024/25 iteration of the Introduction to Bioinformatics course at the\nUniversity of Ljubljana. Over the course of the semester, more than 100\nstudents answered 36 text-based questions, most of which were automatically\ngraded using LLMs. In a blind study, students received feedback from both LLMs\nand human teaching assistants without knowing the source, and later rated the\nquality of the feedback. We conducted a systematic evaluation of six commercial\nand open-source LLMs and compared their grading performance with human teaching\nassistants. Our results show that with well-designed prompts, LLMs can achieve\ngrading accuracy and feedback quality comparable to human graders. Our results\nalso suggest that open-source LLMs perform as well as commercial LLMs, allowing\nschools to implement their own grading systems while maintaining privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing students with individualized feedback through assignments is a\ncornerstone of education that supports their learning and development. Studies\nhave shown that timely, high-quality feedback plays a critical role in\nimproving learning outcomes. However, providing personalized feedback on a\nlarge scale in classes with large numbers of students is often impractical due\nto the significant time and effort required. Recent advances in natural\nlanguage processing and large language models (LLMs) offer a promising solution\nby enabling the efficient delivery of personalized feedback. These technologies\ncan reduce the workload of course staff while improving student satisfaction\nand learning outcomes. Their successful implementation, however, requires\nthorough evaluation and validation in real classrooms. We present the results\nof a practical evaluation of LLM-based graders for written assignments in the\n2024/25 iteration of the Introduction to Bioinformatics course at the\nUniversity of Ljubljana. Over the course of the semester, more than 100\nstudents answered 36 text-based questions, most of which were automatically\ngraded using LLMs. In a blind study, students received feedback from both LLMs\nand human teaching assistants without knowing the source, and later rated the\nquality of the feedback. We conducted a systematic evaluation of six commercial\nand open-source LLMs and compared their grading performance with human teaching\nassistants. Our results show that with well-designed prompts, LLMs can achieve\ngrading accuracy and feedback quality comparable to human graders. Our results\nalso suggest that open-source LLMs perform as well as commercial LLMs, allowing\nschools to implement their own grading systems while maintaining privacy."
                },
                "authors": [
                    {
                        "name": "Pavlin G. Poličar"
                    },
                    {
                        "name": "Martin Špendl"
                    },
                    {
                        "name": "Tomaž Curk"
                    },
                    {
                        "name": "Blaž Zupan"
                    }
                ],
                "author_detail": {
                    "name": "Blaž Zupan"
                },
                "author": "Blaž Zupan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14809v3",
                "updated": "2025-01-24T13:54:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    54,
                    5,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-19T12:57:47Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    57,
                    47,
                    3,
                    354,
                    0
                ],
                "title": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResoFilter: Fine-grained Synthetic Data Filtering for Large Language\n  Models through Data-Parameter Resonance Analysis"
                },
                "summary": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable effectiveness across\nvarious domains, with data augmentation methods utilizing GPT for synthetic\ndata generation becoming prevalent. However, the quality and utility of\naugmented data remain questionable, and current methods lack clear metrics for\nevaluating data characteristics. To address these challenges, we propose\nResoFilter, a novel method that integrates models, data, and tasks to refine\ndatasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter\nfeatures for data selection, offering improved interpretability by representing\ndata characteristics through model weights. Our experiments demonstrate that\nResoFilter achieves comparable results to full-scale fine-tuning using only\nhalf the data in mathematical tasks and exhibits strong generalization across\ndifferent models and domains. This method provides valuable insights for\nconstructing synthetic datasets and evaluating high-quality data, offering a\npromising solution for enhancing data augmentation techniques and improving\ntraining dataset quality for LLMs. For reproducibility, we will release our\ncode and data upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zeao Tu"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Zihan Yao"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "Accepted by NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14497v1",
                "updated": "2025-01-24T13:53:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    53,
                    54,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:53:54Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    53,
                    54,
                    4,
                    24,
                    0
                ],
                "title": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Wanqiu Long"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Victor Gutierrez Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14492v1",
                "updated": "2025-01-24T13:48:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    48,
                    10,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:48:10Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    48,
                    10,
                    4,
                    24,
                    0
                ],
                "title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model\n  Critiques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model\n  Critiques"
                },
                "summary": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback for\nothers by identifying flaws and suggesting improvements. However, evaluating\nthe critique capabilities of LLMs presents a significant challenge due to the\nopen-ended nature of the task. In this work, we introduce a new benchmark\ndesigned to assess the critique capabilities of LLMs. Unlike existing\nbenchmarks, which typically function in an open-loop fashion, our approach\nemploys a closed-loop methodology that evaluates the quality of corrections\ngenerated from critiques. Moreover, the benchmark incorporates features such as\nself-critique, cross-critique, and iterative critique, which are crucial for\ndistinguishing the abilities of advanced reasoning models from more classical\nones. We implement this benchmark using eight challenging reasoning tasks. We\nhave several interesting findings. First, despite demonstrating comparable\nperformance in direct chain-of-thought generation, classical LLMs significantly\nlag behind the advanced reasoning-based model o1-mini across all critique\nscenarios. Second, in self-critique and iterative critique settings, classical\nLLMs may even underperform relative to their baseline capabilities. We hope\nthat this benchmark will serve as a valuable resource to guide future\nadvancements. The code and data are available at\n\\url{https://github.com/tangzhy/RealCritic}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback for\nothers by identifying flaws and suggesting improvements. However, evaluating\nthe critique capabilities of LLMs presents a significant challenge due to the\nopen-ended nature of the task. In this work, we introduce a new benchmark\ndesigned to assess the critique capabilities of LLMs. Unlike existing\nbenchmarks, which typically function in an open-loop fashion, our approach\nemploys a closed-loop methodology that evaluates the quality of corrections\ngenerated from critiques. Moreover, the benchmark incorporates features such as\nself-critique, cross-critique, and iterative critique, which are crucial for\ndistinguishing the abilities of advanced reasoning models from more classical\nones. We implement this benchmark using eight challenging reasoning tasks. We\nhave several interesting findings. First, despite demonstrating comparable\nperformance in direct chain-of-thought generation, classical LLMs significantly\nlag behind the advanced reasoning-based model o1-mini across all critique\nscenarios. Second, in self-critique and iterative critique settings, classical\nLLMs may even underperform relative to their baseline capabilities. We hope\nthat this benchmark will serve as a valuable resource to guide future\nadvancements. The code and data are available at\n\\url{https://github.com/tangzhy/RealCritic}."
                },
                "authors": [
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Zhenyang Xiao"
                    },
                    {
                        "name": "Tian Ding"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14484v1",
                "updated": "2025-01-24T13:37:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    37,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T13:37:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    37,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility"
                },
                "summary": "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06605v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06605v3",
                "updated": "2025-01-24T13:29:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    29,
                    33,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-11T18:11:07Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    18,
                    11,
                    7,
                    5,
                    11,
                    0
                ],
                "title": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon\n  Robotic Manipulation"
                },
                "summary": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient control in long-horizon robotic manipulation is challenging due to\ncomplex representation and policy learning requirements. Model-based visual\nreinforcement learning (RL) has shown great potential in addressing these\nchallenges but still faces notable limitations, particularly in handling sparse\nrewards and complex visual features in long-horizon environments. To address\nthese limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for\nlong-horizon tasks and further introduce RoboHorizon, an LLM-assisted\nmulti-view world model tailored for long-horizon robotic manipulation. In\nRoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage\nsub-tasks based on task language instructions, enabling robots to better\nrecognize long-horizon tasks. Keyframe discovery is then integrated into the\nmulti-view masked autoencoder (MAE) architecture to enhance the robot's ability\nto sense critical task sequences, strengthening its multi-stage perception of\nlong-horizon processes. Leveraging these dense rewards and multi-view\nrepresentations, a robotic world model is constructed to efficiently plan\nlong-horizon tasks, enabling the robot to reliably act through RL algorithms.\nExperiments on two representative benchmarks, RLBench and FurnitureBench, show\nthat RoboHorizon outperforms state-of-the-art visual model-based RL methods,\nachieving a 23.35% improvement in task success rates on RLBench's 4\nshort-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from\nRLBench and 3 furniture assembly tasks from FurnitureBench."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06605v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06605v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04772v2",
                "updated": "2025-01-24T13:10:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    10,
                    15,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-07T09:17:33Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    9,
                    17,
                    33,
                    4,
                    159,
                    0
                ],
                "title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning"
                },
                "summary": "Recent rehearsal-free methods, guided by prompts, generally excel in\nvision-related continual learning (CL) scenarios with continuously drifting\ndata. To be deployable on real-world devices, these methods must contain high\nresource efficiency during training. In this paper, we introduce\nResource-Efficient Prompting (REP), which targets improving the resource\nefficiency of prompt-based rehearsal-free methods. Our key focus is on avoiding\ncatastrophic trade-offs with accuracy while trimming computational and memory\ncosts during prompt learning. We achieve this by exploiting swift prompt\nselection that enhances input data using a carefully provisioned model, and by\ndeveloping adaptive token merging (AToM) and layer dropping (ALD) algorithms\nfor the prompt updating stage. AToM and ALD perform selective skipping across\nthe data and model dimensions without compromising task-specific features while\nlearning new tasks. We validate REP's superior resource efficiency over current\nstate-of-the-art ViT- and CNN-based methods through extensive experiments on\nthree image classification datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent rehearsal-free methods, guided by prompts, generally excel in\nvision-related continual learning (CL) scenarios with continuously drifting\ndata. To be deployable on real-world devices, these methods must contain high\nresource efficiency during training. In this paper, we introduce\nResource-Efficient Prompting (REP), which targets improving the resource\nefficiency of prompt-based rehearsal-free methods. Our key focus is on avoiding\ncatastrophic trade-offs with accuracy while trimming computational and memory\ncosts during prompt learning. We achieve this by exploiting swift prompt\nselection that enhances input data using a carefully provisioned model, and by\ndeveloping adaptive token merging (AToM) and layer dropping (ALD) algorithms\nfor the prompt updating stage. AToM and ALD perform selective skipping across\nthe data and model dimensions without compromising task-specific features while\nlearning new tasks. We validate REP's superior resource efficiency over current\nstate-of-the-art ViT- and CNN-based methods through extensive experiments on\nthree image classification datasets."
                },
                "authors": [
                    {
                        "name": "Sungho Jeon"
                    },
                    {
                        "name": "Xinyue Ma"
                    },
                    {
                        "name": "Kwang In Kim"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Myeongjae Jeon"
                },
                "author": "Myeongjae Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14465v1",
                "updated": "2025-01-24T12:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    54,
                    19,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    54,
                    19,
                    4,
                    24,
                    0
                ],
                "title": "Boundary Value Test Input Generation Using Prompt Engineering with LLMs:\n  Fault Detection and Coverage Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boundary Value Test Input Generation Using Prompt Engineering with LLMs:\n  Fault Detection and Coverage Analysis"
                },
                "summary": "As software systems grow more complex, automated testing has become essential\nto ensuring reliability and performance. Traditional methods for boundary value\ntest input generation can be time-consuming and may struggle to address all\npotential error cases effectively, especially in systems with intricate or\nhighly variable boundaries. This paper presents a framework for assessing the\neffectiveness of large language models (LLMs) in generating boundary value test\ninputs for white-box software testing by examining their potential through\nprompt engineering. Specifically, we evaluate the effectiveness of LLM-based\ntest input generation by analyzing fault detection rates and test coverage,\ncomparing these LLM-generated test sets with those produced using traditional\nboundary value analysis methods. Our analysis shows the strengths and\nlimitations of LLMs in boundary value generation, particularly in detecting\ncommon boundary-related issues. However, they still face challenges in certain\nareas, especially when handling complex or less common test inputs. This\nresearch provides insights into the role of LLMs in boundary value testing,\nunderscoring both their potential and areas for improvement in automated\ntesting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software systems grow more complex, automated testing has become essential\nto ensuring reliability and performance. Traditional methods for boundary value\ntest input generation can be time-consuming and may struggle to address all\npotential error cases effectively, especially in systems with intricate or\nhighly variable boundaries. This paper presents a framework for assessing the\neffectiveness of large language models (LLMs) in generating boundary value test\ninputs for white-box software testing by examining their potential through\nprompt engineering. Specifically, we evaluate the effectiveness of LLM-based\ntest input generation by analyzing fault detection rates and test coverage,\ncomparing these LLM-generated test sets with those produced using traditional\nboundary value analysis methods. Our analysis shows the strengths and\nlimitations of LLMs in boundary value generation, particularly in detecting\ncommon boundary-related issues. However, they still face challenges in certain\nareas, especially when handling complex or less common test inputs. This\nresearch provides insights into the role of LLMs in boundary value testing,\nunderscoring both their potential and areas for improvement in automated\ntesting methods."
                },
                "authors": [
                    {
                        "name": "Xiujing Guo"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Tatsuhiro Tsuchiya"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuhiro Tsuchiya"
                },
                "author": "Tatsuhiro Tsuchiya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14457v1",
                "updated": "2025-01-24T12:41:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    41,
                    30,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:41:30Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    41,
                    30,
                    4,
                    24,
                    0
                ],
                "title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable\n  Neuron Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Gender Bias in LLMs via Interpretable\n  Neuron Editing"
                },
                "summary": "Large language models (LLMs) often exhibit gender bias, posing challenges for\ntheir safe deployment. Existing methods to mitigate bias lack a comprehensive\nunderstanding of its mechanisms or compromise the model's core capabilities. To\naddress these issues, we propose the CommonWords dataset, to systematically\nevaluate gender bias in LLMs. Our analysis reveals pervasive bias across models\nand identifies specific neuron circuits, including gender neurons and general\nneurons, responsible for this behavior. Notably, editing even a small number of\ngeneral neurons can disrupt the model's overall capabilities due to\nhierarchical neuron interactions. Based on these insights, we propose an\ninterpretable neuron editing method that combines logit-based and causal-based\nstrategies to selectively target biased neurons. Experiments on five LLMs\ndemonstrate that our method effectively reduces gender bias while preserving\nthe model's original capabilities, outperforming existing fine-tuning and\nediting approaches. Our findings contribute a novel dataset, a detailed\nanalysis of bias mechanisms, and a practical solution for mitigating gender\nbias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit gender bias, posing challenges for\ntheir safe deployment. Existing methods to mitigate bias lack a comprehensive\nunderstanding of its mechanisms or compromise the model's core capabilities. To\naddress these issues, we propose the CommonWords dataset, to systematically\nevaluate gender bias in LLMs. Our analysis reveals pervasive bias across models\nand identifies specific neuron circuits, including gender neurons and general\nneurons, responsible for this behavior. Notably, editing even a small number of\ngeneral neurons can disrupt the model's overall capabilities due to\nhierarchical neuron interactions. Based on these insights, we propose an\ninterpretable neuron editing method that combines logit-based and causal-based\nstrategies to selectively target biased neurons. Experiments on five LLMs\ndemonstrate that our method effectively reduces gender bias while preserving\nthe model's original capabilities, outperforming existing fine-tuning and\nediting approaches. Our findings contribute a novel dataset, a detailed\nanalysis of bias mechanisms, and a practical solution for mitigating gender\nbias in LLMs."
                },
                "authors": [
                    {
                        "name": "Zeping Yu"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11175v2",
                "updated": "2025-01-24T12:41:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    41,
                    6,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-17T03:28:08Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    28,
                    8,
                    0,
                    169,
                    0
                ],
                "title": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo\n  Cancellation and Noise Suppression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo\n  Cancellation and Noise Suppression"
                },
                "summary": "The proliferation of deep neural networks has spawned the rapid development\nof acoustic echo cancellation and noise suppression, and plenty of prior arts\nhave been proposed, which yield promising performance. Nevertheless, they\nrarely consider the deployment generality in different processing scenarios,\nsuch as edge devices, and cloud processing. To this end, this paper proposes a\ngeneral model, termed SMRU, to cover different application scenarios. The\nnovelty lies in two-fold. First, a multi-scale band split layer and band merge\nlayer are proposed to effectively fuse local frequency bands for lower\ncomplexity modeling. Besides, by simulating the multi-resolution feature\nmodeling characteristic of the classical UNet structure, a novel\nrecurrent-dominated UNet is devised. It consists of multiple variable frame\nrate blocks, each of which involves the causal time down-/up-sampling layer\nwith varying compression ratios and the dual-path structure for inter- and\nintra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of\nMACs, and the experimental results show that the proposed approach yields\ncompetitive or even better performance over existing baselines, and has the\nfull potential to adapt to more general scenarios with varying complexity\nrequirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of deep neural networks has spawned the rapid development\nof acoustic echo cancellation and noise suppression, and plenty of prior arts\nhave been proposed, which yield promising performance. Nevertheless, they\nrarely consider the deployment generality in different processing scenarios,\nsuch as edge devices, and cloud processing. To this end, this paper proposes a\ngeneral model, termed SMRU, to cover different application scenarios. The\nnovelty lies in two-fold. First, a multi-scale band split layer and band merge\nlayer are proposed to effectively fuse local frequency bands for lower\ncomplexity modeling. Besides, by simulating the multi-resolution feature\nmodeling characteristic of the classical UNet structure, a novel\nrecurrent-dominated UNet is devised. It consists of multiple variable frame\nrate blocks, each of which involves the causal time down-/up-sampling layer\nwith varying compression ratios and the dual-path structure for inter- and\nintra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of\nMACs, and the experimental results show that the proposed approach yields\ncompetitive or even better performance over existing baselines, and has the\nfull potential to adapt to more general scenarios with varying complexity\nrequirements."
                },
                "authors": [
                    {
                        "name": "Zhihang Sun"
                    },
                    {
                        "name": "Andong Li"
                    },
                    {
                        "name": "Rilin Chen"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Meng Yu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_doi": "10.1109/SLT61566.2024.10832279",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SLT61566.2024.10832279",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.11175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, Accepted to SLT 2024",
                "arxiv_journal_ref": "2024 IEEE Spoken Language Technology Workshop (SLT), pp. 317-324,\n  2024",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14451v1",
                "updated": "2025-01-24T12:34:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    34,
                    4,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T12:34:04Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    12,
                    34,
                    4,
                    4,
                    24,
                    0
                ],
                "title": "MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to\n  Detect Safety Violation in Autonomous Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to\n  Detect Safety Violation in Autonomous Driving Systems"
                },
                "summary": "Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety\nviolations can result in significant losses. Rigorous testing is essential\nbefore deployment, with simulation testing playing a key role. However, ADSs\nare typically complex, consisting of multiple modules such as perception and\nplanning, or well-trained end-to-end autonomous driving systems. Offline\nmethods, such as the Genetic Algorithm (GA), can only generate predefined\ntrajectories for dynamics, which struggle to cause safety violations for ADSs\nrapidly and efficiently in different scenarios due to their evolutionary\nnature. Online methods, such as single-agent reinforcement learning (RL), can\nquickly adjust the dynamics' trajectory online to adapt to different scenarios,\nbut they struggle to capture complex corner cases of ADS arising from the\nintricate interplay among multiple vehicles. Multi-agent reinforcement learning\n(MARL) has a strong ability in cooperative tasks. On the other hand, it faces\nits own challenges, particularly with convergence. This paper introduces\nMARL-OT, a scalable framework that leverages MARL to detect safety violations\nof ADS resulting from surrounding vehicles' cooperation. MARL-OT employs MARL\nfor high-level guidance, triggering various dangerous scenarios for the\nrule-based online fuzzer to explore potential safety violations of ADS, thereby\ngenerating dynamic, realistic safety violation scenarios. Our approach improves\nthe detected safety violation rate by up to 136.2% compared to the\nstate-of-the-art (SOTA) testing technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety\nviolations can result in significant losses. Rigorous testing is essential\nbefore deployment, with simulation testing playing a key role. However, ADSs\nare typically complex, consisting of multiple modules such as perception and\nplanning, or well-trained end-to-end autonomous driving systems. Offline\nmethods, such as the Genetic Algorithm (GA), can only generate predefined\ntrajectories for dynamics, which struggle to cause safety violations for ADSs\nrapidly and efficiently in different scenarios due to their evolutionary\nnature. Online methods, such as single-agent reinforcement learning (RL), can\nquickly adjust the dynamics' trajectory online to adapt to different scenarios,\nbut they struggle to capture complex corner cases of ADS arising from the\nintricate interplay among multiple vehicles. Multi-agent reinforcement learning\n(MARL) has a strong ability in cooperative tasks. On the other hand, it faces\nits own challenges, particularly with convergence. This paper introduces\nMARL-OT, a scalable framework that leverages MARL to detect safety violations\nof ADS resulting from surrounding vehicles' cooperation. MARL-OT employs MARL\nfor high-level guidance, triggering various dangerous scenarios for the\nrule-based online fuzzer to explore potential safety violations of ADS, thereby\ngenerating dynamic, realistic safety violation scenarios. Our approach improves\nthe detected safety violation rate by up to 136.2% compared to the\nstate-of-the-art (SOTA) testing technique."
                },
                "authors": [
                    {
                        "name": "Linfeng Liang"
                    },
                    {
                        "name": "Xi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zheng"
                },
                "author": "Xi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14431v1",
                "updated": "2025-01-24T11:57:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    57,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:57:39Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    57,
                    39,
                    4,
                    24,
                    0
                ],
                "title": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes\n  Domains"
                },
                "summary": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://anonymous.4open.science/r/Domaino1s-006F/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely applied to downstream domains.\nHowever, current LLMs for high-stakes domain tasks, such as financial\ninvestment and legal QA, typically generate brief answers without reasoning\nprocesses and explanations. This limits users' confidence in making decisions\nbased on their responses. While original CoT shows promise, it lacks\nself-correction mechanisms during reasoning. This work introduces Domain$o1$s,\nwhich enhances LLMs' reasoning capabilities on domain tasks through supervised\nfine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k\ndatasets for fine-tuning models that activate domain-specific reasoning steps\nbased on their judgment. Additionally, we propose Selective Tree Exploration to\nspontaneously explore solution spaces and sample optimal reasoning paths to\nimprove performance. We also introduce PROOF-Score, a new metric for evaluating\ndomain models' explainability, complementing traditional accuracy metrics with\nricher assessment dimensions. Extensive experiments on stock investment\nrecommendation and legal reasoning QA tasks demonstrate Domaino1s's leading\nperformance and explainability. Our code is available at\nhttps://anonymous.4open.science/r/Domaino1s-006F/."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Hanlin Xue"
                    },
                    {
                        "name": "Guanyu Wang"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14427v1",
                "updated": "2025-01-24T11:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    55,
                    57,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:55:57Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    55,
                    57,
                    4,
                    24,
                    0
                ],
                "title": "GraphBC: Improving LLMs for Better Graph Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphBC: Improving LLMs for Better Graph Data Processing"
                },
                "summary": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe that when the order of nodes or\nedges in the natural language description of a graph is shuffled, despite\ndescribing the same graph, model performance fluctuates between high\nperformance and random guessing. Additionally, due to the limited input context\nlength of LLMs, current methods typically randomly sample neighbors of target\nnodes as representatives of their neighborhood, which may not always be\neffective for accurate reasoning. To address these gaps, we introduce GraphBC.\nThis novel model framework features an Order Selector Module to ensure proper\nserialization order of the graph and a Subgraph Sampling Module to sample\nsubgraphs with better structure for better reasoning. Furthermore, we propose\nGraph CoT obtained through distillation, and enhance LLM's reasoning and\nzero-shot learning capabilities for graph tasks through instruction tuning.\nExperiments on multiple datasets for node classification and graph\nquestion-answering demonstrate that GraphBC improves LLMs' performance and\ngeneralization ability on graph tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe that when the order of nodes or\nedges in the natural language description of a graph is shuffled, despite\ndescribing the same graph, model performance fluctuates between high\nperformance and random guessing. Additionally, due to the limited input context\nlength of LLMs, current methods typically randomly sample neighbors of target\nnodes as representatives of their neighborhood, which may not always be\neffective for accurate reasoning. To address these gaps, we introduce GraphBC.\nThis novel model framework features an Order Selector Module to ensure proper\nserialization order of the graph and a Subgraph Sampling Module to sample\nsubgraphs with better structure for better reasoning. Furthermore, we propose\nGraph CoT obtained through distillation, and enhance LLM's reasoning and\nzero-shot learning capabilities for graph tasks through instruction tuning.\nExperiments on multiple datasets for node classification and graph\nquestion-answering demonstrate that GraphBC improves LLMs' performance and\ngeneralization ability on graph tasks."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Hanlin Xue"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Bingce Wang"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08027v2",
                "updated": "2025-01-24T11:53:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    53,
                    43,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-12T13:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "title": "iLLuMinaTE: An LLM-XAI Framework Leveraging Social Science Explanation\n  Theories Towards Actionable Student Performance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iLLuMinaTE: An LLM-XAI Framework Leveraging Social Science Explanation\n  Theories Towards Actionable Student Performance Feedback"
                },
                "summary": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    },
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Bhargav Srinivasa Desikan"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Tanja Käser"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Käser"
                },
                "author": "Tanja Käser",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18442v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18442v4",
                "updated": "2025-01-24T11:39:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    39,
                    32,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-24T14:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    14,
                    2,
                    44,
                    1,
                    359,
                    0
                ],
                "title": "SoK: On the Offensive Potential of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: On the Offensive Potential of AI"
                },
                "summary": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laypeople -- all of which being valuable sources of\ninformation on offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our society increasingly benefits from Artificial Intelligence (AI).\nUnfortunately, more and more evidence shows that AI is also used for offensive\npurposes. Prior works have revealed various examples of use cases in which the\ndeployment of AI can lead to violation of security and privacy objectives. No\nextant work, however, has been able to draw a holistic picture of the offensive\npotential of AI. In this SoK paper we seek to lay the ground for a systematic\nanalysis of the heterogeneous capabilities of offensive AI. In particular we\n(i) account for AI risks to both humans and systems while (ii) consolidating\nand distilling knowledge from academic literature, expert opinions, industrial\nvenues, as well as laypeople -- all of which being valuable sources of\ninformation on offensive AI.\n  To enable alignment of such diverse sources of knowledge, we devise a common\nset of criteria reflecting essential technological factors related to offensive\nAI. With the help of such criteria, we systematically analyze: 95 research\npapers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user\nstudy (N=549) entailing individuals with diverse backgrounds and expertise; and\nthe opinion of 12 experts. Our contributions not only reveal concerning ways\n(some of which overlooked by prior work) in which AI can be offensively used\ntoday, but also represent a foothold to address this threat in the years to\ncome."
                },
                "authors": [
                    {
                        "name": "Saskia Laura Schröer"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    },
                    {
                        "name": "Soheil Human"
                    },
                    {
                        "name": "Pavel Laskov"
                    },
                    {
                        "name": "Hyrum S. Anderson"
                    },
                    {
                        "name": "Edward W. N. Bernroider"
                    },
                    {
                        "name": "Aurore Fass"
                    },
                    {
                        "name": "Ben Nassi"
                    },
                    {
                        "name": "Vera Rimmer"
                    },
                    {
                        "name": "Fabio Roli"
                    },
                    {
                        "name": "Samer Salam"
                    },
                    {
                        "name": "Ashley Shen"
                    },
                    {
                        "name": "Ali Sunyaev"
                    },
                    {
                        "name": "Tim Wadhwa-Brown"
                    },
                    {
                        "name": "Isabel Wagner"
                    },
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "Systematization of Knowledge (SoK) paper. Accepted to the 3rd IEEE\n  Conference on Secure and Trustworthy Machine Learning (SaTML'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18442v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18442v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14417v1",
                "updated": "2025-01-24T11:34:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    34,
                    13,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:34:13Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    34,
                    13,
                    4,
                    24,
                    0
                ],
                "title": "DeepFlow: Serverless Large Language Model Serving at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepFlow: Serverless Large Language Model Serving at Scale"
                },
                "summary": "This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Chao He"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14413v1",
                "updated": "2025-01-24T11:28:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    28,
                    17,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T11:28:17Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    28,
                    17,
                    4,
                    24,
                    0
                ],
                "title": "Context-CrackNet: A Context-Aware Framework for Precise Segmentation of\n  Tiny Cracks in Pavement images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-CrackNet: A Context-Aware Framework for Precise Segmentation of\n  Tiny Cracks in Pavement images"
                },
                "summary": "The accurate detection and segmentation of pavement distresses, particularly\ntiny and small cracks, are critical for early intervention and preventive\nmaintenance in transportation infrastructure. Traditional manual inspection\nmethods are labor-intensive and inconsistent, while existing deep learning\nmodels struggle with fine-grained segmentation and computational efficiency. To\naddress these challenges, this study proposes Context-CrackNet, a novel\nencoder-decoder architecture featuring the Region-Focused Enhancement Module\n(RFEM) and Context-Aware Global Module (CAGM). These innovations enhance the\nmodel's ability to capture fine-grained local details and global contextual\ndependencies, respectively. Context-CrackNet was rigorously evaluated on ten\npublicly available crack segmentation datasets, covering diverse pavement\ndistress scenarios. The model consistently outperformed 9 state-of-the-art\nsegmentation frameworks, achieving superior performance metrics such as mIoU\nand Dice score, while maintaining competitive inference efficiency. Ablation\nstudies confirmed the complementary roles of RFEM and CAGM, with notable\nimprovements in mIoU and Dice score when both modules were integrated.\nAdditionally, the model's balance of precision and computational efficiency\nhighlights its potential for real-time deployment in large-scale pavement\nmonitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accurate detection and segmentation of pavement distresses, particularly\ntiny and small cracks, are critical for early intervention and preventive\nmaintenance in transportation infrastructure. Traditional manual inspection\nmethods are labor-intensive and inconsistent, while existing deep learning\nmodels struggle with fine-grained segmentation and computational efficiency. To\naddress these challenges, this study proposes Context-CrackNet, a novel\nencoder-decoder architecture featuring the Region-Focused Enhancement Module\n(RFEM) and Context-Aware Global Module (CAGM). These innovations enhance the\nmodel's ability to capture fine-grained local details and global contextual\ndependencies, respectively. Context-CrackNet was rigorously evaluated on ten\npublicly available crack segmentation datasets, covering diverse pavement\ndistress scenarios. The model consistently outperformed 9 state-of-the-art\nsegmentation frameworks, achieving superior performance metrics such as mIoU\nand Dice score, while maintaining competitive inference efficiency. Ablation\nstudies confirmed the complementary roles of RFEM and CAGM, with notable\nimprovements in mIoU and Dice score when both modules were integrated.\nAdditionally, the model's balance of precision and computational efficiency\nhighlights its potential for real-time deployment in large-scale pavement\nmonitoring systems."
                },
                "authors": [
                    {
                        "name": "Blessing Agyei Kyem"
                    },
                    {
                        "name": "Joshua Kofi Asamoah"
                    },
                    {
                        "name": "Armstrong Aboah"
                    }
                ],
                "author_detail": {
                    "name": "Armstrong Aboah"
                },
                "author": "Armstrong Aboah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v4",
                "updated": "2025-01-24T11:05:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    5,
                    5,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Hans Christian Farsethås"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "17 pages, 5 figures, 8 tables. Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14371v1",
                "updated": "2025-01-24T10:04:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    4,
                    53,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:04:53Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    4,
                    53,
                    4,
                    24,
                    0
                ],
                "title": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style\n  Subspace Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style\n  Subspace Editing"
                },
                "summary": "We introduce DRESS, a novel approach for generating stylized large language\nmodel (LLM) responses through representation editing. Existing methods like\nprompting and fine-tuning are either insufficient for complex style adaptation\nor computationally expensive, particularly in tasks like NPC creation or\ncharacter role-playing. Our approach leverages the over-parameterized nature of\nLLMs to disentangle a style-relevant subspace within the model's representation\nspace to conduct representation editing, ensuring a minimal impact on the\noriginal semantics. By applying adaptive editing strengths, we dynamically\nadjust the steering vectors in the style subspace to maintain both stylistic\nfidelity and semantic integrity. We develop two stylized QA benchmark datasets\nto validate the effectiveness of DRESS, and the results demonstrate significant\nimprovements compared to baseline methods such as prompting and ITI. In short,\nDRESS is a lightweight, train-free solution for enhancing LLMs with flexible\nand effective style control, making it particularly useful for developing\nstylized conversational agents. Codes and benchmark datasets are available at\nhttps://github.com/ArthurLeoM/DRESS-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DRESS, a novel approach for generating stylized large language\nmodel (LLM) responses through representation editing. Existing methods like\nprompting and fine-tuning are either insufficient for complex style adaptation\nor computationally expensive, particularly in tasks like NPC creation or\ncharacter role-playing. Our approach leverages the over-parameterized nature of\nLLMs to disentangle a style-relevant subspace within the model's representation\nspace to conduct representation editing, ensuring a minimal impact on the\noriginal semantics. By applying adaptive editing strengths, we dynamically\nadjust the steering vectors in the style subspace to maintain both stylistic\nfidelity and semantic integrity. We develop two stylized QA benchmark datasets\nto validate the effectiveness of DRESS, and the results demonstrate significant\nimprovements compared to baseline methods such as prompting and ITI. In short,\nDRESS is a lightweight, train-free solution for enhancing LLMs with flexible\nand effective style control, making it particularly useful for developing\nstylized conversational agents. Codes and benchmark datasets are available at\nhttps://github.com/ArthurLeoM/DRESS-LLM."
                },
                "authors": [
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Yifeng Xu"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "arxiv_comment": "ICLR 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14350v1",
                "updated": "2025-01-24T09:21:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    21,
                    41,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T09:21:41Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    21,
                    41,
                    4,
                    24,
                    0
                ],
                "title": "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition\n  Models from Encoder-Decoder to LLM Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition\n  Models from Encoder-Decoder to LLM Integration"
                },
                "summary": "We present FireRedASR, a family of large-scale automatic speech recognition\n(ASR) models for Mandarin, designed to meet diverse requirements in superior\nperformance and optimal efficiency across various applications. FireRedASR\ncomprises two variants:\n  FireRedASR-LLM: Designed to achieve state-of-the-art (SOTA) performance and\nto enable seamless end-to-end speech interaction. It adopts an\nEncoder-Adapter-LLM framework leveraging large language model (LLM)\ncapabilities. On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters)\nachieves an average Character Error Rate (CER) of 3.05%, surpassing the latest\nSOTA of 3.33% with an 8.4% relative CER reduction (CERR). It demonstrates\nsuperior generalization capability over industrial-grade baselines, achieving\n24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and\nintelligent assistant.\n  FireRedASR-AED: Designed to balance high performance and computational\nefficiency and to serve as an effective speech representation module in\nLLM-based speech models. It utilizes an Attention-based Encoder-Decoder (AED)\narchitecture. On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters)\nachieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still\noutperforming the latest SOTA model with over 12B parameters. It offers a more\ncompact size, making it suitable for resource-constrained applications.\n  Moreover, both models exhibit competitive results on Chinese dialects and\nEnglish speech benchmarks and excel in singing lyrics recognition. To advance\nresearch in speech processing, we release our models and inference code at\nhttps://github.com/FireRedTeam/FireRedASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FireRedASR, a family of large-scale automatic speech recognition\n(ASR) models for Mandarin, designed to meet diverse requirements in superior\nperformance and optimal efficiency across various applications. FireRedASR\ncomprises two variants:\n  FireRedASR-LLM: Designed to achieve state-of-the-art (SOTA) performance and\nto enable seamless end-to-end speech interaction. It adopts an\nEncoder-Adapter-LLM framework leveraging large language model (LLM)\ncapabilities. On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters)\nachieves an average Character Error Rate (CER) of 3.05%, surpassing the latest\nSOTA of 3.33% with an 8.4% relative CER reduction (CERR). It demonstrates\nsuperior generalization capability over industrial-grade baselines, achieving\n24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and\nintelligent assistant.\n  FireRedASR-AED: Designed to balance high performance and computational\nefficiency and to serve as an effective speech representation module in\nLLM-based speech models. It utilizes an Attention-based Encoder-Decoder (AED)\narchitecture. On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters)\nachieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still\noutperforming the latest SOTA model with over 12B parameters. It offers a more\ncompact size, making it suitable for resource-constrained applications.\n  Moreover, both models exhibit competitive results on Chinese dialects and\nEnglish speech benchmarks and excel in singing lyrics recognition. To advance\nresearch in speech processing, we release our models and inference code at\nhttps://github.com/FireRedTeam/FireRedASR."
                },
                "authors": [
                    {
                        "name": "Kai-Tuo Xu"
                    },
                    {
                        "name": "Feng-Long Xie"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14348v1",
                "updated": "2025-01-24T09:18:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    18,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T09:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    18,
                    48,
                    4,
                    24,
                    0
                ],
                "title": "Advancing data-driven broadband seismic wavefield simulation with\n  multi-conditional diffusion model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing data-driven broadband seismic wavefield simulation with\n  multi-conditional diffusion model"
                },
                "summary": "Sparse distributions of seismic sensors and sources pose challenges for\nsubsurface imaging, source characterization, and ground motion modeling. While\nlarge-N arrays have shown the potential of dense observational data, their\ndeployment over extensive areas is constrained by economic and logistical\nlimitations. Numerical simulations offer an alternative, but modeling realistic\nwavefields remains computationally expensive. To address these challenges, we\ndevelop a multi-conditional diffusion transformer for generating seismic\nwavefields without requiring prior geological knowledge. Our method produces\nhigh-resolution wavefields that accurately capture both amplitude and phase\ninformation across diverse source and station configurations. The model first\ngenerates amplitude spectra conditioned on input attributes and subsequently\nrefines wavefields through iterative phase optimization. We validate our\napproach using data from the Geysers geothermal field, demonstrating the\ngeneration of wavefields with spatial continuity and fidelity in both spectral\namplitude and phase. These synthesized wavefields hold promise for advancing\nstructural imaging and source characterization in seismology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse distributions of seismic sensors and sources pose challenges for\nsubsurface imaging, source characterization, and ground motion modeling. While\nlarge-N arrays have shown the potential of dense observational data, their\ndeployment over extensive areas is constrained by economic and logistical\nlimitations. Numerical simulations offer an alternative, but modeling realistic\nwavefields remains computationally expensive. To address these challenges, we\ndevelop a multi-conditional diffusion transformer for generating seismic\nwavefields without requiring prior geological knowledge. Our method produces\nhigh-resolution wavefields that accurately capture both amplitude and phase\ninformation across diverse source and station configurations. The model first\ngenerates amplitude spectra conditioned on input attributes and subsequently\nrefines wavefields through iterative phase optimization. We validate our\napproach using data from the Geysers geothermal field, demonstrating the\ngeneration of wavefields with spatial continuity and fidelity in both spectral\namplitude and phase. These synthesized wavefields hold promise for advancing\nstructural imaging and source characterization in seismology."
                },
                "authors": [
                    {
                        "name": "Zhengfa Bi"
                    },
                    {
                        "name": "Nori Nakata"
                    },
                    {
                        "name": "Rie Nakata"
                    },
                    {
                        "name": "Pu Ren"
                    },
                    {
                        "name": "Xinming Wu"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Mahoney"
                },
                "author": "Michael W. Mahoney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01639v3",
                "updated": "2025-01-24T09:10:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    10,
                    42,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-02T15:09:36Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    9,
                    36,
                    2,
                    276,
                    0
                ],
                "title": "Moral Alignment for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moral Alignment for LLM Agents"
                },
                "summary": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques."
                },
                "authors": [
                    {
                        "name": "Elizaveta Tennant"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR'25), Singapore, Apr 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14334v1",
                "updated": "2025-01-24T08:58:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    58,
                    49,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:58:49Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    58,
                    49,
                    4,
                    24,
                    0
                ],
                "title": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts"
                },
                "summary": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.In this paper, we propose a\nmethodology to estimate the environmental impact of a company's AI portfolio,\nproviding actionable insights without necessitating extensive AI and Life-Cycle\nAssessment (LCA) expertise. Results confirm that large generative AI models\nconsume up to 4600x more energy than traditional models. Our modelling\napproach, which accounts for increased AI usage, hardware computing efficiency,\nand changes in electricity mix in line with IPCC scenarios, forecasts AI\nelectricity use up to 2030. Under a high adoption scenario, driven by\nwidespread Generative AI and agents adoption associated to increasingly complex\nmodels and frameworks, AI electricity use is projected to rise by a factor of\n24.4.Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.In this paper, we propose a\nmethodology to estimate the environmental impact of a company's AI portfolio,\nproviding actionable insights without necessitating extensive AI and Life-Cycle\nAssessment (LCA) expertise. Results confirm that large generative AI models\nconsume up to 4600x more energy than traditional models. Our modelling\napproach, which accounts for increased AI usage, hardware computing efficiency,\nand changes in electricity mix in line with IPCC scenarios, forecasts AI\nelectricity use up to 2030. Under a high adoption scenario, driven by\nwidespread Generative AI and agents adoption associated to increasingly complex\nmodels and frameworks, AI electricity use is projected to rise by a factor of\n24.4.Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals."
                },
                "authors": [
                    {
                        "name": "Clément Desroches"
                    },
                    {
                        "name": "Martin Chauvin"
                    },
                    {
                        "name": "Louis Ladan"
                    },
                    {
                        "name": "Caroline Vateau"
                    },
                    {
                        "name": "Simon Gosset"
                    },
                    {
                        "name": "Philippe Cordier"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Cordier"
                },
                "author": "Philippe Cordier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12032v2",
                "updated": "2025-01-24T08:51:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    51,
                    54,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-21T10:53:17Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    53,
                    17,
                    1,
                    21,
                    0
                ],
                "title": "Multi-Tenant SmartNICs for In-Network Preprocessing of Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Tenant SmartNICs for In-Network Preprocessing of Recommender\n  Systems"
                },
                "summary": "Keeping ML-based recommender models up-to-date as data drifts and evolves is\nessential to maintain accuracy. As a result, online data preprocessing plays an\nincreasingly important role in serving recommender systems. Existing solutions\nemploy multiple CPU workers to saturate the input bandwidth of a single\ntraining node. Such an approach results in high deployment costs and energy\nconsumption. For instance, a recent report from industrial deployments shows\nthat data storage and ingestion pipelines can account for over 60\\% of the\npower consumption in a recommender system. In this paper, we tackle the issue\nfrom a hardware perspective by introducing Piper, a flexible and\nnetwork-attached accelerator that executes data loading and preprocessing\npipelines in a streaming fashion. As part of the design, we define MiniPipe,\nthe smallest pipeline unit enabling multi-pipeline implementation by executing\nvarious data preprocessing tasks across the single board, giving Piper the\nability to be reconfigured at runtime. Our results, using publicly released\ncommercial pipelines, show that Piper, prototyped on a power-efficient FPGA,\nachieves a 39$\\sim$105$\\times$ speedup over a server-grade, 128-core CPU and\n3$\\sim$17$\\times$ speedup over GPUs like RTX 3090 and A100 in multiple\npipelines. The experimental analysis demonstrates that Piper provides\nadvantages in both latency and energy efficiency for preprocessing tasks in\nrecommender systems, providing an alternative design point for systems that\ntoday are in very high demand.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping ML-based recommender models up-to-date as data drifts and evolves is\nessential to maintain accuracy. As a result, online data preprocessing plays an\nincreasingly important role in serving recommender systems. Existing solutions\nemploy multiple CPU workers to saturate the input bandwidth of a single\ntraining node. Such an approach results in high deployment costs and energy\nconsumption. For instance, a recent report from industrial deployments shows\nthat data storage and ingestion pipelines can account for over 60\\% of the\npower consumption in a recommender system. In this paper, we tackle the issue\nfrom a hardware perspective by introducing Piper, a flexible and\nnetwork-attached accelerator that executes data loading and preprocessing\npipelines in a streaming fashion. As part of the design, we define MiniPipe,\nthe smallest pipeline unit enabling multi-pipeline implementation by executing\nvarious data preprocessing tasks across the single board, giving Piper the\nability to be reconfigured at runtime. Our results, using publicly released\ncommercial pipelines, show that Piper, prototyped on a power-efficient FPGA,\nachieves a 39$\\sim$105$\\times$ speedup over a server-grade, 128-core CPU and\n3$\\sim$17$\\times$ speedup over GPUs like RTX 3090 and A100 in multiple\npipelines. The experimental analysis demonstrates that Piper provides\nadvantages in both latency and energy efficiency for preprocessing tasks in\nrecommender systems, providing an alternative design point for systems that\ntoday are in very high demand."
                },
                "authors": [
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17094v2",
                "updated": "2025-01-24T08:46:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    46,
                    41,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-22T17:09:34Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    17,
                    9,
                    34,
                    6,
                    357,
                    0
                ],
                "title": "Analysis on LLMs Performance for Code Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis on LLMs Performance for Code Summarization"
                },
                "summary": "Code summarization aims to generate concise natural language descriptions for\nsource code. Deep learning has been used more and more recently in software\nengineering, particularly for tasks like code creation and summarization.\nSpecifically, it appears that the most current Large Language Models with\ncoding perform well on these tasks. Large Language Models (LLMs) have\nsignificantly advanced the field of code summarization, providing sophisticated\nmethods for generating concise and accurate summaries of source code. This\nstudy aims to perform a comparative analysis of several open-source LLMs,\nnamely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is\nassessed using important metrics such as BLEU\\textsubscript{3.1} and\nROUGE\\textsubscript{3.2}.\n  Through this analysis, we seek to identify the strengths and weaknesses of\neach model, offering insights into their applicability and effectiveness in\ncode summarization tasks. Our findings contribute to the ongoing development\nand refinement of LLMs, supporting their integration into tools that enhance\nsoftware development and maintenance processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code summarization aims to generate concise natural language descriptions for\nsource code. Deep learning has been used more and more recently in software\nengineering, particularly for tasks like code creation and summarization.\nSpecifically, it appears that the most current Large Language Models with\ncoding perform well on these tasks. Large Language Models (LLMs) have\nsignificantly advanced the field of code summarization, providing sophisticated\nmethods for generating concise and accurate summaries of source code. This\nstudy aims to perform a comparative analysis of several open-source LLMs,\nnamely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is\nassessed using important metrics such as BLEU\\textsubscript{3.1} and\nROUGE\\textsubscript{3.2}.\n  Through this analysis, we seek to identify the strengths and weaknesses of\neach model, offering insights into their applicability and effectiveness in\ncode summarization tasks. Our findings contribute to the ongoing development\nand refinement of LLMs, supporting their integration into tools that enhance\nsoftware development and maintenance processes."
                },
                "authors": [
                    {
                        "name": "Md. Ahnaf Akib"
                    },
                    {
                        "name": "Md. Muktadir Mazumder"
                    },
                    {
                        "name": "Salman Ahsan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Ahsan"
                },
                "author": "Salman Ahsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11081v2",
                "updated": "2025-01-24T08:44:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    44,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2024-11-17T14:14:36Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    14,
                    36,
                    6,
                    322,
                    0
                ],
                "title": "The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case\n  Study on Media Bias Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case\n  Study on Media Bias Detection"
                },
                "summary": "High annotation costs from hiring or crowdsourcing complicate the creation of\nlarge, high-quality datasets needed for training reliable text classifiers.\nRecent research suggests using Large Language Models (LLMs) to automate the\nannotation process, reducing these costs while maintaining data quality. LLMs\nhave shown promising results in annotating downstream tasks like hate speech\ndetection and political framing. Building on the success in these areas, this\nstudy investigates whether LLMs are viable for annotating the complex task of\nmedia bias detection and whether a downstream media bias classifier can be\ntrained on such data. We create annolexical, the first large-scale dataset for\nmedia bias classification with over 48000 synthetically annotated examples. Our\nclassifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by\n5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or\noutperforms the model trained on human-labeled data when evaluated on two media\nbias benchmark datasets (BABE and BASIL). This study demonstrates how our\napproach significantly reduces the cost of dataset creation in the media bias\ndomain and, by extension, the development of classifiers, while our subsequent\nbehavioral stress-testing reveals some of its current limitations and\ntrade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High annotation costs from hiring or crowdsourcing complicate the creation of\nlarge, high-quality datasets needed for training reliable text classifiers.\nRecent research suggests using Large Language Models (LLMs) to automate the\nannotation process, reducing these costs while maintaining data quality. LLMs\nhave shown promising results in annotating downstream tasks like hate speech\ndetection and political framing. Building on the success in these areas, this\nstudy investigates whether LLMs are viable for annotating the complex task of\nmedia bias detection and whether a downstream media bias classifier can be\ntrained on such data. We create annolexical, the first large-scale dataset for\nmedia bias classification with over 48000 synthetically annotated examples. Our\nclassifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by\n5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or\noutperforms the model trained on human-labeled data when evaluated on two media\nbias benchmark datasets (BABE and BASIL). This study demonstrates how our\napproach significantly reduces the cost of dataset creation in the media bias\ndomain and, by extension, the development of classifiers, while our subsequent\nbehavioral stress-testing reveals some of its current limitations and\ntrade-offs."
                },
                "authors": [
                    {
                        "name": "Tomas Horych"
                    },
                    {
                        "name": "Christoph Mandl"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Andre Greiner-Petter"
                    },
                    {
                        "name": "Bela Gipp"
                    },
                    {
                        "name": "Akiko Aizawa"
                    },
                    {
                        "name": "Timo Spinde"
                    }
                ],
                "author_detail": {
                    "name": "Timo Spinde"
                },
                "author": "Timo Spinde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14326v1",
                "updated": "2025-01-24T08:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    39,
                    50,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:39:50Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    39,
                    50,
                    4,
                    24,
                    0
                ],
                "title": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Large Language Models in Comprehending and Verifying\n  Concurrent Programs across Memory Models"
                },
                "summary": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As concurrent programming becomes increasingly prevalent, effectively\nidentifying and addressing concurrency issues such as data races and deadlocks\nis critical. This study evaluates the performance of several leading large\nlanguage models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini,\nand Mistral-AI's Large2, in understanding and analyzing concurrency issues\nwithin software programs. Given that relaxed memory models, such as Total Store\nOrder (TSO) and Partial Store Order (PSO), are widely implemented and adapted\nin modern systems, supported even by commodity architectures like ARM and x86,\nour evaluation focuses not only on sequentially consistent memory models but\nalso on these relaxed memory models. Specifically, we assess two main aspects:\nthe models' capacity to detect concurrency problems under a sequentially\nconsistent memory model and their ability to verify the correctness conditions\nof concurrent programs across both sequentially consistent and relaxed memory\nmodels. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests\ndesigned to evaluate Total Store Order (TSO) and Partial Store Order (PSO)\nmemory models. The experimental results reveal that GPT-4, GPT-4o, and\nMistral-AI's Large2 demonstrate a robust understanding of concurrency issues,\neffectively identifying data races and deadlocks when assessed under a\nsequentially consistent memory model. However, despite its superior\nperformance, all selected LLMs face significant challenges verifying program\ncorrectness under relaxed memory models. These LLMs exhibit limitations in\naccurately capturing memory ordering constraints, and their current\ncapabilities fall short in verifying even small programs in these complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Ridhi Jain"
                    },
                    {
                        "name": "Rahul Purandare"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Purandare"
                },
                "author": "Rahul Purandare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09099v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09099v5",
                "updated": "2025-01-24T08:22:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    22,
                    25,
                    4,
                    24,
                    0
                ],
                "published": "2024-02-14T11:20:09Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    11,
                    20,
                    9,
                    2,
                    45,
                    0
                ],
                "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in\n  Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in\n  Large Models"
                },
                "summary": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been increasing attention on the capabilities of\nlarge models, particularly in handling complex tasks that small-scale models\nare unable to perform. Notably, large language models (LLMs) have demonstrated\n``intelligent'' abilities such as complex reasoning and abstract language\ncomprehension, reflecting cognitive-like behaviors. However, current research\non emergent abilities in large models predominantly focuses on the relationship\nbetween model performance and size, leaving a significant gap in the systematic\nquantitative analysis of the internal structures and mechanisms driving these\nemergent abilities. Drawing inspiration from neuroscience research on brain\nnetwork structure and self-organization, we propose (i) a general network\nrepresentation of large models, (ii) a new analytical framework, called\nNeuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and\n(iii) a novel structure-based metric as a proxy for emergent abilities of large\nmodels. By linking structural features to the capabilities of large models,\nNeuroMFA provides a quantitative framework for analyzing emergent phenomena in\nlarge models. Our experiments show that the proposed method yields a\ncomprehensive measure of network's evolving heterogeneity and organization,\noffering theoretical foundations and a new perspective for investigating\nemergent abilities in large models."
                },
                "authors": [
                    {
                        "name": "Xiongye Xiao"
                    },
                    {
                        "name": "Chenyu Zhou"
                    },
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Defu Cao"
                    },
                    {
                        "name": "Yaxing Li"
                    },
                    {
                        "name": "Yi-Zhuo Zhou"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Nikos Kanakaris"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "arxiv_comment": "ICLR 2025: https://openreview.net/forum?id=nt8gBX58Kh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09099v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09099v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14315v1",
                "updated": "2025-01-24T08:18:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    18,
                    56,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:18:56Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    18,
                    56,
                    4,
                    24,
                    0
                ],
                "title": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of\n  Token Perplexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of\n  Token Perplexity"
                },
                "summary": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. In this paper, we present a systematic analysis\nrevealing that fine-tuning with LLM-generated data not only improves target\ntask performance but also reduces out-of-domain (OOD) degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhanced OOD robustness\nstems from a reduced prevalence of high perplexity tokens in LLM-generated\nsequences. Following this hypothesis we showed that masking high perplexity\ntokens in ground truth training data also achieves similar OOD preservation\ncomparable to using LLM-generated data. Extensive experiments across diverse\nmodel architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B,\ncorroborate the consistency of our findings. To the best of our knowledge, this\nwork provides the first mechanistic explanation for the superior OOD robustness\nconferred by LLM-generated training data, offering valuable insights for\ndeveloping more robust fine-tuning strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining consistent model performance across domains is a fundamental\nchallenge in machine learning. While recent work has explored using\nLLM-generated data for fine-tuning, its impact on cross-domain generalization\nremains poorly understood. In this paper, we present a systematic analysis\nrevealing that fine-tuning with LLM-generated data not only improves target\ntask performance but also reduces out-of-domain (OOD) degradation compared to\nfine-tuning with ground truth data. Through analyzing the data sequence in\ntasks of various domains, we demonstrate that this enhanced OOD robustness\nstems from a reduced prevalence of high perplexity tokens in LLM-generated\nsequences. Following this hypothesis we showed that masking high perplexity\ntokens in ground truth training data also achieves similar OOD preservation\ncomparable to using LLM-generated data. Extensive experiments across diverse\nmodel architectures and scales, including Gemma2-2B, Mistral-7B and Llama3-8B,\ncorroborate the consistency of our findings. To the best of our knowledge, this\nwork provides the first mechanistic explanation for the superior OOD robustness\nconferred by LLM-generated training data, offering valuable insights for\ndeveloping more robust fine-tuning strategies."
                },
                "authors": [
                    {
                        "name": "Chao-Chung Wu"
                    },
                    {
                        "name": "Zhi Rui Tam"
                    },
                    {
                        "name": "Chieh-Yen Lin"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05006v2",
                "updated": "2025-01-24T08:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    13,
                    35,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-09T11:35:44Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    11,
                    35,
                    44,
                    4,
                    222,
                    0
                ],
                "title": "COAST: Enhancing the Code Debugging Ability of LLMs through\n  Communicative Agent Based Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COAST: Enhancing the Code Debugging Ability of LLMs through\n  Communicative Agent Based Data Synthesis"
                },
                "summary": "Code debugging is a vital stage of software development, essential for\nensuring the reliability and performance of Large Language Models (LLMs) in\ncode generation task. Human debugging typically follows a multi-stage process,\nwhich includes Bug Localization, Bug Identification, Code Repair, and Code\nRecognition. However, existing code debugging benchmarks predominantly focus on\nthe Code Repair stage, which offers only a limited perspective on evaluating\nthe debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a\ncomprehensive benchmark for evaluating the debugging abilities of LLMs by\nemulating the multi-stage human debugging process. Through evaluating on\nDEBUGEVAL, we observe that 7B-scale models consistently underperform compared\nto their larger counterparts, highlighting their limitations in comprehending\ncode semantics. In this case, we propose the COmmunicative Agent-based data\nSynThesis (COAST) framework, which employs a multi-agent system to generate\nhigh-quality training data for supervised fine-tuning (SFT). Experimental\nresults demonstrate that COAST-generated data outperform human-curated and\nGPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance\ncomparable to GPT-3.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code debugging is a vital stage of software development, essential for\nensuring the reliability and performance of Large Language Models (LLMs) in\ncode generation task. Human debugging typically follows a multi-stage process,\nwhich includes Bug Localization, Bug Identification, Code Repair, and Code\nRecognition. However, existing code debugging benchmarks predominantly focus on\nthe Code Repair stage, which offers only a limited perspective on evaluating\nthe debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a\ncomprehensive benchmark for evaluating the debugging abilities of LLMs by\nemulating the multi-stage human debugging process. Through evaluating on\nDEBUGEVAL, we observe that 7B-scale models consistently underperform compared\nto their larger counterparts, highlighting their limitations in comprehending\ncode semantics. In this case, we propose the COmmunicative Agent-based data\nSynThesis (COAST) framework, which employs a multi-agent system to generate\nhigh-quality training data for supervised fine-tuning (SFT). Experimental\nresults demonstrate that COAST-generated data outperform human-curated and\nGPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance\ncomparable to GPT-3.5."
                },
                "authors": [
                    {
                        "name": "Weiqing Yang"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Minghe Yu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14305v1",
                "updated": "2025-01-24T08:01:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    41,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:01:41Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    41,
                    4,
                    24,
                    0
                ],
                "title": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher\n  Education"
                },
                "summary": "Automated grading has become an essential tool in education technology due to\nits ability to efficiently assess large volumes of student work, provide\nconsistent and unbiased evaluations, and deliver immediate feedback to enhance\nlearning. However, current systems face significant limitations, including the\nneed for large datasets in few-shot learning methods, a lack of personalized\nand actionable feedback, and an overemphasis on benchmark performance rather\nthan student experience. To address these challenges, we propose a Zero-Shot\nLarge Language Model (LLM)-Based Automated Assignment Grading (AAG) system.\nThis framework leverages prompt engineering to evaluate both computational and\nexplanatory student responses without requiring additional training or\nfine-tuning. The AAG system delivers tailored feedback that highlights\nindividual strengths and areas for improvement, thereby enhancing student\nlearning outcomes. Our study demonstrates the system's effectiveness through\ncomprehensive evaluations, including survey responses from higher education\nstudents that indicate significant improvements in motivation, understanding,\nand preparedness compared to traditional grading methods. The results validate\nthe AAG system's potential to transform educational assessment by prioritizing\nlearning experiences and providing scalable, high-quality feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated grading has become an essential tool in education technology due to\nits ability to efficiently assess large volumes of student work, provide\nconsistent and unbiased evaluations, and deliver immediate feedback to enhance\nlearning. However, current systems face significant limitations, including the\nneed for large datasets in few-shot learning methods, a lack of personalized\nand actionable feedback, and an overemphasis on benchmark performance rather\nthan student experience. To address these challenges, we propose a Zero-Shot\nLarge Language Model (LLM)-Based Automated Assignment Grading (AAG) system.\nThis framework leverages prompt engineering to evaluate both computational and\nexplanatory student responses without requiring additional training or\nfine-tuning. The AAG system delivers tailored feedback that highlights\nindividual strengths and areas for improvement, thereby enhancing student\nlearning outcomes. Our study demonstrates the system's effectiveness through\ncomprehensive evaluations, including survey responses from higher education\nstudents that indicate significant improvements in motivation, understanding,\nand preparedness compared to traditional grading methods. The results validate\nthe AAG system's potential to transform educational assessment by prioritizing\nlearning experiences and providing scalable, high-quality feedback."
                },
                "authors": [
                    {
                        "name": "Calvin Yeung"
                    },
                    {
                        "name": "Jeff Yu"
                    },
                    {
                        "name": "King Chau Cheung"
                    },
                    {
                        "name": "Tat Wing Wong"
                    },
                    {
                        "name": "Chun Man Chan"
                    },
                    {
                        "name": "Kin Chi Wong"
                    },
                    {
                        "name": "Keisuke Fujii"
                    }
                ],
                "author_detail": {
                    "name": "Keisuke Fujii"
                },
                "author": "Keisuke Fujii",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14304v1",
                "updated": "2025-01-24T08:01:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    11,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:01:11Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    1,
                    11,
                    4,
                    24,
                    0
                ],
                "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASTER: A Multi-Agent System with LLM Specialized MCTS"
                },
                "summary": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, leading to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot obtain an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present Multi-Agent\nSystem with Tactical Execution and Reasoning using LLM Specialized MCTS\n(MASTER), a novel framework that coordinates agent recruitment and\ncommunication using LLM specialized MCTS. This system autonomously adjusts the\nnumber of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, leading to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot obtain an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present Multi-Agent\nSystem with Tactical Execution and Reasoning using LLM Specialized MCTS\n(MASTER), a novel framework that coordinates agent recruitment and\ncommunication using LLM specialized MCTS. This system autonomously adjusts the\nnumber of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets."
                },
                "authors": [
                    {
                        "name": "Bingzheng Gan"
                    },
                    {
                        "name": "Yufan Zhao"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yusu Li"
                    },
                    {
                        "name": "Shu Xian Teo"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shi"
                },
                "author": "Wei Shi",
                "arxiv_comment": "Accepted by main NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14300v1",
                "updated": "2025-01-24T07:47:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    47,
                    40,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:47:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    47,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large\n  Language Model on Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large\n  Language Model on Knowledge Graph"
                },
                "summary": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes\nthe naive RAG system a step further by integrating graph information, such as\nknowledge graph (KGs), into large-scale language models (LLMs) to mitigate\nhallucination. However, existing GRAG still encounter limitations: 1) simple\nparadigms usually fail with the complex problems due to the narrow and shallow\ncorrelations capture from KGs 2) methods of strong coupling with KGs tend to be\nhigh computation cost and time consuming if the graph is dense. In this paper,\nwe propose the Fast Think-on-Graph (FastToG), an innovative paradigm for\nenabling LLMs to think ``community by community\" within KGs. To do this,\nFastToG employs community detection for deeper correlation capture and two\nstages community pruning - coarse and fine pruning for faster retrieval.\nFurthermore, we also develop two Community-to-Text methods to convert the graph\nstructure of communities into textual form for better understanding by LLMs.\nExperimental results demonstrate the effectiveness of FastToG, showcasing\nhigher accuracy, faster reasoning, and better explainability compared to the\nprevious works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes\nthe naive RAG system a step further by integrating graph information, such as\nknowledge graph (KGs), into large-scale language models (LLMs) to mitigate\nhallucination. However, existing GRAG still encounter limitations: 1) simple\nparadigms usually fail with the complex problems due to the narrow and shallow\ncorrelations capture from KGs 2) methods of strong coupling with KGs tend to be\nhigh computation cost and time consuming if the graph is dense. In this paper,\nwe propose the Fast Think-on-Graph (FastToG), an innovative paradigm for\nenabling LLMs to think ``community by community\" within KGs. To do this,\nFastToG employs community detection for deeper correlation capture and two\nstages community pruning - coarse and fine pruning for faster retrieval.\nFurthermore, we also develop two Community-to-Text methods to convert the graph\nstructure of communities into textual form for better understanding by LLMs.\nExperimental results demonstrate the effectiveness of FastToG, showcasing\nhigher accuracy, faster reasoning, and better explainability compared to the\nprevious works."
                },
                "authors": [
                    {
                        "name": "Xujian Liang"
                    },
                    {
                        "name": "Zhaoquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoquan Gu"
                },
                "author": "Zhaoquan Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12334v3",
                "updated": "2025-01-24T07:40:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    40,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2024-06-18T06:59:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    59,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering"
                },
                "summary": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance."
                },
                "authors": [
                    {
                        "name": "Federico Errica"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14296v1",
                "updated": "2025-01-24T07:33:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    33,
                    39,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:33:39Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    33,
                    39,
                    4,
                    24,
                    0
                ],
                "title": "Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in\n  Relevance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in\n  Relevance Assessment"
                },
                "summary": "The effectiveness of search systems is evaluated using relevance labels that\nindicate the usefulness of documents for specific queries and users. While\nobtaining these relevance labels from real users is ideal, scaling such data\ncollection is challenging. Consequently, third-party annotators are employed,\nbut their inconsistent accuracy demands costly auditing, training, and\nmonitoring. We propose an LLM-based modular classification pipeline that\ndivides the relevance assessment task into multiple stages, each utilising\ndifferent prompts and models of varying sizes and capabilities. Applied to TREC\nDeep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff's\n$\\alpha$ accuracy increase over OpenAI's GPT-4o mini while maintaining a cost\nof about 0.2 USD per million input tokens, offering a more efficient and\nscalable solution for relevance assessment. This approach beats the baseline\nperformance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of\nthe GPT-4o flagship model, measured in $\\alpha$, could be improved by 9.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of search systems is evaluated using relevance labels that\nindicate the usefulness of documents for specific queries and users. While\nobtaining these relevance labels from real users is ideal, scaling such data\ncollection is challenging. Consequently, third-party annotators are employed,\nbut their inconsistent accuracy demands costly auditing, training, and\nmonitoring. We propose an LLM-based modular classification pipeline that\ndivides the relevance assessment task into multiple stages, each utilising\ndifferent prompts and models of varying sizes and capabilities. Applied to TREC\nDeep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff's\n$\\alpha$ accuracy increase over OpenAI's GPT-4o mini while maintaining a cost\nof about 0.2 USD per million input tokens, offering a more efficient and\nscalable solution for relevance assessment. This approach beats the baseline\nperformance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of\nthe GPT-4o flagship model, measured in $\\alpha$, could be improved by 9.7%."
                },
                "authors": [
                    {
                        "name": "Julian A. Schnabel"
                    },
                    {
                        "name": "Johanne R. Trippas"
                    },
                    {
                        "name": "Falk Scholer"
                    },
                    {
                        "name": "Danula Hettiachchi"
                    }
                ],
                "author_detail": {
                    "name": "Danula Hettiachchi"
                },
                "author": "Danula Hettiachchi",
                "arxiv_comment": "WebConf'25, WWW'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14294v1",
                "updated": "2025-01-24T07:24:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    24,
                    23,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:24:23Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    24,
                    23,
                    4,
                    24,
                    0
                ],
                "title": "Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes"
                },
                "summary": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, particularly when these systems fail to operate as\nintended. This study explores the challenge of aligning LLMs with human\nintentions and values, with specific focus on their political inclinations.\nPrevious research has highlighted LLMs' propensity to display political\nleanings, and their ability to mimic certain political parties' stances on\nvarious issues. However, the extent and conditions under which LLMs deviate\nfrom empirical positions have not been thoroughly examined. To address this\ngap, our study systematically investigates the factors contributing to LLMs'\ndeviations from empirical positions on political issues, aiming to quantify\nthese deviations and identify the conditions that cause them.\n  Drawing on cognitive science findings related to representativeness\nheuristics -- where individuals readily recall the representative attribute of\na target group in a way that leads to exaggerated beliefs -- we scrutinize LLM\nresponses through this heuristics lens. We conduct experiments to determine how\nLLMs exhibit stereotypes by inflating judgments in favor of specific political\nparties. Our results indicate that while LLMs can mimic certain political\nparties' positions, they often exaggerate these positions more than human\nrespondents do. Notably, LLMs tend to overemphasize representativeness to a\ngreater extent than humans. This study highlights the susceptibility of LLMs to\nrepresentativeness heuristics, suggeseting potential vulnerabilities to\npolitical stereotypes. We propose prompt-based mitigation strategies that\ndemonstrate effectiveness in reducing the influence of representativeness in\nLLM responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, particularly when these systems fail to operate as\nintended. This study explores the challenge of aligning LLMs with human\nintentions and values, with specific focus on their political inclinations.\nPrevious research has highlighted LLMs' propensity to display political\nleanings, and their ability to mimic certain political parties' stances on\nvarious issues. However, the extent and conditions under which LLMs deviate\nfrom empirical positions have not been thoroughly examined. To address this\ngap, our study systematically investigates the factors contributing to LLMs'\ndeviations from empirical positions on political issues, aiming to quantify\nthese deviations and identify the conditions that cause them.\n  Drawing on cognitive science findings related to representativeness\nheuristics -- where individuals readily recall the representative attribute of\na target group in a way that leads to exaggerated beliefs -- we scrutinize LLM\nresponses through this heuristics lens. We conduct experiments to determine how\nLLMs exhibit stereotypes by inflating judgments in favor of specific political\nparties. Our results indicate that while LLMs can mimic certain political\nparties' positions, they often exaggerate these positions more than human\nrespondents do. Notably, LLMs tend to overemphasize representativeness to a\ngreater extent than humans. This study highlights the susceptibility of LLMs to\nrepresentativeness heuristics, suggeseting potential vulnerabilities to\npolitical stereotypes. We propose prompt-based mitigation strategies that\ndemonstrate effectiveness in reducing the influence of representativeness in\nLLM responses."
                },
                "authors": [
                    {
                        "name": "Sullam Jeoung"
                    },
                    {
                        "name": "Yubin Ge"
                    },
                    {
                        "name": "Haohan Wang"
                    },
                    {
                        "name": "Jana Diesner"
                    }
                ],
                "author_detail": {
                    "name": "Jana Diesner"
                },
                "author": "Jana Diesner",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14291v1",
                "updated": "2025-01-24T07:13:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    13,
                    26,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T07:13:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    13,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches"
                },
                "summary": "Temporal point processes (TPPs) are stochastic process models used to\ncharacterize event sequences occurring in continuous time. Traditional\nstatistical TPPs have a long-standing history, with numerous models proposed\nand successfully applied across diverse domains. In recent years, advances in\ndeep learning have spurred the development of neural TPPs, enabling greater\nflexibility and expressiveness in capturing complex temporal dynamics. The\nemergence of large language models (LLMs) has further sparked excitement,\noffering new possibilities for modeling and analyzing event sequences by\nleveraging their rich contextual understanding. This survey presents a\ncomprehensive review of recent research on TPPs from three perspectives:\nBayesian, deep learning, and LLM approaches. We begin with a review of the\nfundamental concepts of TPPs, followed by an in-depth discussion of model\ndesign and parameter estimation techniques in these three frameworks. We also\nrevisit classic application areas of TPPs to highlight their practical\nrelevance. Finally, we outline challenges and promising directions for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal point processes (TPPs) are stochastic process models used to\ncharacterize event sequences occurring in continuous time. Traditional\nstatistical TPPs have a long-standing history, with numerous models proposed\nand successfully applied across diverse domains. In recent years, advances in\ndeep learning have spurred the development of neural TPPs, enabling greater\nflexibility and expressiveness in capturing complex temporal dynamics. The\nemergence of large language models (LLMs) has further sparked excitement,\noffering new possibilities for modeling and analyzing event sequences by\nleveraging their rich contextual understanding. This survey presents a\ncomprehensive review of recent research on TPPs from three perspectives:\nBayesian, deep learning, and LLM approaches. We begin with a review of the\nfundamental concepts of TPPs, followed by an in-depth discussion of model\ndesign and parameter estimation techniques in these three frameworks. We also\nrevisit classic application areas of TPPs to highlight their practical\nrelevance. Finally, we outline challenges and promising directions for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Quyu Kong"
                    },
                    {
                        "name": "Yixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Zhang"
                },
                "author": "Yixuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09688v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09688v4",
                "updated": "2025-01-24T07:10:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    7,
                    10,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-19T03:53:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts"
                },
                "summary": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task."
                },
                "authors": [
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09688v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09688v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09150v2",
                "updated": "2025-01-24T06:45:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    45,
                    3,
                    4,
                    24,
                    0
                ],
                "published": "2024-08-17T09:49:40Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    9,
                    49,
                    40,
                    5,
                    230,
                    0
                ],
                "title": "CogLM: Tracking Cognitive Development of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogLM: Tracking Cognitive Development of Large Language Models"
                },
                "summary": "Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) Human-like cognitive\nabilities have emerged in advanced LLMs (GPT-4), comparable to those of a\n20-year-old human. (2) The parameter size and optimization objective are two\nkey factors affecting the cognitive levels of LLMs. (3) The performance on\ndownstream tasks is positively correlated with the level of cognitive\nabilities. These findings fill the gap in research on the cognitive abilities\nof LLMs, tracing the development of LLMs from a cognitive perspective and\nguiding the future direction of their evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) Human-like cognitive\nabilities have emerged in advanced LLMs (GPT-4), comparable to those of a\n20-year-old human. (2) The parameter size and optimization objective are two\nkey factors affecting the cognitive levels of LLMs. (3) The performance on\ndownstream tasks is positively correlated with the level of cognitive\nabilities. These findings fill the gap in research on the cognitive abilities\nof LLMs, tracing the development of LLMs from a cognitive perspective and\nguiding the future direction of their evolution."
                },
                "authors": [
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Heda Wang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "arxiv_comment": "NAACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14275v1",
                "updated": "2025-01-24T06:39:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    39,
                    38,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T06:39:38Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    6,
                    39,
                    38,
                    4,
                    24,
                    0
                ],
                "title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and\n  Contamination-Resistant Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and\n  Contamination-Resistant Evaluation"
                },
                "summary": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops"
                },
                "authors": [
                    {
                        "name": "Sadegh Mahdavi"
                    },
                    {
                        "name": "Muchen Li"
                    },
                    {
                        "name": "Kaiwen Liu"
                    },
                    {
                        "name": "Christos Thrampoulidis"
                    },
                    {
                        "name": "Leonid Sigal"
                    },
                    {
                        "name": "Renjie Liao"
                    }
                ],
                "author_detail": {
                    "name": "Renjie Liao"
                },
                "author": "Renjie Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14257v1",
                "updated": "2025-01-24T05:53:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    53,
                    7,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T05:53:07Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    53,
                    7,
                    4,
                    24,
                    0
                ],
                "title": "C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic\n  Techniques"
                },
                "summary": "In recent years, there has been a lot of interest in converting C code to\nRust, to benefit from the memory and thread safety guarantees of Rust. C2Rust\nis a rule-based system that can automatically convert C code to functionally\nidentical Rust, but the Rust code that it produces is non-idiomatic, i.e.,\nmakes extensive use of unsafe Rust, a subset of the language that doesn't have\nmemory or thread safety guarantees. At the other end of the spectrum are LLMs,\nwhich produce idiomatic Rust code, but these have the potential to make\nmistakes and are constrained in the length of code they can process. In this\npaper, we present C2SaferRust, a novel approach to translate C to Rust that\ncombines the strengths of C2Rust and LLMs. We first use C2Rust to convert C\ncode to non-idiomatic, unsafe Rust. We then decompose the unsafe Rust code into\nslices that can be individually translated to safer Rust by an LLM. After\nprocessing each slice, we run end-to-end test cases to verify that the code\nstill functions as expected. We also contribute a benchmark of 7 real-world\nprograms, translated from C to unsafe Rust using C2Rust. Each of these programs\nalso comes with end-to-end test cases. On this benchmark, we are able to reduce\nthe number of raw pointers by up to 38%, and reduce the amount of unsafe code\nby up to 28%, indicating an increase in safety. The resulting programs still\npass all test cases. C2SaferRust also shows convincing gains in performance\nagainst two previous techniques for making Rust code safer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a lot of interest in converting C code to\nRust, to benefit from the memory and thread safety guarantees of Rust. C2Rust\nis a rule-based system that can automatically convert C code to functionally\nidentical Rust, but the Rust code that it produces is non-idiomatic, i.e.,\nmakes extensive use of unsafe Rust, a subset of the language that doesn't have\nmemory or thread safety guarantees. At the other end of the spectrum are LLMs,\nwhich produce idiomatic Rust code, but these have the potential to make\nmistakes and are constrained in the length of code they can process. In this\npaper, we present C2SaferRust, a novel approach to translate C to Rust that\ncombines the strengths of C2Rust and LLMs. We first use C2Rust to convert C\ncode to non-idiomatic, unsafe Rust. We then decompose the unsafe Rust code into\nslices that can be individually translated to safer Rust by an LLM. After\nprocessing each slice, we run end-to-end test cases to verify that the code\nstill functions as expected. We also contribute a benchmark of 7 real-world\nprograms, translated from C to unsafe Rust using C2Rust. Each of these programs\nalso comes with end-to-end test cases. On this benchmark, we are able to reduce\nthe number of raw pointers by up to 38%, and reduce the amount of unsafe code\nby up to 28%, indicating an increase in safety. The resulting programs still\npass all test cases. C2SaferRust also shows convincing gains in performance\nagainst two previous techniques for making Rust code safer."
                },
                "authors": [
                    {
                        "name": "Vikram Nitin"
                    },
                    {
                        "name": "Rahul Krishna"
                    },
                    {
                        "name": "Luiz Lemos do Valle"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12612v2",
                "updated": "2025-01-24T05:52:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    52,
                    51,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-17T07:21:25Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    21,
                    25,
                    1,
                    352,
                    0
                ],
                "title": "Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised\n  generation-verification framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised\n  generation-verification framework"
                },
                "summary": "Graph databases like Neo4j are gaining popularity for handling complex,\ninterconnected data, over traditional relational databases in modeling and\nquerying relationships. While translating natural language into SQL queries is\nwell-researched, generating Cypher queries for Neo4j remains relatively\nunderexplored. In this work, we present an automated, LLM-Supervised, pipeline\nto generate high-quality synthetic data for Text2Cypher. Our Cypher data\ngeneration pipeline introduces LLM-As-Database-Filler, a novel strategy for\nensuring Cypher query correctness, thus resulting in high quality generations.\nUsing our pipeline, we generate high quality Text2Cypher data - SynthCypher\ncontaining 29.8k instances across various domains and queries with varying\ncomplexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and\nQWEN-7B on SynthCypher results in performance gains of up to 40% on the\nText2Cypher test split and 30% on the SPIDER benchmark, adapted for graph\ndatabases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph databases like Neo4j are gaining popularity for handling complex,\ninterconnected data, over traditional relational databases in modeling and\nquerying relationships. While translating natural language into SQL queries is\nwell-researched, generating Cypher queries for Neo4j remains relatively\nunderexplored. In this work, we present an automated, LLM-Supervised, pipeline\nto generate high-quality synthetic data for Text2Cypher. Our Cypher data\ngeneration pipeline introduces LLM-As-Database-Filler, a novel strategy for\nensuring Cypher query correctness, thus resulting in high quality generations.\nUsing our pipeline, we generate high quality Text2Cypher data - SynthCypher\ncontaining 29.8k instances across various domains and queries with varying\ncomplexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and\nQWEN-7B on SynthCypher results in performance gains of up to 40% on the\nText2Cypher test split and 30% on the SPIDER benchmark, adapted for graph\ndatabases."
                },
                "authors": [
                    {
                        "name": "Aman Tiwari"
                    },
                    {
                        "name": "Shiva Krishna Reddy Malay"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Masoud Hashemi"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    }
                ],
                "author_detail": {
                    "name": "Sathwik Tejaswi Madhusudhan"
                },
                "author": "Sathwik Tejaswi Madhusudhan",
                "arxiv_comment": "Accepted at NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14253v1",
                "updated": "2025-01-24T05:36:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    36,
                    53,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T05:36:53Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    36,
                    53,
                    4,
                    24,
                    0
                ],
                "title": "Distributionally Robust Coreset Selection under Covariate Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributionally Robust Coreset Selection under Covariate Shift"
                },
                "summary": "Coreset selection, which involves selecting a small subset from an existing\ntraining dataset, is an approach to reducing training data, and various\napproaches have been proposed for this method. In practical situations where\nthese methods are employed, it is often the case that the data distributions\ndiffer between the development phase and the deployment phase, with the latter\nbeing unknown. Thus, it is challenging to select an effective subset of\ntraining data that performs well across all deployment scenarios. We therefore\npropose Distributionally Robust Coreset Selection (DRCS). DRCS theoretically\nderives an estimate of the upper bound for the worst-case test error, assuming\nthat the future covariate distribution may deviate within a defined range from\nthe training distribution. Furthermore, by selecting instances in a way that\nsuppresses the estimate of the upper bound for the worst-case test error, DRCS\nachieves distributionally robust training instance selection. This study is\nprimarily applicable to convex training computation, but we demonstrate that it\ncan also be applied to deep learning under appropriate approximations. In this\npaper, we focus on covariate shift, a type of data distribution shift, and\ndemonstrate the effectiveness of DRCS through experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coreset selection, which involves selecting a small subset from an existing\ntraining dataset, is an approach to reducing training data, and various\napproaches have been proposed for this method. In practical situations where\nthese methods are employed, it is often the case that the data distributions\ndiffer between the development phase and the deployment phase, with the latter\nbeing unknown. Thus, it is challenging to select an effective subset of\ntraining data that performs well across all deployment scenarios. We therefore\npropose Distributionally Robust Coreset Selection (DRCS). DRCS theoretically\nderives an estimate of the upper bound for the worst-case test error, assuming\nthat the future covariate distribution may deviate within a defined range from\nthe training distribution. Furthermore, by selecting instances in a way that\nsuppresses the estimate of the upper bound for the worst-case test error, DRCS\nachieves distributionally robust training instance selection. This study is\nprimarily applicable to convex training computation, but we demonstrate that it\ncan also be applied to deep learning under appropriate approximations. In this\npaper, we focus on covariate shift, a type of data distribution shift, and\ndemonstrate the effectiveness of DRCS through experiments."
                },
                "authors": [
                    {
                        "name": "Tomonari Tanaka"
                    },
                    {
                        "name": "Hiroyuki Hanada"
                    },
                    {
                        "name": "Hanting Yang"
                    },
                    {
                        "name": "Tatsuya Aoyama"
                    },
                    {
                        "name": "Yu Inatsu"
                    },
                    {
                        "name": "Satoshi Akahane"
                    },
                    {
                        "name": "Yoshito Okura"
                    },
                    {
                        "name": "Noriaki Hashimoto"
                    },
                    {
                        "name": "Taro Murayama"
                    },
                    {
                        "name": "Hanju Lee"
                    },
                    {
                        "name": "Shinya Kojima"
                    },
                    {
                        "name": "Ichiro Takeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Takeuchi"
                },
                "author": "Ichiro Takeuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14250v1",
                "updated": "2025-01-24T05:31:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    31,
                    27,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T05:31:27Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    31,
                    27,
                    4,
                    24,
                    0
                ],
                "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating\n  Real-World Human Jailbreak Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating\n  Real-World Human Jailbreak Behaviors"
                },
                "summary": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Youzhi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youzhi Zhang"
                },
                "author": "Youzhi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14249v1",
                "updated": "2025-01-24T05:27:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    27,
                    46,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T05:27:46Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    27,
                    46,
                    4,
                    24,
                    0
                ],
                "title": "Humanity's Last Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanity's Last Exam"
                },
                "summary": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Josephina Hu"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Sean Shi"
                    },
                    {
                        "name": "Michael Choi"
                    },
                    {
                        "name": "Anish Agrawal"
                    },
                    {
                        "name": "Arnav Chopra"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Ryan Kim"
                    },
                    {
                        "name": "Jason Hausenloy"
                    },
                    {
                        "name": "Oliver Zhang"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Daron Anderson"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Mobeen Mahmood"
                    },
                    {
                        "name": "Fiona Feng"
                    },
                    {
                        "name": "Steven Y. Feng"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Michael Yu"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Chelsea Zou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jessica P. Wang"
                    },
                    {
                        "name": "Pawan Kumar"
                    },
                    {
                        "name": "Oleksandr Pokutnyi"
                    },
                    {
                        "name": "Robert Gerbicz"
                    },
                    {
                        "name": "Serguei Popov"
                    },
                    {
                        "name": "John-Clark Levin"
                    },
                    {
                        "name": "Mstyslav Kazakov"
                    },
                    {
                        "name": "Johannes Schmitt"
                    },
                    {
                        "name": "Geoff Galgon"
                    },
                    {
                        "name": "Alvaro Sanchez"
                    },
                    {
                        "name": "Yongki Lee"
                    },
                    {
                        "name": "Will Yeadon"
                    },
                    {
                        "name": "Scott Sauers"
                    },
                    {
                        "name": "Marc Roth"
                    },
                    {
                        "name": "Chidozie Agu"
                    },
                    {
                        "name": "Søren Riis"
                    },
                    {
                        "name": "Fabian Giska"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Zachary Giboney"
                    },
                    {
                        "name": "Gashaw M. Goshu"
                    },
                    {
                        "name": "Joan of Arc Xavier"
                    },
                    {
                        "name": "Sarah-Jane Crowson"
                    },
                    {
                        "name": "Mohinder Maheshbhai Naiya"
                    },
                    {
                        "name": "Noah Burns"
                    },
                    {
                        "name": "Lennart Finke"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Hyunwoo Park"
                    },
                    {
                        "name": "Francesco Fournier-Facio"
                    },
                    {
                        "name": "John Wydallis"
                    },
                    {
                        "name": "Mark Nandor"
                    },
                    {
                        "name": "Ankit Singh"
                    },
                    {
                        "name": "Tim Gehrunger"
                    },
                    {
                        "name": "Jiaqi Cai"
                    },
                    {
                        "name": "Ben McCarty"
                    },
                    {
                        "name": "Darling Duclosel"
                    },
                    {
                        "name": "Jungbae Nam"
                    },
                    {
                        "name": "Jennifer Zampese"
                    },
                    {
                        "name": "Ryan G. Hoerr"
                    },
                    {
                        "name": "Aras Bacho"
                    },
                    {
                        "name": "Gautier Abou Loume"
                    },
                    {
                        "name": "Abdallah Galal"
                    },
                    {
                        "name": "Hangrui Cao"
                    },
                    {
                        "name": "Alexis C Garretson"
                    },
                    {
                        "name": "Damien Sileo"
                    },
                    {
                        "name": "Qiuyu Ren"
                    },
                    {
                        "name": "Doru Cojoc"
                    },
                    {
                        "name": "Pavel Arkhipov"
                    },
                    {
                        "name": "Usman Qazi"
                    },
                    {
                        "name": "Lianghui Li"
                    },
                    {
                        "name": "Sumeet Motwani"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Edwin Taylor"
                    },
                    {
                        "name": "Johannes Veith"
                    },
                    {
                        "name": "Eric Singer"
                    },
                    {
                        "name": "Taylor D. Hartman"
                    },
                    {
                        "name": "Paolo Rissone"
                    },
                    {
                        "name": "Jaehyeok Jin"
                    },
                    {
                        "name": "Jack Wei Lun Shi"
                    },
                    {
                        "name": "Chris G. Willcocks"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Aleksandar Mikov"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Longke Tang"
                    },
                    {
                        "name": "Xavier Alapont"
                    },
                    {
                        "name": "Justine Leon Uro"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "Emily de Oliveira Santos"
                    },
                    {
                        "name": "Andrey Pupasov Maksimov"
                    },
                    {
                        "name": "Edward Vendrow"
                    },
                    {
                        "name": "Kengo Zenitani"
                    },
                    {
                        "name": "Julien Guillod"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Joshua Vendrow"
                    },
                    {
                        "name": "Vladyslav Kuchkin"
                    },
                    {
                        "name": "Ng Ze-An"
                    },
                    {
                        "name": "Pierre Marion"
                    },
                    {
                        "name": "Denis Efremov"
                    },
                    {
                        "name": "Jayson Lynch"
                    },
                    {
                        "name": "Kaiqu Liang"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Dakotah Martinez"
                    },
                    {
                        "name": "Ben Pageler"
                    },
                    {
                        "name": "Nick Crispino"
                    },
                    {
                        "name": "Dimitri Zvonkine"
                    },
                    {
                        "name": "Natanael Wildner Fraga"
                    },
                    {
                        "name": "Saeed Soori"
                    },
                    {
                        "name": "Ori Press"
                    },
                    {
                        "name": "Henry Tang"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Sean R. Green"
                    },
                    {
                        "name": "Lina Brüssel"
                    },
                    {
                        "name": "Moon Twayana"
                    },
                    {
                        "name": "Aymeric Dieuleveut"
                    },
                    {
                        "name": "T. Ryan Rogers"
                    },
                    {
                        "name": "Wenjin Zhang"
                    },
                    {
                        "name": "Bikun Li"
                    },
                    {
                        "name": "Jinzhou Yang"
                    },
                    {
                        "name": "Arun Rao"
                    },
                    {
                        "name": "Gabriel Loiseau"
                    },
                    {
                        "name": "Mikhail Kalinin"
                    },
                    {
                        "name": "Marco Lukas"
                    },
                    {
                        "name": "Ciprian Manolescu"
                    },
                    {
                        "name": "Subrata Mishra"
                    },
                    {
                        "name": "Ariel Ghislain Kemogne Kamdoum"
                    },
                    {
                        "name": "Tobias Kreiman"
                    },
                    {
                        "name": "Tad Hogg"
                    },
                    {
                        "name": "Alvin Jin"
                    },
                    {
                        "name": "Carlo Bosio"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Brian P Coppola"
                    },
                    {
                        "name": "Tim Tarver"
                    },
                    {
                        "name": "Haline Heidinger"
                    },
                    {
                        "name": "Rafael Sayous"
                    },
                    {
                        "name": "Stefan Ivanov"
                    },
                    {
                        "name": "Joseph M Cavanagh"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Joseph Marvin Imperial"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Shaipranesh Senthilkuma"
                    },
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "Ali Dehghan"
                    },
                    {
                        "name": "Andres Algaba"
                    },
                    {
                        "name": "Brecht Verbeken"
                    },
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Ragavendran P V"
                    },
                    {
                        "name": "Lisa Schut"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Evgenii Zheltonozhskii"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Richard Stanley"
                    },
                    {
                        "name": "Shankar Sivarajan"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "John Maar"
                    },
                    {
                        "name": "Julian Wykowski"
                    },
                    {
                        "name": "Martí Oller"
                    },
                    {
                        "name": "Jennifer Sandlin"
                    },
                    {
                        "name": "Anmol Sahu"
                    },
                    {
                        "name": "Yuzheng Hu"
                    },
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Nasser Heydari"
                    },
                    {
                        "name": "Archimedes Apronti"
                    },
                    {
                        "name": "Kaivalya Rawal"
                    },
                    {
                        "name": "Tobias Garcia Vilchis"
                    },
                    {
                        "name": "Yuexuan Zu"
                    },
                    {
                        "name": "Martin Lackner"
                    },
                    {
                        "name": "James Koppel"
                    },
                    {
                        "name": "Jeremy Nguyen"
                    },
                    {
                        "name": "Daniil S. Antonenko"
                    },
                    {
                        "name": "Steffi Chern"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Pierrot Arsene"
                    },
                    {
                        "name": "Alan Goldfarb"
                    },
                    {
                        "name": "Sergey Ivanov"
                    },
                    {
                        "name": "Rafał Poświata"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Daofeng Li"
                    },
                    {
                        "name": "Donato Crisostomi"
                    },
                    {
                        "name": "Andrea Achilleos"
                    },
                    {
                        "name": "Benjamin Myklebust"
                    },
                    {
                        "name": "Archan Sen"
                    },
                    {
                        "name": "David Perrella"
                    },
                    {
                        "name": "Nurdin Kaparov"
                    },
                    {
                        "name": "Mark H Inlow"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Elliott Thornley"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Vladislav Poritski"
                    },
                    {
                        "name": "Shalev Ben-David"
                    },
                    {
                        "name": "Zachary Berger"
                    },
                    {
                        "name": "Parker Whitfill"
                    },
                    {
                        "name": "Michael Foster"
                    },
                    {
                        "name": "Daniel Munro"
                    },
                    {
                        "name": "Linh Ho"
                    },
                    {
                        "name": "Dan Bar Hava"
                    },
                    {
                        "name": "Aleksey Kuchkin"
                    },
                    {
                        "name": "Robert Lauff"
                    },
                    {
                        "name": "David Holmes"
                    },
                    {
                        "name": "Frank Sommerhage"
                    },
                    {
                        "name": "Keith Schneider"
                    },
                    {
                        "name": "Zakayo Kazibwe"
                    },
                    {
                        "name": "Nate Stambaugh"
                    },
                    {
                        "name": "Mukhwinder Singh"
                    },
                    {
                        "name": "Ilias Magoulas"
                    },
                    {
                        "name": "Don Clarke"
                    },
                    {
                        "name": "Dae Hyun Kim"
                    },
                    {
                        "name": "Felipe Meneguitti Dias"
                    },
                    {
                        "name": "Veit Elser"
                    },
                    {
                        "name": "Kanu Priya Agarwal"
                    },
                    {
                        "name": "Victor Efren Guadarrama Vilchis"
                    },
                    {
                        "name": "Immo Klose"
                    },
                    {
                        "name": "Christoph Demian"
                    },
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Guglielmo Albani"
                    },
                    {
                        "name": "Jeffery Li"
                    },
                    {
                        "name": "Nicolas Daans"
                    },
                    {
                        "name": "Maksim Radionov"
                    },
                    {
                        "name": "Václav Rozhoň"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Christian Stump"
                    },
                    {
                        "name": "Mohammed Berkani"
                    },
                    {
                        "name": "Jacob Platnick"
                    },
                    {
                        "name": "Volodymyr Nevirkovets"
                    },
                    {
                        "name": "Luke Basler"
                    },
                    {
                        "name": "Marco Piccardo"
                    },
                    {
                        "name": "Ferenc Jeanplong"
                    },
                    {
                        "name": "Niv Cohen"
                    },
                    {
                        "name": "Josef Tkadlec"
                    },
                    {
                        "name": "Paul Rosu"
                    },
                    {
                        "name": "Piotr Padlewski"
                    },
                    {
                        "name": "Stanislaw Barzowski"
                    },
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "Aline Menezes"
                    },
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jamie Tucker-Foltz"
                    },
                    {
                        "name": "Jack Stade"
                    },
                    {
                        "name": "Tom Goertzen"
                    },
                    {
                        "name": "Fereshteh Kazemi"
                    },
                    {
                        "name": "Jeremiah Milbauer"
                    },
                    {
                        "name": "John Arnold Ambay"
                    },
                    {
                        "name": "Abhishek Shukla"
                    },
                    {
                        "name": "Yan Carlos Leyva Labrador"
                    },
                    {
                        "name": "Alan Givré"
                    },
                    {
                        "name": "Hew Wolff"
                    },
                    {
                        "name": "Vivien Rossbach"
                    },
                    {
                        "name": "Muhammad Fayez Aziz"
                    },
                    {
                        "name": "Younesse Kaddar"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Robin Zhang"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Antonio Terpin"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Eric Zheng"
                    },
                    {
                        "name": "Avishy Carmi"
                    },
                    {
                        "name": "Adam Jones"
                    },
                    {
                        "name": "Jainam Shah"
                    },
                    {
                        "name": "Ethan D. L. Brown"
                    },
                    {
                        "name": "Kelin Zhu"
                    },
                    {
                        "name": "Max Bartolo"
                    },
                    {
                        "name": "Richard Wheeler"
                    },
                    {
                        "name": "Andrew Ho"
                    },
                    {
                        "name": "Shaul Barkan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Martin Stehberger"
                    },
                    {
                        "name": "Egor Kretov"
                    },
                    {
                        "name": "Kaustubh Sridhar"
                    },
                    {
                        "name": "Zienab EL-Wasif"
                    },
                    {
                        "name": "Anji Zhang"
                    },
                    {
                        "name": "Daniel Pyda"
                    },
                    {
                        "name": "Joanna Tam"
                    },
                    {
                        "name": "David M. Cunningham"
                    },
                    {
                        "name": "Vladimir Goryachev"
                    },
                    {
                        "name": "Demosthenes Patramanis"
                    },
                    {
                        "name": "Michael Krause"
                    },
                    {
                        "name": "Andrew Redenti"
                    },
                    {
                        "name": "Daniel Bugas"
                    },
                    {
                        "name": "David Aldous"
                    },
                    {
                        "name": "Jesyin Lai"
                    },
                    {
                        "name": "Shannon Coleman"
                    },
                    {
                        "name": "Mohsen Bahaloo"
                    },
                    {
                        "name": "Jiangnan Xu"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Sandy Zhao"
                    },
                    {
                        "name": "Ning Tang"
                    },
                    {
                        "name": "Michael K. Cohen"
                    },
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Orr Paradise"
                    },
                    {
                        "name": "Jan Hendrik Kirchner"
                    },
                    {
                        "name": "Stefan Steinerberger"
                    },
                    {
                        "name": "Maksym Ovchynnikov"
                    },
                    {
                        "name": "Jason O. Matos"
                    },
                    {
                        "name": "Adithya Shenoy"
                    },
                    {
                        "name": "Benedito Alves de Oliveira Junior"
                    },
                    {
                        "name": "Michael Wang"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Paolo Giordano"
                    },
                    {
                        "name": "Philipp Petersen"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Priti Shukla"
                    },
                    {
                        "name": "Jonathan Crozier"
                    },
                    {
                        "name": "Antonella Pinto"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Prashant Joshi"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Allison Tee"
                    },
                    {
                        "name": "Jérémy Andréoletti"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Alexander Ivanov"
                    },
                    {
                        "name": "Seri Khoury"
                    },
                    {
                        "name": "Hamid Mostaghimi"
                    },
                    {
                        "name": "Kunvar Thaman"
                    },
                    {
                        "name": "Qijia Chen"
                    },
                    {
                        "name": "Tran Quoc Khánh"
                    },
                    {
                        "name": "Jacob Loader"
                    },
                    {
                        "name": "Stefano Cavalleri"
                    },
                    {
                        "name": "Hannah Szlyk"
                    },
                    {
                        "name": "Zachary Brown"
                    },
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "William Alley"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Ryan Stendall"
                    },
                    {
                        "name": "Max Lamparth"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Hanmeng Xu"
                    },
                    {
                        "name": "Sreenivas Goud Raparthi"
                    },
                    {
                        "name": "Pablo Hernández-Cámara"
                    },
                    {
                        "name": "Freddie Martin"
                    },
                    {
                        "name": "Dmitry Malishev"
                    },
                    {
                        "name": "Thomas Preu"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Marcus Abramovitch"
                    },
                    {
                        "name": "Dominic Williamson"
                    },
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Biró Bálint"
                    },
                    {
                        "name": "M Saiful Bari"
                    },
                    {
                        "name": "Peyman Kassani"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Behzad Ansarinejad"
                    },
                    {
                        "name": "Laxman Prasad Goswami"
                    },
                    {
                        "name": "Yewen Sun"
                    },
                    {
                        "name": "Hossam Elgnainy"
                    },
                    {
                        "name": "Daniel Tordera"
                    },
                    {
                        "name": "George Balabanian"
                    },
                    {
                        "name": "Earth Anderson"
                    },
                    {
                        "name": "Lynna Kvistad"
                    },
                    {
                        "name": "Alejandro José Moyano"
                    },
                    {
                        "name": "Rajat Maheshwari"
                    },
                    {
                        "name": "Ahmad Sakor"
                    },
                    {
                        "name": "Murat Eron"
                    },
                    {
                        "name": "Isaac C. McAlister"
                    },
                    {
                        "name": "Javier Gimenez"
                    },
                    {
                        "name": "Innocent Enyekwe"
                    },
                    {
                        "name": "Andrew Favre D. O."
                    },
                    {
                        "name": "Shailesh Shah"
                    },
                    {
                        "name": "Xiaoxiang Zhou"
                    },
                    {
                        "name": "Firuz Kamalov"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Sherwin Abdoli"
                    },
                    {
                        "name": "Tim Santens"
                    },
                    {
                        "name": "Khalida Meer"
                    },
                    {
                        "name": "Harrison K Wang"
                    },
                    {
                        "name": "Kalyan Ramakrishnan"
                    },
                    {
                        "name": "Evan Chen"
                    },
                    {
                        "name": "Alessandro Tomasiello"
                    },
                    {
                        "name": "G. Bruno De Luca"
                    },
                    {
                        "name": "Shi-Zhuo Looi"
                    },
                    {
                        "name": "Vinh-Kha Le"
                    },
                    {
                        "name": "Noam Kolt"
                    },
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Avi Semler"
                    },
                    {
                        "name": "Emma Rodman"
                    },
                    {
                        "name": "Jacob Drori"
                    },
                    {
                        "name": "Carl J Fossum"
                    },
                    {
                        "name": "Milind Jagota"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Honglu Fan"
                    },
                    {
                        "name": "Tej Shah"
                    },
                    {
                        "name": "Jonathan Eicher"
                    },
                    {
                        "name": "Michael Chen"
                    },
                    {
                        "name": "Kushal Thaman"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Carter Harris"
                    },
                    {
                        "name": "Jason Gross"
                    },
                    {
                        "name": "Ilya Gusev"
                    },
                    {
                        "name": "Asankhaya Sharma"
                    },
                    {
                        "name": "Shashank Agnihotri"
                    },
                    {
                        "name": "Pavel Zhelnov"
                    },
                    {
                        "name": "Siranut Usawasutsakorn"
                    },
                    {
                        "name": "Mohammadreza Mofayezi"
                    },
                    {
                        "name": "Sergei Bogdanov"
                    },
                    {
                        "name": "Alexander Piperski"
                    },
                    {
                        "name": "Marc Carauleanu"
                    },
                    {
                        "name": "David K. Zhang"
                    },
                    {
                        "name": "Dylan Ler"
                    },
                    {
                        "name": "Roman Leventov"
                    },
                    {
                        "name": "Ignat Soroko"
                    },
                    {
                        "name": "Thorben Jansen"
                    },
                    {
                        "name": "Pascal Lauer"
                    },
                    {
                        "name": "Joshua Duersch"
                    },
                    {
                        "name": "Vage Taamazyan"
                    },
                    {
                        "name": "Wiktor Morak"
                    },
                    {
                        "name": "Wenjie Ma"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Tran Đuc Huy"
                    },
                    {
                        "name": "Ruicheng Xian"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Mohanad Mohamed"
                    },
                    {
                        "name": "Julian Noah Leser"
                    },
                    {
                        "name": "Michelle X Yuan"
                    },
                    {
                        "name": "Laila Yacar"
                    },
                    {
                        "name": "Johannes Lengler"
                    },
                    {
                        "name": "Hossein Shahrtash"
                    },
                    {
                        "name": "Edson Oliveira"
                    },
                    {
                        "name": "Joseph W. Jackson"
                    },
                    {
                        "name": "Daniel Espinosa Gonzalez"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Muthu Chidambaram"
                    },
                    {
                        "name": "Timothy Manik"
                    },
                    {
                        "name": "Hector Haffenden"
                    },
                    {
                        "name": "Dashiell Stander"
                    },
                    {
                        "name": "Ali Dasouqi"
                    },
                    {
                        "name": "Alexander Shen"
                    },
                    {
                        "name": "Emilien Duc"
                    },
                    {
                        "name": "Bita Golshani"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Mikalai Uzhou"
                    },
                    {
                        "name": "Alina Borisovna Zhidkovskaya"
                    },
                    {
                        "name": "Lukas Lewark"
                    },
                    {
                        "name": "Mátyás Vincze"
                    },
                    {
                        "name": "Dustin Wehr"
                    },
                    {
                        "name": "Colin Tang"
                    },
                    {
                        "name": "Zaki Hossain"
                    },
                    {
                        "name": "Shaun Phillips"
                    },
                    {
                        "name": "Jiang Muzhen"
                    },
                    {
                        "name": "Fredrik Ekström"
                    },
                    {
                        "name": "Angela Hammon"
                    },
                    {
                        "name": "Oam Patel"
                    },
                    {
                        "name": "Nicolas Remy"
                    },
                    {
                        "name": "Faraz Farhidi"
                    },
                    {
                        "name": "George Medley"
                    },
                    {
                        "name": "Forough Mohammadzadeh"
                    },
                    {
                        "name": "Madellene Peñaflor"
                    },
                    {
                        "name": "Haile Kassahun"
                    },
                    {
                        "name": "Alena Friedrich"
                    },
                    {
                        "name": "Claire Sparrow"
                    },
                    {
                        "name": "Taom Sakal"
                    },
                    {
                        "name": "Omkar Dhamane"
                    },
                    {
                        "name": "Ali Khajegili Mirabadi"
                    },
                    {
                        "name": "Eric Hallman"
                    },
                    {
                        "name": "Mike Battaglia"
                    },
                    {
                        "name": "Mohammad Maghsoudimehrabani"
                    },
                    {
                        "name": "Hieu Hoang"
                    },
                    {
                        "name": "Alon Amit"
                    },
                    {
                        "name": "Dave Hulbert"
                    },
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Simon Weber"
                    },
                    {
                        "name": "Stephen Mensah"
                    },
                    {
                        "name": "Nathan Andre"
                    },
                    {
                        "name": "Anton Peristyy"
                    },
                    {
                        "name": "Chris Harjadi"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Stephen Malina"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "Mustafa Mehkary"
                    },
                    {
                        "name": "Frank Reidegeld"
                    },
                    {
                        "name": "Anna-Katharina Dick"
                    },
                    {
                        "name": "Cary Friday"
                    },
                    {
                        "name": "Jasdeep Sidhu"
                    },
                    {
                        "name": "Wanyoung Kim"
                    },
                    {
                        "name": "Mariana Costa"
                    },
                    {
                        "name": "Hubeyb Gurdogan"
                    },
                    {
                        "name": "Brian Weber"
                    },
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "Tong Jiang"
                    },
                    {
                        "name": "Arunim Agarwal"
                    },
                    {
                        "name": "Chiara Ceconello"
                    },
                    {
                        "name": "Warren S. Vaz"
                    },
                    {
                        "name": "Chao Zhuang"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Andrew R. Tawfeek"
                    },
                    {
                        "name": "Daattavya Aggarwal"
                    },
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Linjie Dai"
                    },
                    {
                        "name": "Evan Kim"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Yuzhou Wang"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Krzysztof Burdzy"
                    },
                    {
                        "name": "Lixin Zhang"
                    },
                    {
                        "name": "Antonio Franca"
                    },
                    {
                        "name": "Diana T. Pham"
                    },
                    {
                        "name": "Kang Yong Loh"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Shreen Gul"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Zhehang Du"
                    },
                    {
                        "name": "Adrian Cosma"
                    },
                    {
                        "name": "Colin White"
                    },
                    {
                        "name": "Robin Riblet"
                    },
                    {
                        "name": "Prajvi Saxena"
                    },
                    {
                        "name": "Jacob Votava"
                    },
                    {
                        "name": "Vladimir Vinnikov"
                    },
                    {
                        "name": "Ethan Delaney"
                    },
                    {
                        "name": "Shiv Halasyamani"
                    },
                    {
                        "name": "Syed M. Shahid"
                    },
                    {
                        "name": "Jean-Christophe Mourrat"
                    },
                    {
                        "name": "Lavr Vetoshkin"
                    },
                    {
                        "name": "Renas Bacho"
                    },
                    {
                        "name": "Vincent Ginis"
                    },
                    {
                        "name": "Aleksandr Maksapetyan"
                    },
                    {
                        "name": "Florencia de la Rosa"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Guillaume Malod"
                    },
                    {
                        "name": "Leon Lang"
                    },
                    {
                        "name": "Julien Laurendeau"
                    },
                    {
                        "name": "Fatimah Adesanya"
                    },
                    {
                        "name": "Julien Portier"
                    },
                    {
                        "name": "Lawrence Hollom"
                    },
                    {
                        "name": "Victor Souza"
                    },
                    {
                        "name": "Yuchen Anna Zhou"
                    },
                    {
                        "name": "Yiğit Yalın"
                    },
                    {
                        "name": "Gbenga Daniel Obikoya"
                    },
                    {
                        "name": "Luca Arnaboldi"
                    },
                    {
                        "name": "Rai"
                    },
                    {
                        "name": "Filippo Bigi"
                    },
                    {
                        "name": "Kaniuar Bacho"
                    },
                    {
                        "name": "Pierre Clavier"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Mara Popescu"
                    },
                    {
                        "name": "Nikita Shulga"
                    },
                    {
                        "name": "Ngefor Mildred Tanwie"
                    },
                    {
                        "name": "Thomas C. H. Lux"
                    },
                    {
                        "name": "Ben Rank"
                    },
                    {
                        "name": "Colin Ni"
                    },
                    {
                        "name": "Alesia Yakimchyk"
                    },
                    {
                        "name": "Huanxu"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Olle Häggström"
                    },
                    {
                        "name": "Emil Verkama"
                    },
                    {
                        "name": "Himanshu Narayan"
                    },
                    {
                        "name": "Hans Gundlach"
                    },
                    {
                        "name": "Leonor Brito-Santana"
                    },
                    {
                        "name": "Brian Amaro"
                    },
                    {
                        "name": "Vivek Vajipey"
                    },
                    {
                        "name": "Rynaa Grover"
                    },
                    {
                        "name": "Yiyang Fan"
                    },
                    {
                        "name": "Gabriel Poesia Reis e Silva"
                    },
                    {
                        "name": "Linwei Xin"
                    },
                    {
                        "name": "Yosi Kratish"
                    },
                    {
                        "name": "Jakub Łucki"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Kevin Joseph Scaria"
                    },
                    {
                        "name": "Freddie Vargus"
                    },
                    {
                        "name": "Farzad Habibi"
                    },
                    {
                        "name": "Long"
                    },
                    {
                        "name": "Lian"
                    },
                    {
                        "name": "Emanuele Rodolà"
                    },
                    {
                        "name": "Jules Robins"
                    },
                    {
                        "name": "Vincent Cheng"
                    },
                    {
                        "name": "Declan Grabb"
                    },
                    {
                        "name": "Ida Bosio"
                    },
                    {
                        "name": "Tony Fruhauff"
                    },
                    {
                        "name": "Ido Akov"
                    },
                    {
                        "name": "Eve J. Y. Lo"
                    },
                    {
                        "name": "Hao Qi"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Ben Segev"
                    },
                    {
                        "name": "Jingxuan Fan"
                    },
                    {
                        "name": "Sarah Martinson"
                    },
                    {
                        "name": "Erik Y. Wang"
                    },
                    {
                        "name": "Kaylie Hausknecht"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Mao Mao"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "David Avagian"
                    },
                    {
                        "name": "Eshawn Jessica Scipio"
                    },
                    {
                        "name": "Muhammad Rehan Siddiqi"
                    },
                    {
                        "name": "Alon Ragoler"
                    },
                    {
                        "name": "Justin Tan"
                    },
                    {
                        "name": "Deepakkumar Patil"
                    },
                    {
                        "name": "Rebeka Plecnik"
                    },
                    {
                        "name": "Aaron Kirtland"
                    },
                    {
                        "name": "Roselynn Grace Montecillo"
                    },
                    {
                        "name": "Stephane Durand"
                    },
                    {
                        "name": "Omer Faruk Bodur"
                    },
                    {
                        "name": "Zahra Adoul"
                    },
                    {
                        "name": "Mohamed Zekry"
                    },
                    {
                        "name": "Guillaume Douville"
                    },
                    {
                        "name": "Ali Karakoc"
                    },
                    {
                        "name": "Tania C. B. Santos"
                    },
                    {
                        "name": "Samir Shamseldeen"
                    },
                    {
                        "name": "Loukmane Karim"
                    },
                    {
                        "name": "Anna Liakhovitskaia"
                    },
                    {
                        "name": "Nate Resman"
                    },
                    {
                        "name": "Nicholas Farina"
                    },
                    {
                        "name": "Juan Carlos Gonzalez"
                    },
                    {
                        "name": "Gabe Maayan"
                    },
                    {
                        "name": "Sarah Hoback"
                    },
                    {
                        "name": "Rodrigo De Oliveira Pena"
                    },
                    {
                        "name": "Glen Sherman"
                    },
                    {
                        "name": "Hodjat Mariji"
                    },
                    {
                        "name": "Rasoul Pouriamanesh"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Gözdenur Demir"
                    },
                    {
                        "name": "Sandra Mendoza"
                    },
                    {
                        "name": "Ismail Alarab"
                    },
                    {
                        "name": "Joshua Cole"
                    },
                    {
                        "name": "Danyelle Ferreira"
                    },
                    {
                        "name": "Bryan Johnson"
                    },
                    {
                        "name": "Hsiaoyun Milliron"
                    },
                    {
                        "name": "Mohammad Safdari"
                    },
                    {
                        "name": "Liangti Dai"
                    },
                    {
                        "name": "Siriphan Arthornthurasuk"
                    },
                    {
                        "name": "Alexey Pronin"
                    },
                    {
                        "name": "Jing Fan"
                    },
                    {
                        "name": "Angel Ramirez-Trinidad"
                    },
                    {
                        "name": "Ashley Cartwright"
                    },
                    {
                        "name": "Daphiny Pottmaier"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "David Outevsky"
                    },
                    {
                        "name": "Stanley Stepanic"
                    },
                    {
                        "name": "Samuel Perry"
                    },
                    {
                        "name": "Luke Askew"
                    },
                    {
                        "name": "Raúl Adrián Huerta Rodríguez"
                    },
                    {
                        "name": "Abdelkader Dendane"
                    },
                    {
                        "name": "Sam Ali"
                    },
                    {
                        "name": "Ricardo Lorena"
                    },
                    {
                        "name": "Krishnamurthy Iyer"
                    },
                    {
                        "name": "Sk Md Salauddin"
                    },
                    {
                        "name": "Murat Islam"
                    },
                    {
                        "name": "Juan Gonzalez"
                    },
                    {
                        "name": "Josh Ducey"
                    },
                    {
                        "name": "Russell Campbell"
                    },
                    {
                        "name": "Maja Somrak"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Eric Vergo"
                    },
                    {
                        "name": "Juehang Qin"
                    },
                    {
                        "name": "Benjámin Borbás"
                    },
                    {
                        "name": "Eric Chu"
                    },
                    {
                        "name": "Jack Lindsey"
                    },
                    {
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "name": "Antoine Jallon"
                    },
                    {
                        "name": "I. M. J. McInnis"
                    },
                    {
                        "name": "Alex Hoover"
                    },
                    {
                        "name": "Sören Möller"
                    },
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "John Lai"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Alexandr Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "arxiv_affiliation": "Tony",
                "author": "Dan Hendrycks",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07796v2",
                "updated": "2025-01-24T05:24:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    24,
                    14,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-12T06:56:52Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    56,
                    52,
                    3,
                    256,
                    0
                ],
                "title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for\n  Efficient Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for\n  Efficient Adaptation"
                },
                "summary": "Resource-constrained IoT devices increasingly rely on deep learning models\nfor inference tasks in remote environments. However, these models experience\nsignificant accuracy drops due to domain shifts when encountering variations in\nlighting, weather, and seasonal conditions. While cloud-based retraining can\naddress this issue, many IoT deployments operate with limited connectivity and\nenergy constraints, making traditional fine-tuning approaches impractical. We\nexplore this challenge through the lens of wildlife ecology, where camera traps\nmust maintain accurate species classification across changing seasons, weather,\nand habitats without reliable connectivity. We introduce WildFit, an autonomous\nin-situ adaptation framework that leverages the key insight that background\nscenes change more frequently than the visual characteristics of monitored\nspecies. WildFit combines background-aware synthesis to generate training\nsamples on-device with drift-aware fine-tuning that triggers model updates only\nwhen necessary to conserve resources. Through extensive evaluation on multiple\ncamera trap deployments, we demonstrate that WildFit significantly improves\naccuracy while greatly reducing adaptation overhead compared to traditional\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-constrained IoT devices increasingly rely on deep learning models\nfor inference tasks in remote environments. However, these models experience\nsignificant accuracy drops due to domain shifts when encountering variations in\nlighting, weather, and seasonal conditions. While cloud-based retraining can\naddress this issue, many IoT deployments operate with limited connectivity and\nenergy constraints, making traditional fine-tuning approaches impractical. We\nexplore this challenge through the lens of wildlife ecology, where camera traps\nmust maintain accurate species classification across changing seasons, weather,\nand habitats without reliable connectivity. We introduce WildFit, an autonomous\nin-situ adaptation framework that leverages the key insight that background\nscenes change more frequently than the visual characteristics of monitored\nspecies. WildFit combines background-aware synthesis to generate training\nsamples on-device with drift-aware fine-tuning that triggers model updates only\nwhen necessary to conserve resources. Through extensive evaluation on multiple\ncamera trap deployments, we demonstrate that WildFit significantly improves\naccuracy while greatly reducing adaptation overhead compared to traditional\napproaches."
                },
                "authors": [
                    {
                        "name": "Mohammad Mehdi Rastikerdar"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Hui Guan"
                    },
                    {
                        "name": "Deepak Ganesan"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Ganesan"
                },
                "author": "Deepak Ganesan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12692v2",
                "updated": "2025-01-24T05:12:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    12,
                    22,
                    4,
                    24,
                    0
                ],
                "published": "2024-11-19T17:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    17,
                    59,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference"
                },
                "summary": "Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p."
                },
                "authors": [
                    {
                        "name": "Jiho Shin"
                    },
                    {
                        "name": "Hoeseok Yang"
                    },
                    {
                        "name": "Youngmin Yi"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Yi"
                },
                "author": "Youngmin Yi",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14232v1",
                "updated": "2025-01-24T04:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    4,
                    40,
                    40,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T04:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    4,
                    40,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "Learning-Augmented Online Control for Decarbonizing Water\n  Infrastructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Control for Decarbonizing Water\n  Infrastructures"
                },
                "summary": "Water infrastructures are essential for drinking water supply, irrigation,\nfire protection, and other critical applications. However, water pumping\nsystems, which are key to transporting water to the point of use, consume\nsignificant amounts of energy and emit millions of tons of greenhouse gases\nannually. With the wide deployment of digital water meters and sensors in these\ninfrastructures, Machine Learning (ML) has the potential to optimize water\nsupply control and reduce greenhouse gas emissions. Nevertheless, the inherent\nvulnerability of ML methods in terms of worst-case performance raises safety\nconcerns when deployed in critical water infrastructures. To address this\nchallenge, we propose a learning-augmented online control algorithm, termed\nLAOC, designed to dynamically schedule the activation and/or speed of water\npumps. To ensure safety, we introduce a novel design of safe action sets for\nonline control problems. By leveraging these safe action sets, LAOC can\nprovably guarantee safety constraints while utilizing ML predictions to reduce\nenergy and environmental costs. Our analysis reveals the tradeoff between\nsafety requirements and average energy/environmental cost performance.\nAdditionally, we conduct an experimental study on a building water supply\nsystem to demonstrate the empirical performance of LAOC. The results indicate\nthat LAOC can effectively reduce environmental and energy costs while\nguaranteeing safety constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water infrastructures are essential for drinking water supply, irrigation,\nfire protection, and other critical applications. However, water pumping\nsystems, which are key to transporting water to the point of use, consume\nsignificant amounts of energy and emit millions of tons of greenhouse gases\nannually. With the wide deployment of digital water meters and sensors in these\ninfrastructures, Machine Learning (ML) has the potential to optimize water\nsupply control and reduce greenhouse gas emissions. Nevertheless, the inherent\nvulnerability of ML methods in terms of worst-case performance raises safety\nconcerns when deployed in critical water infrastructures. To address this\nchallenge, we propose a learning-augmented online control algorithm, termed\nLAOC, designed to dynamically schedule the activation and/or speed of water\npumps. To ensure safety, we introduce a novel design of safe action sets for\nonline control problems. By leveraging these safe action sets, LAOC can\nprovably guarantee safety constraints while utilizing ML predictions to reduce\nenergy and environmental costs. Our analysis reveals the tradeoff between\nsafety requirements and average energy/environmental cost performance.\nAdditionally, we conduct an experimental study on a building water supply\nsystem to demonstrate the empirical performance of LAOC. The results indicate\nthat LAOC can effectively reduce environmental and energy costs while\nguaranteeing safety constraints."
                },
                "authors": [
                    {
                        "name": "Jianyi Yang"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Tongxin Li"
                    },
                    {
                        "name": "Adam Wierman"
                    },
                    {
                        "name": "Shaolei Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shaolei Ren"
                },
                "author": "Shaolei Ren",
                "arxiv_comment": "Accepted by e-Energy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14224v1",
                "updated": "2025-01-24T04:06:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    4,
                    6,
                    50,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T04:06:50Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    4,
                    6,
                    50,
                    4,
                    24,
                    0
                ],
                "title": "Top Ten Challenges Towards Agentic Neural Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top Ten Challenges Towards Agentic Neural Graph Databases"
                },
                "summary": "Graph databases (GDBs) like Neo4j and TigerGraph excel at handling\ninterconnected data but lack advanced inference capabilities. Neural Graph\nDatabases (NGDBs) address this by integrating Graph Neural Networks (GNNs) for\npredictive analysis and reasoning over incomplete or noisy data. However, NGDBs\nrely on predefined queries and lack autonomy and adaptability. This paper\nintroduces Agentic Neural Graph Databases (Agentic NGDBs), which extend NGDBs\nwith three core functionalities: autonomous query construction, neural query\nexecution, and continuous learning. We identify ten key challenges in realizing\nAgentic NGDBs: semantic unit representation, abductive reasoning, scalable\nquery execution, and integration with foundation models like large language\nmodels (LLMs). By addressing these challenges, Agentic NGDBs can enable\nintelligent, self-improving systems for modern data-driven applications, paving\nthe way for adaptable and autonomous data management solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph databases (GDBs) like Neo4j and TigerGraph excel at handling\ninterconnected data but lack advanced inference capabilities. Neural Graph\nDatabases (NGDBs) address this by integrating Graph Neural Networks (GNNs) for\npredictive analysis and reasoning over incomplete or noisy data. However, NGDBs\nrely on predefined queries and lack autonomy and adaptability. This paper\nintroduces Agentic Neural Graph Databases (Agentic NGDBs), which extend NGDBs\nwith three core functionalities: autonomous query construction, neural query\nexecution, and continuous learning. We identify ten key challenges in realizing\nAgentic NGDBs: semantic unit representation, abductive reasoning, scalable\nquery execution, and integration with foundation models like large language\nmodels (LLMs). By addressing these challenges, Agentic NGDBs can enable\nintelligent, self-improving systems for modern data-driven applications, paving\nthe way for adaptable and autonomous data management solutions."
                },
                "authors": [
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yukun Zhou"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Zheye Deng"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Yisen Gao"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Yufei Li"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "12 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06781v2",
                "updated": "2025-01-24T03:37:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    37,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-12T11:35:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    35,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "Eliza: A Web3 friendly AI Agent Operating System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliza: A Web3 friendly AI Agent Operating System"
                },
                "summary": "AI Agent, powered by large language models (LLMs) as its cognitive core, is\nan intelligent agentic system capable of autonomously controlling and\ndetermining the execution paths under user's instructions. With the burst of\ncapabilities of LLMs and various plugins, such as RAG, text-to-image/video/3D,\netc., the potential of AI Agents has been vastly expanded, with their\ncapabilities growing stronger by the day. However, at the intersection between\nAI and web3, there is currently no ideal agentic framework that can seamlessly\nintegrate web3 applications into AI agent functionalities. In this paper, we\npropose Eliza, the first open-source web3-friendly Agentic framework that makes\nthe deployment of web3 applications effortless. We emphasize that every aspect\nof Eliza is a regular Typescript program under the full control of its user,\nand it seamlessly integrates with web3 (i.e., reading and writing blockchain\ndata, interacting with smart contracts, etc.). Furthermore, we show how stable\nperformance is achieved through the pragmatic implementation of the key\ncomponents of Eliza's runtime. Our code is publicly available at\nhttps://github.com/ai16z/eliza.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agent, powered by large language models (LLMs) as its cognitive core, is\nan intelligent agentic system capable of autonomously controlling and\ndetermining the execution paths under user's instructions. With the burst of\ncapabilities of LLMs and various plugins, such as RAG, text-to-image/video/3D,\netc., the potential of AI Agents has been vastly expanded, with their\ncapabilities growing stronger by the day. However, at the intersection between\nAI and web3, there is currently no ideal agentic framework that can seamlessly\nintegrate web3 applications into AI agent functionalities. In this paper, we\npropose Eliza, the first open-source web3-friendly Agentic framework that makes\nthe deployment of web3 applications effortless. We emphasize that every aspect\nof Eliza is a regular Typescript program under the full control of its user,\nand it seamlessly integrates with web3 (i.e., reading and writing blockchain\ndata, interacting with smart contracts, etc.). Furthermore, we show how stable\nperformance is achieved through the pragmatic implementation of the key\ncomponents of Eliza's runtime. Our code is publicly available at\nhttps://github.com/ai16z/eliza."
                },
                "authors": [
                    {
                        "name": "Shaw Walters"
                    },
                    {
                        "name": "Sam Gao"
                    },
                    {
                        "name": "Shakker Nerd"
                    },
                    {
                        "name": "Feng Da"
                    },
                    {
                        "name": "Warren Williams"
                    },
                    {
                        "name": "Ting-Chien Meng"
                    },
                    {
                        "name": "Amie Chow"
                    },
                    {
                        "name": "Hunter Han"
                    },
                    {
                        "name": "Frank He"
                    },
                    {
                        "name": "Allen Zhang"
                    },
                    {
                        "name": "Ming Wu"
                    },
                    {
                        "name": "Timothy Shen"
                    },
                    {
                        "name": "Maxwell Hu"
                    },
                    {
                        "name": "Jerry Yan"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Yan"
                },
                "author": "Jerry Yan",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14189v1",
                "updated": "2025-01-24T02:50:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    50,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T02:50:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    50,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models"
                },
                "summary": "Distributed Constraint Optimization Problems (DCOPs) offer a powerful\nframework for multi-agent coordination but often rely on labor-intensive,\nmanual problem construction. To address this, we introduce VL-DCOPs, a\nframework that takes advantage of large multimodal foundation models (LFMs) to\nautomatically generate constraints from both visual and linguistic\ninstructions. We then introduce a spectrum of agent archetypes for solving\nVL-DCOPs: from a neuro-symbolic agent that delegates some of the algorithmic\ndecisions to an LFM, to a fully neural agent that depends entirely on an LFM\nfor coordination. We evaluate these agent archetypes using state-of-the-art\nLLMs (large language models) and VLMs (vision language models) on three novel\nVL-DCOP tasks and compare their respective advantages and drawbacks. Lastly, we\ndiscuss how this work extends to broader frontier challenges in the DCOP\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Constraint Optimization Problems (DCOPs) offer a powerful\nframework for multi-agent coordination but often rely on labor-intensive,\nmanual problem construction. To address this, we introduce VL-DCOPs, a\nframework that takes advantage of large multimodal foundation models (LFMs) to\nautomatically generate constraints from both visual and linguistic\ninstructions. We then introduce a spectrum of agent archetypes for solving\nVL-DCOPs: from a neuro-symbolic agent that delegates some of the algorithmic\ndecisions to an LFM, to a fully neural agent that depends entirely on an LFM\nfor coordination. We evaluate these agent archetypes using state-of-the-art\nLLMs (large language models) and VLMs (vision language models) on three novel\nVL-DCOP tasks and compare their respective advantages and drawbacks. Lastly, we\ndiscuss how this work extends to broader frontier challenges in the DCOP\nliterature."
                },
                "authors": [
                    {
                        "name": "Saaduddin Mahmud"
                    },
                    {
                        "name": "Dorian Benhamou Goldfajn"
                    },
                    {
                        "name": "Shlomo Zilberstein"
                    }
                ],
                "author_detail": {
                    "name": "Shlomo Zilberstein"
                },
                "author": "Shlomo Zilberstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14186v1",
                "updated": "2025-01-24T02:29:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    29,
                    27,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T02:29:27Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    29,
                    27,
                    4,
                    24,
                    0
                ],
                "title": "GeoSim.AI: AI assistants for numerical simulations in geomechanics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoSim.AI: AI assistants for numerical simulations in geomechanics"
                },
                "summary": "The ability to accomplish tasks via natural language instructions is one of\nthe most efficient forms of interaction between humans and technology. This\nefficiency has been translated into practical applications with generative AI\ntools now allowing users to get things done through natural language queries.\nThe emergence of advanced Large Language Models (LLMs) marks a pivotal shift in\nthis direction. With ongoing advancements in the field of generative AI,\nintegrating natural language commands into sophisticated technical fields in\nscience and engineering is becoming increasingly feasible. This paper\nintroduces GeoSim.AI - a suite of AI assistants for numerical simulations in\ngeomechanics - thereby demonstrating the transformative potential of generative\nAI in geotechnical engineering. We investigate how AI assistants powered by\nLLMs can streamline the process of creating complex simulation inputs and\ninterpreting results by translating natural language instructions or image\ninputs into precise technical commands and scripts. This approach aims to\nbridge the gap between human intent and the intricate requirements of numerical\nmodeling tools, potentially revolutionizing how researchers and engineers\ninteract with simulation software. We present demonstrations involving AI\nassistants for performing slope stability analyses in various software\npackages. The demonstrations highlight the potential of this technology to\nsignificantly enhance productivity and accessibility in computational\ngeomechanics. GeoSim.AI is under active development, continuously expanding the\nsuite of AI assistants for various numerical simulation problems in\ngeotechnical engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to accomplish tasks via natural language instructions is one of\nthe most efficient forms of interaction between humans and technology. This\nefficiency has been translated into practical applications with generative AI\ntools now allowing users to get things done through natural language queries.\nThe emergence of advanced Large Language Models (LLMs) marks a pivotal shift in\nthis direction. With ongoing advancements in the field of generative AI,\nintegrating natural language commands into sophisticated technical fields in\nscience and engineering is becoming increasingly feasible. This paper\nintroduces GeoSim.AI - a suite of AI assistants for numerical simulations in\ngeomechanics - thereby demonstrating the transformative potential of generative\nAI in geotechnical engineering. We investigate how AI assistants powered by\nLLMs can streamline the process of creating complex simulation inputs and\ninterpreting results by translating natural language instructions or image\ninputs into precise technical commands and scripts. This approach aims to\nbridge the gap between human intent and the intricate requirements of numerical\nmodeling tools, potentially revolutionizing how researchers and engineers\ninteract with simulation software. We present demonstrations involving AI\nassistants for performing slope stability analyses in various software\npackages. The demonstrations highlight the potential of this technology to\nsignificantly enhance productivity and accessibility in computational\ngeomechanics. GeoSim.AI is under active development, continuously expanding the\nsuite of AI assistants for various numerical simulation problems in\ngeotechnical engineering."
                },
                "authors": [
                    {
                        "name": "Yared W. Bekele"
                    }
                ],
                "author_detail": {
                    "name": "Yared W. Bekele"
                },
                "author": "Yared W. Bekele",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14182v1",
                "updated": "2025-01-24T02:22:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    22,
                    42,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T02:22:42Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    22,
                    42,
                    4,
                    24,
                    0
                ],
                "title": "Post-hoc Spurious Correlation Neutralization with Single-Weight\n  Fictitious Class Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-hoc Spurious Correlation Neutralization with Single-Weight\n  Fictitious Class Unlearning"
                },
                "summary": "Neural network training tends to exploit the simplest features as shortcuts\nto greedily minimize training loss. However, some of these features might be\nspuriously correlated with the target labels, leading to incorrect predictions\nby the model. Several methods have been proposed to address this issue.\nFocusing on suppressing the spurious correlations with model training, they not\nonly incur additional training cost, but also have limited practical utility as\nthe model misbehavior due to spurious relations is usually discovered after its\ndeployment. It is also often overlooked that spuriousness is a subjective\nnotion. Hence, the precise questions that must be investigated are; to what\ndegree a feature is spurious, and how we can proportionally distract the\nmodel's attention from it for reliable prediction. To this end, we propose a\nmethod that enables post-hoc neutralization of spurious feature impact,\ncontrollable to an arbitrary degree. We conceptualize spurious features as\nfictitious sub-classes within the original classes, which can be eliminated by\na class removal scheme. We then propose a unique precise class removal\ntechnique that employs a single-weight modification, which entails negligible\nperformance compromise for the remaining classes. We perform extensive\nexperiments, demonstrating that by editing just a single weight in a post-hoc\nmanner, our method achieves highly competitive, or better performance against\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network training tends to exploit the simplest features as shortcuts\nto greedily minimize training loss. However, some of these features might be\nspuriously correlated with the target labels, leading to incorrect predictions\nby the model. Several methods have been proposed to address this issue.\nFocusing on suppressing the spurious correlations with model training, they not\nonly incur additional training cost, but also have limited practical utility as\nthe model misbehavior due to spurious relations is usually discovered after its\ndeployment. It is also often overlooked that spuriousness is a subjective\nnotion. Hence, the precise questions that must be investigated are; to what\ndegree a feature is spurious, and how we can proportionally distract the\nmodel's attention from it for reliable prediction. To this end, we propose a\nmethod that enables post-hoc neutralization of spurious feature impact,\ncontrollable to an arbitrary degree. We conceptualize spurious features as\nfictitious sub-classes within the original classes, which can be eliminated by\na class removal scheme. We then propose a unique precise class removal\ntechnique that employs a single-weight modification, which entails negligible\nperformance compromise for the remaining classes. We perform extensive\nexperiments, demonstrating that by editing just a single weight in a post-hoc\nmanner, our method achieves highly competitive, or better performance against\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Shahin Hakemi"
                    },
                    {
                        "name": "Naveed Akhtar"
                    },
                    {
                        "name": "Ghulam Mubashar Hassan"
                    },
                    {
                        "name": "Ajmal Mian"
                    }
                ],
                "author_detail": {
                    "name": "Ajmal Mian"
                },
                "author": "Ajmal Mian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14179v1",
                "updated": "2025-01-24T02:12:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    12,
                    8,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T02:12:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    12,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "AI Chatbots as Professional Service Agents: Developing a Professional\n  Identity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Chatbots as Professional Service Agents: Developing a Professional\n  Identity"
                },
                "summary": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach."
                },
                "authors": [
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Kangwei Shi"
                    },
                    {
                        "name": "Yidong Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Chai"
                },
                "author": "Yidong Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14170v1",
                "updated": "2025-01-24T01:38:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    1,
                    38,
                    37,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T01:38:37Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    1,
                    38,
                    37,
                    4,
                    24,
                    0
                ],
                "title": "Argos: Agentic Time-Series Anomaly Detection with Autonomous Rule\n  Generation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argos: Agentic Time-Series Anomaly Detection with Autonomous Rule\n  Generation via Large Language Models"
                },
                "summary": "Observability in cloud infrastructure is critical for service providers,\ndriving the widespread adoption of anomaly detection systems for monitoring\nmetrics. However, existing systems often struggle to simultaneously achieve\nexplainability, reproducibility, and autonomy, which are three indispensable\nproperties for production use. We introduce Argos, an agentic system for\ndetecting time-series anomalies in cloud infrastructure by leveraging large\nlanguage models (LLMs). Argos proposes to use explainable and reproducible\nanomaly rules as intermediate representation and employs LLMs to autonomously\ngenerate such rules. The system will efficiently train error-free and\naccuracy-guaranteed anomaly rules through multiple collaborative agents and\ndeploy the trained rules for low-cost online anomaly detection. Through\nevaluation results, we demonstrate that Argos outperforms state-of-the-art\nmethods, increasing $F_1$ scores by up to $9.5\\%$ and $28.3\\%$ on public\nanomaly detection datasets and an internal dataset collected from Microsoft,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observability in cloud infrastructure is critical for service providers,\ndriving the widespread adoption of anomaly detection systems for monitoring\nmetrics. However, existing systems often struggle to simultaneously achieve\nexplainability, reproducibility, and autonomy, which are three indispensable\nproperties for production use. We introduce Argos, an agentic system for\ndetecting time-series anomalies in cloud infrastructure by leveraging large\nlanguage models (LLMs). Argos proposes to use explainable and reproducible\nanomaly rules as intermediate representation and employs LLMs to autonomously\ngenerate such rules. The system will efficiently train error-free and\naccuracy-guaranteed anomaly rules through multiple collaborative agents and\ndeploy the trained rules for low-cost online anomaly detection. Through\nevaluation results, we demonstrate that Argos outperforms state-of-the-art\nmethods, increasing $F_1$ scores by up to $9.5\\%$ and $28.3\\%$ on public\nanomaly detection datasets and an internal dataset collected from Microsoft,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Yifan Xiong"
                    },
                    {
                        "name": "Jonathan Mace"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yigong Hu"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06252v3",
                "updated": "2025-01-24T01:26:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    1,
                    26,
                    30,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-09T01:19:21Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    19,
                    21,
                    3,
                    9,
                    0
                ],
                "title": "Transformer-Squared: Self-adaptive LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-Squared: Self-adaptive LLMs"
                },
                "summary": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\nTransformer-Squared, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, Transformer-Squared employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific 'expert' vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod consistently outperforms ubiquitous approaches such as LoRA, with fewer\nparameters and greater efficiency. Furthermore, Transformer-Squared\ndemonstrates versatility across different LLM architectures and modalities,\nincluding vision-language tasks. Transformer-Squared represents a significant\nleap forward, offering a scalable, efficient solution for enhancing the\nadaptability and task-specific performance of LLMs, paving the way for truly\ndynamic, self-organizing AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\nTransformer-Squared, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, Transformer-Squared employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific 'expert' vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod consistently outperforms ubiquitous approaches such as LoRA, with fewer\nparameters and greater efficiency. Furthermore, Transformer-Squared\ndemonstrates versatility across different LLM architectures and modalities,\nincluding vision-language tasks. Transformer-Squared represents a significant\nleap forward, offering a scalable, efficient solution for enhancing the\nadaptability and task-specific performance of LLMs, paving the way for truly\ndynamic, self-organizing AI systems."
                },
                "authors": [
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Edoardo Cetin"
                    },
                    {
                        "name": "Yujin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yujin Tang"
                },
                "author": "Yujin Tang",
                "arxiv_comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14875v3",
                "updated": "2025-01-24T01:15:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    1,
                    15,
                    16,
                    4,
                    24,
                    0
                ],
                "published": "2024-02-21T18:25:25Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    18,
                    25,
                    25,
                    2,
                    52,
                    0
                ],
                "title": "What's in a Name? Auditing Large Language Models for Race and Gender\n  Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What's in a Name? Auditing Large Language Models for Race and Gender\n  Bias"
                },
                "summary": "We employ an audit design to investigate biases in state-of-the-art large\nlanguage models, including GPT-4. In our study, we prompt the models for advice\ninvolving a named individual across a variety of scenarios, such as during car\npurchase negotiations or election outcome predictions. We find that the advice\nsystematically disadvantages names that are commonly associated with racial\nminorities and women. Names associated with Black women receive the least\nadvantageous outcomes. The biases are consistent across 42 prompt templates and\nseveral models, indicating a systemic issue rather than isolated incidents.\nWhile providing numerical, decision-relevant anchors in the prompt can\nsuccessfully counteract the biases, qualitative details have inconsistent\neffects and may even increase disparities. Our findings underscore the\nimportance of conducting audits at the point of LLM deployment and\nimplementation to mitigate their potential for harm against marginalized\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ an audit design to investigate biases in state-of-the-art large\nlanguage models, including GPT-4. In our study, we prompt the models for advice\ninvolving a named individual across a variety of scenarios, such as during car\npurchase negotiations or election outcome predictions. We find that the advice\nsystematically disadvantages names that are commonly associated with racial\nminorities and women. Names associated with Black women receive the least\nadvantageous outcomes. The biases are consistent across 42 prompt templates and\nseveral models, indicating a systemic issue rather than isolated incidents.\nWhile providing numerical, decision-relevant anchors in the prompt can\nsuccessfully counteract the biases, qualitative details have inconsistent\neffects and may even increase disparities. Our findings underscore the\nimportance of conducting audits at the point of LLM deployment and\nimplementation to mitigate their potential for harm against marginalized\ncommunities."
                },
                "authors": [
                    {
                        "name": "Alejandro Salinas"
                    },
                    {
                        "name": "Amit Haim"
                    },
                    {
                        "name": "Julian Nyarko"
                    }
                ],
                "author_detail": {
                    "name": "Julian Nyarko"
                },
                "author": "Julian Nyarko",
                "arxiv_comment": "62 pages, 34 tables, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14063v2",
                "updated": "2025-01-24T00:29:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    0,
                    29,
                    35,
                    4,
                    24,
                    0
                ],
                "published": "2024-12-18T17:08:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    8,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software\n  Verification"
                },
                "summary": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven."
                },
                "authors": [
                    {
                        "name": "Kyle Thompson"
                    },
                    {
                        "name": "Nuno Saavedra"
                    },
                    {
                        "name": "Pedro Carrott"
                    },
                    {
                        "name": "Kevin Fisher"
                    },
                    {
                        "name": "Alex Sanchez-Stern"
                    },
                    {
                        "name": "Yuriy Brun"
                    },
                    {
                        "name": "João F. Ferreira"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Emily First"
                    }
                ],
                "author_detail": {
                    "name": "Emily First"
                },
                "author": "Emily First",
                "arxiv_comment": "In Proceedings of the 47th International Conference on Software\n  Engineering (ICSE), Ottawa, ON, Canada, April 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; I.2.7; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10989v3",
                "updated": "2025-01-24T00:14:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    0,
                    14,
                    55,
                    4,
                    24,
                    0
                ],
                "published": "2024-10-14T18:17:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    17,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Liger Kernel: Efficient Triton Kernels for LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liger Kernel: Efficient Triton Kernels for LLM Training"
                },
                "summary": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel."
                },
                "authors": [
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Yun Dai"
                    },
                    {
                        "name": "Vignesh Kothapalli"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Siyu Zhu"
                    },
                    {
                        "name": "Steven Shimizu"
                    },
                    {
                        "name": "Shivam Sahni"
                    },
                    {
                        "name": "Haowen Ning"
                    },
                    {
                        "name": "Yanning Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yanning Chen"
                },
                "author": "Yanning Chen",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06394v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06394v4",
                "updated": "2025-01-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    22,
                    33,
                    58,
                    3,
                    23,
                    0
                ],
                "published": "2024-12-09T11:22:59Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    22,
                    59,
                    0,
                    344,
                    0
                ],
                "title": "GameArena: Evaluating LLM Reasoning through Live Computer Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GameArena: Evaluating LLM Reasoning through Live Computer Games"
                },
                "summary": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild."
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Qiyu Li"
                    },
                    {
                        "name": "Anze Xie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Haojian Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06394v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06394v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12853v2",
                "updated": "2025-01-23T22:22:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    22,
                    22,
                    13,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-10T21:59:01Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    59,
                    1,
                    3,
                    284,
                    0
                ],
                "title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in\n  Multi-Agent Debate Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity of Thought Elicits Stronger Reasoning Capabilities in\n  Multi-Agent Debate Frameworks"
                },
                "summary": "Large language models (LLMs) excel in natural language generation but often\nconfidently produce incorrect responses, especially in tasks like mathematical\nreasoning. Chain-of-thought prompting, self-verification, and multi-agent\ndebate are among the strategies proposed to improve the reasoning and factual\naccuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find\nthat multi-agent debate helps at any model scale, and that diversity of thought\nelicits stronger reasoning in debating LLMs. Across various model sizes,\nperformance on mathematical reasoning tasks benefits most when diverse trained\nmodels are used. Remarkably, after 4 rounds of debate, a diverse set of\nmedium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms\nGPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3\ninstances of Gemini-Pro are used, performance only reaches 82%. Finally, this\ndiverse set of medium-capacity models sets a new state-of-the-art performance\non the ASDiv benchmark (94%). These results underscore the idea that the future\nof AI is agentic, with diverse cooperating agents yielding emergent\ncapabilities beyond even the most powerful individual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in natural language generation but often\nconfidently produce incorrect responses, especially in tasks like mathematical\nreasoning. Chain-of-thought prompting, self-verification, and multi-agent\ndebate are among the strategies proposed to improve the reasoning and factual\naccuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find\nthat multi-agent debate helps at any model scale, and that diversity of thought\nelicits stronger reasoning in debating LLMs. Across various model sizes,\nperformance on mathematical reasoning tasks benefits most when diverse trained\nmodels are used. Remarkably, after 4 rounds of debate, a diverse set of\nmedium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms\nGPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3\ninstances of Gemini-Pro are used, performance only reaches 82%. Finally, this\ndiverse set of medium-capacity models sets a new state-of-the-art performance\non the ASDiv benchmark (94%). These results underscore the idea that the future\nof AI is agentic, with diverse cooperating agents yielding emergent\ncapabilities beyond even the most powerful individual models."
                },
                "authors": [
                    {
                        "name": "Mahmood Hegazy"
                    }
                ],
                "author_detail": {
                    "name": "Mahmood Hegazy"
                },
                "author": "Mahmood Hegazy",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12783v2",
                "updated": "2025-01-23T21:37:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    21,
                    37,
                    0,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-22T10:35:36Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    35,
                    36,
                    2,
                    22,
                    0
                ],
                "title": "Cost Optimization for Serverless Edge Computing with Budget Constraints\n  using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Optimization for Serverless Edge Computing with Budget Constraints\n  using Deep Reinforcement Learning"
                },
                "summary": "Serverless computing adopts a pay-as-you-go billing model where applications\nare executed in stateless and shortlived containers triggered by events,\nresulting in a reduction of monetary costs and resource utilization. However,\nexisting platforms do not provide an upper bound for the billing model which\nmakes the overall cost unpredictable, precluding many organizations from\nmanaging their budgets. Due to the diverse ranges of serverless functions and\nthe heterogeneous capacity of edge devices, it is challenging to receive\nnear-optimal solutions for deployment cost in a polynomial time. In this paper,\nwe investigated the function scheduling problem with a budget constraint for\nserverless computing in wireless networks. Users and IoT devices are sending\nrequests to edge nodes, improving the latency perceived by users. We propose\ntwo online scheduling algorithms based on reinforcement learning, incorporating\nseveral important characteristics of serverless functions. Via extensive\nsimulations, we justify the superiority of the proposed algorithm by comparing\nwith an ILP solver (Midaco). Our results indicate that the proposed algorithms\nefficiently approximate the results of Midaco within a factor of 1.03 while our\ndecision-making time is 5 orders of magnitude less than that of Midaco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing adopts a pay-as-you-go billing model where applications\nare executed in stateless and shortlived containers triggered by events,\nresulting in a reduction of monetary costs and resource utilization. However,\nexisting platforms do not provide an upper bound for the billing model which\nmakes the overall cost unpredictable, precluding many organizations from\nmanaging their budgets. Due to the diverse ranges of serverless functions and\nthe heterogeneous capacity of edge devices, it is challenging to receive\nnear-optimal solutions for deployment cost in a polynomial time. In this paper,\nwe investigated the function scheduling problem with a budget constraint for\nserverless computing in wireless networks. Users and IoT devices are sending\nrequests to edge nodes, improving the latency perceived by users. We propose\ntwo online scheduling algorithms based on reinforcement learning, incorporating\nseveral important characteristics of serverless functions. Via extensive\nsimulations, we justify the superiority of the proposed algorithm by comparing\nwith an ILP solver (Midaco). Our results indicate that the proposed algorithms\nefficiently approximate the results of Midaco within a factor of 1.03 while our\ndecision-making time is 5 orders of magnitude less than that of Midaco."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Peiyuan Guan"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Amir Taherkordi"
                    },
                    {
                        "name": "Fen Hou"
                    },
                    {
                        "name": "Lin X. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Lin X. Cai"
                },
                "author": "Lin X. Cai",
                "arxiv_comment": "This paper has been accepted by IEEE ICC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14105v1",
                "updated": "2025-01-23T21:32:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    21,
                    32,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T21:32:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    21,
                    32,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note\n  Sectioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note\n  Sectioning"
                },
                "summary": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility."
                },
                "authors": [
                    {
                        "name": "Joshua Davis"
                    },
                    {
                        "name": "Thomas Sounack"
                    },
                    {
                        "name": "Kate Sciacca"
                    },
                    {
                        "name": "Jessie M Brain"
                    },
                    {
                        "name": "Brigitte N Durieux"
                    },
                    {
                        "name": "Nicole D Agaronnik"
                    },
                    {
                        "name": "Charlotta Lindvall"
                    }
                ],
                "author_detail": {
                    "name": "Charlotta Lindvall"
                },
                "author": "Charlotta Lindvall",
                "arxiv_comment": "Our code is publicly available on github (\n  https://github.com/lindvalllab/MedSlice )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07341v2",
                "updated": "2025-01-23T21:26:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    21,
                    26,
                    2,
                    3,
                    23,
                    0
                ],
                "published": "2024-07-10T03:25:47Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    3,
                    25,
                    47,
                    2,
                    192,
                    0
                ],
                "title": "A Guide To Effectively Leveraging LLMs for Low-Resource Text\n  Summarization: Data Augmentation and Semi-supervised Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Guide To Effectively Leveraging LLMs for Low-Resource Text\n  Summarization: Data Augmentation and Semi-supervised Approaches"
                },
                "summary": "Existing approaches for low-resource text summarization primarily employ\nlarge language models (LLMs) like GPT-3 or GPT-4 at inference time to generate\nsummaries directly; however, such approaches often suffer from inconsistent LLM\noutputs and are difficult to adapt to domain-specific data in low-resource\nscenarios. In this work, we propose two novel methods to effectively utilize\nLLMs for low-resource text summarization: 1) MixSumm, an LLM-based data\naugmentation regime that synthesizes high-quality documents (short and long)\nfor few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling\nstrategy for sample-efficient semi-supervised text summarization. Specifically,\nMixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new\ndocuments by mixing topical information derived from a small seed set, and PPSL\nleverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels\nin a semi-supervised learning setup. We evaluate our methods on the TweetSumm,\nWikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation\nmetric, and ROUGE scores to measure the quality of generated summaries. Our\nexperiments on extractive and abstractive summarization show that MixSumm and\nPPSL achieve competitive ROUGE scores as a fully supervised method with 5% of\nthe labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches for low-resource text summarization primarily employ\nlarge language models (LLMs) like GPT-3 or GPT-4 at inference time to generate\nsummaries directly; however, such approaches often suffer from inconsistent LLM\noutputs and are difficult to adapt to domain-specific data in low-resource\nscenarios. In this work, we propose two novel methods to effectively utilize\nLLMs for low-resource text summarization: 1) MixSumm, an LLM-based data\naugmentation regime that synthesizes high-quality documents (short and long)\nfor few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling\nstrategy for sample-efficient semi-supervised text summarization. Specifically,\nMixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new\ndocuments by mixing topical information derived from a small seed set, and PPSL\nleverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels\nin a semi-supervised learning setup. We evaluate our methods on the TweetSumm,\nWikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation\nmetric, and ROUGE scores to measure the quality of generated summaries. Our\nexperiments on extractive and abstractive summarization show that MixSumm and\nPPSL achieve competitive ROUGE scores as a fully supervised method with 5% of\nthe labeled data."
                },
                "authors": [
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Olga Vechtomova"
                    },
                    {
                        "name": "Issam H. Laradji"
                    }
                ],
                "author_detail": {
                    "name": "Issam H. Laradji"
                },
                "author": "Issam H. Laradji",
                "arxiv_comment": "Accepted to NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14101v1",
                "updated": "2025-01-23T21:20:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    21,
                    20,
                    10,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T21:20:10Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    21,
                    20,
                    10,
                    3,
                    23,
                    0
                ],
                "title": "StreamingRAG: Real-time Contextual Retrieval and Generation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingRAG: Real-time Contextual Retrieval and Generation Framework"
                },
                "summary": "Extracting real-time insights from multi-modal data streams from various\ndomains such as healthcare, intelligent transportation, and satellite remote\nsensing remains a challenge. High computational demands and limited knowledge\nscope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)\non these data streams. Traditional Retrieval-Augmented Generation (RAG) systems\naddress knowledge limitations of these models, but suffer from slow\npreprocessing, making them unsuitable for real-time analysis. We propose\nStreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG\nconstructs evolving knowledge graphs capturing scene-object-entity\nrelationships in real-time. The knowledge graph achieves temporal-aware scene\nrepresentations using MM-LLMs and enables timely responses for specific events\nor user queries. StreamingRAG addresses limitations in existing methods,\nachieving significant improvements in real-time analysis (5-6x faster\nthroughput), contextual accuracy (through a temporal knowledge graph), and\nreduced resource consumption (using lightweight models by 2-3x).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting real-time insights from multi-modal data streams from various\ndomains such as healthcare, intelligent transportation, and satellite remote\nsensing remains a challenge. High computational demands and limited knowledge\nscope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)\non these data streams. Traditional Retrieval-Augmented Generation (RAG) systems\naddress knowledge limitations of these models, but suffer from slow\npreprocessing, making them unsuitable for real-time analysis. We propose\nStreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG\nconstructs evolving knowledge graphs capturing scene-object-entity\nrelationships in real-time. The knowledge graph achieves temporal-aware scene\nrepresentations using MM-LLMs and enables timely responses for specific events\nor user queries. StreamingRAG addresses limitations in existing methods,\nachieving significant improvements in real-time analysis (5-6x faster\nthroughput), contextual accuracy (through a temporal knowledge graph), and\nreduced resource consumption (using lightweight models by 2-3x)."
                },
                "authors": [
                    {
                        "name": "Murugan Sankaradas"
                    },
                    {
                        "name": "Ravi K. Rajendran"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    }
                ],
                "author_detail": {
                    "name": "Srimat T. Chakradhar"
                },
                "author": "Srimat T. Chakradhar",
                "arxiv_comment": "Accepted and Presented at AI4Sys, HPDC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11747v2",
                "updated": "2025-01-23T20:45:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    20,
                    45,
                    47,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-20T21:10:22Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    10,
                    22,
                    0,
                    20,
                    0
                ],
                "title": "Optimizing Pretraining Data Mixtures with LLM-Estimated Utility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Pretraining Data Mixtures with LLM-Estimated Utility"
                },
                "summary": "Large Language Models improve with increasing amounts of high-quality\ntraining data. However, leveraging larger datasets requires balancing quality,\nquantity, and diversity across sources. After evaluating nine baseline methods\nunder both compute- and data-constrained scenarios, we find token-count\nheuristics outperform manual and learned mixes, indicating that simple\napproaches accounting for dataset size and diversity are surprisingly\neffective. Building on this insight, we propose two complementary approaches:\nUtiliMax, which extends token-based heuristics by incorporating utility\nestimates from reduced-scale ablations, achieving up to a 10.6x speedup over\nmanual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs\nto estimate data utility from small samples, matching ablation-based\nperformance while reducing computational requirements by $\\sim$200x. Together,\nthese approaches establish a new framework for automated, compute-efficient\ndata mixing that is robust across training regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models improve with increasing amounts of high-quality\ntraining data. However, leveraging larger datasets requires balancing quality,\nquantity, and diversity across sources. After evaluating nine baseline methods\nunder both compute- and data-constrained scenarios, we find token-count\nheuristics outperform manual and learned mixes, indicating that simple\napproaches accounting for dataset size and diversity are surprisingly\neffective. Building on this insight, we propose two complementary approaches:\nUtiliMax, which extends token-based heuristics by incorporating utility\nestimates from reduced-scale ablations, achieving up to a 10.6x speedup over\nmanual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs\nto estimate data utility from small samples, matching ablation-based\nperformance while reducing computational requirements by $\\sim$200x. Together,\nthese approaches establish a new framework for automated, compute-efficient\ndata mixing that is robust across training regimes."
                },
                "authors": [
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "name": "Punit Singh Koura"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Frank Zhang"
                    },
                    {
                        "name": "Todor Mihaylov"
                    }
                ],
                "author_detail": {
                    "name": "Todor Mihaylov"
                },
                "author": "Todor Mihaylov",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14073v1",
                "updated": "2025-01-23T20:20:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    20,
                    20,
                    20,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T20:20:20Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    20,
                    20,
                    20,
                    3,
                    23,
                    0
                ],
                "title": "LLMs are Vulnerable to Malicious Prompts Disguised as Scientific\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Vulnerable to Malicious Prompts Disguised as Scientific\n  Language"
                },
                "summary": "As large language models (LLMs) have been deployed in various real-world\nsettings, concerns about the harm they may propagate have grown. Various\njailbreaking techniques have been developed to expose the vulnerabilities of\nthese models and improve their safety. This work reveals that many\nstate-of-the-art proprietary and open-source LLMs are vulnerable to malicious\nrequests hidden behind scientific language. Specifically, our experiments with\nGPT4o, GPT4o-mini, GPT-4, LLama3-405B-Instruct, Llama3-70B-Instruct, Cohere,\nGemini models on the StereoSet data demonstrate that, the models' biases and\ntoxicity substantially increase when prompted with requests that deliberately\nmisinterpret social science and psychological studies as evidence supporting\nthe benefits of stereotypical biases. Alarmingly, these models can also be\nmanipulated to generate fabricated scientific arguments claiming that biases\nare beneficial, which can be used by ill-intended actors to systematically\njailbreak even the strongest models like GPT. Our analysis studies various\nfactors that contribute to the models' vulnerabilities to malicious requests in\nacademic language. Mentioning author names and venues enhances the\npersuasiveness of some models, and the bias scores can increase as dialogues\nprogress. Our findings call for a more careful investigation on the use of\nscientific data in the training of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) have been deployed in various real-world\nsettings, concerns about the harm they may propagate have grown. Various\njailbreaking techniques have been developed to expose the vulnerabilities of\nthese models and improve their safety. This work reveals that many\nstate-of-the-art proprietary and open-source LLMs are vulnerable to malicious\nrequests hidden behind scientific language. Specifically, our experiments with\nGPT4o, GPT4o-mini, GPT-4, LLama3-405B-Instruct, Llama3-70B-Instruct, Cohere,\nGemini models on the StereoSet data demonstrate that, the models' biases and\ntoxicity substantially increase when prompted with requests that deliberately\nmisinterpret social science and psychological studies as evidence supporting\nthe benefits of stereotypical biases. Alarmingly, these models can also be\nmanipulated to generate fabricated scientific arguments claiming that biases\nare beneficial, which can be used by ill-intended actors to systematically\njailbreak even the strongest models like GPT. Our analysis studies various\nfactors that contribute to the models' vulnerabilities to malicious requests in\nacademic language. Mentioning author names and venues enhances the\npersuasiveness of some models, and the bias scores can increase as dialogues\nprogress. Our findings call for a more careful investigation on the use of\nscientific data in the training of LLMs."
                },
                "authors": [
                    {
                        "name": "Yubin Ge"
                    },
                    {
                        "name": "Neeraja Kirtane"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Dilek Hakkani-Tür"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tür"
                },
                "author": "Dilek Hakkani-Tür",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v8",
                "updated": "2025-01-23T20:19:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    20,
                    19,
                    40,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for zero-shot molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with domain\n  feedback for zero-shot molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule. Despite Large Language Models (LLMs)\nholding the potential to efficiently simulate this task by using natural\nlanguage to direct the optimization, straightforwardly utilizing them shows\nlimited performance. In this work, we facilitate utilizing LLMs in an iterative\nparadigm by proposing a simple yet effective domain feedback provider, namely\n$\\text{Re}^2$DF. In detail, $\\text{Re}^2$DF harnesses an external toolkit,\nRDKit, to handle the molecule hallucination, if the modified molecule is\nchemically invalid. Otherwise, $\\text{Re}^2$DF verifies whether the modified\nmolecule meets the objective, if not, its desired properties are computed and\ncompared to the original one, establishing reliable domain feedback with\ncorrect direction and distance towards the objective to explicitly guide the\nLLM to refine the modified molecule. We conduct experiments across both single-\nand multi-property objectives with 2 thresholds, where $\\text{Re}^2$DF shows\nsignificant improvements. Notably, for 20 single-property objectives,\n$\\text{Re}^2$DF enhances Hit ratio by 16.96% and 20.76% under loose\n(\\texttt{l}) and strict (\\texttt{s}) thresholds, respectively. For 32\nmulti-property objectives, $\\text{Re}^2$DF enhances Hit ratio by 6.04% and\n5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule. Despite Large Language Models (LLMs)\nholding the potential to efficiently simulate this task by using natural\nlanguage to direct the optimization, straightforwardly utilizing them shows\nlimited performance. In this work, we facilitate utilizing LLMs in an iterative\nparadigm by proposing a simple yet effective domain feedback provider, namely\n$\\text{Re}^2$DF. In detail, $\\text{Re}^2$DF harnesses an external toolkit,\nRDKit, to handle the molecule hallucination, if the modified molecule is\nchemically invalid. Otherwise, $\\text{Re}^2$DF verifies whether the modified\nmolecule meets the objective, if not, its desired properties are computed and\ncompared to the original one, establishing reliable domain feedback with\ncorrect direction and distance towards the objective to explicitly guide the\nLLM to refine the modified molecule. We conduct experiments across both single-\nand multi-property objectives with 2 thresholds, where $\\text{Re}^2$DF shows\nsignificant improvements. Notably, for 20 single-property objectives,\n$\\text{Re}^2$DF enhances Hit ratio by 16.96% and 20.76% under loose\n(\\texttt{l}) and strict (\\texttt{s}) thresholds, respectively. For 32\nmulti-property objectives, $\\text{Re}^2$DF enhances Hit ratio by 6.04% and\n5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14066v1",
                "updated": "2025-01-23T20:01:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    20,
                    1,
                    33,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T20:01:33Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    20,
                    1,
                    33,
                    3,
                    23,
                    0
                ],
                "title": "Efficient 2D CT Foundation Model for Contrast Phase Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient 2D CT Foundation Model for Contrast Phase Classification"
                },
                "summary": "Purpose: The purpose of this study is to harness the efficiency of a 2D\nfoundation model to develop a robust phase classifier that is resilient to\ndomain shifts.\n  Materials and Methods: This retrospective study utilized three public\ndatasets from separate institutions. A 2D foundation model was trained on the\nDeepLesion dataset (mean age: 51.2, s.d.: 17.6; 2398 males) to generate\nembeddings from 2D CT slices for downstream contrast phase classification. The\nclassifier was trained on the VinDr Multiphase dataset and externally validated\non the WAW-TACE dataset. The 2D model was also compared to three 3D supervised\nmodels.\n  Results: On the VinDr dataset (146 male, 63 female, 56 unidentified), the\nmodel achieved near-perfect AUROC scores and F1 scores of 99.2%, 94.2%, and\n93.1% for non-contrast, arterial, and venous phases, respectively. The `Other'\ncategory scored lower (F1: 73.4%) due to combining multiple contrast phases\ninto one class. On the WAW-TACE dataset (mean age: 66.1, s.d.: 10.0; 185\nmales), the model showed strong performance with AUROCs of 91.0% and 85.6%, and\nF1 scores of 87.3% and 74.1% for non-contrast and arterial phases. Venous phase\nperformance was lower, with AUROC and F1 scores of 81.7% and 70.2%\nrespectively, due to label mismatches. Compared to 3D supervised models, the\napproach trained faster, performed as well or better, and showed greater\nrobustness to domain shifts.\n  Conclusion: The robustness of the 2D Foundation model may be potentially\nuseful for automation of hanging protocols and data orchestration for clinical\ndeployment of AI algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: The purpose of this study is to harness the efficiency of a 2D\nfoundation model to develop a robust phase classifier that is resilient to\ndomain shifts.\n  Materials and Methods: This retrospective study utilized three public\ndatasets from separate institutions. A 2D foundation model was trained on the\nDeepLesion dataset (mean age: 51.2, s.d.: 17.6; 2398 males) to generate\nembeddings from 2D CT slices for downstream contrast phase classification. The\nclassifier was trained on the VinDr Multiphase dataset and externally validated\non the WAW-TACE dataset. The 2D model was also compared to three 3D supervised\nmodels.\n  Results: On the VinDr dataset (146 male, 63 female, 56 unidentified), the\nmodel achieved near-perfect AUROC scores and F1 scores of 99.2%, 94.2%, and\n93.1% for non-contrast, arterial, and venous phases, respectively. The `Other'\ncategory scored lower (F1: 73.4%) due to combining multiple contrast phases\ninto one class. On the WAW-TACE dataset (mean age: 66.1, s.d.: 10.0; 185\nmales), the model showed strong performance with AUROCs of 91.0% and 85.6%, and\nF1 scores of 87.3% and 74.1% for non-contrast and arterial phases. Venous phase\nperformance was lower, with AUROC and F1 scores of 81.7% and 70.2%\nrespectively, due to label mismatches. Compared to 3D supervised models, the\napproach trained faster, performed as well or better, and showed greater\nrobustness to domain shifts.\n  Conclusion: The robustness of the 2D Foundation model may be potentially\nuseful for automation of hanging protocols and data orchestration for clinical\ndeployment of AI algorithms."
                },
                "authors": [
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Tejas Sudharshan Mathai"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Xinya Wang"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Zhiyong Lub"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lub"
                },
                "author": "Zhiyong Lub",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13387v3",
                "updated": "2025-01-23T19:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    37,
                    24,
                    3,
                    23,
                    0
                ],
                "published": "2024-10-17T09:39:10Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    9,
                    39,
                    10,
                    3,
                    291,
                    0
                ],
                "title": "CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk\n  Generation for Large Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk\n  Generation for Large Language Model Applications"
                },
                "summary": "The rise of end-user applications powered by large language models (LLMs),\nincluding both conversational interfaces and add-ons to existing graphical user\ninterfaces (GUIs), introduces new privacy challenges. However, many users\nremain unaware of the risks. This paper explores methods to increase user\nawareness of privacy risks associated with LLMs in end-user applications. We\nconducted five co-design workshops to uncover user privacy concerns and their\ndemand for contextual privacy information within LLMs. Based on these insights,\nwe developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk\nGeneration), a just-in-time contextual assistant designed to help users\nidentify sensitive information, summarize relevant privacy policies, and\nhighlight potential risks when sharing information with LLMs. We evaluated the\nusability and usefulness of CLEAR across two example domains: ChatGPT and the\nGemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and\nimproves users' understanding of data practices and privacy risks. We also\ndiscussed LLM's duality in posing and mitigating privacy risks, offering design\nand policy implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of end-user applications powered by large language models (LLMs),\nincluding both conversational interfaces and add-ons to existing graphical user\ninterfaces (GUIs), introduces new privacy challenges. However, many users\nremain unaware of the risks. This paper explores methods to increase user\nawareness of privacy risks associated with LLMs in end-user applications. We\nconducted five co-design workshops to uncover user privacy concerns and their\ndemand for contextual privacy information within LLMs. Based on these insights,\nwe developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk\nGeneration), a just-in-time contextual assistant designed to help users\nidentify sensitive information, summarize relevant privacy policies, and\nhighlight potential risks when sharing information with LLMs. We evaluated the\nusability and usefulness of CLEAR across two example domains: ChatGPT and the\nGemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and\nimproves users' understanding of data practices and privacy risks. We also\ndiscussed LLM's duality in posing and mitigating privacy risks, offering design\nand policy implications."
                },
                "authors": [
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Daodao Zhou"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Toby Jia-jun Li"
                    },
                    {
                        "name": "Yaxing Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yaxing Yao"
                },
                "author": "Yaxing Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14046v1",
                "updated": "2025-01-23T19:26:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    26,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T19:26:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    26,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "LLM-guided Instance-level Image Manipulation with Diffusion U-Net\n  Cross-Attention Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Instance-level Image Manipulation with Diffusion U-Net\n  Cross-Attention Maps"
                },
                "summary": "The advancement of text-to-image synthesis has introduced powerful generative\nmodels capable of creating realistic images from textual prompts. However,\nprecise control over image attributes remains challenging, especially at the\ninstance level. While existing methods offer some control through fine-tuning\nor auxiliary information, they often face limitations in flexibility and\naccuracy. To address these challenges, we propose a pipeline leveraging Large\nLanguage Models (LLMs), open-vocabulary detectors, cross-attention maps and\nintermediate activations of diffusion U-Net for instance-level image\nmanipulation. Our method detects objects mentioned in the prompt and present in\nthe generated image, enabling precise manipulation without extensive training\nor input masks. By incorporating cross-attention maps, our approach ensures\ncoherence in manipulated images while controlling object positions. Our method\nenables precise manipulations at the instance level without fine-tuning or\nauxiliary information such as masks or bounding boxes. Code is available at\nhttps://github.com/Palandr123/DiffusionU-NetLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of text-to-image synthesis has introduced powerful generative\nmodels capable of creating realistic images from textual prompts. However,\nprecise control over image attributes remains challenging, especially at the\ninstance level. While existing methods offer some control through fine-tuning\nor auxiliary information, they often face limitations in flexibility and\naccuracy. To address these challenges, we propose a pipeline leveraging Large\nLanguage Models (LLMs), open-vocabulary detectors, cross-attention maps and\nintermediate activations of diffusion U-Net for instance-level image\nmanipulation. Our method detects objects mentioned in the prompt and present in\nthe generated image, enabling precise manipulation without extensive training\nor input masks. By incorporating cross-attention maps, our approach ensures\ncoherence in manipulated images while controlling object positions. Our method\nenables precise manipulations at the instance level without fine-tuning or\nauxiliary information such as masks or bounding boxes. Code is available at\nhttps://github.com/Palandr123/DiffusionU-NetLLM"
                },
                "authors": [
                    {
                        "name": "Andrey Palaev"
                    },
                    {
                        "name": "Adil Khan"
                    },
                    {
                        "name": "Syed M. Ahsan Kazmi"
                    }
                ],
                "author_detail": {
                    "name": "Syed M. Ahsan Kazmi"
                },
                "author": "Syed M. Ahsan Kazmi",
                "arxiv_comment": "Presented at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14040v1",
                "updated": "2025-01-23T19:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    14,
                    11,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T19:14:11Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    14,
                    11,
                    3,
                    23,
                    0
                ],
                "title": "Global Perspectives of AI Risks and Harms: Analyzing the Negative\n  Impacts of AI Technologies as Prioritized by News Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Perspectives of AI Risks and Harms: Analyzing the Negative\n  Impacts of AI Technologies as Prioritized by News Media"
                },
                "summary": "Emerging AI technologies have the potential to drive economic growth and\ninnovation but can also pose significant risks to society. To mitigate these\nrisks, governments, companies, and researchers have contributed regulatory\nframeworks, risk assessment approaches, and safety benchmarks, but these can\nlack nuance when considered in global deployment contexts. One way to\nunderstand these nuances is by looking at how the media reports on AI, as news\nmedia has a substantial influence on what negative impacts of AI are discussed\nin the public sphere and which impacts are deemed important. In this work, we\nanalyze a broad and diverse sample of global news media spanning 27 countries\nacross Asia, Africa, Europe, Middle East, North America, and Oceania to gain\nvaluable insights into the risks and harms of AI technologies as reported and\nprioritized across media outlets in different countries. This approach reveals\na skewed prioritization of Societal Risks followed by Legal & Rights-related\nRisks, Content Safety Risks, Cognitive Risks, Existential Risks, and\nEnvironmental Risks, as reflected in the prevalence of these risk categories in\nthe news coverage of different nations. Furthermore, it highlights how the\ndistribution of such concerns varies based on the political bias of news\noutlets, underscoring the political nature of AI risk assessment processes and\npublic opinion. By incorporating views from various regions and political\norientations for assessing the risks and harms of AI, this work presents\nstakeholders, such as AI developers and policy makers, with insights into the\nAI risks categories prioritized in the public sphere. These insights my guide\nthe development of more inclusive, safe, and responsible AI technologies that\naddress the diverse concerns and needs across the world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging AI technologies have the potential to drive economic growth and\ninnovation but can also pose significant risks to society. To mitigate these\nrisks, governments, companies, and researchers have contributed regulatory\nframeworks, risk assessment approaches, and safety benchmarks, but these can\nlack nuance when considered in global deployment contexts. One way to\nunderstand these nuances is by looking at how the media reports on AI, as news\nmedia has a substantial influence on what negative impacts of AI are discussed\nin the public sphere and which impacts are deemed important. In this work, we\nanalyze a broad and diverse sample of global news media spanning 27 countries\nacross Asia, Africa, Europe, Middle East, North America, and Oceania to gain\nvaluable insights into the risks and harms of AI technologies as reported and\nprioritized across media outlets in different countries. This approach reveals\na skewed prioritization of Societal Risks followed by Legal & Rights-related\nRisks, Content Safety Risks, Cognitive Risks, Existential Risks, and\nEnvironmental Risks, as reflected in the prevalence of these risk categories in\nthe news coverage of different nations. Furthermore, it highlights how the\ndistribution of such concerns varies based on the political bias of news\noutlets, underscoring the political nature of AI risk assessment processes and\npublic opinion. By incorporating views from various regions and political\norientations for assessing the risks and harms of AI, this work presents\nstakeholders, such as AI developers and policy makers, with insights into the\nAI risks categories prioritized in the public sphere. These insights my guide\nthe development of more inclusive, safe, and responsible AI technologies that\naddress the diverse concerns and needs across the world."
                },
                "authors": [
                    {
                        "name": "Mowafak Allaham"
                    },
                    {
                        "name": "Kimon Kieslich"
                    },
                    {
                        "name": "Nicholas Diakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Diakopoulos"
                },
                "author": "Nicholas Diakopoulos",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14037v1",
                "updated": "2025-01-23T19:06:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    6,
                    26,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T19:06:26Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    19,
                    6,
                    26,
                    3,
                    23,
                    0
                ],
                "title": "Leveraging Large Language Models to Analyze Emotional and Contextual\n  Drivers of Teen Substance Use in Online Discussions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Analyze Emotional and Contextual\n  Drivers of Teen Substance Use in Online Discussions"
                },
                "summary": "Adolescence is a critical stage often linked to risky behaviors, including\nsubstance use, with significant developmental and public health implications.\nSocial media provides a lens into adolescent self-expression, but interpreting\nemotional and contextual signals remains complex. This study applies Large\nLanguage Models (LLMs) to analyze adolescents' social media posts, uncovering\nemotional patterns (e.g., sadness, guilt, fear, joy) and contextual factors\n(e.g., family, peers, school) related to substance use. Heatmap and machine\nlearning analyses identified key predictors of substance use-related posts.\nNegative emotions like sadness and guilt were significantly more frequent in\nsubstance use contexts, with guilt acting as a protective factor, while shame\nand peer influence heightened substance use risk. Joy was more common in\nnon-substance use discussions. Peer influence correlated strongly with sadness,\nfear, and disgust, while family and school environments aligned with\nnon-substance use. Findings underscore the importance of addressing emotional\nvulnerabilities and contextual influences, suggesting that collaborative\ninterventions involving families, schools, and communities can reduce risk\nfactors and foster healthier adolescent development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adolescence is a critical stage often linked to risky behaviors, including\nsubstance use, with significant developmental and public health implications.\nSocial media provides a lens into adolescent self-expression, but interpreting\nemotional and contextual signals remains complex. This study applies Large\nLanguage Models (LLMs) to analyze adolescents' social media posts, uncovering\nemotional patterns (e.g., sadness, guilt, fear, joy) and contextual factors\n(e.g., family, peers, school) related to substance use. Heatmap and machine\nlearning analyses identified key predictors of substance use-related posts.\nNegative emotions like sadness and guilt were significantly more frequent in\nsubstance use contexts, with guilt acting as a protective factor, while shame\nand peer influence heightened substance use risk. Joy was more common in\nnon-substance use discussions. Peer influence correlated strongly with sadness,\nfear, and disgust, while family and school environments aligned with\nnon-substance use. Findings underscore the importance of addressing emotional\nvulnerabilities and contextual influences, suggesting that collaborative\ninterventions involving families, schools, and communities can reduce risk\nfactors and foster healthier adolescent development."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Hailong Jiang"
                    },
                    {
                        "name": "Yulan Wang"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Karin G. Coifman"
                    }
                ],
                "author_detail": {
                    "name": "Karin G. Coifman"
                },
                "author": "Karin G. Coifman",
                "arxiv_comment": "28 pages, 9 figures with an appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]